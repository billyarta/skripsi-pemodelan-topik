# -*- coding: utf-8 -*-
"""Web Crawling & Scraping ETD UGM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fq3p7RCZH2xk08Tce9aJk4xxDA2oNUtf
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Crawling Link Dokumen"""

!pip install selenium

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# 
# # Add debian buster
# cat > /etc/apt/sources.list.d/debian.list <<'EOF'
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main
# EOF
# 
# # Add keys
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A
# 
# apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg
# apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg
# apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg
# 
# # Prefer debian repo for chromium* packages only
# # Note the double-blank lines between entries
# cat > /etc/apt/preferences.d/chromium.pref << 'EOF'
# Package: *
# Pin: release a=eoan
# Pin-Priority: 500
# 
# 
# Package: *
# Pin: origin "deb.debian.org"
# Pin-Priority: 300
# 
# 
# Package: chromium*
# Pin: origin "deb.debian.org"
# Pin-Priority: 700
# EOF

!apt-get update
!apt-get install chromium chromium-driver

from selenium import webdriver
import time
from bs4 import BeautifulSoup
import pickle

def web_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--verbose")
    options.add_argument('--no-sandbox')
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument("--window-size=1300, 800")
    options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=options)
    return driver

def collecting_repository(URL, num_page, file_name, prodi):
  print("collecting all repository from prodi {}".format(prodi))
  print("consists of {} page(s)".format(num_page))
  print("- - - - - - - - - - - - - - - - - - - - - - - - - - -")
  # mendefinisikan tempat untuk menyimpan link repository
  link_repository = []
  # menghubungkan dengan driver chrome
  driver = web_driver()
  driver.get(URL)
  time.sleep(10)
  driver.save_screenshot("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Screenshot/{}_screenshot_page_1.png".format(prodi))
  # menjalankan untuk tiap halaman web
  for page in range(num_page):
  # mengambil konten web
    content = driver.page_source
    # membuat parsing html
    soup = BeautifulSoup(content, "html.parser")
    results = soup.find(id="example_wrapper")
    repository_elements = results.find_all("div", class_="col-lg-12 col-md-12 col-sm-12 col-xs-12")
    for repository_element in repository_elements:
      link = repository_element.find("a")['href']
      link_repository.append(link)
    if len(link_repository) > (page)*10:
      print("collecting repository page-{} succeed...".format(page+1))
    # menuju halaman web selanjutnya
    if page < num_page-1:
      next_click = driver.find_element("link text", "Next")
      next_click.click()
      time.sleep(10)
      driver.save_screenshot("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Screenshot/{}_screenshot_page_{}.png".format(prodi, page+2))
  driver.quit()
  # menyimpan list berisi link repository
  open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/{}_".format(prodi)+file_name, "wb")
  pickle.dump(link_repository, open_file)
  open_file.close()
  print("- - - - - - - - - - - - - - - - - - - - - - - - - - -")
  print("done collecting all repository from page-1 to page-{}".format(page+1))
  print("total {} repository".format(len(link_repository)))
  print("-----------------------------------------------------")
  print("repository will save to {}".format("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/{}_".format(prodi)+file_name))

"""#### Prodi 5262 (S1 STATISTIKA) -> 449 repository"""

prodi = "5262" # S1 Statistika
URL = "http://etd.repository.ugm.ac.id/home/result_pencarianlanjut/search?key_one=penJudul&keyword_one=a&logic_two=AND&key_two=penJudul&keyword_two=&logic_three=AND&key_three=penJudul&keyword_three=&tahunawal=&tahunakhir=&jeniskarya=&prodi={}".format(prodi)
num_page = 45
file_name = "link_repository"
collecting_repository(URL, num_page, file_name, prodi)

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/5262_link_repository", "rb")
link_repository_5262 = pickle.load(open_file)
open_file.close()

link_repository_5262

len(link_repository_5262)

"""#### Menggabungkan semua link_repository"""

link_repository = link_repository_5262

set([x for x in link_repository if link_repository.count(x) > 1])

len(link_repository)

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_link_repository", "wb")
pickle.dump(link_repository, open_file)
open_file.close()

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_link_repository", "rb")
link_repository = pickle.load(open_file)
open_file.close()

link_repository

"""### Scraping Per Judul"""

import requests
from bs4 import BeautifulSoup

def collecting_documents(list_link):
  print("collecting all documents from {} repository".format(len(list_link)))
  print("- - - - - - - - - - - - - - - - - - - - - - - - - - -")
  k = 1
  # mendefinisikan tempat untuk menyimpan dokumen tugas akhir
  titles = []
  authors = []
  years = []
  abstracts_in = []
  abstracts_en = []
  keywords = []
  all_documents = [titles, authors, years, abstracts_in, abstracts_en, keywords]
  name_documents = ['titles', 'authors', 'years', 'abstracts_in', 'abstracts_en', 'keywords']
  # membuat header untuk kebutuhan user agent pada request
  headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'}
  # menjalankan untuk tiap repository
  for rep in list_link:
    # membuat request URL
    URL = rep
    page = requests.get(URL, headers)
    # membuat parsing html
    soup = BeautifulSoup(page.content, "html.parser")
    results = soup.find(id="body")
    document_elements = results.find_all("div", class_="container")
    # mengambil data sesuai kebutuhan (judul, pengarang, tahun, abstrak_in, abstrak_en, kata_kunci)
    for document_element in document_elements:
      title = document_element.find("p", class_="title").text.strip()
      author = document_element.find("p", class_="penulis").text.strip()
      year = document_element.find("strong", class_="detail-karya-akhir").text.strip()
      abstract_in = document_element.find("p", class_="abstrak").text.strip()
      abstract_en = document_element.find("p", class_="abstrak").findNext('p').text.strip()
      keyword = document_element.find("p", class_="keyword").text.strip()
      # menggabungkan seluruh data
      titles.append(title)
      authors.append(author)
      years.append(year)
      abstracts_in.append(abstract_in)
      abstracts_en.append(abstract_en)
      keywords.append(keyword)
    if k % 10 == 0:
      print("collecting {} documents succeed...".format(k))
    k += 1
  # menyimpan list berisi judul, pengarang, tahun, abstrak_in, abstrak_en, kata_kunci
  j = 0
  for docs in all_documents:
    open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_"+name_documents[j], "wb")
    pickle.dump(docs, open_file)
    open_file.close()
    j += 1
  print("- - - - - - - - - - - - - - - - - - - - - - - - - - -")
  print("done collecting all documents from {} repository".format(len(list_link)))
  print("total {} documents titles".format(len(titles)))
  print("total {} documents authors".format(len(authors)))
  print("total {} documents years".format(len(years)))
  print("total {} documents abstracts_in".format(len(abstracts_in)))
  print("total {} documents abstracts_en".format(len(abstracts_en)))
  print("total {} documents keywords".format(len(keywords)))
  print("-----------------------------------------------------")
  for docs in name_documents:
    print("documents {} will save to {}".format(docs, "/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_"+docs))

list_link = link_repository.copy()
collecting_documents(list_link)

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_titles", "rb")
titles = pickle.load(open_file)
open_file.close()
titles = [x.replace("\r\n"," ") for x in titles]
titles = [x.replace("\n"," ") for x in titles]
titles

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_authors", "rb")
authors = pickle.load(open_file)
open_file.close()
authors = [x.replace("\r\n"," ") for x in authors]
authors = [x.replace("\n"," ") for x in authors]
authors

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_years", "rb")
years = pickle.load(open_file)
open_file.close()
years = [x.replace("\r\n"," ") for x in years]
years = [x.replace("\n"," ") for x in years]
years

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_abstracts_in", "rb")
abstracts_in = pickle.load(open_file)
open_file.close()
abstracts_in = [x.replace("\r\n"," ") for x in abstracts_in]
abstracts_in = [x.replace("\n"," ") for x in abstracts_in]
abstracts_in

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_abstracts_en", "rb")
abstracts_en = pickle.load(open_file)
open_file.close()
abstracts_en = [x.replace("\r\n"," ") for x in abstracts_en]
abstracts_en = [x.replace("\n"," ") for x in abstracts_en]
abstracts_en

open_file = open("/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/all_keywords", "rb")
keywords = pickle.load(open_file)
open_file.close()
keywords = [x.replace("\r\n"," ") for x in keywords]
keywords = [x.replace("\n"," ") for x in keywords]
keywords

"""### Memindahkan ke Dataframe"""

import pandas as pd

documents_thesis = pd.DataFrame(data={'link_repository':link_repository, 'titles':titles, 'authors':authors, 'years':years, 'abstracts_in':abstracts_in, 'abstracts_en':abstracts_en, 'keywords':keywords})

documents_thesis

documents_thesis.to_csv('/content/drive/MyDrive/Tugas Akhir/Data Scraping/Data/documents_thesis.csv', index=False)