link_repository,titles,authors,years,abstracts_in,abstracts_en,keywords
http://etd.repository.ugm.ac.id/home/detail_pencarian/209155,Model Pohon Regresi dengan Pendekatan Kredibilitas dan Aplikasinya pada Asuransi Kendaraan Bermotor,"AULADIENA SHAUMU RODHIYATIN, Danang Teguh Qoyyimi, M.Sc., Ph.D",2022 | Skripsi | S1 STATISTIKA,"Dalam perhitungan premi terdapat salah satu metode yang digunakan yaitu perhitungan premi dengan menggunakan teori kredibilitas. Terdapat beberapa teori kredibilitas yang telah dikembangkan, salah satunya teori kredibilitas klasik BÃƒÂ¼hlmann-Straub. Dalam skripsi ini akan dibahas mengenai penerapan metode machine learning ke dalam teori kredibilitas yang selanjutnya disebut dengan pohon regresi dengan pendekatan kredibilitas. Risiko individu akan dibagi ke dalam beberapa sub-kolektif dengan menggunakan algoritma pohon regresi dengan pendekatan kredibilitas berdasarkan fungsi kerugian BÃƒÂ¼hlmann-Straub kemudian akan diprediksi premi bersih individu dari masing-masing sub-kolektif. Metode ini cukup efektif karena dapat memanfaatkan berbagai informasi kovariat yang ada sehingga dapat meminimalkan kesalahan prediksi dalam perhitungan premi bersih individu.","One of the methods used in premium calculation is premium calculation using credibility theory. There are several credibility theories that have been developed, one of which is the classic BÃƒÂ¼hlmann-Straub credibility theory. In this thesis, we discuss the application of machine learning methods into credibility theory, hereinafter referred to as regression tree with credibility approach. Individual risk is divided into several sub-collectives using a credibility regression tree algorithm based on credibility loss and then the classical BÃƒÂ¼hlmann-Straub credibility formula is applied to predict individual net premiums within each subcollective. This method is quite effective because it can take advantage of various available covariate information to minimize prediction errors in the calculation of individual net premiums.","Kata Kunci : premi, teori kredibilitas, BÃƒÂ¼hlmann-Straub, pohon regresi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/214787,PEMODELAN TOPIK PADA DATA REVIEW MENGGUNAKAN TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY DAN LATENT DIRICHLET ALLOCATION,"ARUNG RIZKY ANENURAT, Drs. Danardono, MPH., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Dalam upaya membatasi pertumbuhan Covid-19 dengan mendeteksi alur penyebarannya serta mengetahui setiap kontak erat yang terjadi antara individu, pemerintah melalui Kominfo meluncurkan aplikasi PeduliLindungi. Pada awal diluncurkannya aplikasi PeduliLindungi masih banyak keluhan dari masyarakat mengenai penggunaan aplikasi tersebut. Sehingga aplikasi PeduliLindungi terus diperbaharui untuk memperbaiki kekurangan dan menambah fitur yang masih dianggap kurang. Meski telah mengalami perbaharuan masih banyak keluhan dan saran yang masuk dari masyarakat. Banyaknya kritik dan saran yang disampaikan, tidak mudah bagi pengembang aplikasi PeduliLindungi untuk mengetahui topik apa saja yang dibahas dan perlu menjadi perhatian. Diperlukan metode untuk memodelkan topik apa saja yang dibahas dalam ulasan aplikasi PeduliLindungi pada Google Play Store. Pemodelan topik yang popular digunakan adalah Latent Dirichlet Allocation (LDA). Pada skripsi ini penulis menggabungkan metode pemodelan topik LDA dengan algoritma Term Frequency-Inversed Document Frequency (TF-IDF). Algoritma TF-IDF merupakan perkalian antara TF dan IDF. Perhitungan estimasi dari distribusi posterior menggunakan algoritma Variational Expectation Maximization (VEM). Hasil pemodelan topik ini berupa proporsi topik pada korpus, probabilitas kata pada setiap topik, dan proporsi topik dalam dokumen/ulasan.","In an effort to limit the growth of Covid-19 by detecting the flow of its spread and knowing every close contact that occurs between individuals, the government through the Ministry of Communication and Information has launched the PeduliLindung application. At the beginning of the launch of the PeduliLindung application, there were still many complaints from the public regarding the use of the application. So the PeduliLindung application continues to be updated to correct deficiencies and add features that are still considered lacking. Even though it has been updated, there are still many complaints and suggestions from the community. With so many criticisms and suggestions submitted, it is not easy for PeduliLindung application developers to find out what topics are discussed and need attention. A method is needed to model what topics are covered in the PeduliLindungi app review on the Google Play Store. The popular modeling topic used is Latent Dirichlet Allocation (LDA). In this thesis, the author combines the LDA topic modeling method with the Term Frequency-Inversed Document Frequency (TF-IDF) algorithm. The TF-IDF algorithm is a multiplication between TF and IDF. The estimation calculation of the posterior distribution uses the Variational Expectation Maximization (VEM) algorithm. The results of this topic modeling are the proportion of topics in the corpus, the probability of words in each topic, and the proportion of topics in the document/review.","Kata Kunci : Pemodelan topik, Latent Dirichlet Allocation, TF-IDF, Variational Expectation Maximization."
http://etd.repository.ugm.ac.id/home/detail_pencarian/215812,ANALISIS CHURN PADA PELANGGAN PRODUK SUSU MENGGUNAKAN METODE EXTREME LEARNING MACHINE,"MOCHAMAD RAIHAN M, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Dalam menjalankan bisnis hampir di seluruh industri sudah mulai bersaing dengan menerapkan customer relationship management (CRM). Sebelumnya CRM hanya diterapkan pada industri telekomunikasi yang berbasis langganan. Pada dasarnya CRM digunakan karena untuk mendapatkan pelanggan baru diperlukan biaya yang lebih banyak daripada mempertahankan pelanggan yang sudah ada, sehingga alat analisis yang akurat diperlukan untuk mendukung implementasi CRM. Pada skripsi ini dibahas bagaimana analisis untuk memprediksi pelanggan yang sudah tidak lagi membeli/menggunakan (churn) sebuah produk, sehingga lebih lanjut perusahaan dapat melakukan evaluasi atau memutuskan kebijakan yang lebih baik. Skripsi ini akan memprediksi pelanggan yang churn dengan algoritma extreme learning machine (ELM), backpropagation, dan regresi logistik. Studi kasus ini menggunakan data pelanggan salah satu produk susu. Hasil menunjukan bahwa nilai akurasi yang paling tinggi didapatkan dengan regresi logistik, lalu waktu komputasi yang paling cepat menggunakan extreme learning machine.","In running a business, almost all industries have started to compete by implementing customer relationship management (CRM). Previously, CRM was only applied to the subscription-based telecommunications industry. Basically CRM is used because getting new customers costs more than retaining existing customers, so accurate analytical tools are needed to support CRM implementation. This thesis discusses how to analyze how to predict customers who no longer buy/churn a product, so that the company can further evaluate or decide on a better policy. This thesis will predict customer churn with extreme learning machine (ELM) algorithm, backpropagation, and logistic regression. This case study uses customer data for one of the dairy products. The results show that the highest accuracy value is obtained by logistic regression, then the fastest computation time using extreme learning machine.","Kata Kunci : Customer Churn, Backpropagation, Extreme Learning Machine, Klasifikasi, Regresi logistik"
http://etd.repository.ugm.ac.id/home/detail_pencarian/210706,Aplikasi XGBoost pada Prediksi Cadangan Klaim Produk Asuransi dengan Loss Development Factors Tak Stabil,"RUTH CORNELIA N, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Semua kegiatan manusia tidak terlepas dari risiko. Salah satu cara untuk mengatasi risiko adalah dengan mentransfer risiko tersebut kepada perusahaan asuransi dimana pihak tertanggung harus membayarkan premi dan pada saat terjadi kejadian yang merugikan bagi pihak tertanggung, pihak tertanggung dapat melakukan klaim. Adanya penundaan pelaporan klaim maupun penundaan waktu sampai klaim diselesaikan membuat perusahaan asuransi wajib untuk menyiapkan suatu dana cadangan, salah satunya adalah cadangan klaim. Hingga saat ini, dalam menghitung besarnya cadangan klaim yang perlu disiapkan perusahaan, kebanyakan aktuaris di Indonesia masih menggunakan metode Chain-Ladder. Padahal, Chain-Ladder tidak dapat menangkap pola dengan baik apabila data memiliki loss development factors yang tidak stabil. Di sisi lain, salah satu algoritma dari metode machine learning yang cukup sering diteliti dalam pencadangan klaim adalah CART. Namun, CART tidak begitu baik karena tidak robust. Oleh karena itu, penulis mengenalkan algoritma XGBoost yang merupakan ensemble model dari beberapa model CART untuk memprediksi cadangan klaim dari data run-off triangle. Berdasarkan hasil penelitian ini, diperoleh kesimpulan bahwa algoritma XGBoost mampu mengungguli metode Chain-Ladder, CART, dan Random Forest dalam memprediksi cadangan klaim pada run-off triangle dengan loss development factors yang tidak stabil.","All human activities are inseparable from risk. One way to overcome the risk is to transfer the risk to an insurance company where the insured party has to pay a premium and in the event of an adverse event for the insured, the insured party can make a claim. The existence of delays in reporting claims and delays in time until claims are resolved makes insurance companies obliged to prepare a reserve fund, one of which is a claim reserve. Until now, in calculating the amount of claim reserves that need to be prepared by the company, most actuaries in Indonesia still use the Chain-Ladder method. In fact, Chain-Ladder cannot capture patterns well if the data has unstable loss development factors. On the other hand, one of the algorithms from machine learning methods that is quite often researched in claims reserve is CART. However, CART is not very good because it is not robust. Therefore, the author introduces the XGBoost algorithm which is an ensemble model of several CART models to predict claim reserves from the run-off triangle data. Based on the results of this study, it is concluded that the XGBoost algorithm is able to outperform the Chain-Ladder, CART, and Random Forest methods in predicting claims reserves on run-off triangle that has unstable loss development factors.","Kata Kunci : Cadangan Klaim, Chain-Ladder, Extreme Gradient Boosting, Loss Development Factors Tak Stabil, Regression Tree"
http://etd.repository.ugm.ac.id/home/detail_pencarian/212500,VALUASI OBLIGASI DENGAN PEMODELAN SUKU BUNGA CIR PENDEKATAN MODEL MERTON,"PUTRI DWI RAMADHANI, Dr.Abdurakhman, S.Si., M.Si.",2022 | Skripsi | S1 STATISTIKA,"Obligasi merupakan surat hutang jangka panjang yang berisi perjanjian antara penerbit obligasi dan pemegang obligasi (investor) untuk nantinya penerbit obligasi akan membayar hutang pokok saat jatuh tempo dan pembayaran kupon sesuai dengan waktu yang disepakati sebelumnya. Kupon merupakan imbal hasil untuk investor dari penerbit obligasi. Dalam pelaksanaannya, berinvestasi obligasi selain memberikan keuntungan juga memiliki potensi risiko kerugian bagi investor sabagai pemegang obligasi yaitu salah satunya risiko kredit. Pengukuran risiko kredit pertama kali dikembangkan oleh Merton (1974) dengan memodifikasi Model Black-Scholes. Pada model Merton diasumsikan bahwa kondisi obligasi tanpa kupon (zero coupon bond) dan perusahaan diasumsikan hanya dapat mengalami kebangkrutan pada saat jatuh tempo dengan kata lain jika penerbit obligasi tidak mampu memenuhi kewajiban pembayaran hutang pada saat jatuh tempo, maka perusahaan tersebut default atau mengalami kebangkrutan. Suku bunga stokastik yang bebas risiko yang digunakan di dalam perhitungan risiko kredit obligasi ini mengikuti model suku bunga Cox - Ingersoll - Ross (CIR). Setelah dilakukan pengaplikasian terhadap aset PT Bank Pan Indonesia Tbk dan aset PT Bank Capital Indonesia Tbk, hasilnya menunjukkan bahwa kedua perusahaan dianggap masih dapat memenuhi pembayaran kewajibannya pada saat jatuh tempo.","Bonds are financial instruments in the form of debt securities that are approved by bond issuer and investor where the bond issuer required to pay the coupon and the principal value at maturity. Coupons are returns for investors from the bond issuer. In practice, investing in bond instruments will get many profit but also has risk that can harm investors as the bondholders, one of them is credit risk. Credit risk structural model for the first time was developed by Merton (1974) with the assumptions that exist in the Black-Scholes Model. In the Merton Model assumed bonds condition is without coupon or free coupon and when the firm as the bond issuer cannot pay the debt at the maturity the firm can be said to be default or bankrupt. The free-risk interest rate that used in this credit risk valuation is following CIR rate. After applying it to the asset of PT Bank Pan Indonesia Tbk and PT Bank Capital Indonesia Tbk, the results show that both companies are considered to be able to full their obligations at the maturity","Kata Kunci : Risiko Kredit, Model Merton, Suku Bunga Cox-Ingersoll-Ross, Obligasi Berkupon Nol."
http://etd.repository.ugm.ac.id/home/detail_pencarian/210968,Penentuan Bobot Portofolio dengan Strategi Maximum Diversification Berdasarkan Algoritma K-Means++,"DITTA NOVITASARI, Dr. Adhitya Ronnie Effendie, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Investasi merupakan kegiatan menempatkan dana yang dimiliki saat ini pada suatu aset dengan tujuan untuk memperoleh keuntungan di masa mendatang. Dalam investasi, terdapat risiko yang tidak dapat dihindari disamping pengembalian yang diharapkan. Risiko tersebut dapat diminimalkan dengan cara diversifikasi. Strategi maximum diversification, yang diperkenalkan oleh Choueifaty dan Coignard (2008) adalah strategi alokasi aset berbasis risiko yang menggunakan ukuran diversifikasi sebagai dasar dalam konstruksi portofolio. Strategi ini membentuk portofolio semaksimal mungkin terdiversifikasi dengan cara memaksimalkan rasio volatilitas tertimbang aset terhadap volatilitas portofolio. Pada penelitian ini akan dibahas suatu metode gabungan dari algoritma K-Means++ dengan strategi maximum diversification dalam penentuan bobot portofolio. Algoritma K-Means++ digunakan untuk mengelompokkan saham berdasarkan kemiripan karakteristiknya meliputi harga saham, return on asset, return on equity, debt to equity ratio dan net profit margin sedangkan strategi maximum diversification digunakan untuk menentukan bobot portofolio. Studi kasus penelitian ini menggunakan data closing price saham harian periode Januari 2021 sampai dengan Desember 2021 dari 17 saham LQ45. Nilai return masing-masing saham dihitung kemudian dibentuk portofolio menggunakan strategi maximum diversification berdasarkan algoritma K-Means++. Kinerja portofolio tersebut kemudian dibandingkan dengan portofolio maximum diversification biasa menggunakan rasio sharpe. Hasil yang diperoleh menunjukkan bahwa portofolio dengan strategi maximum diversification berdasarkan algoritma K-Means++ lebih baik daripada portofolio maximum diversification biasa.","Investment is the activity of placing funds that are currently held in order to obtain profit in the future. In every investment, there will be unavoidable risks in addition to the expected return. This risk can be minimized by diversification. The maximum diversification strategy, introduced by Choueifaty and Coignard (2008) is a risk-based asset allocation strategy that uses diversification measure in portfolio construction. This strategy creates the most diversified portfolio by maximizing the ratio of the weighted average of asset volatilities to portfolio volatility. In this study, we will discuss a combined method of the K-Means++ algorithm with a maximum diversification strategy to determine portfolio weight. K-Means++ algorithm is used to group stocks based on their similar characteristics including stock price, return on asset, return on equity, debt to equity ratio and net profit margin while the maximum diversification strategy is used to determine portfolio weights. This research case study uses daily stock closing price data for the period of January 2021 to December 2021 from 17 stocks of LQ45 index. The return of each stock is calculated and then a portfolio is formed using the maximum diversification strategy based on the K-Means++ algorithm. The portfolio performance of that method is compared with the ordinary maximum diversification portfolio using the sharpe ratio. The result show that the portfolio using the maximum diversification strategy based on the K-Means++ algorithm is better than the ordinary maximum diversification portfolio.","Kata Kunci : portofolio, maximum diversification, algoritma K-Means++, rasio sharpe"
http://etd.repository.ugm.ac.id/home/detail_pencarian/210714,Penentuan Premi Murni Asuransi Kendaraan Bermotor berdasarkan Jarak Tempuh (Pay-As-You-Drive Insurance) dengan Tree-Based Machine Learning,"DHESTAR BAGUS W, Dr. Gunardi, M.Si.",2022 | Skripsi | S1 STATISTIKA,"Sistem penentuan premi asuransi kendaraan bermotor di Indonesia saat ini masih belum cukup adil bagi nasabah, terutama terkait jarak tempuh. Terdapat subsidi silang antara nasabah yang jarang menggunakan kendaraannya dengan yang sering menggunakan. Penelitian yang dilakukan oleh Ferreira dan Minikel (2012) menargetkan permasalahan ini dan menggunakan Generalized Linear Models (GLM) untuk menghitung premi murni asuransi kendaraan bermotor berdasarkan jarak tempuh atau biasa disebut pay-as-you-drive insurance. Meskipun GLM sering digunakan dalam pemodelan di dunia asuransi, ketergantungan metode GLM terhadap asumsi-asumsi yang harus terpenuhi dan ketidakmampuan dalam menangkap pola non-linear menjadi kelemahan utamanya. Penelitian ini akan memanfaatkan tree-based machine learning yaitu Random Forest dan Gradient Boosting Machine dalam perhitungan premi murni pay-as-you-drive insurance dan membuka black-box machine learning sehingga memiliki kemampuan interpretasi yang sama dengan GLM. Diperoleh kesimpulan bahwa algoritma Gradient Boosting Machine mampu menghasilkan model yang memiliki RMSE terendah, baik itu untuk pemodelan frekuensi klaim maupun severity klaim. Selain itu, diperoleh kesimpulan juga bahwa asuransi PAYD lebih baik daripada asuransi kendaraan bermotor tradisional.","The current system for determining auto-insurance premium in Indonesia is still not fair enough for the customer, especially related to mileage. There are cross-subsidized between the low-mileage customers and high-mileage customers. In 2012, Ferreira and Minikel target this problems and used Generalized Linear Models (GLM) to calculate the pure premium based on mileage or known as pay-as-you-drive insurance. Although GLM is often used when modelling in insurance, the dependence of GLM on assumptions and its inability to capture non-linear pattern is the main weaknesses of GLM. This research would use tree-based machine learning, e.g. Random Forest and Gradient Boosting Machine, in calculating the pure premium of pay-as-you-drive insurance and opening the black-box of machine learning so that it has the same interpretation capabilities as the GLM models. It is concluded that the Gradient Boosting Machine algorithm is able to produce a model that has the lowest RMSE, both for modeling claim frequency and claim severity. In addition, it is also concluded that PAYD insurance is better than traditional motor vehicle insurance.","Kata Kunci : Gradient Boosting Machine, Generalized Linear Models, Pay-as-you-drive Insurance, Random Forest"
http://etd.repository.ugm.ac.id/home/detail_pencarian/214308,Robust Jackknife Ridge Regression dengan Estimator Least Absolute Deviations (LAD) untuk Mengatasi Masalah Multikolinearitas dan Pencilan pada Regresi Linear Berganda,"GABRIELLA A L TOBING, Dr. Herni Utami, S.Si., M.Si.",2022 | Skripsi | S1 STATISTIKA,"Dalam ilmu statistika, salah satu metode analisis yang paling sering digunakan adalah analisis regresi. Analisis regresi merupakan metode yang digunakan untuk mengetahui pengaruh antara variabel independen terhadap variabel dependen. Untuk mengestimasi parameter model regresi digunakan metode kuadrat terkecil. Metode ini memerlukan beberapa asumsi klasik yang harus terpenuhi, salah satunya adalah tidak terjadi multikolinearitas antar variabel independen. Jika terjadi multikolinearitas pada model regresi, penggunaan metode kuadrat terkecil akan menghasilkan estimator yang bersifat bias dan memberikan kesimpulan yang kurang tepat.  Metode ridge regression dapat digunakan untuk mengatasi masalah multikolinearitas. Metode ini ditempuh dengan cara menambah tetapan bias k pada diagonal utama matriks Z'Z. Meskipun ridge regression memiliki sifat yang optimal dalam menangani masalah multikolinearitas, estimatornya bersifat bias. Metode jackknife ridge regression dapat digunakan untuk mengurasi bias dari ridge regression. Akan tetapi jackknife ridge regression kurang tepat digunakan untuk data yang memiliki pencilan. Untuk mengatasinya dapat menggunakan robust regression dengan estimator Least Absolute Deviations (LAD). Jadi untuk mengatasi masalah multikolinearitas dan pencilan secara bersamaan digunakan metode robust jackknife ridge regression dengan estimator LAD, di mana robust jackknife ridge regression merupakan hasil pengembangan dari robust ridge regression.  Penelitian ini menggunakan data persentase penduduk miskin berdasarkan kabupaten/kota di Provinsi Sumatera Utara pada tahun 2019 dan faktor-faktor yang mempengaruhinya sebagai studi kasus. Berdasarkan kriteria pemilihan model terbaik, yaitu nilai Mean Square Error (MSE), Akaike Information Criterion (AIC), dan Bayesian Information Criterion (BIC), metode robust jackknife ridge regression dengan estimator LAD lebih baik dibandingkan metode robust ridge regression dengan estimator LAD.","In statistics, one of the most frequently used analytical method is regression analysis. Regression analysis is the method that used to determine the effect of the independent variables on the dependent variable. To estimate the parameters of the regression model, the ordinary least squares method is used. This method requires several classical assumptions that must be fulfilled, one of which is that there is no multicollinearity between independent variables. If there is multicollinearity in the regression model, the use of ordinary least squares method will result in a biased estimator and give inaccurate conclusions.  The ridge regression method is used to solve the multicollinearity problem. The concept is the addition of bias constant k on the main diagonal of the Z'Z matrix. Although ridge regression has optimal properties in dealing with multicollinearity problems, the estimator is biased. Jackknife ridge regression method can be used to reduce the bias of ridge regression. However, jackknife ridge regression is not appropriate for data that has outliers. To overcome this, robust regression can be used with the Least Absolute Deviations (LAD) estimator. So to overcome the problem of multicollinearity and outliers simultaneously, the robust jackknife ridge regression method is used with the LAD estimator, where robust jackknife ridge regression is the development of the robust ridge regression.  This study uses data on the percentage of poor people based on districts/cities in North Sumatra Province in 2019 and the factors that influence it as a case study. Based on the criteria for selecting the best model, such as the value of Mean Square Error (MSE), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC), the robust jackknife ridge regression method with the LAD estimator is better than the robust ridge regression method with the LAD estimator.","Kata Kunci : Multikolinearitas, Pencilan, Ridge regression, Jackknife, Robust, Estimator Least Absolute Deviations (LAD)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/214822,Optimasi Aplikasi Bayesian Structural Time Series (BSTS) dalam Peramalan Harga Saham Melalui Pemilihan State Components,"BENITA KATARINA, Dr. Drs. Gunardi, S.Si., M.Si.",2022 | Skripsi | S1 STATISTIKA,"Dalam perdagangan saham, prediksi harga menjadi salah satu topik yang tidak pernah habis untuk dibahas. Keberadaannya dianggap cukup penting karena dapat membantu para investor untuk meminimalisir kerugian yang mungkin timbul. Sejatinya, prediksi harga saham merupakan hal yang sulit dilakukan, namun metode-metode peramalan yang telah ditemukan setidaknya dapat membantu memberikan gambaran akan masa depan dengan lebih baik. Autoregressive Integrated Moving Average (ARIMA) merupakan metode peramalan klasik yang lazim digunakan meski kurang cocok untuk diterapkan pada data yang bervolatilitas tinggi dan kompleks seperti saham. Pada beberapa literatur, ARIMA bahkan memiliki nilai kesalahan yang lebih besar dalam meramalkan harga saham dibandingkan dengan metode klasik lain yang lebih sederhana, yaitu Double Exponential Smoothing (Holt). Perluasan dari model ARIMA untuk data musiman, yaitu Seasonal Autoregressive Integrated Moving Average (SARIMA), juga menghasilkan nilai kesalahan yang lebih besar dibandingkan Triple Exponential Smoothing (Holt-Winters). Bayesian Structural Time Series (BSTS) yang dikembangkan oleh Scott dan Varian pada tahun 2013 dapat menjadi metode alternatif yang dapat digunakan untuk meramalkan harga saham. Metode ini memiliki fleksibilitas yang tinggi dan pandai menggambarkan pola-pola pada deret waktu sehingga cocok digunakan untuk data saham yang kompleks. Berdasarkan hasil yang diperoleh dari penelitian ini, BSTS memiliki nilai kesalahan yang lebih kecil dibandingkan dengan Holt-Winters dan SARIMA. Namun, untuk menghasilkan model dengan nilai kesalahan yang kecil, diperlukan pemilihan state components yang tepat dalam pembentukan modelnya.","In stock trading, price prediction is a never-ending topic to discuss. This prediction is considered important because it can help investors in minimizing losses that may appear. Stock price prediction is indeed a difficult thing to do, but the forecasting methods that have been found can help provide a better picture of the future. Autoregressive Integrated Moving Average (ARIMA) is a classical forecasting method that is commonly used, even though this method is not suitable for data with high volatility and complexity, such as stocks. In some literature, ARIMA even produces larger error values in forecasting stock prices compared to simpler classical methods, that is Double Exponential Smoothing (Holt). The extension of the ARIMA model for seasonal data, namely the Seasonal Autoregressive Integrated Moving Average (SARIMA), also produces larger error values than Triple Exponential Smoothing (Holt-Winters). The Bayesian Structural Time Series (BSTS) which was developed by Scott and Varian in 2013 can be an alternative method that can be used to forecast stock prices. This method has high flexibility and is good at describing time series patterns, so it is suitable for complex stock data. Based on the results obtained from this study, BSTS has smaller error values than Holt-Winters and SARIMA. However, to produce a model with high accuracy, the proper selection of state components has to be done.","Kata Kunci : Bayesian Structural Time Series, Seasonal Autoregressive Integrated Moving Average, Holt-Winters, peramalan harga saham"
http://etd.repository.ugm.ac.id/home/detail_pencarian/217146,UJI HIPOTESIS KESAMAAN MEAN BROWN-FORSYTHE UNTUK POPULASI HETEROSKEDASTIK,"VISHY ANAND CANDAYA, Danang Teguh Qoyyimi., M.Sc., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Analisis Variansi (ANAVA) merupakan salah satu metode statistika yang digunakan untuk menguji ada tidaknya perbedaan rata-rata yang signifikan antar level faktor. Dalam klasik ANAVA, digunakan statistik uji CF yang mana tidak robust dalam kondisi pelanggaran asumsi kesamaan variansi antar level faktor. Kemudian terdapat statistik uji F* yang merupakan pengembangan dari statistik uji CF. Statistik uji F* ini kemudian digunakan dalam Uji Brown-Forsythe. Selanjutnya kedua metode ini akan dibandingkan kinerjanya dalam olah data riil dan studi simulasi menggunakan metode simulasi Monte Carlo untuk membandingkan nilai type I error rate. Hasil simulasi menunjukkan bahwa kekuatan Uji Brown-Forsythe lebih baik dibandingkan uji ANAVA klasik.","Analysis of Variance (ANOVA) is one of the statistical methods used to test whether or not there is a significant average difference between level factors. In ANOVA, CF test statistic is used which is not robust in the condition of violating the assumption of similarity between factor levels. Therefore, the F* test statistic is used which is an extension of the CF test statistic. Then, the F* test statistic is used in the Brown-Forsythe test. Furthermore, the performance of these two methods is compared in real data processing and study simulation using the Monte Carlo simulation to compare the value of the type I error rate. The simulation show that the strength of the Brown-Forsythe test is better than the classical ANOVA .","Kata Kunci : Analisis Variansi, Statistik Uji CF, Statistik Uji F*, Uji Brown-Forsythe, Type I Error Rate."
http://etd.repository.ugm.ac.id/home/detail_pencarian/211789,Support Vector Machine dengan Stochastic Gradient Descent Training (SVM-SGD) pada Analisis Sentimen,"NUR ISFANDIARIE P, Drs. Danardono, MPH., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Salah satu upaya pemerintah Indonesia untuk menanggulangi Covid-19 adalah dengan memperkuat 3T (Testing, Tracing, dan Treatment). Pelaksanaan 3T secara digital didukung dengan penggunaan aplikasi buatan Kemenkominfo yaitu PeduliLindungi. Tempat umum (pusat perbelanjaan, restoran, dsb.) kini mewajibkan seluruh pengunjung untuk scan QR code PeduliLindungi. Selain itu, banyak fitur lainnya yang mendorong masyarakat Indonesia menggunakan PeduliLindungi. Penting bagi pihak pengembang PeduliLindungi untuk menganalisis opini pengguna terhadap PeduliLindungi melalui ulasan yang ditulis langsung oleh pengguna salah satunya yang dapat diakses pada Google Play. Diperlukan metode untuk mengklasifikasikan opini pengguna (positif dan negatif) secara otomatis mengingat banyaknya ulasan yang masuk setiap harinya. Metode klasifikasi yang dapat digunakan adalah Support Vector Machine dan metode hybrid Support Vector Machine dengan Stochastic Gradient Descent Training. Karena data yang digunakan berbentuk teks, maka penting untuk mempertimbangkan teknik representasi vektor yang sesuai dengan metode klasifikasi yang digunakan, pada penelitian ini diujikan dua teknik yaitu BoW dan TF-IDF. Diperoleh metode SVM dengan vektor TF-IDF adalah yang paling baik diantara model lainnya, karena memiliki nilai akurasi, recall, presisi, dan f1-score paling tinggi yaitu nilai akurasi 95.68%, presisi 95.93%, recall 95.40%, dan f1-score 95.67%.","One of the Indonesian government's efforts to tackle Covid-19 is to strengthen 3T (Testing, Tracing, and Treatment). The implementation of 3T digitally is supported by the use of an application made by the Ministry of Communication and Information, namely PeduliLindungi. Public places (shopping centers, restaurants, etc.) now require all visitors to scan the PeduliLindungi QR code. In addition, there are many other features that encourage Indonesian people to use PeduliLindungi. It is important for PeduliLindungi developers to analyze user opinions on PeduliLindungi through reviews written directly by users, one of which can be accessed on Google Play. A method is needed to classify user opinions (positive and negative) automatically considering the number of reviews that come in every day. The classification methods that can be used are the Support Vector Machine and the hybrid Support Vector Machine with Stochastic Gradient Descent Training method. Because the data used is in the form of text, it is important to consider vector representation techniques that are in accordance with the classification method used. In this study, two techniques were tested, namely BoW and TF-IDF. The SVM method with the TF-IDF vector is the best among other models, because it has the highest accuracy, recall, precision, and f1-score values, with 95.68% accuracy, 95.93% precision, 95.40% recall, and 95.67% f1-score.","Kata Kunci : analisis sentimen, Support Vector Machine, Support Vector Machine dengan Stochastic Gradient Descent Training, BoW, TF-IDF"
http://etd.repository.ugm.ac.id/home/detail_pencarian/211304,Model Regresi Robust Hurdle Poisson untuk Mengatasi Overdispersi dan Outlier,"Septiana Rizki, Dr. Abdurakhman, S.Si., M.Si",2022 | Skripsi | S1 STATISTIKA,"Analisis regresi yang dapat digunakan untuk memodelkan data cacah adalah regresi Poisson. Model regresi Poisson memiliki asumsi equidispersi, dimana mean variabel respon sama dengan variansinya. Apabila data variabel respon mengalami overdispersi, yaitu kondisi variansi data lebih besar dari mean data, maka digunakan metode alternatif yaitu regresi Hurdle Poisson. Model ini terdiri atas dua bagian yang independen, yaitu model logit dan model Truncated Poisson. Namun metode tersebut tidak dapat mengontrol outlier pada data. Oleh karena itu, untuk mengatasi overdispersi dan outlier secara bersama-sama dapat digunakan model regresi Robust Hurdle Poisson. Berdasarkan studi kasus diperoleh kesimpulan bahwa regresi Robust Hurdle Poisson lebih baik dibandingkan regresi Hurdle Poisson melalui perbandingan kurva sensitivitas","A regression analysis that can be used to model count data is the Poisson regression. This regression model assumes equidispersion, in which the mean and the variance of the response variable data, namely the variance is greater than the mean, then an alternative method, the Hurdle Poisson regression, is used. This particular model consists of two independent components, namely the logit model and the Truncated Poisson model. However, this method cannot control outliers. Therefore, to overcome both overdispersion and outliers, the Robust Hurdle Poisson regression model can be used. Based on the case study that had been conducted, it can be concluded that the Robust Hurdle Poisson regression is better than the Hurdle Poisson regression by comparing the sensitivity curves.","Kata Kunci : regresi Poisson, overdispersi, outlier, robust Hurdle Poisson"
http://etd.repository.ugm.ac.id/home/detail_pencarian/213097,Estimasi Bayesian pada Model Regresi Zero-Inflated Generalized Poisson (ZIGP) pada Kasus Difteri di Pulau Jawa dengan Simulasi Markov Chain Monte Carlo (MCMC),"AISYAH UMMU SHABRINA, Drs. Danardono, MPH., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Model regresi Poisson merupakan salah satu bentuk analisis regresi yang digunakan untuk memodelkan data cacah (count data) dan termasuk ke dalam Generalized Linear Model (GLM). Terdapat asumsi yang harus terpenuhi pada regresi Poisson yaitu data harus bersifat equidispersi atau kesamaan antara nilai mean dan variansinya. Namun, pada kenyataannya sering terjadi keadaan dimana nilai variansi lebih besar dari nilai mean atau disebut dengan overdispersi. Salah satu faktor penyebab overdispersi yaitu terlalu banyak dijumpai nilai nol pada variabel respon data cacah. Dalam tugas akhir ini akan dibahas salah satu alternatif dalam mengatasi masalah tersebut yaitu dengan menggunakan model regresi Zero-Inflated Generalized Poisson (ZIGP). Pendugaan parameter dalam model regresi ini dilakukan berdasarkan metode Bayesian dengan bantuan simulasi Markov Chain Monte Carlo (MCMC). Model terbaik yang diukur berdasarkan nilai Deviance Information Criterion (DIC) selanjutnya akan digunakan dalam memodelkan data jumlah kasus penyakit difteri di Pulau Jawa yang bersifat overdispersi akibat nilai nol yang berlebih.","The Poisson regression, that is included in the Generalized Linear Model (GLM), is a type of regression analysis used to model count data. In Poisson regression, there is an assumption that the data must have equidispersion, or that the mean and variance values should be similar. However, there are many cases in which the variance value is greater than the mean value, which is known as overdispersion. One of the factors contributing to overdispersion is the presence of too many zero values in the response variable of the count data. In this final project, one solution to this problem will be discussed, namely the Zero-Inflated Generalized Poisson (ZIGP) regression model. The Bayesian approach is used to estimate the parameters in this regression model, which is done using Markov Chain Monte Carlo (MCMC) simulation. The best model, as determined by the Deviance Information Criterion (DIC) value, will be used to model data on the number of diphtheria cases in Java, which is overdispersed due to excessive zero values.","Kata Kunci : Zero-Inflated Generalized Poisson (ZIGP), Bayesian, MCMC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/217967,PERBANDINGAN ALGORITMA CGD LASSO DENGAN LARS LASSO UNTUK MENGANALISIS DATA BERDIMENSI TINGGI,"VINCENTIUS RYO S, Prof. Dr.rer.nat. Dedi Rosadi S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Data sudah menjadi dasar dari seluruh operasi statistik. Semua analisis pasti membutuhkan data. Pada analisis tersebut ada banyak asumsi-asumsi berhubungan dengan data yang harus dipenuhi. Contoh analisis tersebut adalah analisis regresi. Salah satu asumsi yang harus dipenuhi dalam analisis regresi adalah asumsi non-multikolinearitas, yaitu asumsi bahwa setiap variabel independen pada data tidak mempengaruhi variabel independen lainnya. Namun pada praktiknya tidak semua data dapat memenuhi asumsi tersebut. Contoh data yang memiliki masalah tersebut adalah data yang digunakan pada penelitian ini adalah data covid-19. Data berdimensi tinggi (p>n) tersebut memiliki masalah multikolinearitas sehingga tidak dapat dianalisis menggunakan metode regresi biasa. Maka untuk itu perlu dilakukan metode alternatif yang dapat digunakan pada data yang tidak memenuhi asumsi tersebut. Salah satu metode alternatif tersebut adalah analisis LASSO. Analisis LASSO adalah metode regresi alternatif yang dapat digunakan untuk mengatasi masalah multikolinearitas. Dalam metode LASSO ada berbqgai algoritma yang dapat digunakan sesuai dengan situasi dan kondisi. Algoritma yang digunakan dalam penelitian ini adalah algoritma gradient descent dan LARS. Tujuan dari penelitian ini adalah untuk menentukan algoritma LASSO yang paling efektif dalam menganalisis data covid-19.  Untuk menentukan metode yang terbaik maka dilihat dari nilai MSE dan MAE terkecil, dan nilai R^2 terbesar. Data covid-19 yang digunakan adalah data pada tanggal 1 februari 2022 sampai tanggal 15 maret 2022. Berdasarkan metriks evaluasi yang diperoleh dapat dilihat bahwa nilai metriks evaluasi algoritma LARS lebih  baik dari algoritma lainnya. Hal tersebut mengindikasikan bahwa metode LARS LASSO adalah metode yang paling efektif untuk menganalisis data covid-19.  Dengan menggunnakan metode LARS LASSO, didapatkan hasil bahwa faktor faktor yang mempengaruhi nilai reproduction rate adalah new  cases, new  deaths  smoothed, total  deaths  per  million, total  tests  per  thousand, new  tests, new  tests  smoothed, positive  rate, total  vaccinations, dan new  vaccinations  smoothed.","Data is the basis of all statistical operations. All analysis needs data. In statistical analysis there are many assumptions related to the data that must be met. An example of such analysis is regression analysis. One of the assumptions that must be met in regression analysis is the assumption of non-multicollinearity, namely the assumption that each independent variable in the data does not affect the other independent variables. However, in practice not all data can meet these assumptions. An example of data that has this problem is the data used in this study, the data on COVID-19. This high-dimensional data (p>n) has a multicollinearity problem so that it cannot be analyzed using the usual regression method. Therefore, it is necessary to use alternative methods that can be used on data that do not meet these assumptions. One such alternative method is LASSO analysis. LASSO analysis is an alternative regression method that can be used to solve the problem of multicollinearity. In the LASSO method there are various algorithms that can be used according to the situation. The algorithm used in this study is the gradient descent algorithm and LARS. The purpose of this study was to determine the most effective LASSO algorithm in analyzing COVID-19 data. To determine the best method, it is seen from the smallest MSE and MAE values, and the largest R^2 value. The covid-19 data used is data from February 1, 2022 to March 15, 2022.  Based on the evaluation metric obtained, it can be seen that the evaluation metric value of the LARS algorithm is better than other algorithms. This indicates that the LARS LASSO method is the most effective method for analyzing COVID-19 data. By using the LARS LASSO method, it is found that the factors that affect the reproduction rate are new cases, new deaths smoothed, total deaths per million, total tests per thousand, new tests, new tests smoothed, positive rate, total vaccinations, and new vaccinations. smoothed.","Kata Kunci : Covid-19, LASSO, LARS LASSO, CGD LASSO"
http://etd.repository.ugm.ac.id/home/detail_pencarian/211569,Analisis Klasifikasi Menggunakan Metode Gradient Boosting Machine (GBM) dan Light Gradient Boosting Machine (LGBM),"SANIA DWIKI N, Dr. Herni Utami, S.Si., M.Si.",2022 | Skripsi | S1 STATISTIKA,"Pohon keputusan (decision tree) adalah salah satu metode dalam analisis  klasifikasi. Salah satu algoritma dalam machine learning yang dapat digunakan  untuk melakukan analisis klasifikasi dengan metode decision tree yaitu metode  Gradient Boosting Machine (GBM). GBM adalah algoritma ensemble yang  bekerja dengan cara memperkecil kesalahan model secara bertahap. Di sisi lain,  perkembangan pesat dalam bidang teknologi mengakibatkan ketersediaan data  yang semakin banyak dan kompleks. Ukuran data pun mengalami peningkatan  dari hari ke hari. Hal ini mempengaruhi kecepatan dalam analisis sehingga  kecepatan analisis juga sangat diperhitungkan. Oleh karena itu Light Gradient  Boosting Machine (LGBM) muncul untuk mengatasi hal tersebut sebagai  modifikasi dari aplikasi GBM. Dari hasil penelitan ini, diperoleh kesimpulan  bahwa berdasarkan tingginya nilai pada performa klasifikasi, GBM merupakan  metode kalsifikasi yang lebih baik digunakan pada data dengan ukuran sampel  yang kecil sedangkan LGBM lebih baik digunakan pada data dengan ukuran  sampel yang besar. LGBM rentan mengalami overfitting apabila diterapkan pada  data dengan ukuran sampel yang kecil. Kemudian waktu yang dibutuhkan oleh  metode LGBM dalam melakukan pembelajaran jauh lebih singkat dibandingkan  waktu yang dibutuhkan metode GBM pada dataset yang sama dengan sampel  besar.","Decision tree is one of the methods in classification analysis. One of the  algorithms in machine learning that can be used to perform classification analysis  with the decision tree method is the Gradient Boosting Machine (GBM) method.  GBM is an ensemble that works by gradually reducing model errors. On the other  hand, rapid developments in technology have resulted in the availability of more  and more complex data. The size of the data is also increasing day by day. This  affects the speed in the analysis so that the speed of the analysis is also important.  Therefore the Light Gradient Boosting Machine (LGBM) emerged to overcome  this as a modification of the GBM method. From the results of this study, it was  concluded that based on the increase in the value of classification performance,  GBM is a classification method that is better used on data with a small sample  size while LGBM is better used on data with a big sample size. LGBM is prone to  overfitting when applied to data with a small sample size. Then the time required  by the LGBM method in conducting learning is much shorter than the time  required by the GBM method on the same datasets with large samples.","Kata Kunci : Analisis Klasifikasi, Gradient Boosting Machine, Light Gradient  Boosting Machine, Machine Learning"
http://etd.repository.ugm.ac.id/home/detail_pencarian/210547,Prediksi Harga Saham Menggunakan Metode Kernel Principal Component Analysis-Long Short Term Memory,"M ADELFT RAMADHAN, Prof. Dr.rer.nat Dedi Rosadi, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Prediksi pasar saham tidak hanya bertujuan untuk meramalkan harga atau arah pasar untuk membantu pengambilan keputusan investasi yang lebih baik, tetapi juga untuk mencegah kekacauan pada pasar saham yang dapat menyebabkan kerusakan pada perkembangan pasar modal yang sehat. Analisa pergerakan pasar saham merupakan suatu hal yang sangat sulit dilakukan oleh peneliti dan investor. Utamanya disebabkan karena pada dasarnya pasar saham merupakan suatu sistem yang dinamis, nonlinear, nonstationary, nonparametric, noisy, dan chaotic. Faktanya pasar saham dipengaruhi oleh banyak sekali faktor yang saling berhubungan. Faktor-faktor tersebut berinteraksi dengan cara yang sangat kompleks. Principal component analysis (PCA) merupakan metode untuk melakukan reduksi dimensi dengan tetap mempertahankan informasi penting dalam data. PCA melakukan reduksi dimensi secara linier, ketika data memiliki struktur yang lebih kompleks dimana tidak dapat direpresentasikan dalam linear subspace, PCA memberikan hasil yang kurang baik. Kernel principal component analysis (KPCA) dikembangkan untuk melakukan reduksi dimensi nonlinier. Long Short-Term Memory (LSTM) pengembangan dari recurrent neural network untuk mengatasi masalah vanishing dan exploding gradients. LSTM mengganti hidden cell dengan memory cell yang di dalamnya terdapat gates untuk mengontrol jalannya informasi dalam model. Metode LSTM dapat mempelajari dengan lebih baik pola dan informasi dalam data. Oleh karena itu, kombinasi metode Kernel Principal Component Analysis-Long Short Term Memory (KPCA-LSTM) dipilih untuk memprediksi harga saham jangka pendek dengan menggunakan berbagai indikator teknikal yang saling berkorelasi dan kompleks sebagai variabel independennya. Berdasarkan analisis yang dilakukan, didapatkan bahwa metode KPCA-LSTM menghasilkan performa prediksi paling baik dibandingkan metode tanpa reduksi dimensi, maupun PCA-RNN, KPCA-RNN dan PCA-LSTM.","Stock market predictions aim not only to forecast prices or market direction to help make better investment decisions, but also to prevent chaos in the stock market that could cause damage to healthy capital market developments. Analysis of stock market movements is a very difficult thing for researchers and investors to do. Mainly because basically the stock market is a dynamic, nonlinear, nonstationary, nonparametric, noisy, and chaotic system. In facts the stock market is influenced by many interrelated factors. These factors interact in very complex ways. Principal component analysis (PCA) is a method to perform dimension reduction while retaining important information in the data. PCA performs linear dimension reduction, when the data has a more complex structure which cannot be represented in linear subspace, PCA gives poor results. Kernel principal component analysis (KPCA) was developed to reduce nonlinear dimensions. Long Short-Term Memory (LSTM) development of recurrent neural network to overcome the problem of vanishing and exploding gradients. LSTM replaces hidden cells with memory cells in which use gates to control the flow of information in the model. The LSTM method can better study the patterns and information in the data. Therefore, the combination of the Kernel Principal Component Analysis-Long Short Term Memory (KPCA-LSTM) method was chosen to predict short-term stock prices by using various highly correlated and complex technical indicators as independent variables. Based on the analysis, it was found that the KPCA-LSTM method produced the best prediction performance compared to the methods without dimension reduction, as well as PCA-RNN, KPCA-RNN and PCA-LSTM.","Kata Kunci : prediksi harga saham, reduksi dimensi, Kernel Principal Component Analysis, Long Short-Term Memory"
http://etd.repository.ugm.ac.id/home/detail_pencarian/211062,"METODE LONG SHORT-TERM MEMORY (LSTM), GATED RECURRENT UNIT (GRU), DAN CONVOLUTIONAL LONG SHORT-TERM MEMORY (CONV-LSTM) UNTUK PERAMALAN DATA RUNTUN WAKTU  (Studi Kasus: Jumlah Kasus Positif Harian COVID-19 di Indonesia)","JESSLYN MARCELLINA, Prof. Dr. rer. nat. Dedi Rosadi, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Pada penghujung tahun 2019, ditemukan virus corona baru bernama Coronavirus Disease 2019 (COVID-19) di China. Virus ini sangat mudah menyebar sehingga menjadi pandemi di berbagai negara di dunia, salah satunya Indonesia. Pandemi COVID-19 menjadi permasalahan yang serius, maka pemodelan dan peramalan secara akurat terhadap jumlah kasus positif harian sangat penting untuk memahami dan membantu melakukan manajemen risiko untuk pengendalian terhadap wabah. Data jumlah kasus positif harian COVID-19 memiliki ketidakpastian dan kompleksitas dinamika dalam deret waktu, sehingga metode klasik seperti Autoregressive Integrated Moving Average (ARIMA) akan sulit untuk menghasilkan performa peramalan yang baik. Oleh karena itu, para peneliti berusaha mengembangkan beberapa model alternatif dengan pendekatan machine learning yang dapat mengatasi permasalahan tersebut, salah satunya adalah model jaringan saraf tiruan.  Dalam skripsi ini, digunakan tiga metode jaringan saraf pengembangan dari arsitektur Recurrent Neural Network (RNN) diantaranya Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), dan Convolutional Long Short-Term Memory (Conv-LSTM). Arsitektur model dibentuk menggunakan dua lapisan tersembunyi dengan mencari kombinasi terbaik, yaitu pada arsitektur LSTM dan GRU digunakan kombinasi jumlah neuron sebesar 25, 50, 75, dan 100, sedangkan pada arsitektur Conv-LSTM digunakan kombinasi jumlah convolutional filter sebesar 32, 64, 128, dan 256. Lebih lanjut, dibentuk kombinasi arsitektur berdasarkan fungsi aktivasi yang digunakan dan penggunaan dropout. Berdasarkan analisis yang telah dilakukan, didapatkan bahwa model Conv-LSTM menghasilkan performa yang lebih baik dibandingkan dengan metode lainnya.","At the end of 2019, a new corona virus named Coronavirus Disease 2019 (COVID-19) was discovered in China. This virus rapidly spreading and becoming a pandemic that happened to various countries in the world, including Indonesia. This pandemic becomes a serious problem, so modeling and forecasting accurately the daily new COVID-19 cases is very important to understand and help carry out risk management for the outbreak control. The daily new COVID-19 cases data has uncertainty and dynamics complexity in the time series, so classic methods such as Autoregressive Integrated Moving Average (ARIMA) will be difficult to produce good forecasting performance. Therefore, the researchers are trying to develop several alternative models with a machine learning approach that can overcome these problems, one of which is an artificial neural network model. In this thesis, three methods of neural network development from Recurrent Neural Network (RNN) architecture will be used, which are Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Long Short-Term Memory (Conv-LSTM). The model architecture is formed using two hidden layers by looking for the best combination, LSTM and GRU architectures uses number of neurons combination of 25, 50, 75, and 100 while Conv-LSTM architecture uses number of convolutional filters combination of 32, 64, 128, and 256. Furthermore, combinations of architecture were formed based on the activation function used and the dropout usage. Based on the analysis that has been conducted, it is found that the Conv-LSTM model produces better performance than other methods.","Kata Kunci : Jumlah kasus positif harian COVID-19, peramalan, dropout, fungsi aktivasi, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Convolutional Long Short-Term Memory (Conv-LSTM)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/211577,Penyetelan Hyperparameter Extreme Gradient Boosting Menggunakan Bayesian Optimization untuk Klasifikasi Credit Scoring,"ARUM SEKAR MURDAYA, Dr. Abdurakhman, S.Si., M.Si.",2022 | Skripsi | S1 STATISTIKA,"Latar Belakang Credit scoring (penilaian kredit) merupakan suatu sistem yang diterapkan oleh suatu lembaga pembiayaan atau bank untuk menilai kelayakan peminjam atau debitur yang mengajukan pinjaman. Tujuannya adalah untuk mengantisipasi kredit macet yang ditimbulkan oleh kelalaian debitur dan untuk memilih debitur yang berpotensi untuk lancar dalam pelunasan pinjaman, sehingga lembaga pembiayaan atau bank dapat terhindar dari risiko kerugian. Pada dekade terakhir, pendekatan metode ensemble banyak diimplementasikan dalam model klasifikasi credit scoring dengan tujuan untuk meningkatkan akurasi model. Salah satunya adalah algoritma extreme gradient boosting (XGB) yang selain dapat meningkatkan akurasi model, juga mampu untuk mengatasi ketidakseimbangan data. Algoritma XGB memiliki hyperparameter yang banyak dan berpengaruh pada pembangunan modelnya. Oleh karenanya pada penelitian ini, diajukan sebuah metode untuk model klasifikasi credit scoring berbasis algoritma XGB dengan penyetelan hyperparameter menggunakan Bayesian optimization (XGB-BO). Metode Pemodelan memiliki dua tahapan utama. Pertama, dilakukan data pre-processing berupa penanganan data hilang, encoding, dan pembobotan lebih pada kelas minoritas untuk mengatasi ketidakseimbangan data. Kedua, penyetelan hyperparameter menggunakan Bayesian optimization diaplikasikan pada algoritma XGB. Model kemudian dievaluasi menggunakan tiga dataset credit scoring publik yaitu dataset HMEQ, Taiwan, dan Credit Risk.  Beberapa metode penyetelan lain yaitu default, grid search, dan random search digunakan sebagai pembanding untuk mengetahui metode mana yang memiliki performa lebih baik.   Hasil Penyetelan hyperparameter XGB menggunakan Bayesian optimization pada dataset HMEQ dan Taiwan menunjukkan performa akurasi, sensitivitas, spesifisitas, presisi, dan skor F1 yang lebih baik dibandingkan teknik grid search, random search, maupun penyetelan default. Pada dataset Credit Risk model XGB-BO mengalami penurunan sensitivitas sebesar 1% jika dibandingkan dengan model default, namun untuk keempat ukuran evaluasi lainnya menunjukkan nilai yang lebih unggul dibandingkan teknik grid search, random search, maupun penyetelan default. Waktu komputasi Bayesian optimization lebih cepat dari grid search, tapi lebih lambat 0,8 menit dari random search.  Kesimpulan Model klasifikasi credit scoring dengan algoritma XGB-BO menggunakan dataset credit scoring pada skripsi ini menunjukkan kinerja yang lebih baik dibandingkan teknik grid search, random search, maupun penyetelan default.","Background Credit scoring is a system applied by a financial institution or bank to assess the eligibility of a borrower or debtor applying for loans. The purpose of this scoring is to anticipate bad loans caused by debtor negligence and to select debtors who have the potential to repay loans successfully. Thus, financial institutions or banks can avoid the risk of loss. In the last decade, the ensemble method has been widely implemented in credit scoring modeling to improve the accuracy of the assessment. One of them is the extreme gradient boosting (XGB) algorithm which in addition to increasing model accuracy, is also able to overcome imbalanced data. XGB algorithm has many hyperparameters that are crucial to constructing its models. This study proposed for credit scoring classification model based on the XGB algorithm with hyperparameter tuning using Bayesian optimization (XGB-BO). Methods The model mainly comprises two steps. First, data pre-processing is employed to handle missing values, encoding, and handle imbalanced data with weighting more on the minority class. Second, Bayesian optimization is applied to tune the hyperparameter of the  XGB classifier. The model is evaluated using three public credit scoring data, that are HMEQ, Taiwan, and Credit Risk datasets. Several other hyperparameter tuning methods, namely default, grid search, and random search are used as comparisons to find out which method has better performance. Results Hyperparameter tuning in XGB using Bayesian optimization on HMEQ and Taiwan datasets showed better accuracy, sensitivity, specificity, precision, and F1 score performance than grid search, random search, and default tuning. In the Credit Risk dataset, the XGB-BO model experienced a 1% decrease in sensitivity when compared to the default model, but the other four evaluation measures showed higher values than grid search, random search, and default tuning. Bayesian optimization computation time is faster than grid search, but 0.8 minutes slower than random search. Conclusions Credit scoring modeling with XGB-BO algorithm using credit scoring data in this thesis shows better performance than grid search, random search, and default techniques.","Kata Kunci : Bayesian optimization, credit scoring, extreme gradient boosting, hyperparameter, XGB."
http://etd.repository.ugm.ac.id/home/detail_pencarian/210557,Regresi Nonparametrik Multirespon dengan Estimator Spline Truncated Terbobot pada Data Longitudinal,"GADING KUSUMA A, Drs.Danardono,MPH.,Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Analisis regresi nonparametrik digunakan ketika dengan menggunakan scatter plot dapat diperoleh kesimpulan bahwa pola data yang dimiliki tidak membentuk pola yang jelas atau tidak beraturan. Salah satu metode analisis regresi nonparametrik yang memiliki interpretasi statistik dan interpretasi visual yang sangat khusus serta baik adalah Spline Truncated. Spline dikembangkan ke dalam berbagai penelitian dengan menggunakan data longitudinal. Seringkali dalam kondisi permasalahan nyata untuk mendapatkan kesimpulan lebih spesifik perlu menggunakan data longitudinal yang melibatkan 2 atau lebih variabel respon dan variabel prediktor yang disebut sebagai multirespon dan multiprediktor. Dalam suatu analisis regresi nonparametrik pada data longitudinal apabila melibatkan variabel respon lebih dari 1 maka terdapat korelasi dari setiap respon pada subjek yang sama. Oleh karena itu, untuk melakukan analisis regresi nonparametrik spline truncated multirespon pada data longitudinal menggunakan metode estimasi Weighted Least Square. Pada penelitian ini digunakan studi kasus kemiskinan di Pulau Jawa pada tahun 2018-2020. Diperoleh titik knot optimal yaitu titik knot 3 dengan menggunakan metode GCV yang mana nilai GCV paling minimum adalah 0,0007220848. Dengan memasukkan nilai titik knot optimal ke dalam program R yang telah dibuat, diperoleh estimasi model untuk masing-masing provinsi sebanyak 3 dan segmentasi model yang terbentuk untuk setiap variabel prediktor di masing-masing provinsi adalah 7.","Nonparametric regression analysis is used when using scatter plot can be drawn to a conclusion that the data patterns that have do not form a clear or irregular pattern. One method of nonparametric regression analysis that has a statistical interpretation and a very spesific visual interpretation is spline truncated. Spline was developed into various studies using longitudinal data. Often in real problems for a spesific conclusion need to longitudinal data who involve two or more variables of response and predictor variables referred to as multiresponse and multipredictor variables. In a longitudinal data nonparametric regression analysis when it involves more that 1 response variables there is a correlation of each response on the same subject. Therefore, for regression analysis nonparametric spline truncated multirespons to longitudinal data using advanced estimated method with Weighted Least Square. In this study used the poverty case study on the Java Island in 2018-2020. The optimum knots is knots 3 using the GCV method in which the minimum GCV value is 0,0007220848. By inserting optimal knots value into the R program which has been made available for estimates of models for each province is 3 and segmentation of models made for every predictor variable in each province is 7.","Kata Kunci : Regresi Nonparametrik, Spline Truncated, Data Longitudinal, Multirespon, Kemiskinan di Pulau Jawa Tahun 2018-2020, 3 Model, 7 Segmentasi Model"
http://etd.repository.ugm.ac.id/home/detail_pencarian/212620,PENERAPAN REGRESI TERBOBOTI GEOGRAFIS DAN TEMPORAL (Studi Kasus: Indeks Pembangunan Manusia Provinsi Jawa Tengah Tahun 2016-2019),"OKTSA DWIKA R, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2022 | Skripsi | S1 STATISTIKA,"Regresi Terboboti Geografis dan Temporal (RTGT) merupakan pengembangan dari Regresi Terboboti Geografis (RTG). RTG merupakan pengembangan dari regresi berganda untuk data yang memiliki heterogenitas spasial, sedangkan RTGT digunakan untuk mengatasi ketidakstasioneran suatu data baik dari sisi spasial dan temporal secara bersamaan. Estimator RTGT akan berbeda pada setiap lokasi dan waktu. Proses estimasi parameter dalam RTGT menggunakan Weighted Least Square (WLS). Pada penelitian ini dilakukan analisis pada Indeks Pembangunan Manusia Provinsi Jawa Tengah Tahun 2016-2019 dengan RTGT, kemudian dibandingkan dengan model regresi berganda (Ordinary Least Square) dan model RTG. Berdasarkan analisis yang dilakukan diperoleh kesimpulan bahwa model RTGT menghasilkan performa yang lebih baik dibandingkan model regresi berganda dan RTG, dilihat dari nilai koefisien determinasi (R^2) dan nilai Mean Square Error (MSE).","Geographically and Temporally Weighted Regression (GTWR) is a development of Geographically Weighted Regression (GWR). GWR is a development of multiple regression for data with spatial heterogeneity. GTWR is used to simultaneously overcome the non-stationary data from both spatial and temporal perspective. The GTWR estimator will be different at each location and time. The parameter estimation process in GTWR uses Weighted Least Square (WLS). In this study, an analysis was conducted on Human Development Index of Central Java Province in 2016-2019 with GTWR, then compared it with multiple regression (Ordinary Least Square) model and GWR model. Based on the analysis, it can be concluded that the GTWR model produces better performance than the multiple regression model and GWR model, as seen from the value of the coefficient of determination (R^2) and the value of Mean Square Error (MSE).","Kata Kunci : Regresi Terboboti Geografis dan Temporal, data spasial, heterogenitas spasial, heterogenitas temporal, Weighted Least Square"
http://etd.repository.ugm.ac.id/home/detail_pencarian/212623,Penerapan Model Black-Litterman dengan Estimasi EGARCH-M dalam Portofolio Saham LQ-45,"NAILIS ARUM TAZILLA, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Konsep pembobotan portofolio saham dengan tujuan untuk memaksimalkan tingkat pengembalian dan meminimalkan kerugian dapat memberikan masalah baru yang berkaitan dengan pandangan atau keyakinan investor menilai suatu saham. Perbedaan pandangan tiap investor dalam menilai saham cenderung berdampak pada kontribusi masing-masing saham dalam mendapatkan keuntungan portofolio. Oleh karena itu, terdapat pembobotan model Black-Litterman yang menggabungkan return market equilibrium dari data historis return dengan pandangan subjektif investor pada kinerja aset untuk mengatasi masalah tersebut. Salah satu estimasi yang dapat dilakukan dalam pembentukan pandangan investor adalah model EGARCH-M yang mampu memberikan nilai expected return sebagai input views dalam pembentukan model Black-Litterman serta mengatasi masalah heteroskedastisitas dan efek leverage yang umumnya terdapat pada data finansial. Dalam studi kasus skripsi ini, digunakan portofolio dari 5 saham LQ-45 yang berbeda sektor, yaitu ERAA, KLBF, PTBA, PGAS, dan BBNI. Pembentukan model Black-Litterman dengan estimasi EGARCH-M (1,1) dari nilai skalar tau yang berbeda memberikan pengaruh terhadap keuntungan dan risiko pada kinerja portofolio. Pemilihan portofolio optimal dari model tersebut menggunakan ukuran Sharpe Ratio yang menghasilkan bahwa model Black-Litterman dengan estimasi EGARCH-M sebagai pembentukan views serta tau=0,001 mempunyai kinerja portofolio yang lebih baik dibandingkan lainnya.","The concept of portfolio weights with the aim of maximizing return and minimizing risk can provide new problems related to the views or confidence investors for value stock. Differences in the views of each investor in assessing stocks tend to have an impact on the contribution of each stock in obtaining portfolio profits. Therefore, there is Black-Litterman model portfolio weights that combines the market equilibrium return of historical data with the subjective view of investors on asset performance to overcome the problem. One of the estimates that can be done in the formation of investor views is the EGARCH-M model which is able to provide expected return value as input views in the formation of the Black-Litterman model and overcome the problem of heteroskedasticity and leverage effects that are generally found in financial data. In this undergraduate thesis, a portfolio of five LQ-45 stocks were used in different sectors, which are ERAA, KLBF, PTBA, PGAS, dan BBNI. The formation of the Black-Litterman model with an estimated EGARCH-M (1,1) of different scalar tau  give impact on the profit and risk on portfolio performance. The optimal portfolio selection of the model uses a Sharpe Ratio measure that results in that the Black-Litterman model with EGARCH-M estimates as views formation and tau=0.001 has better portfolio performance than others.","Kata Kunci : Black-Litterman, EGARCH-M (1,1), Sharpe Ratio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/211358,OPTIMISASI BANDWIDTH GEOGRAPHICALLY WEIGHTED  REGRESSION (GWR) PADA PEMODELAN HARGA BIDANG TANAH  MENGGUNAKAN MULTISCALE-GWR,"KARINA DYAH P, Dr. Adhitya Ronnie Effendie, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Geographically Weighted Regression merupakan pemodelan regresi lokal yang  ditimbulkan oleh heterogenitas spasial, sehingga menghasilkan model parameter  yang bervariasi secara spasial pada proses spasial yang sama. Namun, model GWR  tidak dapat mengatasi adanya kemungkinan proses spasial yang berbeda pada  variabel independen dalam mempengaruhi variabel dependen. Kemungkinan  tersebut mampu diatasi oleh Multiscale Geographically Weighted Regression dimana mengadaptasi Generalized Additive Model dengan algoritma backfitting.  Kedua metode tersebut digunakan untuk pemodelan pada harga bidang tanah di kawasan industri Argodadi. Penelitian ini menggunakan fixed bandwidth bi-square optimal pada kriteria AICc dengan algoritma golden section search. GWR  memperoleh bandwidth tunggal untuk seluruh asosiasi yang terjadi. Sedangkan,  MGWR menghasilkan vektor bandwidth yang menggambarkan proses spasial yang  berbeda pada model. Pemodelan GWR harga bidang tanah di daerah tersebut  menghasilkan nilai AICc = 5469.218, R2 = 0.629, dan Adj. R2 = 0.578. Sementara,  pemodelan MGWR menghasilkan nilai AICc = 5456.360, R2 = 0.649, dan adj. R2  = 0.603. Informasi diagnostik tersebut menunjukkan bahwa model MGWR lebih  baik daripada model GWR.","Geographically Weighted Regression is a local regression modeling caused by  spatial heterogeneity, that results are a spatially varying parameters model in the  same spatial process. Nevertheless, the GWR model can't solve the possibility of  different spatial processes on the independent variables in influencing a dependent  variable. This possibility can be solved by Multiscale Geographically Weighted  Regression which adapts the Generalized Additive Model with Backfitting  Algorithm. Both methods are used for modeling the land parcels price in Argodadi  industrial area. The research uses the optimal fixed bandwidth bi-square on the  AICc criteria with a Golden Section Search Algorithm. GWR obtains single  bandwidth for all associations that occur. Meanwhile, MGWR obtains a bandwidth  vector that describes different spatial processes in the model. GWR model of land  parcels price in the area produced an AICc = 5469.218, R2 = 0.629, and adj. R2 = 0.578. Meanwhile, the MGWR model produced an AICc = 5456.360, R2 = 0.649,  and adj. R2 = 0.603. The diagnostic information shows that the MGWR model is  better than the GWR model.","Kata Kunci : Bandwidth,GWR, Multiscale, backfitting, GAM"
http://etd.repository.ugm.ac.id/home/detail_pencarian/218015,SEGMENTASI PEMAIN SEPAK BOLA PADA DATA CAMPURAN KATEGORIK DAN NUMERIK DENGAN METODE ENSEMBEL FUZZY C-MEANS DAN K-MODES (STUDI KASUS: PENGELOMPOKAN PEMAIN SEPAK BOLA EROPA),"MUHAMMAD GUFRON A, Drs. Danardono, MPH., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Analisis klaster merupakan salah satu metode pengelompokan data berdasarkan kemiripan dari suatu data. Pengelompokan pada data campuran numerik dan kategorik menjadi lebih kompleks karena perbedaan tipe data sehingga harus menggunakan ukuran kemiripan yang berbeda. Salah satu metode pengelompokan data campuran adalah metode ensembel. Metode ensemble melakukan pengelompokan dengan memisahkan data terlebih dahulu sehingga tidak menghilangkan informasi penting pada data. Pada penelitian ini, digunakan metode ensemble fuzzy c-means dan k-modes. Metode fuzzy c-means mengelompokkan data numerik dengan melihat jarak Euclidean antara objek dengan titik pusat klaster. Metode k-modes mengelompokkan data kategorik dengan melihat jarak Hamming antara objek dengan titik pusat klaster. Pada penelitian ini digunakan beberapa validasi klaster untuk menentukan jumlah klaster yang optimum pada tiap metode. Metode ensemble fuzzy c-means dan k-modes diaplikasikan untuk melihat segmentasi pada data pemain sepak bola di English Premier League, La Liga, Bundesliga, Seria A, dan Ligue 1 musim 2021/2022. Didapatkan 19 segmen pemain sepak bola di lima liga Eropa dengan masing-masing karakteristiknya.","Cluster analysis is one of the methods of grouping data based on the similarity of a data. Grouping on numerical and categorical mixed data becomes more complex due to differences in data types so it must use different similarity measures. One oh the methods of grouping mixed data is ensemble method. The ensemble metod performs grouping by splitting the data, so as not to eliminate important information on the data. In this study, the ensemble fuzzy c-means and k-modes methdoes were used. The fuzzy c-means method group numerical data by Euclidean distance between the object and the center point of the cluster. The k-modes method group categorical data by Hamming distance between the object and the center point of the cluster. In this study, several cluster validations were used to determine the optimum number of clusters in each method. The ensemble fuzzy c-means and k-modes methods are applied to see the segmentation on football player data in English Premier League, La Liga, Bundesliga, Serie A, and Ligue 1 season 2021/2022. The are 19 segments of football players in five European leagues with each of their characteristics.","Kata Kunci : analisis klaster, data campuran, ensemble, fuzzy c-means, k-modes"
http://etd.repository.ugm.ac.id/home/detail_pencarian/209315,Optimisasi Portofolio Menggunakan Algoritma Perturbed Particle Swarm Optimization (PPSO) terhadap Saham pada saat Sebelum dan Selama Pandemi,"INTAN PUSPITANINGRUM, Dr. Abdurakhman, S.Si., M.Si",2022 | Skripsi | S1 STATISTIKA,"Investor melakukan investasi dengan tujuan untuk memperoleh keutungan yang optimal. Salah satu cara yang dilakukan para investor adalah dengan melakukan diversifikasi portofolio ke beberapa saham. Portofolio saham dilakukan untuk menyebarkan sumber perolehan return dan kemungkinan risiko. Penelitian ini bertujuan untuk menganalisis apakah jika suatu saham memiliki performa yang sedang tidak baik, maka saham lain yang memiliki performa baik akan menutupi kerugian tersebut sehingga kerugian yang diperoleh tidak terlalu besar. Algoritma Perturbed Particle Swarm Optimization (PPSO) merupakan pengembangan dari algoritma Particle Swarm Optimization (PSO) yang digunakan untuk optimisasi portofolio. Dalam PPSO terdapat pembaruan strategi pada partikel baru dan konsep perturbed gbest di dalam kerumuman. Algoritma ini dimodelkan berdasarkan perilaku sosial organisme seperti pada kehidupan burung berkelompok untuk memecahkan masalah pengoptimalan. Data yang dipakai dalam penelitian ini adalah data saham mingguan dari lima saham yang masuk dalam index IDX30, yaitu saham yang memiliki likuiditas tinggi dan kapitalisasi pasar besar. Data diambil dari sebelum pandemi (Januari sampai Desember 2019)  serta selama pandemi (April 2020 sampai Maret 2021). Pada penelitian ini akan dibandingkan kinerja algoritma Perturbed Particle Swarm Optimization (PPSO) dengan algoritma pendahulunya yaitu Particle Swarm Optimization (PSO) yang diukur dengan menggunakan sharpe ratio.  Hasil yang diperoleh adalah portofolio saham menggunakan algoritma PPSO memiliki nilai sharpe ratio pada saham sebelum pandemi memiliki nilai sebesar 1,609596 sedangkan pada saham selama pandemi memiliki nilai sebesar 1,501114. Adapun algoritma PSO untuk saham sebelum pandemi diperoleh nilai sebesar 1,434777 sedangkan untuk saham selama pandemi diperoleh nilai sebesar 0,9053139. Dengan demikian dapat disimpulkan jika algoritma PPSO lebih baik jika dibandingkan dengan algoritma PSO.","Investors invest with the aim of obtaining optimal profits. One way that investors can do is diversify their portfolio into several stocks. The stock portfolio is carried out to spread the sources of return and possible risks. This study aims to analyze whether if a stock has a bad performance, then other stocks that have good performance will cover the loss so that the loss is not too large. The Perturbed Particle Swarm Optimization (PPSO) algorithm is a development of the Particle Swarm Optimization (PSO) algorithm which is used for portfolio optimization. In PPSO there is a strategy update on new particles and the concept of perturbed gbest in the crowd. This algorithm is modeled based on the social behavior of organisms such as flocking birds to solve optimization problems. The data used in this study is weekly stock data from five stocks included in the IDX30 index, which is stocks that have high liquidity and large market capitalization. The data taken from before the pandemic (January to December 2019) and during the pandemic (April 2020 to March 2021). In this study, the performance of the Perturbed Particle Swarm Optimization (PPSO) algorithm will be compared with its predecessor algorithm, namely Particle Swarm Optimization (PSO) which is measured using the sharpe ratio. The results obtained are that the stock portfolio using the PPSO algorithm has a sharpe ratio value in stocks before the pandemic has a value of 1.609596 while in stocks during pandemic it has a value of 1.501114. The PSO algorithm for stocks before the pandemic obtained a value of 1.434777 while for stocks during pandemic a value of 0.9053139 was obtained. Thus, it can be concluded that the PPSO algorithm is better than the PSO algorithm.","Kata Kunci : Algortima Perturbed Particle Swarm Optimization, Algoritma Particle Swarm Optimization, Saham, Pandemi, Sharpe Ratio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/212644,REGRESI CONWAY-MAXWELL-POISSON DENGAN PARAMETERISASI RATA-RATA UNTUK KASUS OVERDISPERSI PADA RESPON CACAH (Studi Kasus : Jumlah Kasus Kejahatan di Jawa Tengah Tahun 2021),"MEISAL IKHLAS R, Dr. Herni Utami, S.Si., M.Si.",2022 | Skripsi | S1 STATISTIKA,"Regresi Conway-Maxwell-Poisson (CMP) memiliki kelebihan dalam mengatasi kondisi overdispersi sekaligus underdispersi pada respon cacah sebagai alternatif terhadap penyimpangan sifat kesamaan nilai mean dan variansi (ekuidispersi) dalam model Poisson. Kondisi over-under dispersi mengakibatkan hasil estimasi parameter mengandung bias yang selanjutnya dapat berimplikasi pada kesalahan inferensi terhadap signifikansi parameternya. Sebagai bentuk peningkatan performa model CMP standar, maka muncul modifikasi model regresi CMP dengan parameterisasi rata-rata sebagai model mean sehingga memiliki interpretabilitas yang lebih baik. Estimasi parameter model dilakukan dengan metode Maximum Likelihood dengan melakukan optimisasi melalui iterasi Fisher Scoring.  Sebagai aplikasi model maka penelitian ini mengambil studi kasus tentang jumlah kasus kejahatan di Provinsi Jawa Tengah Tahun 2021. Penurunan kasus kejahatan yang terjadi di wilayah tersebut selama pandemi COVID-19 mendorong penyelidikan terhadap berbagai faktor yang mempengaruhi tindak kejahatan berdasarkan karakteristik daerah. Berdasarkan hasil pengujian, diketahui bahwa performa model regresi CMP dengan parameterisasi rata-rata dalam memodelkan jumlah kejahatan lebih unggul dibandingkan model Poisson melalui Likelihood Ratio Test (LRT) dan ukuran kebaikan model yaitu, nilai Akaike Information Criterion (AIC) dan Schwart Bayesian Criterion (SBC).","Conway-Maxwell-Poisson (CMP) regression has the advantage of handling the overdispersion and underdispersion conditions for count response as an alternative for the violation of the similarity between mean and variance (equidispersion) property in the Poisson model. The over-under dispersion causes the parameter estimation results to contain bias, which in turn might have implications for inference errors on the significance of the parameters. As a form of improvement of the standard CMP model, a modified CMP regression model appears with the mean-parametrized as the model mean so that it has better interpretability. The estimation of model parameters is carried out using the Maximum Likelihood method by optimizing it through Fisher Scoring iterations. As an application of this model, this study takes a look on the crime total in Central Java in 2021. The decline of crime cases that occurred in that area during the COVID-19 prompted investigations into various factors based on regional characteristics. According to the results, it was shown that the performance of the Mean-Parametrized CMP regression model in modeling total crime is superior to the Poisson model through the Likelihood Ratio Test (LRT) and some criterion for model selection, e.g. the Akaike Information Criterion (AIC) and Schwart Bayesian Criterion (SBC).","Kata Kunci : overdispersi, Conway-Maxwell-Poisson, parameterisasi rata-rata"
http://etd.repository.ugm.ac.id/home/detail_pencarian/212653,PEMODELAN TOPIK PADA MEDIA SOSIAL MENGGUNAKAN METODE LDA2VEC,"DINDA ASYIFA K, Drs. Danardono, MPH., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Pesatnya perkembangan teknologi dan meningkatnya pengguna internet telah membuat aktivitas pada seluruh aspek kehidupan beralih ke digital, tidak terkecuali dalam aspek ekonomi. Meningkatnya aktivitas belanja secara daring membuat para pelaku usaha terus bersaing dalam mencakup konsumen dengan memerhatikan kebutuhan dan juga perilakunya yang dapat tercermin pada sosial media. Untuk menemukan topik tersembunyi pada pembicaraan konsumen di sosial media tersebut, tugas akhir ini membahas pemodelan topik mengenai pembicaraan belanja daring pada Twitter menggunakan metode LDA2Vec. LDA2vec merupakan metode hibrida antara Latent Dirichlet Allocation (LDA) dan word2vec. Metode penyematan kata word2vec mempelajari vektor kata yang memberikan informasi mengenai bagaimana kata-kata berkorelasi secara semantik ditambahkan bersamaan dengan vektor dokumen yang dipelajari dalam LDA sehingga terbentuk vektor konteks dan kemudian digunakan dalam memprediksi kata. Hasil dari pemodelan topik merupakan topik pembicaraan mengenai belanja daring pada media sosial Twitter, nilai koherensi yang mengukur kesamaan semantik antara kata dalam hasil pemodelan topik, proporsi topik pada korpus, probabilitas kata pada topik, serta proporsi topik dalam dokumen.","The rapid development of technology and the increasing number of internet users have made activities in all aspects of life shift to digital, including the economic aspect. The increase in online shopping activities makes business actors continue to compete in covering consumers by paying attention to their needs and behavior which can be reflected on social media. To find hidden topics in consumer conversations on social media, this final project discusses topic modeling regarding online shopping conversations on Twitter using the LDA2Vec. LDA2vec is a hybrid method between Latent Dirichlet Allocation (LDA) and word2vec. The word embedding method word2vec which learns word vectors that provide information about how semantically correlated words are added together with the document vectors studied in LDA to form context vectors and then used to predict words. The results of the topic modeling are topics of conversation about online shopping on Twitter social media, coherence score that measure the semantic similarity between words in the topic modeling results, the proportion of topics in the corpus, the probability of words on the topic, and the proportion of topics in the document.","Kata Kunci : pemodelan topik, LDA2vec, word2vec, LDA"
http://etd.repository.ugm.ac.id/home/detail_pencarian/213166,OPTIMISASI PORTOFOLIO SAHAM MENGGUNAKAN METODE  MULTI OBJEKTIF ALGORITMA GENETIKA NSGA-II,"FAUZUL AKMAL HUDA, Dr. Abdurakhman, S.Si., M.Si",2022 | Skripsi | S1 STATISTIKA,"Investasi adalah komitmen atas sejumlah dana atau sumber daya lainnya yang dilakukan pada saat ini, dengan tujuan memperoleh sejumlah keuntungan di masa mendatang. Investasi dapat dilakukan melalui berbagai jenis aset, salah satunya saham. Investasi saham yang dilakukan tentunya memiliki risiko, sehingga diperlukan manajemen investasi dan analisa yang tepat sehingga mampu meminimalkan risiko yang mungkin terjadi. Pembentukan portofolio saham merupakan salah satu langkah dalam meminimalkan risiko yang terjadi.  Portofolio saham merupakan sekumpulan saham-saham yang ditelah di analisa dan kemudian dipilih untuk membentuk portofolio yang diharapkan dapat mendatangkan keuntungan di masa mendatang. Investor dapat menggunakan berbagi metode dalam membentuk portofolio saham yang efisien dan optimal, salah satunya metode non-dominated sorting genetic algorithm-II. Metode tersebut merupakan salah satu evolutionary algorithm yang populer digunakan pada permasalahan optimisasi yang bersifat multi objektif, di mana pada konsep ini diharapkan mampu memaksimalkan return dan meminimalkan risiko pada portofolio. Algoritma genetika NSGA-II memiliki beberapa parameter yang digunakan, antara lain ukuran populasi, jumlah generasi, probabilitas crossover, dan probabilitas mutasi.  Hasil akhir ini dari algoritma tersebut mampu menghasilkan bobot-bobot saham pada portofolio yang dapat digunakan investor dalam melakukan investasi saham. Metode NSGA-II pada kesempatan ini diterapkan terhadap pembentukan 3 portofolio saham di mana setiap portofolio berasal dari sektor saham perusahaan yang berbeda. Metode multi objektif juga akan digunakan dalam membentuk 3 portofolio tersebut sebagai metode pembanding. Penilaian kinerja portofolio tersebut dilakukan dengan  menggunakan metode sharpe ratio untuk melihat metode manakah yang mampu memberikan kinerja portofolio yang lebih baik.","Investment is a commitment to the amount of funds or other resources made at this time, by obtaining a number of benefits in the future. Investments can be made through various types of assets, one of which is stocks. Investments that are made certainly have risks, so proper investment management and analysis are needed so that they are able to generate risks that may occur. portfolio is one step in the bidding that occurs. Stock portfolios are stocks that have been analyzed and then selected to form a portfolio that is expected to be profitable in the future. Investors can use the sharing method in forming an efficient and optimal portfolio, one of which is the non-dominant-II sorting genetic algorithm method. This method is one of the popular evolutionary algorithms used in multi objektif optimization problems, where this concept is expected to be able to maximize returns and risks in the portfolio. The NSGA-II genetic algorithm has several parameters used, including population size, number of generations, crossover probability, and mutation probability. The final result of the algorithm is able to produce stock weights in the portfolio that investors can use in investing in stocks. The NSGA-II method on this occasion is applied to the formation of 3 portfolios where each portfolio comes from shares of a different company sector. The multi objective method will also be used in forming the 3 portfolios as a comparison method. The portfolio performance assessment is carried out using the Sharpe ratio method to see which method is able to provide a better portfolio performance.","Kata Kunci : timisasi portofolio, NSGA-II , mean variance, sharpe ratio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/211122,Metode Dependent Nearest Neighbor (dNN) untuk Peningkatan Performa Klasifikasi Data Kategorik,"INTAN ADHELIA P, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Analisis klasifikasi adalah metode yang digunakan untuk memprediksi kelas atau kategori berdasarkan suatu data yang kelasnya telah diketahui sebelumnya. Metode k Nearest Neighbor (kNN) merupakan salah satu metode yang banyak digunakan dalam analisis klasifikasi karena kemudahan dan kesederhanaan algortimanya. Namun metode ini tidak luput dari kekurangan, yakni penentuan k yang sulit ditentukan dan penentuan berdasarkan nilai k mengakibatkan, suatu data bisa diklasifikasikan menjadi suatu kelas tertentu, meskipun memiliki jarak yang jauh. Metode Dependent Nearest Neighbor (dNN) merupakan metode yang dapat menentukan tetangga terdekat berdasarkan kemiripan dan ketergantungannya. Pada metode dNN, tetangga terdekat terpilih merupakan sampel yang berada didalam Dependency Region (DR). DR merupakan daerah yang terbentuk dari parameter jari-jari dan suatu sudut. Penelitian ini bertujuan untuk membandingkan performa yang dihasilkan oleh kNN dan dNN menggunakan tiga dataset. Berdasarkan analisis yang telah dilakukan, akurasi yang dihasilkan oleh metode dNN lebih besar daripada metode kNN.","Classification analysis is a method used to predict classes or categories based on data whose class has been known. k Nearest Neighbor (kNN) method is one of the most widely used methods in classification because of the ease and simplicity of the algorithm. However, this method have some weakness, there are determination of k which is difficult to determine and the determination class based on the value of k makes a data being classified into a certain class, even though it has a long distance. The Dependent Nearest Neighbor (dNN) method is a method that can determine the nearest neighbor based on similarities and dependencies. In the dNN method, the selected nearest neighbor is a sample that is in the Dependency Region (DR). DR is an area formed from the parameters of the radius and an angle. This study aims to compare the performance generated by kNN and dNN using three datasets. Based on the analysis that has been done, the accuracy produced by the dNN method is greater than the kNN method.","Kata Kunci : klasifikasi, kNN, dNN, performa"
http://etd.repository.ugm.ac.id/home/detail_pencarian/215241,Perbandingan Performa Regresi Logistik Biner dan Decision Tree C4.5 dalam Klasifikasi Menggunakan Metode Bootstrap Aggregating (Bagging),"BELLA VEBRYA ARIFIN, Dr. Herni Utami, S.Si., M.Si.",2022 | Skripsi | S1 STATISTIKA,Data mining adalah proses memodelkan data dalam jumlah besar serta mencari pola untuk memperoleh informasi yang berguna. Salah satu teknik dalam data mining adalah klasifikasi. Klasifikasi digunakan untuk mengelompokkan data secara sistematis. Beberapa metode dalam klasifikasi adalah regresi logistik dan decision tree C4.5. Mayoritas metode dalam teknik klasifikasi mempunyai kelemahan dalam menangani dataset yang memiliki ketidakseimbangan kelas sehingga dapat menurunkan performa klasifikasi. Namun dalam data mining sering kali muncul ketidakseimbangan kelas. Metode bootstrap aggregating (bagging) merupakan salah satu metode ensemble untuk membentuk klasifikasi yang lebih stabil dan meningkatkan performa klasifikasi. Pada penelitian ini digunakan metode bagging untuk klasifikasi pada regresi logistik dan decision tree C4.5 dengan beberapa kali replikasi bootstrap. Metode regresi logistik dan decision tree C4.5 tanpa bagging digunakan sebagai pembanding untuk mengetahui metode mana yang memiliki performa terbaik. Dari hasil analisis menggunakan data Pima Indians Diabetes diperoleh kesimpulan bahwa penerapan metode bagging pada regresi logistik dan decision tree C4.5 mengalami peningkatan performa dibandingkan metode yang tidak menggunakan bagging. Dan penerapan bagging pada decision tree C4.5 mendapatkan hasil analisis dengan performa terbaik dibanding yang lain.,"Data mining is the process of modeling large amounts of data and looking for patterns to obtain useful information. One of the techniques in data mining is classification. Classification is used to group data systematically. Several methods in the classification are logistic regression and decision tree C4.5. The majority of methods in the classification technique have weaknesses in handling datasets that have class imbalance which can reduce classification performance. However, in data mining, class imbalance often arises. The bootstrap aggregating (bagging) method is one of the ensemble methods to form a more stable classification and improve classification performance. In this study, the bagging method was used for classification in logistic regression and decision tree C4.5 with several bootstrap replications. The logistic regression method and decision tree C4.5 without bagging were used as comparisons to find out which method had the best performance. From the results of the analysis using Pima Indians Diabetes data, it was concluded that the application of the bagging method to logistic regression and decision tree C4.5 experienced an increase in performance compared to methods that did not use bagging. And the application of bagging in decision tree C4.5 gets the analysis results with the best performance compared to others.","Kata Kunci : Klasifikasi, Ketidakseimbangan Kelas, Regresi Logistik, Decision Tree C4.5, Bootstrap Aggregrating (Bagging)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/213987,ANALISIS KLASTER MENGGUNAKAN ALGORITMA CLAM (CLUSTERING LARGE APPLICATION USING METAHEURISTICS) UNTUK DATASET BESAR DENGAN PENCILAN,"NAURA GHINA AS-SHOFA, Vemmie Lestari Nastiti, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Analisis klaster non-hirarki atau partitioning methods merupakan metode analisis klaster yang jumlah klasternya ditentukan diawal dan setiap klaster memiliki titik pusat klaster. K-medoids adalah salah satu metode partitioning methods dengan medoid sebagai pusat klaster, dimana medoid merupakan objek yang letaknya terpusat pada suatu klaster, sehingga tangguh terhadap pencilan. Algoritma k medoids yang digunakan dalam penelitian ini adalah Clustering Large Application Using Metaheuristics (CLAM), di mana CLAM merupakan pengembangan dari algoritma Clustering Large Application based on Randomized Search (CLARANS) dalam meningkatkan kualitas analisis klaster dengan mengaplikasikan hybrid metaheuristik antara Tabu Search (TS) dan Variable Neighborhood Search (VNS). Metode CLAM menggunakan empat parameter, yaitu numlocal untuk membatasi iterasi, maxneighbor untuk membatasi neighbor pada suatu node, tls untuk membatasi node yang disimpan dalam tabu list dan setradius untuk mengatur jadwal pengurangan linear.   Metode analisis klaster terbaik untuk mengelompokkan kecamatan yang ada di Pulau Sumatera berdasarkan ketersediaan SD dan standar proses SD adalah metode CLAM dengan k = 6, numlocal = 2, maxneighbor = 154, tls = 50 dan setradius = 100-10:5. Pada studi kasus, dapat diketahui bahwa berdasarkan nilai overall average silhouette width metode CLAM lebih baik dibandingkan metode CLARANS.","Non-hierarchical clustering or partitioning methods is a cluster analysis method that the number of clusters is determined at the beginning and each cluster has a center cluster. K-medoids is one of the partitioning methods with the medoid as its center cluster, where medoid is the most centrally located object in a cluster, which is robust to outliers. The k-medoids algorithm used in this study is Clustering Large Application Using Metaheuristics (CLAM), where CLAM is a development of the Clustering Large Application based on Randomized Search (CLARANS) algorithm in improving the quality of cluster analysis by using hybrid metaheuristics between Tabu Search (TS ) and Variable Neighborhood Search (VNS). The CLAM method uses four parameters, named as numlocal to limit  iterations, maxneighbor to limit neighbors to a node, tls to limit nodes stored in the tabu list and setradius to set a linear reduction schedule.  The best cluster analysis method for classifying sub-districts on the island of Sumatra based on elementary school availability and elementary school process standards is the CLAM method with k=6, numlocal = 2, maxneighbor = 154, tls = 50 and setradius = 100-10:5. In the case study, it can be seen that based on the overall average silhouette width value, the CLAM method is better than the CLARANS method.","Kata Kunci : k-medoids, Clustering Large Application Using Metaheuristics,  pencilan, dataset besar, silhouette width, Clustering Large Application based on  Randomized Search, Tabu Search, Variable Neighborhood Search"
http://etd.repository.ugm.ac.id/home/detail_pencarian/212721,Metode Optimasi Portfolio Global Minimum Semivariance pada Klaster Saham Syariah Indonesia di Periode COVID-19,"BERNADET HILGA P P, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2022 | Skripsi | S1 STATISTIKA,"Indonesia yang merupakan negara dengan mayoritas penduduk beragama Muslim, memiliki potensi perkembangan industri syariah yang besar. Salah satu kebutuhan perkembangan industri ini adalah investasi syariah, dimana Indonesia sudah memiliki indeks komposit syariah yaitu ISSI. Meskipun performa dari indeks ini secara umum masih lebih rendah dari indeks komposit saham IHSG, beberapa penelitian telah menemukan bahwa saham syariah menunjukkan performa yang baik daripada saham konvensional di periode krisis.   Pembentukan portfolio investasi merupakan cara yang umum untuk melakukan diversifikasi investasi sehingga diperoleh risiko yang minimal. Dalam penelitian ini, akan dicari pembentukan portfolio saham-saham ISSI yang optimal di periode COVID-19. Saham-saham ISSI yang selalu masuk ke dalam Daftar Efek Syariah selama tahun 2019 dikelompokkan kedalam 3 klaster berdasarkan risiko volatilitas downside semi standar deviasi dan risiko sistematik beta menggunakan K-Means clustering. Beberapa kombinasi portfolio berisikan 10 saham dengan Sortino Rasio terbaik dari setiap klaster kemudian diberikan pembobotan optimal menggunakan Global Minimum Semivariance. Perhitungan volatilitas dan pembobotan optimal dilakukan dengan menggunakan data tahun 2019, kemudian bobot tersebut digunakan untuk simulasi investasi menggunakan harga saham ditutup di awal tahun 2020 dan dilihat performanya di akhir tahun 2020 dan 2021. Hasil yang diperoleh yaitu bahwa portfolio yang berisikan klaster dengan semi standar deviasi kecil dan beta sedang memberikan performa yang sangat baik di akhir 2020 dan tetap memberikan hasil yang sangat baik di akhir 2021. Portfolio yang memberikan pengembalian paling besar di akhir tahun 2020 dan 2021 merupakan portfolio kombinasi terbobot dari ketiga klaster.","Indonesia, a country with its majority of population being Moslem, has a huge potential of sharia industry growth. One of the needs for this growth is sharia investments, where Indonesia already have a sharia composite index called Indeks Saham Syariah Indonesia (ISSI). Even though generally this index still performed below IHSG, some researches found that sharia stocks showed better performance than conventional stocks in crisis periods.   The creation of investment portfolio is a general way to do investment diversification with the goal of minimizing risk. In this study, we would try to find the optimal portfolio consists of ISSI stocks in COVID-19 period. ISSI stocks that consistently in the Daftar Efek Syariah throughout 2019 are being grouped into 3 clusters by their semi standard deviation as their downside volatility risk and their beta as their systematic risk with K-Means clustering. Some portfolio combinations of 10 stocks with best Sortino Ratio from each clusters are being optimized using Global Minimum Semivariance. The calculation of volatility and optimization are using 2019 data, then the weights are used to simulate investation using the stock's closing price in 2020. Then, the performance would be seen in the end of 2020 and 2021. The result was that the portfolios consists of stocks with small semi standard deviation and medium beta performed very well in the end of 2020 and 2021. The portfolio giving the best return by the end of 2020 and 2021 was the portfolio consist of the weighted combination of all 3 clusters.","Kata Kunci : optimasi portfolio, analisis klaster, K-Means, minimum semivariance, risiko volatilitas"
http://etd.repository.ugm.ac.id/home/detail_pencarian/210674,IMPLEMENTASI EXTREME GRADIENT BOOSTING (XGBOOST) DALAM MENANGANI IMBALANCED CLASS PADA ANALISIS KLASIFIKAS,"ANIENDYARATRI D M K, Prof. Dr.rer.nat., Dedi Rosadi, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Teknologi membawa solusi atas permasalahan klasifikasi dalam data analisis, salah satunya adalah permasalahan imbalanced class dalam analisis klasifikasi. Imbalanced class merupakan keadaan di mana salah satu kelas dalam variabel dependen memiliki jumlah yang lebih banyak secara signifikan dibanding kelas yang lain. Keadaan ini mampu menimbulkan model yang bias akibat kurangnya kemampuan model untuk memprediksi kelas dengan jumlah observasi lebih sedikit (kelas minor). Penanganan dengan pendekatan algoritma berupa penggunaan algoritma ensemble learning dan modifikasinya diterapkan untuk mengatasi permasalan tersebut. Extreme Gradient Boosting (XGBoost) merupakan metode ensemble learning yang membentuk beberapa decision tree secara berurutan untuk memperbaiki kesalahan yang dibentuk model sebelumnya. Modifikasi algoritma XGBoost dengan mengaplikasikan pembobotan, Weighted-Loss, dan Focal-Loss. Pada skripsi ini dilakukan analisis pada Bank Marketing dengan menggunakan XGBoost dan modifikasinya, kemudian dibandingkan dengan model dasarnya, decision tree, dan model ensemble learning lain, yaitu AdaBoost dan Random Forest. Dari analisis yang dilakukan diperoleh kesimpulan bahwa Weighted-XGBoost menghasilkan performa yang paling baik.","Technology brings solutions to classification problem in data analysis, such as the problem of imbalanced class in classification analysis. Imbalanced class is a condition where one of the classes in the dependent variable has significantly more numbers than the other classes. This situation can lead to a biased model due to the modelâ€™s lack ability to predict classes with fewer observations (minority class). With an algorithmic approach, ensemble learning and algorithm modification, are applied to overcome these problems. Extreme Gradient Boosting (XGBoost) is an ensemble learning method that build several decision trees in sequence to correct errors made by the previous model. For handling the imbalanced class, modification of the XGBoost algorithm is done by applying weighting, Weighted-Loss, dan Focal-Loss. In this thesis we analyze Bank Marketing data using XGBoost and its modifications, then compared with basic model, decision tree, and other ensemble learning models, namely AdaBoost and Random Forest. From the analysis, it was concluded that Weighted XGBoost has  the best performance.","Kata Kunci : klasifikasi, data tidak seimbang, XGBoost, Weighted Loss, Focal-Loss"
http://etd.repository.ugm.ac.id/home/detail_pencarian/210677,Peningkatan Performa Klasifikasi Algoritma C4.5 Menggunakan Split Feature Reduction dan Bootstrap Aggregating (Bagging),"HAFIZHAH FARAH AZHAR, Prof. Dr.rer.nat Dedi Rosadi, S.Si., M.Sc.",2022 | Skripsi | S1 STATISTIKA,"Dalam machine learning, salah satu algoritma klasifikasi yang tergolong dalam kategori supervised learning adalah algoritma C4.5. Pada dataset dengan jumlah fitur yang besar, dimungkinkan adanya fitur-fitur yang tidak relevan dengan proses klasifikasi yang menyebabkan performa model klasifikasi menjadi tidak optimal. Split feature reduction adalah salah satu metode pemilihan fitur yang bekerja berdasarkan pemeringkatan pada nilai gain yang kemudian membagi data menjadi beberapa skema split yang mungkin terbentuk. Dibandingkan dengan metode klasifikasi lain, model pohon keputusan termasuk dalam metode yang tidak stabil dikarenakan perubahan kecil dalam data training dapat menyebabkan perubahan signifikan pada model klasifikasi. Selain itu, terdapat risiko terjadinya overfitting pada data dengan variansi besar. Oleh karena itu, diterapkan metode bagging untuk meningkatkan stabilitas dan performa klasifikasi. Dari hasil analisis, diketahui bahwa bagging meningkatkan performa klasifikasi yakni akurasi, presisi, sensitivitas, spesifisitas, dan skor F-1, serta adanya pembagian data menjadi beberapa split mampu memilih kombinasi fitur mana yang menghasilkan performa terbaik.","In machine learning, one of the classification algorithms belonging to the supervised learning category is the C4.5 algorithm. In datasets with a large number of features, it is possible that there are features that are not relevant to the classification process which causes the performance of the classification model to be not optimal. Split feature reduction is a feature selection method that works based on ranking on the gain value which then divides the data into several possible split schemes. Compared to other classification methods, the decision tree model is an unstable method because small changes in the training data can cause significant changes to the classification model. In addition, there is a risk of overfitting on data with large variances. Therefore, the bagging method is applied to improve the stability and classification performance. From the results of the analysis, it is known that bagging improves classification performance, namely accuracy, precision, sensitivity, specificity, and F-1 score, as well as the division of data into several splits to be able to choose which combination of features produces the best performance.","Kata Kunci : Klasifikasi, C4.5, Bootstrap Aggregating (Bagging), Split Feature Reduction."
http://etd.repository.ugm.ac.id/home/detail_pencarian/195335,Distribusi Quasi Poisson Lindley Dan Aplikasi Pada Data Overdispersi,"Astri Paoziyah Sapari, Prof. Drs. Subanar, Ph.D",2021 | Skripsi | S1 STATISTIKA,"Mengolah data yang memiliki keadaan overdispersi dengan menggunakan distribusi Poisson telah dinilai tidak efektif. Hal itu disebabkan karena nilai estimasi parameter yang dihasilkan akan bersifat bias. Oleh karena itu banyak para peneliti memodifikasi dan memperbarui distribusi probabilitas yang telah ada untuk mengatasi keadaan overdispersi pada data cacah. Distribusi Quasi Poisson Lindley adalah distribusi probabilitas campuran baru yang diperoleh dari menggabungkan distribusi Poisson dengan distribusi Quasi Lindley. Distribusi ini pertama kali dikenalkan oleh Shanker dan Mishra (2016). Tujuan dari pembentukan distribusi campuran baru ini untuk menyajikan distribusi alternatif lain yang dapat digunakan untuk menganalisis data cacah yang mengalami overdispersi. Distribusi Quasi Poisson Lindley ini akan diaplikasikan pada data kelahiran mati bayi di Kabupaten Gunungkidul tahun 2012-2013. Estimasi parameter yang akan digunakan adalah Metode Maksimum Likelihood dengan iterasi Newton-Raphson kemudian dengan menggunakan nilai Akaike Information Criterion (AIC) dan Schwart Bayesian Criterion (SBC) akan dibandingkan antara distribusi Quasi Poisson Lindley, distribusi Poisson Lindley, dan distribusi Poisson untuk melihat distribusi yang terbaik.","Processing overdispersion data using the Poisson distribution has been considered ineffective. This is because the estimated parameter values generated will be biased. Therefore, many researchers modify and update existing probability distributions to overcome overdispersion states in the count data. The Quasi Poisson Lindley distribution is a new mixed probability distribution obtained by compounding the Poisson distribution with the Quasi Lindley distribution. This distribution was first introduced by Shanker and Mishra (2016). Quasi Poisson Lindley distribution will be applied to fetal death data Gunungkidul Regency in 2012-2013. Parameter of Quasi Poisson Lindley distribution will be estimated using Maximum Likelihood Method with Newton-Raphson iteration then using the Akaike Information Criterion (AIC) and Schwart Bayesian Criterion (SBC) values, will be compared between the Quasi Poisson Lindley distribution, the Poisson Lindley distribution, and the Poisson distribution to see which the best model distribution for fetal death data Gunungkidul Regency in 2012-2013.","Kata Kunci : Distribusi Lindley, Distribusi Poisson, Distribusi Quasi Poisson Lindley, Overdispersi/Lindley Distribution, Poisson Distribution, Poisson Lindley Distribution, Quasi Poisson Lindley Distribution, Overdispresion"
http://etd.repository.ugm.ac.id/home/detail_pencarian/198157,Optimisasi Portofolio Saham Indeks IDX 30 Menggunakan Model Black Litterman Berdasarkan Klaster K-Means,"MIA CARRA, Dr. Abdurakhman, M.Si",2021 | Skripsi | S1 STATISTIKA,"Investasi merupakan aktivitas penempatan sejumlah dana yang dimiliki dengan harapan akan memperoleh keuntungan di masa depan. Perlu diingat bahwa dalam setiap investasi juga pasti ada risiko yang harus ditanggung investor. Oleh sebab itu, pemilihan portofolio dalam investasi menjadi hal yang penting untuk dilakukan sehingga menghasilkan tingkat pengembalian yang diharapkan dengan risiko yang masih dapat ditoleransi. Pada penelitian ini akan dilakukan suatu teknik sehingga mampu membentuk portofolio yang optimal, yaitu dengan menggabungkan analisis klaster K-means dengan model Black Litterman. Model Black Litterman merupakan metode perhitungan portfolio yang mampu mengkombinasikan market return dengan pandangan investor.  Data yang digunakan adalah data closing price saham mingguan dari 10 saham IDX 30. Data tersebut dikelompokan ke dalam 3 klaster dan dipilih perwakilan dari tiap klaster, sehingga terbentuk suatu portfolio yang terdiri dari 3 saham. Selanjutnya boobot portfolio dihitung menggunakan model Black Litterman. Kinerja portfolio dihitung dengan melihat nilai investasi selama 5 hari ke depan.","Investment is an activity to place fund in order to making profits in the future. However, in every investment, there will always be risks that must be borne by the investor. Therefore, forming a portfolio is important to produce high expected rate with tolerable risks. In this study, a special technique is used by combining K-means cluster analysis with black litterman model.  This model enables investors to combine the market return and their views. The data used in this study is  weekly closing price from 10 stocks of IDX 30 index. This data is divided into 3 clusters and each cluster will have a stock representative that will form a portfolio. The weight of each stock in this portfolio will be calculated using black litterman model and the portfolio performace will be calculated by determining the investment value for five days ahead.","Kata Kunci : Optimisasi portofolio, analisis klaster, Black Litterman"
http://etd.repository.ugm.ac.id/home/detail_pencarian/198417,MODEL HYBRID GARCH DAN MULTI LAYER PERCEPTRON NEURAL  NETWORK (GARCH-MLPNN) UNTUK PREDIKSI INDEKS HARGA  SAHAM GABUNGAN DI BURSA EFEK INDONESIA,"ARETA SABILA, Prof. Dr. rer. nat. Dedi Rosadi, S.Si,  M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Kemajuan pasar modal menggambarkan tingginya investasi yang ditanamkan para investor dan tingginya modal yang dapat digunakan untuk pertumbuhan perekonomian di negara tersebut. Penting untuk mengetahui indeks harga saham karena investor akan mendapat gambaran performa harga saham pada emiten-emiten yang dimilikinya. Untuk dapat memperoleh gambaran kondisi pasar kedepan maka perlu mengetahui indeks harga saham yang akan datang melalui peramalan. Teknologi sistem jaringan syaraf tiruan dengan algoritma Backpropagation telah diimplementasikan dalam berbagai aplikasi terutama dalam hal peramalan.   Pengujian menggunakan hybrid GARCH-ANN dilakukan untuk mendapatkan hasil ramalan yang diharapkan dapat meningkatkan akurasi dari peramalan harga penutupan saham harian. Hasil dari metode ini menunjukkan bahwa model yang diusulkan memberikan hasil prediksi harga penutupan saham harian menjadi lebih akurat dari model GARCH individu. Dalam kasus ini, nilai Means Square Error (MSE) untuk GARCH(1,2) model volatilitas sebesar 33,96395x10-5 dan hybrid GARCH-ANN model volatilitas adalah sebesar 4,011141x10-6.","Capital market describes its investorsÃƒÂ¢Ã¯Â¿Â½Ã¯Â¿Â½s high investment and its high capital which can be used for the economic growth of a country. Stock price performance will be unknown from the issuers that the investor own, therefore it is necessary for them to understand the stock price index. Predicting the future stock price index to get a better overview of future market conditions is done through forecasting.  The use of artificial neural network system using backpropagation algorithm has been implemented for various methods of forecasting, one of which is GARCH-ANN. As an attempt to obtain an increased accuracy in forecasting daily closing stock prices, an examination using GARCH-ANN hybrid is carried out. The results of this method indicates that the proposed model provides a more accurate prediction of daily stock prices than the individual GARCH model. It is found that the value of Means Squared Error (MSE) for GARCH volatility model is 33,96395x10-5 and hybrid GARCH-ANN volatility model is 4,011141x10-6","Kata Kunci : Jaringan Syaraf Tiruan, Runtun Waktu, GARCH, Algoritma  Backpropagation, Neural Network, Time Series, GARCH, Backpropagation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/203539,Analisis Sentimen Terhadap Review Drama dari Twitter Menggunakan Metode K-Nearest Neighbor (K-NN) dengan Penanganan Data Imbalanced,"ANISAH CAHYANI, Drs. Danardono, MPH., Ph.D.",2021 | Skripsi | S1 STATISTIKA,"Media sosial memiliki peran sangat penting untuk komunikasi secara online antar satu orang dengan orang lain. Salah satu media sosial yang populer adalah Twitter, dengan Twitter masyarakat dapat menyatakan pendapat atau opini secara bebas terkait suatu topik, menjual suatu barang, dan hal lainnya. Salah satu pendapat atau opini yang sering dibicarakan masyarakat di Twitter adalah serial drama yang sedang populer. Banyaknya pengguna dan review dari penonton drama di Twitter menyebabkan banyak data yang terkumpul, sehingga sulit untuk melihat gambaran sentimen secara jelas. Oleh karena itu, pada penelitian ini akan menganalisis Twitter berbahasa Indonesia yang memiliki opini atau sentimen tentang serial drama, agar data yang tersedia dapat dimanfaatkan untuk keperluan-keperluan terkait review drama. Pada penelitian ini dilakukan klasifikasi sentimen menggunakan K-Nearest Neighbor serta penanganan data imbalanced dengan random oversampling, random undersampling, dan random over-under sampling. Penanganan data imbalanced dilakukan agar prediksi klasifikasi tidak cenderung pada kelas mayoritas dan mengabaikan kelas minoritas. Dari analisis yang dilakukan, didapatkan kesimpulan bahwa metode klasifikasi terbaik untuk review drama dari Twitter adalah metode K-NN dengan random oversampling (k=1) karena dapat meningkatkan nilai akurasi, sensitivitas, dan f-measure, serta memiliki nilai spesifisitas yang cukup baik.","Social media has a significant role in online communication between one person and another. One of the popular social media is Twitter, with Twitter can express freely opinions related to a topic, sell an item, and other things. One of the opinions often talks about on Twitter is the currently popular drama series. A large number of users and reviews from drama viewers on Twitter means that a lot of data is collected, making it difficult to see a clear picture of the sentiment. Therefore, this study will analyze Twitter Indonesian-language has opinions or sentiment about drama series so that the available data can be used for purposes related to drama review. In this study conducted sentiment classification using K-Nearest Neighbor and imbalanced data handling with random oversampling, random undersampling, and random over-under sampling. Handle imbalanced data so that the classification results not tend to predict the majority class and ignore the minority class. From the analysis, can be concluded that the best classification method for a review drama from Twitter is K-NN with random oversampling (k = 1) because it can increase the value of accuracy, sensitivity, f-measure, and the value of specifity is good.","Kata Kunci : analisis sentimen, K-Nearest Neighbor, data imbalanced, random   oversampling, random undersampling,  random over-under sampling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/199700,MODEL MULTI-STATUS DALAM MENENTUKAN PREMI ASURANSI LONG TERM CARE,"CHELSEA JUSTIANI, Dr. Adhitya Ronnie Effendie, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Asuransi Long Term Care adalah salah satu jenis asuransi kesehatan yang memberikan perlindungan bagi tertanggung yang menderita penyakit kronis atau cacat tubuh. Salah satu produk asuransi Long Term Care adalah Annuity as A Rider Benefit, produk asuransi ini memberikan perlindungan tambahan berupa anuitas untuk tertanggung dengan beberapa ketentuan. Dalam tugas akhir ini akan dibahas perhitungan premi asuransi LTC dengan produk Annuity as A Rider Benefit dengan menggunakan model multi status (tiga status), yaitu sehat, sakit dan meninggal. Perhitungan premi juga menggunakan probabilitas transisi rantai markov. Data yang digunakan adalah data sampel polis asuransi kesehatan tahun 2016 Badan Penyelenggara Jaminan Sosial (BPJS) Kesehatan. Dalam tugas akhir ini akan membandingkan premi yang harus tertanggung bayar dengan tingkat suku bunga, umur, jenis kelamin, dan masa pembayaran premi yang berbeda-beda. Kenaikan tingkat suku bunga dan masa pembayaran premi maka akan menghasilkan nilai premi yang semakin rendah. Kenaikan umur saat awal mengikuti asuransi akan mengakibatkan nilai premi yang semakin tinggi.","Long term care insurance is a type of health insurance that provides protection for the insured suffering from chronic diseases or disabilities. One of the Long Term Care insurance products is Annuity as A Rider Benefit. This insurance product provides additional protection in the form of an annuity for the insured with several conditions. In this final project will discuss the calculation of Long Term Care insurance premiums with the product Annuity as A Rider Benefit using the multiple state model (three states), namely health, illness and death. The premium calculation also uses the Markov chain transition probability. The data used are sample data on health insurance policies in 2016 by the Badan Penyelenggara Jaminan Sosial (BPJS). In this final project will compare the premium that the insured must pay with different interest rates, age, gender, and premium payment period. An increase in interest rates and the period of premium payment will result in a lower premium value. The increase in age at the beginning of joining the insurance will result in a higher premium value.","Kata Kunci : premi, long term care, Annuity as A Rider Benefit, model multi status"
http://etd.repository.ugm.ac.id/home/detail_pencarian/197918,Metode Deep Learning Menggunakan Algoritma Convolutional Neural Network untuk Klasifikasi Citra,"ARIELA NISA ADIANA, Yunita Wulansari, S.Si, M.Sc",2021 | Skripsi | S1 STATISTIKA,"Kemacetan lalu lintas merupakan kondisi kendaraan yang ditandai dengan kecepatan yang lebih lambat, waktu perjalanan yang lebih lama, dan antrian kendaraan yang meningkat. Salah satu alternatif untuk mengurangi terjadinya kemacetan dan kecelakaan di Yogyakarta yaitu dengan dibangunnya jalur khusus bagi pengendara roda dua. Sistem kanalisasi atau jalur khusus untuk kendaraan roda dua ini apabila diberlakukan akan mubazir jika pengawasannya masih dilakukan secara manual sehingga diperlukan suatu kegiatan pengawasan penggunaan jalur lalu lintas secara otomatis, salah satunya yaitu dengan menggunakan citra/gambar untuk melakukan klasifikasi jenis-jenis kendaraan. Deep Learning mengenalkan metode Convolutional Neural Network yang memiliki kinerja yang baik dalam pengenalan pola maupun klasifikasi pada citra.  Pada skripsi ini akan dibahas mengenai metode Deep Learning menggunakan algoritma Convolutional Neural Network dengan menerapkan max-pooling sebagai operasi pooling dan ReLU sebagai fungsi aktivasi. Studi kasus pada skripsi ini menggunakan data citra dua kategori kendaraan yaitu mobil dan sepeda motor yang diambil dari situs Kaggle. Kegiatan yang dilakukan selama penelitian terdiri dari pengumpulan data citra, preprocessing citra, pelatihan model klasifikasi, perbandingan jenis metode regularisasi pada Neural Network serta melakukan pemilihan parameter yang tepat untuk mendapatkan model terbaik. Berdasarkan hasil pembahasan didapatkan tingkat akurasi sebesar 100% pada proses training dan 92.5% pada proses testing sehingga performa dari model dengan menerapkan metode regularisasi dropout pada penelitian ini dapat dikatakan optimal dalam mengklasifikasikan citra kendaraan.","Traffic congestion is a condition in transport that is characterized by slower speeds, longer trip times, and increased vehicular queueing. One of the alternatives to reduce traffic congestion and accidents in Yogyakarta is the construction of a special lane for two-wheeled riders. By enforcing the canalization system or special lanes for two-wheeled vehicles, will be redundant if the supervision is still carried out manually so that an automatic traffic control activity is required, one of which is by using images to classify vehicle types. Deep Learning introduces the Convolutional Neural Network method which has a good performance in pattern recognition and classification in images.  In this paper, we will discuss the deep learning method using the Convolutional Neural Network algorithm by implementing max-pooling as a pooling operation and ReLU as an activation function. The case study in this paper uses images data from two categories of vehicles, namely cars and motorbikes which are taken from the Kaggle site. The activities carried out during the research consisted of image data collection, image preprocessing, classification model training, comparison of regularization methods on the Neural Network and selecting the right parameters to get the best model. Based on the results of the discussion, the accuracy rate is 100% in the training process and 92.5% in the testing process so that the performance of the model by applying the dropout regularization method in this study can be said to be optimal in classifying vehicle images.","Kata Kunci : Deep Learning, Convolutional Neural Network, Klasifikasi Gambar, Kendaraan, Metode Regularisasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/197666,Metode Neural Network Autoregression (NNAR) untuk Peramalan Data Runtun Waktu (Studi Kasus: Indeks Kualitas Udara Harian di Jakarta Pusat dan Jakarta Selatan),"ARIF SETYAWAN, Vemmie Nastiti Lestari, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Pencemaran udara masih menjadi permasalahan yang dialami banyak negara di dunia. Menurut WHO, pencemaran udara menyebabkan sekitar 7 juta kematian di seluruh dunia setiap tahunnya. Tingkat pencemaran udara dapat diukur dengan indeks kualitas udara atau air quality index (AQI). Nilai indeks kualitas udara berperan penting dalam manajemen kualitas udara, salah satunya untuk memberikan peringatan kesehatan. Skripsi ini membahas peramalan indeks kualitas udara dengan dua pendekatan yaitu klasik dengan metode Autoregression (AR) dan pembelajaran mesin dengan metode Neural Network Autoregression (NNAR). Metode NNAR adalah pengembangan dari jaringan syaraf tiruan dengan struktur feed-forward, hanya terdiri dari tiga lapisan, dan menggunakan fungsi aktivasi sigmoid. Dengan data historis indeks kualitas udara harian di kota Jakarta Pusat dan kota Jakarta Selatan pada rentang waktu Januari 2018 hingga Desember 2020, diperoleh kesimpulan performa metode NNAR jauh lebih unggul dari metode AR klasik. Performa metode diukur dengan Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), dan Mean Absolute Scaled Error (MASE). Diperoleh hasil berturut-turut untuk kedua kota yaitu 1,243; 1,091; 0,042; dan 5,49; 4,926; 0,237.","Air pollution is still a problem in many countries across the world. According to WHO, air pollution is estimated to cause 7 million deaths throughout the world each year. Air pollution levels can be measured by the air quality index (AQI). The air quality index has an important role in air quality management, such as health early warning system. This undergraduate thesis discusses air quality index forecasting using two approaches, classic approach using the autoregression (AR) method and machine learning approach using the Neural Network Autoregression (NNAR) method. The NNAR method is the advancement of an artificial neural network with feed-forward structures, consists of only three layers, and using the sigmoid activation function. Using the historical data of air quality index in Central Jakarta and South Jakarta during 2018 January until 2020 December, we found that NNAR outperformed classic AR. The performance of these methods measured with Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), dan Mean Absolute Scaled Error (MASE). The results are, for both cities, respectively 1.243; 1.091; 0.042; and 5.49; 4.926; 0.237.","Kata Kunci : Indeks Kualitas Udara, Peramalan, Air Quality Index, Forecast, Autoregression, Neural Network Autoregression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/198441,Pengaplikasian Extreme Learning Machine untuk Peramalan Data Time Series,"SHELLY PRIMANGGARA WARDHANI, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Artificial neural network (ANN) atau jaringan saraf tiruan adalah metode yang bertujuan untuk mensimulasikan jaringan saraf biologis otak manusia. Metode ini telah banyak digunakan untuk peramalan, regresi, hingga klasifikasi. Banyaknya manfaat dari metode ini membuat ANN terus dikembangkan sehingga dapat menghasilkan hasil yang akurat. Pada dasarnya, algoritma paling umum yang digunakan untuk memperbaharui parameter jaringan saraf tiruan adalah backpropagation. Algoritma ini memiliki kekurangan learning speed nya yang cukup lambat. Extreme learning machine diperkenalkan untuk mengatasi masalah tersebut, serta dipercaya menghasilkan hasil peramalan yang lebih akurat. Extreme learning machine adalah metode pembelajaran jaringan saraf tiruan feedforward dengan satu hidden layer, dikenal juga sebagai Single Layer-Hidden Feedforward Neural Network (SLFN). ELM memiliki keunggulan dalam learning speed dan akurasinya yang lebih baik. Bobot serta bias awal dari ELM dipilih secara acak, kemudian untuk bobot serta bias akhir dilakukan dengan perhitungan analitis Moore-Penrose Generalized Invers, sehingga tidak dilakukan iterasi serta mengurangi kompleksitas komputasi. Skripsi ini akan meramalkan harga penutupan IHSG menggunakan ANN dengan algoritma backpropagation dan ELM. Didapatkan nilai RMSE, MAPE, dan MAE yang lebih rendah serta waktu komputasi yang lebih cepat menggunakan algoritma ELM dibandingkan backpropagation.","Artificial neural network (ANN) is a method that simulate the biological neural network of human brain. This method had been widely used for forecasting, regression, and classification. ANN continue to be developed so it can produce more accurate prediction. Basically, the most common algorithm used to update neural network parameters is backpropagation. This algorithm has some flaws, one of them is a slow learning speed. Extreme learning machine was introduced to fix this problem and believed to produce more accurate results. Extreme learning machine is a feedforward neural network learning method with one hidden layer, also known as single layer-hidden feedforward neural network (SLFN). ELM has advantages in learning speed and better accuracy. The initial weights and bias were randomly selected, then the final weights and bias were computed analytically with Moore-Penrose Generalized Inverse, so there is no iteration and reduced computation complexity. In this undergraduate thesis, will predict the close price of IHSG using ANN with backpropagation and ELM algorithms. A lower value of RMSE, MAPE, and MAE obtained using ELM algorithm, as well as faster computational time compared to backpropagation.","Kata Kunci : Artificial Neural Network, Backpropagation, Extreme Learning Machine, Peramalan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/206642,METODE HIGH ORDER FUZZY TIME SERIES DENGAN MENGGUNAKAN JARINGAN SYARAF TIRUAN UNTUK MEMPREDIKSI JUMLAH HARIAN POSITIF COVID-19,"DINI RAMADHANI, Prof.Dr.rer.nat Dedi Rosadi, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Pada tahun 2019 World Healh Organization (WHO) pertama kali menemukan virus corona jenis baru kemudian diberi nama SARS-CoV-2 yang menyebabkan penyakit COVID-19. Virus ini sangat mudah menyebar. Jumlah kasus harian positif COVID-19 di Indonesia tinggi.Bukan hanya Indonesia, hal ini juga terjadi di negara negara lainnya. Pandemi COVID-19 menjadi permasalahan yang besar. Manajemen resiko yang dapat dilakukan untuk memantau penyebaran dan penularan COVID-19 dapat dilakukan dengan peramalan. Data jumlah harian terkonfirmasi positif COVID-19 memiliki ketidakpastian dan kompleksitas dinamika dalam deret waktu di negara negara. Pemilihan data latih dilihat dari pola yang hampir sama dengan data tes. Hal ini diperlukan untuk mengurangi ketidakpastian dan kompleksitas yang ada. Kemudian, akan digunakan metode High Order Fuzzy Time Series Back Propagation, High Order Fuzzy Time Series Resilient Propagation serta Double Exponential Smoothing (Brown) untuk memprediksi jumlah harian positif COVID-19 di Indonesia dan Malaysia. Didapatkan bahwa metode High Order Fuzzy Time Series dengan menggunakan jaringan syaraf tiruan dengan menggunakan resilient propagation lebih akurat dari pada back propagation serta resilient propagation serta Double Exponential Smoothing (Brown) lebih akurat dari pada kedua metode tersebut namun tidak mengikuti pola grafik data asli.","In 2019 the World Health Organization (WHO) first discovered a new coronavirus (SARS-CoV-2) that causes COVID-19 disease. This virus is rapid to spread. daily number of positive COVID-19 in Indonesia is high. Not only happens in Indonesia, but also in other countries. The COVID-19 pandemic is a big problem. Risk management that can be held to monitor the spread and transmission of COVID-19 can be done by forecasting. The daily number of data has confirmed positives COVID-19 uncertaintly and complexity dynamics in the time series in the countries. The selection of training data has been seen from a pattern that almost accurate as the test data needed to reduce the uncertainty and complexity existing. Then the High Order Fuzzy Time Series methods Back Propagation and High Order Fuzzy Time Series methods Resilient Propagation, and Double Exponential Smoothing (Brown) will be used to predict daily number of positive COVID-19 in Indonesia and Malaysia. It was found that back propagation algorithms is more accurate than resilient propagation algorithms and Double Exponential Smoothing (Brown) are more accurate than the two methods but do not follow the graph pattern of the original data.","Kata Kunci : Jumlah harian positif COVID-19, Prediksi, High Order Fuzzy Time Series dengan menggunakan jaringan syaraf tiruan, Back propagation , Resilient propagation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/202293,Peningkatan Performa Klasifikasi pada Regresi Logistik Biner Menggunakan Metode Bootstrap Aggregating (Bagging),"RINA LISTIYOWATI, Drs. Danardono, MPH., Ph.D.",2021 | Skripsi | S1 STATISTIKA,"Klasifikasi merupakan salah satu teknik yang digunakan untuk mengelompokkan suatu data yang disusun secara sistematis. Dalam statistika terdapat beberapa metode klasifikasi, salah satunya adalah regresi logistik. Klasifikasi dengan regresi logistik memuat pendugaan parameter yang tidak stabil, artinya jika terdapat perubahan dalam dataset menyebabkan perubahan yang signifikan pada model. Metode bootstrap aggregating (bagging) merupakan salah satu metode ensemble yang bertujuan meningkatkan stabilitas dan performa klasifikasi. Pada penelitian ini digunakan metode bagging untuk klasifikasi pada regresi logistik dengan beberapa kali replikasi bootstrap. Metode regresi logistik tanpa bagging digunakan sebagai pembanding untuk mengetahui metode mana yang memiliki performa lebih baik. Dari hasil analisis menggunakan dua buah dataset yang berbeda diperoleh kesimpulan bahwa penerapan metode bagging pada regresi logistik mampu meningkatkan performa klasifikasi berdasarkan nilai akurasi dan error rate.","Classification is one of the techniques used to classify data that is arranged systematically. In statistics, there are several classification methods, one of which is logistic regression. Classification with logistic regression contains unstable parameter estimates, meaning that if there is a change in the dataset it causes a significant change in the model. The bootstrap aggregating (bagging) method is one of the ensemble methods that aim to improve the stability and performance of the classification. In this study, the bagging method was used for classification in logistic regression with several bootstrap replications. The logistic regression method without bagging was used as a comparison to find out which method had better performance. From the results of the analysis using two different datasets, it was concluded that the application of the bagging method in logistic regression was able to improve classification performance based on the value of accuracy and error rate.","Kata Kunci : Klasifikasi, Regresi Logistik, Bootstrap Aggregating (Bagging)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/200249,Perbandingan Kinerja Grafik Pengendali Progressive Mean dan Grafik Pengendali DOB (Decision On Belief)  Pada Pengendalian Kualitas Produk,"ANDY GIOFANNY MARBUN, Dr. Abdurachman, M.Si.",2021 | Skripsi | S1 STATISTIKA,"Seiring berkembangnya zaman, karakteristik lingkungan dunia usaha semakin menuntut kebijakan serta kepiawaian manajemen suatu perusahaan dalam menghasilkan suatu produk yang berkualitas. Hal ini  dikarenakan  tingkat  kepuasan konsumen sangat dipengaruhi oleh kualitas dari suatu produk. Dalam suatu proses produksi, kualitas dari produk perlu dikendalikan agar selalu sesuai dengan target yang telah ditetapkan oleh perusahaan yang sesuai dengan kepuasan konsumen. Pengendalian Kualitas Statistik sangat diperlukan guna mendeteksi adanya permasalahan dalam suatu produk yang nantinya akan mempengaruhi kualitas dari produk tersebut. Grafik pengendali merupakan salah satu alat yang sering digunakan untuk mengendalikan kualitas dari produk.  Permasalahan  kualitas statistik seperti pergeseran proses yang kecil atau oleh sebab-sebab yang tak terduga dapat di deteksi oleh grafik pengendali. Beberapa metode grafik pengendali yang digunakan dalam mengontrol kualitas produk adalah metode Grafik Pengendali Progressive mean dan Grafik Pengendali Decision On Belief, dimana kedua Grafik Pengendali ini sangat cocok untuk dipakai untuk suatu data produksi yang berasumsinormal. Pada penelitian ini akan dilihat performa dari grafik pengendali Progressive Mean dan dibandingkan dengan grafik pengendali Decision On Belief. Setelah dibandingkan kedua grafik didapat kesimpulan bahwa Decision On Belief merupakan grafik pengendali yang lebih baik dan lebih cepat dalam mendeteksi adanya data yang tidak terkendali atau berada dalam keadaan out of control .","Along with the times, the characteristics of the business environment increasingly demand policies and management expertise of a company in producing a quality product. This is because the level of consumer satisfaction is strongly influenced by the quality of a product. In a production process, the quality of the product needs to be controlled so that it is always in accordance with the targets set by the company in accordance with customer satisfaction. Statistical Quality Control is very necessary in order to detect any problems in a product that will affect the quality of the product. The control chart is a tool that is often used to control the quality of the product. Statistical quality problems such as small process shifts or unforeseen causes can be detected by the control chart. Several control chart methods used to control product quality are the Progressive Mean Control Chart method and the Decision On Belief Control Chart method, where these two Control Charts are very suitable to be used for production data that assumes normality. 	In this study, the performance of the Progressive Mean control chart will be seen and compared with the Decision On Belief control chart. After comparing the two of them, it can be concluded that Decision On Belief is a better and faster control chart in detecting data that is out of control or is in an out of control state.","Kata Kunci : Grafik Pengendali, Progressive Mean, PMi, Decision On Belief, B(Oi)."
http://etd.repository.ugm.ac.id/home/detail_pencarian/206668,Grafik Pengendali Individual atau Laney untuk Mengatasi Permasalahan Dispersi pada Grafik Pengendali Atribut Klasik,"MUHAMAD AZMI RIZKI RAMDANI, Dr. Herni Utami, S.Si., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Grafik pengendali klasik bagi data atribut (grafik P, grafik C, dll.) didasarkan pada asumsi distribusi binomial/poisson. Namun, ada banyak jenis data atribut yang tidak tepat apabila digambarkan oleh distribusi binomial/poisson. Terkadang variabilitas dari data atribut tidak sesuai dengan variabilitas dari asumsi binomial/poisson. Hal tersebut dapat mengantarkan pada permasalahan dispersi (overdispersi/underdispersi) apabila grafik atribut klasik digunakan untuk menggambarkan proses berjenis data atribut. Saat ini, terdapat dua solusi bagi permasalahan ini yakni ganti grafik pengendali atribut klasik dengan grafik laney atau individual. 	Penelitian ini dilakukan untuk mengetahui dampak dari permasalahan dispersi ini dan untuk mengetahui cara kerja grafik individual atau laney dalam mengatasi permasalahan tersebut. Hasil penelitian menunjukkan bahwa overdispersi akan memunculkan peringatan tak terkendali palsu (false alarm) sedangkan underdispersi akan menyebabkan tidak tampaknya peringatan tak terkendali (invisible alarm). Keduanya akan memberikan hasil analisis yang salah sehingga berdampak negatif pada pengambilan keputusan. Hasil penelitian juga menunjukkan bahwa grafik laney menjadi solusi bagi grafik P dan U sedangkan solusi bagi grafik NP dan C adalah grafik individual.","The classic control chart for attribute data (P-chart, C-chart, etc.) is based on the assumption of a binomial/poisson distribution. However, there are many types of attribute data that are not properly described by the binomial/poisson distribution. Sometimes the variability of the attribute data does not match the variability of the binomial/poisson assumption. This can lead to the dispersion problem (overdispersion/underdispersion) if the classical attribute chart is used to describe the process of attribute data type. Currently, there are two solutions to this problem, namely replacing the classic attribute control chart with a laney or individual chart. 	This research was conducted to determine the impact of this dispersion problem and to find out how individual or laney chart work in overcoming these problem. The results show that overdispersion will cause false alarms, while underdispersion will cause invisible alarms. Both will give wrong analysis results so that it has a negative impact on decision making. The results also show that the laney chart is the solution for the P and U-chart, while the solution for the NP and C-chart is an individual chart.","Kata Kunci : attribute control chart, overdispersion, underdispersion, individual control chart, laney control chart."
http://etd.repository.ugm.ac.id/home/detail_pencarian/204878,"Metode Black-Scholes, Ekspansi Gram-Charlier, dan Ekspansi Gram-Charlier dengan Pembagian Dividen dalam Menentukan Harga Opsi Beli Tipe Eropa","MERRY ANDHANI, Dr. Abdurakhman, S.Si., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Metode Black-Scholes untuk menentukan harga opsi merupakan metode yang telah banyak digunakan dalam dunia finansial. Metode Black-Scholes mengasumsikan return saham berdistribusi normal dengan volatilitas konstan dan tanpa pembagian dividen. Hal tersebut menjadikan metode Black-Scholes kurang akurat untuk menentukan harga opsi karena pada kenyataannya sering ditemukan saham yang tidak berdistribusi normal dan rutin membagikan dividen. Pada penelitian ini akan ditentukan model untuk menentukan harga opsi beli tipe Eropa dengan menggunakan metode ekspansi Gram-Charlier dengan pembagian dividen yang mengikutsertakan nilai skewness dan kurtosis dan mengikutsertakan dividen. Selanjutnya, akan dilakukan perbandingan harga opsi beli tipe Eropa menggunakan metode ekspansi Gram-Charlier dengan pembagian dividen dengan metode Black-Scholes, metode ekspansi Gram-Charlier dan harga opsi di pasar. Data yang digunakan adalah data closing price saham harian dari saham Kellogg Company, The Kraft Heinz Company, Unilever PLC, Starbucks Corporation, dan McDonald's Corporation. Akan ditentukan harga opsi beli tipe Eropa masing-masing saham tersebut dengan beberapa nilai kontrak menggunakan metode ekspansi Gram-Charlier dengan pembagian dividen dan selanjutnya dibandingkan dengan harga opsi beli menggunakan metode Black-Scholes, metode ekspandi Gram-Charlier dan harga opsi di pasar. Diperoleh kesimpulan bahwa metode ekspansi Gram-Charlier dengan pembagian dividen yang mengikutsertakan nilai skewness, kurtosis dan pembagian dividen merupakan metode yang cukup akurat untuk mengestimasi nilai opsi beli tipe Eropa yang dibuktikan dengan harga opsi beli tipe Eropa dengan metode ekspansi Gram-Charlier dengan pembagian dividen lebih mendekati harga opsi pasar dan memiliki nilai root mean square error (RMSE) paling kecil dibandingkan dengan metode Black-Scholes dan metode ekspansi Gram-Charlier.","The Black-Scholes method for determining option prices is a method that has been widely used in the financial world. The Black-Scholes method assumes that stock returns are normally distributed with constant volatility and without dividend distribution. This makes the Black-Scholes method less accurate for determining option prices because in reality, stocks are often found not normally distributed and regularly distribute dividends. In this study, a model will be determined to determine the price of a European type of call option using the Gram-Charlier expansion method with dividend distribution that includes skewness and kurtosis values and includes dividends. Next, we will compare the prices of European type call options using the Gram-Charlier expansion method with dividend distribution with the Black-Scholes method, the Gram-Charlier method and the price of options in the market. The data used is daily stock closing price data from Kellogg Company, The Kraft Heinz Company, Unilever PLC, Starbucks Corporation, and McDonald's Corporation stocks. The European-type call option price will be determined for each share with several contract values using the Gram-Charlier expansion method with dividend distribution and then compared with the call option price using the Black-Scholes method, the Gram-Charlier method and the option price in the market. It is concluded that the Gram-Charlier expansion method with dividend distribution which includes the value of skewness, kurtosis and dividend distribution is a fairly accurate method for estimating the value of a European type of call option as evidenced by the price of a European type of call option using the Gram-Charlier expansion method with dividend distribution closest to the market option price and has the smallest root mean square error (RMSE) compared to the Black-Scholes method and the Gram-Charlier expansion method.","Kata Kunci : harga opsi, Black-Scholes, ekspansi Gram-Charlier, skewness, kurtosis, dividen."
http://etd.repository.ugm.ac.id/home/detail_pencarian/200285,MODIFIED TWO-PARAMETER ESTIMATOR DALAM MODEL REGRESI LINEAR,"ANTONIUS INDO KRISTIKALA, Multicollinearity,  Liu Estimator, Ridge Regression Estimator, Two-Parameter Estimator, Modified Two-Parameter Estimator",2021 | Skripsi | S1 STATISTIKA,"Analisis regresi bertujuan untuk mengetahui hubungan variabel dependen dengan satu atau lebih variabel independen. Terdapat asumsi pada analisis regresi yaitu tidak terdapat multikolinearitas. Multikolinearitas merupakan suatu keadaan dimana terdapat hubungan linear antara beberapa atau semua variabel independen dalam model. Jika asumsi tersebut tidak terpenuhi, maka akan menyebabkan hasil estimasi menggunakan metode kuadrat terkecil menjadi tidak valid.  	Untuk mengatasi masalah multikolinearitas pada model regresi linear, dikembangkan metode Modified Two-Parameter Estimator yang merupakan pengembangan dari metode Two-Parameter Estimator. Dimana metode tersebut menggunakan parameter bias dari Ridge Regression dan parameter bias dari Liu Estimator. Studi kasus dalam skripsi ini menggunakan data penelitian pertumbuhan bayi di Kelurahan Namelo, Kota Masohi, Kabupaten Maluku Tengah oleh Wasilaine (2014). Diperoleh hasil bahwa pada kasus tersebut metode Modified Two-Parameter Estimator lebih efisien digunakan untuk mengatasi masalah multikolinearitas dibandingkan dengan metode Two-Parameter Estimator, Ridge Regression Estimator, dan Liu Estimator dengan melihat kriteria Mean Square Error (MSE).","Regression analysis is used to determine the relationship between a dependent variable and one or more independent variables.  There is an assumption in the regression analysis, called no multicollinearity. Multicollinearity is a condition when there is a linear relationship between some or all of the independent variables in the model. When this assumptions are not fulfilled, the least squareâ€™s estimation result would be invalid. 	To solve the multicollinearity problem in the linear regression model, the Modified Two-Parameter Estimator method was developed which is an extension of the Two-Parameter Estimator method. This method uses the bias parameter from the Ridge Regression and the bias parameter from the Liu Estimator. The case study of this undergraduate thesis uses data on infant growth ini Namelo, Masohi, Maluku Tengah by Wasiliane (2014). The result showed that in this case the Modified Two-Parameter Estimator method was more efficient to   solve the multicollinearity problem     compared to the Two-Parameter Estimator, Ridge Regression, and Liu Estimator methods based on the MSE criteria.","Kata Kunci : Multikolinearitas, Liu Estimator, Ridge Regression Estimator, Two-Parameter Estimator, Modified Two-Parameter Estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/207459,Optimisasi Portofolio Menggunakan Gabungan Optimisasi Kawanan Partikel dan Algoritma Genetik,"WILLIAM MARULI P, Prof. Drs. Subanar, Ph.D",2021 | Skripsi | S1 STATISTIKA,"Dalam melakukan investasi, seorang investor akan berusaha mendapatkan keuntungan yang sesuai dengan risiko yang dihadapinya, hal ini bisa dilakukan dengan melakukan diversifikasi portofolio. Diversifikasi portofolio sendiri dapat diartikan sebagai pembentukan portofolio sedemikan rupa sehingga aset yang dimiliki dapat memberi keuntungan dengan risiko minimum. Hal tersebut dilakukan dengan cara membagi bobot aset ke dalam portofolio, sebagaimana dikatakan pepatah, â€jangan letakkan semua telur dalam keranjang yang sama.â€ Gabungan algoritma genetik (GA) dan optimisasi kawanan partikel  (PSO) merupakan sebuah algoritma hibrida yang menggabungkan prinsip GA dan PSO dalam memebahkan sebuah permasalahan optimisasi. GA yang bekerja layaknya seleksi alam dalam menentukan solusi terbaik dari banyak solusi dalam berbagai permasalahan, digabungkan dengan PSO yang bekerja layaknya sebuah kawanan burung atau ikan yang saling bekerja sama dalam usahanya menemukan sebuah makanan (solusi). Optimisasi portofolio dengan kedua algoritma yang saling melengkapi tersebut diharapkan dapat membantu investor dalam menentukan bobot optimal sebuah portofolio. Skripsi ini akan membahas proses optimisasi portofolio menggunakan gabungan optimisasi kawanan partikel dan algoritma genetic pada studi kasus lima saham indeks LQ-45 jika dibandingkan dengan optimisasi menggunakan metode mean variance.","In investment, an investor will seek to gain profits which relevant to the risk heâ€™s willing to face, this can be done with a method called portfolio diversification. Portfolio diversification itself can be described as forming a portfolio so that an asset can give the most profit with minimum risk. This is done by splitting assets based on their weight in the portfolio, as the saying goes, â€œdonâ€™t put all eggs in the same basket.â€ The integration of genetic algorithm (GA) and particle swarm optimization (PSO) is a hybrid algorithm that combines the principal of both GA and PSO in solving optimization problems. GA works by applying natural selection rule for finding the best solution out of many in a set of problems, itâ€™s combined with PSO which works by applying the principles of flock of birds or fish in its quest to find food (solution). Portfolio optimization done by applying algorithms that complete each other is expected to help investors finding an optimal weight for a portfolio. This thesis will examine the process of portfolio optimization using the integration of particle swarm optimization and genetic algorithm on case study of five LQ-45 stocks when compared to optimization using the mean variance method.","Kata Kunci : Gabungan GA-PSO, Optimisasi Kawanan Partikel, Algoritma Genetik, Optimisasi Portofolio, Mean Variance, Sharpe Ratio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/207209,PENDEKATAN HYBRID JARINGAN SARAF TIRUAN DENGAN BLACK SCHOLES MENGGUNAKAN ALGORITMA RESILIENT BACKPROPAGATION DALAM PENENTUAN HARGA OPSI BELI TIPE EROPA,"MARIA HUMIRAS R, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Kemampuan model Black-Scholes yang ditemukan oleh Black dan Scholes (1973) dalam menghasilkan nilai harga opsi yang wajar dengan mengikuti proses stokastik tertentu dan gerak geometrik Brown telah digunakan selama lebih dari lima puluh tahun keberadaannya. Meskipun model Black-Scholes berguna dalam penentuan harga opsi, namun pada beberapa kasus, perbedaan yang signifikan tetap ada antara harga opsi pasar dengan nilai hasil perhitungannya. Sehingga, ditemukan metode baru yaitu metode hybrid jaringan saraf tiruan dengan Black Scholes (Boek, dkk. 1995). Metode ini didasarkan pada augmentasi model konvensional yaitu Black Scholes dengan jaringan saraf tiruan yang dilatih dengan melihat perbedaan antara model standar dengan harga opsi aktual. Penelitian ini dilakukan untuk menyelidiki akurasi model hibrida yang menggabungkan jaringan saraf tiruan dengan Black Scholes dalam menentukan harga opsi menggunakan resillient backpropagation yaitu metode yang memanfaatkan informasi gradien lokal untuk melakukan perubahan bobot dan bias jaringan. Selanjutnya, dilakukan perbandingan antara harga opsi yang diperoleh melalui metode Black-Scholes, Artificial Neural Network, dan Hybrid Artificial Neural Network-Black Scholes. Dengan menggunakan nilai RMSE (Root Mean Squared Error) sebagai performa metode penentuan harga opsi, diperoleh hasil bahwa model Hybrid Artificial Neural Network-Black Scholes lebih baik dibandingkan dengan model Black-Scholes dan Artificial Neural Network.","The ability of the Black-Scholes model discovered by Black and Scholes (1973) in generating reasonable fair value for option prices by following certain stochastic processes and Brownian geometric motions has been used for more than fifty years of its existence. Despite usefulness of Black-Scholes model, in some cases, significant discrepancies remain between the market option prices and their calculated values. Thus the new hybrid method of artificial neural networks with Black Scholes was found (Boek, dkk. 1995). This method is based on the augmentation of a conventional model Black Scholes with an Artificial Neural Network trained on the difference between the standard model and the actual option price. This study was conducted to investigate the accuracy of a hybrid model that combines artificial neural networks with Black Scholes in determining option prices using resilient backpropagation, which is a method that utilizes local gradient information to make changes to network weights and biases. Furthermore, a comparison is made between the option prices obtained through the Black-Scholes method, Artificial Neural Network, and Hybrid Artificial Neural Network-Black Scholes. By using the RMSE (Root Mean Squared Error) value as the performance of the option pricing method, the result is that the Hybrid Artificial Neural Network-Black Scholes model is better than the Black-Scholes and Artificial Neural Network models.","Kata Kunci : Harga opsi beli, Black-Scholes, Hybrid Artificial Neural Network-Black Scholes, resillient backpropagation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/197484,ANALISIS SENTIMEN DENGAN RECURRENT NEURAL NETWORK DAN GATED RECURRENT UNIT PADA DATA IMBALANCED,"RANA ARIBAH, Dr. Herni Utami, S.Si., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Media sosial merupakan wadah yang biasa digunakan untuk mengungkapkan suatu opini. Salah satu media sosial yang populer adalah twitter. Sosial media dapat menjadi wadah perusahaan berbagi informasi dan mendapat feedback dari konsumen dengan cara yang mudah dan cepat. Kebutuhan masyarakat akan layanan internet terus bertambah. First Media merupakan salah satu perusahaan yang menyediakan jasa tersebut. Bagi perusahaan, opini konsumen penting dianalisis untuk evaluasi dan pengembangan produk. Diperlukan metode untuk mengklasifikasikan tanggapan ke dalam sentimen secara otomatis. Pada penelitian ini menggunakan data tweet tentang First Media, dimana tweet merupakan data sekuensial, yaitu data yang mempunyai urutan. Metode yang dapat memproses data sekuensial adalah Recurrent Neural Network dan Gated Recurrent Unit. Pada penelitian ini dilakukan teknik random oversampling untuk menangani data tidak seimbang. Berdasarkan eksperimen dimensi word embedding, jumlah neuron, fungsi aktivasi, dan penggunaan dropout pada data latih yang seimbang dan tidak seimbang menggunakan kedua metode tersebut, diperoleh hasil bahwa pembentukan model pada metode Gated Recurrent Unit dengan pelatihan menggunakan data tidak seimbang mempunyai performa yang paling baik dengan rata-rata akurasi sebesar 93,27%, presisi sebesar 76,02%, sensitifitas sebesar 61,56%, spesifisitas sebesar 97,45%, dan f-1 score sebesar 67,90%.","Social media is a platform which is regularly used to express opinion(s). One of the well-known social media platforms is Twitter. The implication of social media are vary, one of them are for companies to share information and provide feedback from customers in an effortless and fast way. Consequently, this demand will increase the public's need for internet service. First Media is one of the leading companies that provides internet service in Indonesia. In order to evaluate their service and to develop their company, it is necessary to analyze customers opinion. A new method is needed to automatically classify responses into sentiments. This study is conducted using tweet about First Media, in which involving sequential data where the data have order. Methods that are able to process sequential data are Recurrent Neural Networks and Gated Recurrent Units. In our study, random oversampling is applied to handle imbalanced data. Based on experiments word embedding dimensions, number of neurons, activation function, and dropout usage on balanced and imbalanced data training using both methods, it is found that the modeling of the Gated Recurrent Unit method with training using imbalanced data have the best performance with mean of the accuracy is 93.27%, precision 76.02%, sensitivity 61.56%, specificity 97.45%, and f-1 score 67.90%.","Kata Kunci : analisis sentimen, recurrent neural network, gated recurrent unit, random oversampling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/202870,PEMODELAN TOPIK UNTUK MEDIA SOSIAL MENGGUNAKAN  BITERM TOPIC MODEL,"MARIA GAETANA A T, Yunita Wulan Sari, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Bermedia sosial merupakan aktivitas berinternet yang paling digemari oleh pengguna internet Indonesia. Salah satu media sosial yang paling sering digunakan adalah Twitter. Pesatnya pertumbuhan media sosial tersebut menghasilkan informasi berbasis teks yang semakin besar. Dengan banyaknya data berbasis teks, metode analisis teks menyediakan cara yang efektif untuk menggali informasi penting dari data tersebut. Salah satu metode analisis teks yang dapat digunakan adalah pemodelan topik. Pemodelan topik merupakan analisis teks yang bermanfaat dalam pemodelan data tekstual, dengan tujuan menemukan topik yang tersembunyi di dalamnya. Pemodelan topik yang akan dibahas dan digunakan untuk menganalisis data pada skripsi ini adalah Biterm Topic Model (BTM). BTM merupakan pengembangan dari Latent Dirichlet Allocation (LDA) yang secara efektif dapat memodelkan topik pada teks pendek seperti Twitter, berdasarkan pada kumpulan biterm dari keseluruhan dokumen. Estimasi parameter yang digunakan dalam model adalah metode Bayesian. Perhitungan estimasi dari distribusi posterior menggunakan algoritma Gibbs sampling. Penerapan BTM pada skripsi ini dilakukan pada data tekstual berupa postingan dari tiga akun berita di Twitter yaitu @detikcom, @CNNIndonesia, dan @BBCIndonesia untuk mengetahui berita apa yang dominan dibicarakan akun-akun berita tersebut dalam periode tertentu. Hasil pemodelan topik ini berupa proporsi topik pada korpus, probabilitas kata pada setiap topik, dan proporsi topik dalam dokumen/tweets.","Social media is the most popular internet activity among Indonesian internet users. One of the most used social media is Twitter. The rapid growth of social media produces text-based information that is always increasing in amount. With the large amount of text-based data, text analytics provide an effective way to dig useful information from that. One of the text analytic methods that can be used is topic modeling. Topic model is a useful text analysis in modeling textual data, in order to find the hidden topics in it. Topic models that will be discussed and used to analyze data in this thesis is Biterm Topic Model (BTM). BTM is an extension of Latent Dirichlet Allocation (LDA) which can effectively model topics on short texts such as Twitter, based on collections of biterm on the entire documents. Parameter estimation used in this model is the Bayesian method. The estimated calculation of the posterior distribution using Gibbs sampling algorithm. In this paper, BTM probability model is applied for textual data from three news accounts in Twitter that is @detikcom, @CNNIndonesia, and @BBCIndonesia in order to know what dominant topics were being talked by those accounts in a given period. The outcome of this topic model is proportion of topic in corpus, probability of word in each topic, and proportion of topic in document/tweets.","Kata Kunci : analisis teks, pemodelan topik, Biterm Topic Model, Gibbs sampling, Twitter"
http://etd.repository.ugm.ac.id/home/detail_pencarian/206198,Penerapan Estimasi Liu dalam Menangani Masalah Multikolinearitas pada Model Regresi Logistik dengan Pembatasan Linear Stokastik,"DENIS ADITYA, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Analisis regresi logistik merupakan salah satu analisis statistika yang memodelkan hubungan antara variabel independen dengan variabel dependen yang berupa data dikotomi. Pada pembentukan model regresi logistik, komponen parameter diestimasi menggunakan metode Maximum Likelihood Estimation (MLE). Namun, metode MLE kurang tepat digunakan jika terdapat ketidaksesuaian hipotesis terhadap hasil analisis yang menyebabkan peneliti kesulitan untuk memilih antara mempertahankan hipotesisnya atau menerima hasil analisis. Metode MLE juga kurang tepat digunakan jika terdapat masalah multikolinearitas pada model yang meningkatkan variansi dan menghasilkan estimator yang tidak akurat. Untuk menangani ketidaksesuaian hipotesis terhadap hasil analisis, digunakan metode Stochastic Restriction Maximum Likelihood Estimation (SRMLE) yaitu dengan memberikan pembatasan linear stokastik pada estimator MLE. Sedangkan untuk menangani masalah multikolinearitas, digunakan metode Liu Estimation (LE) yaitu dengan memberikan penalti pada estimator MLE. Jika suatu model regresi terdapat 2 masalah tersebut, digunakan metode Stochastic Restriction Liu Estimation (SRLE) yaitu dengan menerapkan metode LE pada estimator SRMLE. Pada skripsi ini, metode SRLE akan diaplikasikan untuk memodelkan status pinjaman online yang dikelola oleh LendingClub dan akan dibandingkan dengan metode MLE, SRMLE, LE berdasarkan nilai Mean Square Error (MSE).","Logistic regression analysis is a statistical analysis that models the relationship between the independent variable and the dependent variable in the form of dichotomous data. In the formation of the logistic regression model, the parameter components are estimated using the Maximum Likelihood Estimation (MLE) method. However, MLE method is not appropriate to use if there is a mismatch between the hypothesis and the results of the analysis, which makes it difficult for researchers to choose between defending their hypothesis or accepting the results of the analysis. MLE method is also less appropriate to use if there is a multicollinearity problem in the model that increases the variance and produces an inaccurate estimator. To deal with the discrepancy between the hypothesis and the results of the analysis, Stochastic Restriction Maximum Likelihood Estimation (SRMLE) method is used, by providing a stochastic linear restriction on the MLE estimator. Meanwhile, to deal with multicollinearity problems, Liu Estimation (LE) method is used, by giving a penalty on the MLE estimator. If a regression model there are 2 problems, Stochastic Restriction Liu Estimation (SRLE) method is used, by applying the LE method to the SRMLE estimator. In this thesis, SRLE method will be applied to model the online loan status managed by LendingClub and will be compared with the MLE, SRMLE, LE methods based on Mean Square Error (MSE) value.","Kata Kunci : Model regresi logistik, Maximum Likelihood Estimation, Pembatasan Linear Stokastik, Stochastic Restriction Maximum Likelihood Estimation, Multikolinearitas, Liu Estimation, Stochastic Restriction Liu Estimation, Mean Square Error"
http://etd.repository.ugm.ac.id/home/detail_pencarian/194944,PEMILIHAN MODEL PADA DATA LONGITUDINAL DENGAN DATA HILANG MENGGUNAKAN MISSING LONGITUDINAL INFORMATION CRITERION (MLIC),"PARAMITHA KURNIAJATI, Zulaela, Dipl.Med.Stats., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Data hilang sering muncul di bidang klinis dan epidemiologi, khususnya dalam studi longitudinal yang menggabungkan antara pengamatan cross section dan time series. Persamaan estimasi tergeneralisasi (GEE) telah menjadi alat yang populer untuk analisis regresi marjinal dengan data longitudinal. Namun, metodologi pemilihan model untuk GEE belum dikembangkan secara sistematis untuk memungkinkan analisis dengan data hilang. Maka diusulkan Missing Longitudinal Information Criterion (MLIC) untuk pemilihan mean model berdasarkan pada estimasi persamaan estimasi tergeneralisasi terboboti (WGEE) ketika hasil data adalah subjek dengan pola data hilang yang monoton dengan asumsi hilangnya secara acak (MAR). Pada perangkat lunak R telah tersedia paket wgeesel yang mengimplementasikan model marjinal dan menyediakan kriteria informasi yang ada untuk pemilihan model WGEE pada mean marjinal atau struktur korelasi. Performa dari metode ini akan ditunjukkan melalui simulasi dan analisis menggunakan data nyata yaitu data angka prevalensi pemakaian kontrasepsi (CPR) di 33 provinsi di Indonesia pada tahun 2008-2012.","Missing data arise frequently in clinical and epidemiological fields, particularly in longitudinal studies that combine cross section and time series observations. Generalized Estimating Equation (GEE) has been a popular tool for marginal regression analysis with longitudinal data. However, model selection methodologies for GEE have not been systematically developed to allow for missing data. So, the Missing Longitudinal Information Criterion (MLIC) for selection of the mean model in Weighted Generalized Estimating Equation (WGEE) when the outcome data are subject with monotone missing data pattern assuming Missing at Random ( MAR) are proposed. R software provides a wgeesel package which implements marginal model fitting and provide existing information criteria for WGEE model selection on marginal mean or correlation structures. The performance of this method will be demonstrated through simulation and analysis of real data on Contraceptive Prevalence Rate (CPR) in 33 provinces in Indonesia during 2008-2012","Kata Kunci : persamaan estimasi tergeneralisasi terboboti, data longitudinal, data hilang, pemilihan model, wgeesel, weighted generalized estimating equation, longitudinal data, missing data, model selection"
http://etd.repository.ugm.ac.id/home/detail_pencarian/196747,METODE CONVENTIONAL TUNING DAN FAST TUNING DALAM EFISIENSI PENENTUAN HYPERPARAMETER UNTUK KLASIFIKASI TEKS,"NAUFAL HUSAIN REZA, Yunita Wulan Sari,S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Dalam machine learning, penentuan hyperparameter menjadi permasalahan yang cukup kompleks. Banyak pendekatan telah dicoba untuk bisa menentukan nilai hyperparameter dengan optimal, namun cenderung memakan waktu komputasi. Banyaknya waktu komputasi yang terbuang tentu menjadikan kinerja menjadi tidak efektif. Pada skripsi ini, diajukan sebuah metode untuk mempersingkat waktu komputasi yang disebut fast tuning. Fast tuning merupakan suatu metode baru yang dapat mempersingkat waktu komputasi sebagai pengembangan dari metode sebelumnya yaitu conventional tuning. Metode ini bekerja dengan menggunakan sistem grid search dan diterapkan pada kasus klasifikasi teks dengan menggunakan metode K-Nearest Neighbor (KNN) dan Best Match 25 (BM25), dengan tiga hyperparameter yang perlu ditentukan nilainya. Berdasarkan hasil analisisdengan beberapa percobaan diperoleh bahwa metode ini mampu mengurangi waktu komputasi secara signifikan.","In machine learning, hyperparameters tuning is a quite complex problem. Several approaches have been adopted for optimal hyperparameter tuning, but they tend to take computation time. The amount of wasted computing time certainly makes performance ineffective. In this thesis, a method is proposed to shorten the computation time, called fast tuning. Fast tuning is a new method that can make efficient computation as a development from previous method that called conventional tuning. This method works by using a grid search system and applied in text classification using the K-Nearest Neighbor (KNN) and Best Match 25 (BM25) methods, with three hyperparameters whose values need to be tuned. After some analysis with some experiments we can conclude that this method is able to reduce computation time significantly.","Kata Kunci :  KNN, BM25, grid search, hyperparameter, fast tuning, conventional tuning"
http://etd.repository.ugm.ac.id/home/detail_pencarian/207245,Penanganan Data Tidak Seimbang Menggunakan Borderline Synthetic Minority Oversampling Technique (Borderline-SMOTE) pada Analisis Klasifikasi,"IKA RAHMATUNNISA, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Data dan informasi terus mengalami pertambahan seiring dengan berkembangnya tekonologi digital. Ketersediaan data menjadi semakin banyak dan kompleks. Data tidak seimbang merupakan masalah yang sering muncul dalam analisis klasifikasi. Data tidak seimbang merupakan kondisi di mana distribusi kelas data yang tidak seimbang. Saat melakukan analisis klasifikasi pada data tidak seimbang, model klasifikasi yang dihasilkan cenderung memprediksi data yang berasal dari kelas mayoritas mengakibatkan performa klasifikasi menjadi kurang baik. Metode Borderline-SMOTE dapat diterapkan untuk menangani permasalahan data tidak seimbang. Metode Borderline-SMOTE merupakan metode yang dikembangkan dari metode SMOTE. Borderline-SMOTE merupakan metode oversampling yang menyeimbangkan data dengan membentuk instance sintetis dari kelas minoritas di area borderline. Pada skripsi ini dilakukan penerapan metode Borderline-SMOTE pada data yang tidak seimbang dengan menggunakan metode klasifikasi Random Forest dan Naive Bayes pada dua dataset. Dari analisis yang dilakukan penanganan data tidak seimbang dengan Borderline-SMOTE secara umum dapat meningkatkan performa klasifikasi metode Random Forest dan Naive Bayes.","Data and information continue to increase along with the development of digital technology. The availability of the data is becoming more and more complex. Imbalanced data is a problem that is often found in classification analysis. Imbalanced data is a condition where the distribution of data classes is not balanced.  When classifying imbalanced data, the classification model tends to predict data that comes from the majority class resulting in poor classification performance. Borderline-SMOTE can be used to solve that imbalanced data problem. Borderline-SMOTE method is a method developed form the SMOTE method. Borderline-SMOTE is an oversampling method that creates a synthetic instance of the minority class in the borderline area. In this thesis, the Borderline-SMOTE method is applied to imbalanced data using the Random Forest and Naive Bayes classification methods on two datasets. From the analysis, we can conclude that handling imbalanced data with Borderline-SMOTE method can improve the classification performance of Random Forest and Naive Bayes methods.","Kata Kunci : data tidak seimbang, klasifikasi, Borderline-SMOTE, oversampling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/199065,Penerapan Kombinasi SMOTE dan Tomek Links untuk Klasifikasi Data Tidak Seimbang dengan Metode Random Forest,"RONA AGUSTIKA, Vemmie Nastiti Lestari, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Perkembangan pesat di bidang teknologi berbanding lurus dengan ketersediaan data yang menjadi lebih banyak dan kompleks. Namun seringkali ditemui data yang memiliki distribusi kelas yang tidak seimbang. Melakukan klasifikasi pada data tidak seimbang mengakibatkan model klasifikasi yang dihasilkan cenderung memprediksi kelas mayoritas dan mengabaikan kelas minoritas. Terdapat beberapa metode untuk mengatasi data tidak seimbang antara lain oversampling dan undersampling. SMOTE merupakan metode oversampling yang menyeimbangkan data dengan membuat instance sintetis untuk kelas minoritas. Sedangkan Tomek Links merupakan metode undersampling yang menghapus data dari kelas mayoritas yang memiliki karakteristik yang serupa. Namun Tomek Links hanya menghapus instance yang didefinisikan sebagai ""Tomek Links"" sehingga data yang dianalisis tidak dapat seimbang dan dalam penerapannya metode tersebut dikombinasikan dengan metode lain. Pada skripsi ini dilakukan penerapan metode kombinasi SMOTE dan Tomek Links pada data yang tidak seimbang dengan menggunakan metode klasifikasi Random Forest pada tiga dataset. Dari analisis yang dilakukan, diperoleh kesimpulan bahwa penerapan metode kombinasi SMOTE dan Tomek Links menghasilkan performa yang lebih baik dari pada metode SMOTE dan metode Tomek Links untuk analisis klasifikasi Random Forest.","The rapid development in technology is directly proportional to the availability of data which is becoming more numerous and complex. However, we often encounter data that has an unbalanced class distribution. Classifying the unbalanced data results in the resulting classification model that tends to predict the majority class and ignore the minority class. There are several methods to deal with unbalanced data, including oversampling and undersampling. SMOTE is an oversampling method that balances data by creating synthetic instances for minority classes. Meanwhile, Tomek Links is an undersampling method that removes data from majority classes that have similar characteristics. However, Tomek Links only removes instances defined as ""Tomek Links"" so that the analyzed data cannot be balanced. In practice, this method is combined with other methods. In this thesis, the application of the combination of SMOTE and Tomek Links method is carried out on unbalanced data using the Random Forest classification method on three dataset. From the analysis, we can conclude that the combination of SMOTE and Tomek Links method results in better performance than SMOTE and Tomek Links for Random Forest classification analysis.","Kata Kunci : klasifikasi, data tidak seimbang, SMOTE, Tomek Links, Random Forest"
http://etd.repository.ugm.ac.id/home/detail_pencarian/198557,Estimator Liu pada Regresi Logistik Multinomial,"RACHEL CHIEKO S, Drs. Zulaela, Dipl. Med. Stats., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Analisis regresi logistik digunakan untuk menggambarkan hubungan antara variabel independen dan variabel dependen yang bersifat kategotik. Terdapat asumsi yang harus dipenuhi untuk melakukan analisis regresi logistik multinomial, yaitu tidak ada multikolinearitas atau tidak ada korelasi antar variabel independen. Adanya multikolinearitas akan menyebabkan estimasi parameter regresi menjadi tidak akurat yang berakibat pada nilai rata-rata kuadrat error menjadi besar. Sayangnya, metode maksimum likelihood yang digunakan untuk mengestimasi parameter tidak dapat menangani masalah multikolinearitas yang ada. Akan tetapi, terdapat estimator lain yang dapat menangani masalah multikolinearitas, salah satunya adalah estimator Liu. Estimator Liu pada regresi logistik multinomial akan diterapkan pada data kerang abalon dari UCI Machine Learning Repository untuk melihat hubungan antara sex kerang abalon dengan ukuran fisik kerang abalon sebagai variabel independen yang memiliki multikolinearitas. Selanjutnya, akan dibandingkan nilai rata-rata kuadrat error dari estimator Ridge dan estimator Liu. Pada akhirnya, hasil menunjukkan bahwa nilai kuadrat error dari estimator Liu lebih kecil daripada nilai kuadrat error dari estimator Ridge. Terbukti bahwa estimator Liu dapat menangani masalah multikolinearitas.","Logistic regression is used to describe the relationship between independent variables and categorical dependent variable. There is an assumption that must to be fulfilled to perform multinomial logistic regression, that is no multicollinearity or no correlation between the independent variables. The existence of multicollinearity will cause the inaccurate regression parameter estimation which result in a large mean square error (MSE). Unfortunately, maximum likelihood method that used to estimate parameter can not conquer the multicollinearity problem. However, there is another estimator that conquer the multicollinearity problem, one of them is Liu estimator. Liu estimator for multinomial logistic regression will be applied to abalone dataset from the UCI Machine Learning Repository to observe the relationship between abalone's sex and abalone's physical measurement as the independent variables that has multicollinearity. Hereinafter, the MSE of Ridge estimator and Liu estimator will be compared. In the end, the obtained result indicate that the MSE of Liu estimator is smaller than the MSE of Ridge estimator. It is proven that the Liu estimator conquer multicollinearity problem.","Kata Kunci : regresi logistik multinomial, multikolinearitas, maksimum likelihood, estimator Ridge, estimator Liu, rata-rata kuadrat error"
http://etd.repository.ugm.ac.id/home/detail_pencarian/196514,OPTIMISASI PERKIRAAN HARGA SAHAM MENGGUNAKAN PENDEKATAN HYBRID HARMONY SEARCH - ARTIFICIAL NEURAL NETWORK,"GHINA HANIFAH,  Prof. Dr. rer. nat. Dedi Rosadi, S.Si., M.Sc",2021 | Skripsi | S1 STATISTIKA,"Artificial neural network merupakan metode yang dapat digunakan untuk memprediksi salah satunya dalam bidang ekonomi, yaitu memprediksi harga penutupan saham. Algoritma yang paling umum digunakan pada artificial neural network yaitu backpropagation standard. Metode artificial neural network memiliki beberapa kekurangan, yaitu jaringan syaraf tiruan tidak mampu memilih banyaknya neuron dan variabel masukkan mana yang dapat meminimalkan galat. Harmony search merupakan salah satu metode metaheuristik yang dapat menyelesaikan masalah pengoptimalan.  Harmony search dapat digunakan untuk mengatasi kekurangan dari jaringan syaraf tiruan yaitu dapat menentukan banyaknya neuron pada lapisan tersembunyi dan memilih variabel masukkan mana yang dapat meminimalkan galat. Dengan menggunakan metode hybrid harmony search-artificial neural network yang dilakukan pada data pelatihan, didapatkan nilai MSE dan MAPE untuk data pelatihan dan data pengujian pada metode hybrid harmony search-artificial neural network lebih kecil dibandingkan metode artificial neural network.","Artificial neural network is a method that can be used to predict one of them in the economic field, namely predicting the closing price of shares. The algorithm most commonly used in artificial neural networks is the backpropagation standard. The artificial neural network method has several shortcomings, namely the artificial neural network is unable to choose the number of neurons and which input variables can minimize the error. Harmony search is a metaheuristic method that can solve optimization problems.  Harmony search can be used to overcome the shortcomings of an artificial neural network, which is to determine the number of neurons in the hidden layer and choose which input variables can minimize the error. By using the hybrid harmony search-artificial neural network method, the MSE and MAPE values for the training data and the test data for the hybrid harmony search-artificial neural network method are smaller than the artificial neural network method.","Kata Kunci : Harmony search, Backpropagation standard, Metaheuristik, Indikator Teknikal, Prediksi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/195494,OPTIMISASI PORTOFOLIO MEAN-SEMIVARIANCE BERDASARKAN ANALISIS KLASTER K-MEANS,"Fitria Rahmadani Harahap, Drs. Zulaela, Dipl.Med.Stats, M.Si.",2021 | Skripsi | S1 STATISTIKA,"Pada masa sekarang kegiatan investasi sudah tidak asing untuk dilakukan. Investasi merupakan kegiatan mengalokasikan atau menanamkan sumberdaya (resources) saat ini (sekarang), dengan harapan mendapatkan manfaat di kemudian hari (masa datang).  Kegiatan investasi dapat dilakukan pada aset riil dan aset keuangan. Terdapat dua bagian utama yang diharapkan dalam investasi yaitu tingkat pengembalian (return) dan risiko (risk). Risiko dapat diminimumkan dengan cara melakukan pembentukan suatu portofolio. Portofolio merupakan kumpulan aset baik perorangan maupun perusahaan yang disusun untuk mencapai suatu tujuan investasi.  Salah satu metode pembentukan portofolio yang dapat digunakan yaitu dengan melakukan optimisasi portofolio mean-semivariance berdasaarkan analisis klaster K-Means. Optimisasi portofolio mean-semivariance tidak memerlukan asumsi apapun. Pada skripsi ini menggunakan data saham bulanan pada periode Oktober 2015 sampai dengan Oktober 2020. Saham-saham yang diamati yaitu BBTN, LPPF, WIKA, MNCN, EXCL, TLKM, UNTR dan UNVR. Pengukuran kinerja portofolio diukur dengan menggunakan Risk Adjusted Return. Kesimpulan yang diperoleh dari hasil penelitian menggunakan saham-saham tersebut yaitu optimisasi portofolio mean-semivariance berdasarkan analisis klaster K-Means lebih baik digunakan dibanding dengan mean-semivariance tanpa analisis klaster K-Means.","At this time investment activities are often to be carried out. Investment is an activity of allocating or investing resources now, with the hope of getting benefits in the future. Investment activities can be carried out on real assets and financial assets. There are two main parts expected in investment, return and risk. Risk can be minimized by forming a portfolio. A portfolio is a collection of assets, both individuals and companies, which are arranged to achieve an investment goal. One of the methods of portfolio formation that can be used is to optimize the mean-semivariance portfolio based on the K-Means cluster analysis. Mean-semivariance portfolio optimization does not require any assumptions. This thesis uses monthly stock data for the period October 2015 to October 2020. The stocks observed are BBTN, LPPF, WIKA, MNCN, EXCL, TLKM, UNTR and UNVR. Portfolio performance measurement is measured using Risk Adjusted Return. The conclusion obtained from the research results using these stocks is that the optimization of mean-semivariance portfolios based on K-Means cluster analysis is better to use than mean-semivariance without K-Means cluster analysis.","Kata Kunci : portofolio, analisis klaster, K-Means, mean-semivariance, heuristic approach"
http://etd.repository.ugm.ac.id/home/detail_pencarian/198057,Principal Component Liu-Type Estimator untuk Mengatasi Masalah Multikolineritas pada Analisis Regresi Logistik,"RENO TRI PRIMADANI, Dr. Gunardi, M.Si",2021 | Skripsi | S1 STATISTIKA,Analisis regresi adalah analisis yang digunakan untuk mengetahui ada tidaknya hubungan antara satu variabel yang disebut variabel dependen dengan satu atau beberapa variabel yang disebut variabel independen sehingga dapat ditaksir atau diramalkan. Terdapat beberapa model analisis regresi salah satunya adalah analisis regresi logistik. Metode yang sering digunakan untuk analisis regresi logistik adalah Maximum Likelihood Estimator. Namun metode tersebut memerlukan beberapa asumsi yang harus dipenuhi salah satunya adalah multikolinearitas. Pada skripsi ini akan dibahas mengenai metode estimasi parameter untuk mengatasi masalah multikolineritas pada regresi logistik.  Metode yang digunakan adalah Principal Component Liu-Type Estimator (PCLTE) yang merupakan gabungan dari Principal Component Analysis (PCA) dan Liu-Type Estimator (LTE). Studi kasus ini menggunakan data kemampuan melunasi kredit dan faktor-faktor yang mempengaruhinya pada BPRS Khasanah Ummat Purwokerto. Hasilnya menunjukkan bahwa metode PCLTE menghasilkan nilai (Mean Square Error) MSE lebih kecil dibandingkan dengan metode yang lain.,"Regression analysis is an analysis that used to determine whether there is  relationship between a variable called  the dependent variable and one or more variables  called the independent variable so that it can estimated or predicted. There are several regression analysis models, one of them is logistic regression analysis. The method often used for logistic regression analysis is the Maximum Likelihood Estimator. However, that method need some assumptions that have to be fulfilled, like there is no multicollinearity. This paper will be discussed about the parameter estimation method to solve multicollinearity problem in logistic regression. The method used is Principal Component Liu-Type Estimator (PCLTE) which is a combination of Principal Component Analysis (PCA) and Liu-Type Estimator (LTE). This case study uses data on the ability to repay credit and the factors that influence it at BPRS Khasanah Ummat Purwokerto. The results show that the PCLTE method produces smaller MSE (Mean Square Error) values compared to other methods.","Kata Kunci : analisis regresi, regresi logistik, multikolinearitas, estimator Liu-Type, Principal Component Liu-Type Estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/206508,"Analisis Klasifikasi Tutur Menggunakan Convolutional Neural Network, Deep Long Short Term Memory, dan Regresi Logistik Multinomial","MUHAMMAD ALLAFA BUDI PRATAMA, Dr. Abdurakhman, S.Si., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Kesalahan pada Â¬auto-generated subtitle Youtube menggunakan Bahasa Indonesia masih sering terjadi dan merugikan penikmat Youtube yang mengalami gangguan pendengaran sehingga pembelajaran mengenai klasifikasi tutur dilakukan. Penelitian ini bertujuan mempelajari klasifikasi tutur dengan menggunakan data suara pengucapan kata barat, selatan, timur, dan utara. Penelitian ini melalui dua tahapan besar yaitu ekstraksi ciri dan analisis klasifikasi. Ekstraksi ciri data suara yang digunakan yaitu ekstraksi ciri Mel Frequency Ceptrum Coefficient (MFCC). Analisis klasifikasi yang digunakan yaitu Convolutional Neural Network (CNN), Deep Long Short Term Memory (DLSTM), dan regresi logistik multinomial. Ketiga metode klasifikasi ini akan dibandingkan performa modelnya berdasarkan nilai akurasi dan loss function dengan tiap metode dilatih dengan 3 skenario pembagian data training, validation, dan testing. Pada analisis CNN, dibangun 597.316 parameter untuk dilatih dan menghasilkan klasifikasi terbaik yaitu pada skenario pembagian jumlah data training, data validation, dan data testing sebesar 64:16:20 dengan nilai akurasi sebesar 97,31% pada data training, 91,11% pada data validation, dan 99,07% pada data testing. Pada analisis DLSTM, dibangun 215.644 parameter untuk dilatih dan menghasilkan klasifikasi terbaik yaitu pada skenario pembagian jumlah data training, data validation, dan data testing sebesar 64:16:20 dengan nilai akurasi sebesar 91,31% pada data training, 84,63% pada data validation, dan 98,24% pada data testing. Pada analisis regresi logistik multinomial, dibentuk 3 model berdasarkan skenario dan menghasilkan klasifikasi terbaik yaitu pada skenario pembagian jumlah data training, data validation, dan data testing sebesar 64:16:20 dengan nilai akurasi sebesar 27,06% pada data training, 24,03% pada data validation, dan 24,92% pada data testing.Didapatkan kesimpulan hasil klasifikasi menggunakan metode CNN lebih baik jika dibandingkan dengan metode DLSTM, terbukti dari plot akurasi dan loss function yang menunjukan hasil klasifikasi DLSTM kurang stabil pada skenario pembagian jumlah data training, data validation, dan data testing sebesar 64:16:20.","Errors in auto-generated Youtube subtitles using Indonesian still often occur and are detrimental to Youtube viewers who have hearing loss, so learning about speech classification is carried out. This study aims to study speech classification by using voice data for the pronunciation of the words west, south, east, and north. This research went through two major stages, namely feature extraction and classification analysis. Feature extraction of voice data used is feature extraction of Mel Frequency Ceptrum Coefficient (MFCC). The classification analysis used is Convolutional Neural Network (CNN), Deep Long Short Term Memory (DLSTM), and multinomial logistic regression. The three classification methods will compare the performance of the model based on the value of accuracy and loss function with each method being trained with 3 scenarios for sharing data, training, validation, and testing. In the CNN analysis, 597,316 parameters were built to be trained and resulted in the best classification, namely in the scenario of dividing the amount of training data, validation data, and testing data of 64:16:20 with an accuracy value of 97.31% on training data, 91.11% on data validation, and 99.07% on data testing. In the DLSTM analysis, 215,644 parameters were built to be trained and produced the best classification, namely in the scenario of dividing the amount of training data, validation data, and testing data of 64:16:20 with an accuracy value of 91.31% on training data, 84.63% on data validation, and 98.24% on data testing. In multinomial logistic regression analysis, 3 models were formed based on scenarios and resulted in the best classification, namely in the scenario of dividing the amount of training data, data validation, and data testing of 64:16:20 with an accuracy value of 27.06% on training data, 24.03 % on data validation, and 24.92% on data testing. It is concluded that the classification results using the CNN method are better than the DLSTM method, as evidenced by the accuracy and loss function plots which show that the DLSTM classification results are less stable in the scenario of dividing the amount of training data, data validation, and data testing of 64:16:20.","Kata Kunci : Speech Classification, Convolutional Neural Network, Deep Long Short Term Memory, Regresi logistik multinomial, MFCC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/206021,ANALISIS SENTIMEN PERUSAHAAN GOOGLE MENGGUNAKAN BI-DIRECTIONAL LONG SHORT TERM MEMORY PADA DATA TIDAK SEIMBANG,"VISCO OOI'NCO, Dr. Danang Teguh Qoyyimi, S.Si. M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Perkembangan teknologi telah mengubah cara masyarakat mengonsumsi informasi. Media informasi yang dulunya berupa media cetak seperti koran dan majalah telah berubah menjadi digital. Selain itu, informasi yang diterbitkan oleh ratusan penerbit dapat diakses dalam waktu yang singkat. Melakukan analisis manual pada data tekstual yang terbit setiap saat seperti itu merupakan pekerjaan yang sulit dan membutuhkan waktu yang sangat banyak. Analisis sentimen merupakan proses analisis yang diotomatisasi dengan menggunakan komputasi untuk menemukan sentimen positif, netral atau negatif dari data tekstual. Analisis sentimen digunakan secara luas untuk mendapatkan persepsi atau opini dari komentar sosial media, hasil survei kuesioner dan ulasan suatu produk. Dalam beberapa tahun ini, deep learning mendapat perhatian para akademisi karena performa modelnya yang tinggi dalam menangani berbagai hal. Salah satu model Deep Learning yang cukup populer digunakan dalam menangani data teks seperti analisis sentimen adalah Long Short Term Memory, selanjutnya disingkat LSTM. LSTM merupakan perkembangan dari Recurrent Neural Network yang mengatasi masalah kesulitan dalam menyimpan informasi yang terlalu lama (long-term dependency problem). LSTM ini dapat digunakan untuk melakukan analisis pada informasi teks seperti judul berita dengan performa yang tinggi karena memiliki gates yang dapat menyimpan dan menyeleksi informasi. Pada penelitian ini, dilakukan pembentukan model klasifikasi sentimen dari judul berita Google menggunakan LSTM dan variannya yaitu Bi-LSTM, Stacked LSTM dan Stacked Bi-LSTM. Selain itu, teknik oversampling digunakan dalam menangani data yang tidak seimbang. Berdasarkan hasil pembentukan model menggunakan beberapa kombinasi dimensi word embedding, jumlah neuron dan penggunakan dropout, diperoleh hasil bahwa model pada stacked Bi-LSTM pada data seimbang memiliki performa paling baik dengan nilai f1-score (micro) sebesar 89,66% ; f1-score (macro) sebesar 87,26% dan f1-score (weighted) sebesar 89,57%.","Technological developments are changing the way we consume information. Informative media that used to be in the form of print media such as newspapers and magazines have turned into digital. In addition, we can also access information published by hundreds of publishers in a short time. Performing manual analysis on such huge amount of textual data is a difficult task and takes a lot of time. Sentiment analysis is an automated analysis process using computation to find positive, neutral or negative sentiments from textual data. Sentiment analysis is widely used to get perceptions or opinions from social media comments, questionnaire survey results and product reviews.  In recent years, deep learning has received the attention of academics because of high performance of its models in dealing various things. One of deep learning models thats is quite popular used in handling text data such as sentiment analysis is Long Short Term Memory, namely LSTM. LSTM is a type of Recurrent Neural Network architecture that is refined to overcome the problem of difficulty in storing information that is too old (long-term dependency problem). LSTM can be used to analyze textual information such as news headlines with high performance because it has gates that store and filter information. In this study, sentiment classification models were developed from Google news headline using LSTM and its variants, namely Bi-LSTM, Stacked LSTM and Stacked Bi-LSTM. In addition, oversampling technique is used in handling imbalanced data. After developing models using several combinations of word embedding dimensions, number of neurons and the use of dropout, the results show that stacked Bi-LSTM model on balanced data has the best performance with the f1-score (micro) value of 89.66%; f1-score (macro) of 87.26% and f1-score (weighted) of 89.57%.","Kata Kunci : sentiment analysis, deep learning, long short-term memory, bidirectional LSTM, imbalanced data"
http://etd.repository.ugm.ac.id/home/detail_pencarian/200649,Analisis Klaster Hierarki untuk Data Kategorik dengan Algoritma Divisive Hierarchical Clustering of Categorical Data (DHCC) (Studi Kasus: Pengelompokkan Anggota UKM Mapagama),"MUHAMMAD BURHAN AZIZ, Dr. Abdurakhman, M.Si.",2021 | Skripsi | S1 STATISTIKA,"Analisis klaster adalah analisis yang bertujuan untuk mengelompokkan objek-objek yang memiliki tingkat kemiripan tinggi ke dalam suatu kelompok. Analisis klaster pada data kategorik menjadi lebih kompleks karena tidak ada ukuran kemiripan yang memiliki makna berarti diantara objek kategorik. Pada tugas akhir ini, akan dipaparkan suatu metode analisis klaster untuk data kategorik, yaitu algoritma DHCC. Algoritma ini berjalan dengan pendekatan atas-bawah. Pemisahan klaster berdasarkan analisis multikorespondensi pada matriks indikator yang menunjukkan kemunculan nilai kategorik pada suatu objek. Total inersia atau rata-rata jarak Chi-square pada perhitungan analisis multikorespondensi yang berupa jumlah kuadrat dari elemen matriks residual standar dapat dipandang sebagai ukuran heterogenitas suatu klaster dari prespektif analisis klaster. Dimensi pertama dari ruang yang ditransformasi berdasarkan MCA menjelaskan variansi terbesar dari rata-rata jarak Chi-square sehingga pemisahan klaster berdasarkan dimensi pertama akan menurunkan rata-rata jarak Chi-Square. Unsur optimisasi pada algoritma DHCC berjalan dengan memperbaiki pemisahan hasil dari analisis multikorespondensi. Metode ini diaplikasikan untuk melakukan segmentasi anggota UKM Mapagama periode 2020/2021. Diperoleh 8 segmen dari anggota UKM Mapagama dengan masing-masing karakteristiknya. Optimisasi yang terdapat pada algoritma DHCC terbukti meningkatkan kualitas hasil analisis klaster yang lebih baik.","Cluster analysis is an analysis that aims to group objects that have a high level of similarity into a group. Cluster analysis on categorical data becomes more complex because there is no significant similarity measure between categorical objects. In this final project, a cluster analysis method for categorical data will be presented, namely the DHCC algorithm. This algorithm runs with a top-down approach. Cluster separation is based on multiple correspondence analysis on an indicator matrix that shows the occurrence of categorical values in an object. The total inertia or the average Chi-square distance in the calculation of multiple correspondence analysis is the sum squares of elements from the standard residual matrix elements can be viewed as a measure of the heterogeneity of a cluster from the perspective of cluster analysis. The first dimension of the transformed space based on MCA explains the largest variance of the average Chi-square distance so that the cluster separation based on the first dimension will decrease the average Chi-Square distance. The optimization element in the DHCC algorithm works by improving the separation of results from multiple correspondence analysis. This method was applied to segment Mapagama UKM members for the 2020/2021 period. Obtained 8 segments from members of UKM Mapagama with their respective characteristics. The optimization contained in the DHCC algorithm is proven to improve the quality of the results of better cluster analysis.","Kata Kunci : Analisis klaster hierarki, Data kategorik, Analisis multikorespondensi, Algoritma DHCC, Segmentasi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/197322,Robust Jackknife Ridge Regression dengan Estimator Least Trimmed Squares (LTS) untuk Mengatasi Masalah Multikolinearitas dan Pencilan,"JESSICA PRILIA P M, Dr. Herni Utami, M.Si.",2021 | Skripsi | S1 STATISTIKA,"Dalam ilmu statistika mengenal sebuah metode untuk mengetahui pengaruh variabel independen terhadap variabel dependen yaitu analisis regresi. Teknik dasar dalam analisis regresi adalah metode kuadrat terkecil untuk mendapatkan penduga parameter regresi atau estimator. Metode ini memerlukan beberapa asumsi klasik yang harus dipenuhi, salah satunya adalah tidak terjadi multikolinearitas. Jika dalam model regresi terdapat multikolinearitas, maka penggunaan metode kuadrat terkecil akan menghasilkan estimator yang bersifat bias dan kesimpulan yang kurang baik. Metode ridge regression digunakan untuk mengatasi masalah multikolinearitas. Konsep dari ridge regression adalah penambahan tetapan bias k pada diagonal utama matriks Z^' Z. Namun estimator yang diperoleh pada ridge regression merupakan estimator yang bersifat bias. Untuk mengurangi bias, dapat menggunakan jackknife ridge regression. Akan tetapi jackknife ridge regression kurang tepat digunakan untuk data yang memiliki pencilan. Untuk mengatasinya dapat menggunakan robust regression dengan estimator Least Trimmed Squares (LTS). Sehingga untuk mengatasi masalah multikolinearitas dan pencilan secara bersamaan dapat menggunakan metode robust ridge regression dan robust jackknife ridge regression berdasarkan estimator LTS. Dalam penelitian ini menggunakan data tingkat kemiskinan kabupaten/kota di Provinsi Jawa Tengah tahun 2017 beserta faktor-faktor yang mempengaruhinya sebagai studi kasus, Berdasarkan hasil pada studi kasus dan pemilihan model terbaik, yaitu nilai MSE untuk estimator, MSE untuk model, AIC, dan BIC menunjukkan bahwa metode robust jackknife ridge regression dengan estimator LTS lebih baik dibandingkan dengan metode robust ridge regression dengan estimator LTS.","Regression analysis is a statistical analysis used to find out the effect of independent variables toward dependent variables. The basic technique of regression analysis is the least-squares method to obtain predictor of regression parameters or estimator. This method requires some traditional assumptions that must be fulfilled, namely the absence of multicollinearity. The multicollinearity in the regression model results in bias estimator and invalid conclusions. The ridge regression method is used to solve the multicollinearity problem. The concept is the addition of bias constant k on the main diagonal of the Z^' Z matrix. However, the estimator obtained in the ridge regression is biased.  Jackknife ridge regression can be used to reduce bias. However, jackknife ridge regression is not appropriate for data that has outliers. Therefore, robust regression with the Least Trimmed Squares (LTS) estimator is applied. Regarding the problem of multicollinearity and outliers, Robust Ridge regression and robust jackknife ridge regression based on LTS estimator are applied. The data of this study is the poverty level of districts/cities in Central Java Province in 2017 and the influencing factors a case study. Based on case study and choosing the best model, there is the MSE for the estimator, MSE for the model, AIC, and BIC revealed the robust jackknife ridge regression method with LTS estimator was better than the robust ridge regression method with LTS estimator.","Kata Kunci : multikolinearitas, pencilan, ridge regression, jackknife ridge regression, robust, estimator Least Trimmed Squares (LTS)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/201167,FUNGSI PEMBOBOT KERNEL TETAP GAUSSIAN DAN FUNGSI KERNEL ADAPTIF BISQUARE PADA MODEL GEOGRAPHICALLY WEIGHTED NEGATIVE BINOMIAL REGRESSION (GWNBR),"ULIN NUHA NABILA, Yunita Wulan Sari, S.Si., M.Sc",2021 | Skripsi | S1 STATISTIKA,"Salah satu masalah yang sering ditemukan pada pemodelan data cacah adalah terjadinya overdispersi. Permasalahan overdispersi dapat menyebabkan kesalahan dalam pengambilan kesimpulan. Masalah overdispersi juga dapat ditemukan pada regresi spasial. Geographically Weighted Negative Binomial Regression (GWNBR) merupakan satu metode yang efektif untuk mengatasi overdispersi pada data cacah yang memiliki aspek spasial. Pada pengestimasian parameter model GWNBR diperlukan matriks pembobot spasial. Dalam mencari elemen-elemen pada matriks pembobot spasial, digunakan fungsi kernel. Pada penelitian ini, akan dibandikan dua fungsi pembobot spasial yaitu fungsi kernel tetap gaussian dan fungsi kernel adaptif bisquare untuk data jumlah kasus penyakit Tuberculosis (TB) di Kabupaten/Kota Provinsi Jawa Timur. Hasil penelitian menyimpulkan bahwa model GWNBR dengan pembobot fungsi kernel adaptif bisquare memberikan peforma yang lebih baik dibandingkan model GWNBR dengan pembobot fungsi kernel tetap gaussian.","One of the problems that is often found in count data modelling is overdispersion. Overdispersion is able to lead to error in conclusion. Overdispersion case is able to be found in spatial regression. Geographically Weighted Negative Binomial Regression (GWNBR) is an effective method that is able to resolve overdispersion in count data with spatial aspect. In estimating the parameters of GWNBR model, spatial weighting matrix are required. Kernel function is used to find the elements in spatial weighting matrix. In this research, two weighting functions will be compared, namely Fixed Gaussian Kernel Function and Adaptive Kernel Bisquare Function for Tuberculosis data in district/city of East Java. The result of this research concludes that GWNBR model with Adaptive Kernel Bisquare Weighting function gives better performance than GWNBR model with Kernel Fixed Gaussian Weighting Fucntion.","Kata Kunci : heterogenitas spasial, overdispersi, GWNBR, fungsi kernel tetap gaussian, kernel adaptif bisquare"
http://etd.repository.ugm.ac.id/home/detail_pencarian/197857,REGRESI ROBUST RIDGE MENGGUNAKAN ESTIMATOR M DAN ESTIMATOR LEAST MEDIAN SQUARE (LMS) PADA DATA DENGAN MULTIKOLINEARITAS DAN PENCILAN,"ADELIA TRI S, Yunita Wulan Sari, S.Si, M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah suatu analisis statistik yang bertujuan untuk  memodelkan dan mengetahui pola hubungan antara variabel dependen dan variabel  independen sehingga nilai variabel dependen dapat diduga berdasarkan nilai  variabel independennya. Salah satu cara untuk mengestimasi koefisien regresi  adalah dengan metode Ordinary Least Square (OLS). Namun, metode ini  memerlukan beberapa asumsi yang harus dipenuhi, seperti tidak terjadi  multikolinearitas di antara variabel-variabel independen dan tidak ada pencilan  pada data. Pada skripsi ini akan dibahas dua metode yang dapat menangani masalah  multikolinearitas dan pencilan secara bersamaan. Metode ini merupakan gabungan  dari metode regresi ridge dan estimator pada regresi robust, yaitu estimator M dan  estimator LMS. Kemudian akan dilakukan perbandingan antara kedua metode  tersebut dengan studi kasus data Pendapatan Asli Daerah (PAD) di Jawa Tengah beserta faktor-faktor yang mempengaruhinya. Didapatkan kesimpulan bahwa  regresi robust ridge menggunakan estimator LMS lebih baik dalam memodelkan  pengaruh jumlah penduduk, PDRB atas dasar harga konstan, PDRB atas dasar  harga berlaku, dan realisasi pajak daerah terhadap Pendapatan Asli Daerah (PAD)  di Jawa Tengah karena memberikan nilai MSE, AIC, dan BIC yang lebih kecil dibandingkan regresi robust ridge menggunakan estimator M.","Regression analysis is a statistical analysis that aims to modelling and  determine the pattern of the relationship between the dependent variable and the  independent variable so that the value of the dependent variable can be estimated  based on the value of the independent variable. One way to estimate the regression  coefficient is the Ordinary Least Square method (OLS). However, this method  requires some assumptions that must be fulfilled, such as there is no  multicollinearity among the independent variables and no outliers on the data. In this paper will discussed two methods that can handle multicollinearity  and outlier problems simultaneously. This method is a combination of ridge  regression method and robust regression estimator, that is the M estimator and the  LMS estimator. Then a comparison will be made between these methods with the case study of Indonesian inflation data and the factors that influence it. It can be  concluded that the robust ridge regression using LMS estimator is better in  modelling the effect of the BI rate, the US dollar exchange rate against the rupiah,  the money supply, world oil prices, and foreign debt on inflation in Indonesia  because it gives the values of MSE, AIC, and BIC that are smaller than the robust  ridge regression using M estimator","Kata Kunci :  multikolinearitas, pencilan, regresi robust ridge, estimator M,  estimator LMS."
http://etd.repository.ugm.ac.id/home/detail_pencarian/197606,METODE PRINCIPAL COMPONENT ANALYSIS-BACK PROPAGATION ARTIFICIAL NEURAL NETWROK (PCA-BP ANN) UNTUK PREDIKSI HARGA SAHAM PT. BANK RAKYAT INDONESIA,"ENGGAL DWI MULYANINGTYAS, Prof. Drs. Subanar, Ph.D.",2021 | Skripsi | S1 STATISTIKA,"Investasi adalah penanaman modal untuk satu atau lebih aktiva atau aset atau yang dimiliki dan biasanya berjangka waktu lama dengan harapan mendapatkan keuntungan di masa-masa yang akan datang. Pemerintah, melalui Bursa Efek Indonesia, meluncurkan program Yuk Nabung Saham untuk mengajak masyarakat sebagai calon investor untuk berinvestasi di pasar modal dengan membeli saham secara rutin dan berkala. Pada umumnya, investor menganalisis harga saham dengan memerhatikan berbagai variabel yang secara natural saling berkorelasi. Principal Component Analysis merupakan salah satu analisis multivariate yang dapat mereduksi variabel data awal menjadi dimensi yang lebih kecil dengan tetap mempertahankan informasi yang terdapat dalam data dan menghilangkan korelasi antar variabel. Artificial Neural Network merupakan salah satu pendekatan yang dapat menjelaskan hubungan antara variabel-variabel yang dipilih dengan harga close saham. Oleh karena itu, metode Principal Component Analysis-Artificial Neural Network dipilih untuk memprediksi harga saham yang memiliki beberapa variabel independen yang saling berkorelasi dan bersifat kompleks. Dengan menggunakan RMSE, diperoleh hasil bahwa metode Principal Component Analysis-Artificial Neural Network lebih baik dibandingkan dengan metode Artificial Neural Network.","Investments are capital investments for one or more assets and are usually long-term with the expectation of getting return in the future. The government, through the Indonesia Stock Exchange, launched the Yuk Nabung Saham program to invite the public as potential investors to invest in the capital market by buying stocks regularly and periodically. In general, investors analyze stock prices by paying attention to the various variables that are naturally correlated. Principal Component Analysis is a multivariate analysis that can reduce the initial data variables into smaller dimensions while maintaining the information contained in the data and eliminating the correlation between variables. Artificial Neural Network is an approach that can explain the relationship between the selected variables and the close stock price. Therefore, the Principal Component Analysis-Artificial Neural Network method was chosen to predict stock prices which have several independent variables that are correlated and complex. By using RMSE, the result shows that the Principal Component Analysis-Artificial Neural Network method is better than the Artificial Neural Network method.","Kata Kunci : harga saham, Principal Component Analysis, Back Propagation, Artificial Neural Network"
http://etd.repository.ugm.ac.id/home/detail_pencarian/202220,PENENTUAN ATURAN BERBASIS DATA PADA PROSES UNDERWRITING ASURANSI KENDARAAN BERMOTOR,"KHAIRUL IKHSAN N, Danang Teguh Qoyyimi, S.Si., M.Sc., Ph.D.",2021 | Skripsi | S1 STATISTIKA,"Laporan laba rugi dari suatu perusahaan asuransi kedepannya akan mengacu kepada IFRS 17 yang akan menggantikan pendahulunya yaitu IFRS 4. Salah satu perubahan yang terjadi yaitu pada IFRS 17 kerugian akan diakui sepenuhnya diawal, sedangkan pengakuan laba akan diamortisasikan menggunakan contractual service margin. Hal ini merubah sistem pengakuan laba rugi yang sebelumnya diakui semuanya diawal sebagaimana aturan pada IFRS 4. Dengan melihat perbedaan kesanggupan antara perusahaan asuransi untuk menanggung klaim besar dan kebiasaan underwriter yang tidak mencatat bisnis yang tidak menjadi polis maka proses evaluasi dari proses underwriting dilakukan dengan melihat polis polis mana yang akan memaksimalkan keuntungan jika polis diterima. Proses evaluasi akan menolak polis-polis mana yang merugikan yang diproyeksikan akan menyebabkan distribusi dari klaim menjadi heavy tail. Proses ini dipilih dikarenakan tidak adanya pencatatan bisnis yang tertolak sebelum menjadi suatu polis. Setelah mendapatkan polis mana yang diterima dan polis mana yang ditolak, dibuat model klasifikasi yang dapat membantu underwriter dalam mengklasifikasi bisnis baru yang akan datang. Melihat data asuransi memiliki ukuran data yang besar (big data), terdapat data yang hilang dan terjadi ketidakseimbangan data maka penggunaan metode gradient boosting dan metode lightgbm adalah pilihan yang tepat untuk membuat model klasifikasi dengan kendala tersebut. Gradient boosting adalah metode pemodelan fungsi kerugian yang memanfaatkan gabungan dari beberapa pembelajaran lemah dengan bobot yang sama yang menghasilkan model pembelajaran yang kuat. LightGBM adalah metode pengembangan dari gradient boosting yang akan mengefisiensikan algoritma pemrograman.","Profit and loss statement from an insurance company in the future will refer to IFRS 17 which will replace its predecessor IFRS 4. One of the rules that have changed is that in IFRS 17, losses will be fully recognized at the date of initial recognition , while profit will be amortized using the contractual service margin. This rules will changes the profit and loss recognition system which was previously recognized at the date of initial recognition if refetred to IFRS 4. By looking at the difference between the insurance companies ability to cover large claims and the habits of underwriters that did not record businesses that not become policies, the underwriting evaluation process is carried out by looking at which policies will maximize profits if the policy is accepted. The evaluation process will reject which policies are detrimental which are projected to cause the distribution of the claim to be heavy tail. This process was chosen because there were no business records that were rejected before becoming a policies. After getting which policies were accepted and which policies were rejected, A classification model is built that can help underwriters to classify new businesses that are coming. Considering that insurance data has a large amount of data (Big Data), missing data and data imbalance, using the gradient boosting method and lightgbm method are the right choises to build a classification model with these problem. Gradient boosting is a method that modeling a loss function that utilizes a combination of several weak learner with the same weight which results a strong learning model. LightGBM is a method that streamlines the gradient boosting programming algorithm.","Kata Kunci : IFRS 17, IFRS 4, Pengakuan kerugian, Underwriter, Pembatasan, Net Present Value (NPV), Gradient boosting (Gboost), LightGBM, Boosting"
http://etd.repository.ugm.ac.id/home/detail_pencarian/195052,Regresi Buckley-James dengan Boosting untuk Data Tersensor Kanan,"MUHAMMAD RIZKI RAMADHAN, Drs. Danardono, M.P.H.,Ph.D.",2021 | Skripsi | S1 STATISTIKA,"Ekspresi gen yang diperoleh dari hasil DNA microarray menghasilkan jumlah yang sangat banyak. Untuk melihat ekspresi gen mana yang mempengaruhi penyakit suatu pasien menjadi kesulitan tersendiri. Metode Boosting telah banyak digunakan pada klasifikasi dan juga regresi untuk memilih variabel independen dalam pembentukan suatu model. Kesulitan bertambah jika pada data tersebut mengandung data yang tersensor. Salah satu metode yang sering digunakan untuk menangani data yang tersensor adalah regresi Cox atau yang biasa dikenal dengan Cox's proportional hazard model yang diperkenalkan oleh Cox (1972). Regresi Cox tersebut membutuhkan asumsi hazard proporsional dimana semua individu dianggap memiliki satu nilai baseline hazard yang sama lalu kemudian nilai dari baseline hazard tersebut berubah sesuai dengan karakteristik pada masing - masing individu. Jika asumsi tersebut sulit untuk terpenuhi, regresi Buckley-James dapat menjadi salah satu alternatif untuk mengestimasi nilai variabel dependen yang mengandung data tersensor.  Regresi Buckley-James dengan metode Boosting dapat digunakan untuk mengatasi permasalahan data tersensor dengan jumlah variabel independen sangat banyak dimana pada studi kasus ini bertujuan untuk memprediksi tingkat survival dari seorang pasien penderita kanker limfoma yang telah menjalani kemoterapi dengan melihat ekspresi gen hasil DNA microarray.  Dari hasil analisis diperoleh Regresi Buckley-James dengan metode L2 boosting menghasilkan 26 ekspresi gen terpilih, sedangkan untuk twin boosting menghasilkan 10 ekspresi gen terpilih.","Gene expression obtained from the DNA microarray yields a very large number. Which gene expression influences a patient's disease becomes a difficulty. Boosting has been widely used in classification and regression to select independent variables in the modelling. The difficulty increase if the data contains censored data. One method that is often used to handle censored data is Cox regression or commonly known as Cox's proportional hazard where all individuals are deemed to have the same baseline hazard value then changes according to the characteristics of each individual. If these assumption are difficult to fulfill, the Buckley-James regression can be an alternatives to estimate the value of the dependent variable containing censored data.  Buckley-James regression with the Boosting method can be used to overcome the problem of censored data with a large number of independents variables which in this case study aims to predict the survival rate of a patient with lymphoma cancer who has undergone chemotherapy by lookin at the expression of the resulting gene DNA microarray.  From the analysis, it was obtained that the Buckley-James regression using the L2 boosting resulted in in 26 selected gene expression, while the twin boosting method resulted in 10 selected gene expressions.","Kata Kunci : Survival, data tersensor, Buckley-James estimator, boosting, seleksi variabel"
http://etd.repository.ugm.ac.id/home/detail_pencarian/205807,PENENTUAN HARGA OPSI BELI TIPE EROPA MENGGUNAKAN MODEL VOLATILITAS STOKASTIK HESTON DENGAN PERANGKAT LUNAK PYTHON DAN R,"SALSABILA ALYA ANDRA, Dr. Gunardi, S.Si., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Penentuan harga opsi beli tipe eropa merupakan hal menarik oleh para ekonom maupun para matematikawan dan statistisi. Mereka melakukan penelitian mengenai bagaimana membuat suatu model yang dapat mengakomodasi kondisi pasar dan tidak bias pada hal-hal seperti moneyness, skewness dan kurtosis dari data saham. Model yang umum digunakan oleh para trader opsi adalah model Black-Scholes. Model Black-Scholes cukup populer karena memberikan suatu solusi closed-form yang memudahkan trader dalam pengaplikasiannya. Namun terdapat kekurangan pada model Black-Scholes yaitu asumsi bahwa volatilitas yang konstan padahal hal tersebut hampir mustahil terjadi di pasar nyata. Berbagai model penentuan harga opsi yang mengusung volatilitas stokastik pun banyak dikembangkan oleh peneliti, salah satunya yaitu model volatilitas stokastik Heston.  Model volatilitas stokastik Heston mmengasumsikan bahwa volatilitas tidak konstan, melainkan  mengikuti proses stokastik. Volatilitas pada model ini mengikuti proses CIR yang didefinisikan sebagai jumlahan kuadrat dari proses Ornstein-Uhlenbeck. Pada skripsi ini, dilakukan kalibrasi paremeter model volatilitas stokastik Heston dengan metode Levenberg-Marquadt. Kemudian, dilakukan estimasi harga opsi dengan model volatilitas stokastik Heston dan model Black-Scholes. Selanjutnya akan dibandingkan peforma kedua model Black-Scholes dengan membandingkan nilai mean absolute error-nya. Terdapat empat opsi beli tipe eropa yang akan digunakan sebagai studi kasus dalam skripsi ini yaitu opsi Google, opsi Netflix, opsi Ebay, dan opsi Walmart dengan waktu jatuh tempo dan nilai kesepakatan yang berbeda-beda.","The pricing of european call options is of interest to economists as well as mathematicians and statisticians. They do research about how to make a model that can accommodate market behaviour and is not biased on things such as moneyness, skewness and kurtosis of stock data. The model commonly used by options traders is the Black-Scholes model. The Black-Scholes model is quite popular because it provides a closed-form solution that makes it easier for traders to apply. However, there is a weakness in the Black-Scholes model, namely the assumption that volatility is constant even though this assumption is almost impossible in the real market. Various models of option pricing that carry stochastic volatility have also been developed by many researchers, one of which is Heston's stochastic volatility model.  Heston's stochastic volatility model assumes that volatility is not constant, but follows a stochastic process. Volatility in this model follows the CIR process which is defined as the sum of the squares of the Ornstein-Uhlenbeck process. In this thesis, the parameters of the Heston stochastic volatility model are calibrated using the Levenberg-Marquadt method. Then, the option price estimation is carried out using the Heston stochastic volatility model and the Black-Scholes model. Furthermore, the performance of the two models will be compared by comparing the mean absolute error value of those models. There are four european call options that will be used as case studies in this thesis, namely the Google option, the Netflix option, the Ebay option, and the Walmart option with different maturity dates and strikes.","Kata Kunci : Opsi Beli Tipe Eropa, Volatilitas, Stokastik, Heston, Model Black-Scholes."
http://etd.repository.ugm.ac.id/home/detail_pencarian/198128,REGRESI SEMIPARAMETRIK BIRESPON MENGGUNAKAN ESTIMATOR TRUNCATED SPLINE,"GESIA FIFI YEKONIA, Drs. Zulaela, Dipl. Med. Stats., M.Si.",2021 | Skripsi | S1 STATISTIKA,"Dalam analisis regresi multiprediktor dapat ditemukan kurva regresi yang berbentuk linear dan ada pula yang pola kurva regresinya tidak diketahui. Selain itu, dalam analisis regresi, variabel respon yang ingin diteliti dapat berjumlah lebih dari satu dengan maksud mendapatkan kesimpulan yang lebih menyeluruh. Jika demikian keadaannya, maka regresi semiparametrik birespon dapat digunakan. Salah satu estimator dalam pemodelan semiparametrik adalah truncated spline. Penelitian ini bertujuan untuk memperoleh bentuk model regresi semiparametrik birespon menggunakan estimator truncated spline dalam mengestimasi kurva regresi nonparametriknya serta megimplementasikannya pada data riil menggunakan software R. Data yang digunakan adalah data sekunder 34 provinsi di Indonesia tahun 2019, dengan persentase penduduk miskin dan indeks kedalaman kemiskinan sebagai variabel respon, rata-rata lama sekolah dan persentase rumah tangga yang memiliki akses terhadap sanitasi layak sebagai variabel prediktor komponen parametrik, dan harapan lama sekolah, persentase rumah tangga yang memiliki akses terhadap sumber air minum layak, dan tingkat pengangguran terbuka sebagai variabel prediktor komponen nonparametrik. Estimasi model persentase penduduk miskin dan indeks kedalaman kemiskinan menghasilkan nilai MSE sebesar 2,692663 dan koefisien determinasi (R-square) sebesar 92,18315%.","In multipredictor regression analysis, it can be found that some regression curves are linear and some have unknown patterns of regression curves. In addition, in regression analysis, in order to get more comprehensive model, researcher can use more than one response variable. Therefore, biresponse semiparametric regression suitable for that situation. One of the estimators in semiparametric modelling is a truncated spline. This study aims to obtain a biresponse semiparametric regression model using truncated spline estimator in estimating the nonparametric regression curve and implementing it on real data. The data used in this study is secondary data from 34 provinces in Indonesia in 2019, with the proportion of poor people and poverty gap index as response variables, mean years school and the proportion of households that have access to proper sanitation as parametric components, and the expectation of school years, the proportion of households that have access to decent drinking water sources, and unemployment rate as nonparametric components. The regression model of the proportion of poor people and poverty gap index using software R results the value of MSE and R-square are 2.692663 and 92.18315% respectively.","Kata Kunci : regresi semiparametrik, birespon, truncated spline, kemiskinan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/197619,OPTIMASI PORTOFOLIO MULTI OBJEKTIF MENGGUNAKAN METODE ANT COLONY OPTIMIZATION,"LARAS SEKAR KINASIH, Dr. Adhitya Ronnie Efendi, S.Si., M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Investasi merupakan kegiatan menempatkan dana pada suatu aset dalam periode waktu tertentu dengan harapan bisa memperoleh keuntungan. Aset yang dapat diinvestasikan ada beragam, salah satunya ialah aset finansial berupa saham. Kegiatan investasi tidak dapat dihindarkan dari risiko investasi. Untuk meminimalkan risiko dalam investasi saham diperlukan manajemen investasi dengan cara membentuk portofolio. Portofolio merupakan gabungan dari beberapa aset atau saham. Investor dapat mengurangi tingkat risiko dengan membentuk portofolio yang efisien dan optimal. Salah satu metode yang dapat digunakan yaitu ant colony optimization.  Metode ant colony optimization terinspirasi oleh perilaku semut dalam mencari rute untuk memperoleh makanannya. Semut mengandalkan rangsangan suatu zat kimia yang disebut feromon dalam mencari makannya. Setiap semut yang berjalan pada suatu rute akan mengeluarkan zat tersebut. Sehingga semut lain dapat memanfaatkannya untuk mengetahui rute terbaik yang ditempuh. Proses tersebut dimanfaatkan untuk melakukan optimasi pada portofolio. Metode tersebut menghasilkan suatu jalur kelompok aset saham yang efisien. Selanjutnya dipilih perwakilan dari masing-masing klaster yang sesuai dengan karakterisik kelompok tersebut berdasarkan pusat klaster. Kemudian dilakukan pembobotan dengan menggunakan portofolio multi objektif. Pada penelitian ini digunakan data closing price saham bulanan dari 29 saham IDX-30. Asumsi yang harus dipenuhi sesuai dengan asumsi metode tersebut yakni asumsi normalitas return. Sehingga terdapat 22 saham yang akan dibentuk jalur kelompok aset saham. Berdasarkan hasil tersebut terdapat 3 saham perwakilan yang akan dimasukkan ke dalam portofolio. Dilakukan kombinasi koefisien pembobot (k) sebagai pilihan mean return yang diharapkan dan risiko yang diinginkan oleh investor. Berdasarkan hasil tersebut metode ant colony optimization menghasilkan suatu jalur yang menunjukkan kelompok aset saham yang memiliki tingkat kesamaan yang tinggi dalam satu kelompok. Hal tersebut dibuktikan dengan terbentuknya 3 kelompok saham yang memiliki nilai return tinggi dan risiko yang rendah. Selain itu, metode ini dapat mengelompokkan saham yang memiliki return negatif ke dalam satu kelompok. Kemudian terdapat kombinasi bobot portofolio yang dapat dipilih investor untuk memperoleh keuntungan sesuai yang diharapkan.","Investment is the activity of placing funds in an asset for a certain period of time with the hope of making a profit. There are various assets that can be invested, one of which is financial assets in the form of stocks. Investment activities cannot be avoided from investment risk. To minimize risk in stock investment, investment management is needed by forming a portfolio. A portfolio is a combination of several assets or stocks. Investors can reduce the level of risk by forming an efficient and optimal portfolio. One method that can be used is ant colony optimization. The ant colony optimization method is inspired by the behavior of ants in finding a route to get their food. Ants rely on the stimulation of a chemical called a pheromone to find food. Any ants traveling on a route will release this substance. So that other ants can use it to find out the best route to take. This process is used to optimize the portfolio. This method produces an efficient line of stock assets group. Then representatives are selected from each cluster according to the characteristics of the group based on the cluster center. Then the weighting is done using a multi-objective portfolio. This study used data on monthly closing price of 29 IDX-30 stocks. The assumptions that must be met are in accordance with the assumptions of the method, namely the assumption of normality of return. So that there are 22 stock that will be formed as the asset stock group line. Based on these results, there are 3 representative stocks that will be included in the portfolio. A weighting coefficient (k) is combined as a choice of the mean expected return and the risk desired by investors. Based on these results the ant colony optimization method produces a path that shows groups of stocks assets that have a high level of similarity in one group. This is evidenced by the formation of 3 groups of stocks that have high return values and low risks. In addition, this method can classify stocks that have negative returns into one group. Then there is a combination of portfolio weights that investors can choose to get the expected profit.","Kata Kunci : optimisasi portofolio, analisis klaster, ant colony optimization, portofolio multi objektif."
http://etd.repository.ugm.ac.id/home/detail_pencarian/205299,Aplikasi SMOTE+ENN pada Analisis Klasifikasi dengan Data Tidak Seimbang untuk Pengklasifikasian Kandidat Pulsar,"WOLFGANG MAY PANCA A, Dr. Drs. Gunardi, M.Si.",2021 | Skripsi | S1 STATISTIKA,"Analisis klasifikasi merupakan suatu teknik untuk memprediksi label kelas dari data obeservasi dengan menggunakan model klasifikasi yang terbentuk. Analisis klasifikasi yang paling sederhana adalah klasifikasi biner. Dalam dunia nyata, pada analisis klasifikasi biner seringkali ditemukan kasus imbalanced data. Imbalanced data, dalam klasifikasi biner, merupakan kondisi pada suatu dataset dengan proporsi data antara kelas positif dan kelas negatif yang tidak seimbang, di mana jumlah sampel pada kelas positif (main class of interest) jauh kurang dari kelas negatif. Kondisi tersebut dapat menyebabkan penurunan performa klasifikasi. Kondisi imbalanced data tidak semata-mata menjadi satu-satunya faktor permasalahan yang menyebabkan penurunan performa pengklasifikasi, meskipun hal tersebut terjadi pada data yang tidak seimbang. Terdapat faktor lain yang juga dapat menurunkan performa dari algoritma pembelajar, salah satunya adalah tingkat data tumpang tindih antar kelas atau class overlapping. Maka dari itu, untuk menangani kondisi permasalahan tersebut pada penelitian ini digunakan metode Synthetic Minority Oversampling Technique + Edited Nearest Neighbor (SMOTE+ENN).  	Metode SMOTE+ENN bekerja dengan menerapkan algoritma SMOTE terlebih dahulu dan dilanjutkan dengan penerapan algoritma ENN. SMOTE membangkitkan sampel sintetis sebagai sampel pengamatan tambahan pada kelas minoritas. Kemudian, ENN melakukan modifikasi dataset dengan cara pembersihan dataset dari sampel pengamatan yang memiliki label kelas berbeda dari mayoritas label kelas K sampel tetangga terdekatnya. Studi kasus pada penelitian ini menggunakan dataset HTRU2 Data Set. Dalam penelitian ini, diperoleh hasil bahwa performa klasifikasi, yang diukur dengan indikator g-mean, dari analisis klasifikasi dengan menerapkan SMOTE+ENN lebih baik daripada analisis klasifikasi tanpa penerapan teknik resampling dan dengan penerapan SMOTE.","Classification analysis is a technique used for predicting class labels of instances using a constructed classification model. The simplest classification analysis is binary classification. In real, imbalanced data is usually happening in binary classification. In binary classification, imbalanced data is an imbalanced proportion between the positive class and the negative class, where the number of samples in the positive class is too few than in the negative class. That condition can decrease the classification performance. Imbalanced data is not solely being the factor that can decrease the classification performance, even it happens in an imbalanced dataset. Other factors can decrease the classification performance, such as class overlapping. Therefore, Synthetic Minority Oversampling Technique + Edited Nearest Neighbor (SMOTE+ENN) is used in this research.  SMOTE+ENN works by doing the SMOTE algorithm first and then continuing with the ENN algorithm. SMOTE creates synthetic instances as adding instances for the minority class. Then, ENN modifies the dataset by delete instances that have class labels different from the majority class labels of its K nearest neighbors. The case study in this research uses HTRU2 Data Set. The result in this research is classification analysis with application SMOTE+ENN has better classification performance, based on g-mean, than classification analysis without application resampling technique and with application SMOTE.","Kata Kunci : Klasifikasi, Data Tidak Seimbang, Class Overlapping, SMOTE, ENN, g-mean"
http://etd.repository.ugm.ac.id/home/detail_pencarian/201471,ESTIMASI MAXIMUM LIKELIHOOD MODEL LINEAR PANEL EFEK TETAP DENGAN KOMPONEN LAG SPASIAL,"ANANDYA DEVAN A, Dr. Danang Teguh Qoyyimi, S.Si. M.Sc.",2021 | Skripsi | S1 STATISTIKA,"Indeks Pembangunan Manusia (IPM) merupakan indikator kinerja pembangunan berdasarkan tiga dimensi pokok pembangunan manusia, yaitu dimensi kesehatan, dimensi pendidikan, dan standar hidup layak. Capaian IPM Provinsi Jawa Tengah tahun 2012-2017 mengalami kenaikan sebesar 3.31%. Kendati demikian apabila dibandingkan dengan provinsi lain di Pulau Jawa, capaian IPM Provinsi Jawa Tengah pada tahun 2017 hanya menempati posisi kelima dari total enam provinsi. Peta tematik IPM Provinsi Jawa Tengah menurut kabupaten/kota dari tahun 2012-2017 menunjukan terdapat pola sebaran dimana kabupaten/kota yang berdekatan memiliki nilai IPM yang relatif sama. Hal ini mengindikasikan terdapat faktor interaksi spasial antar kabupaten/kota. Sehingga, untuk mengetahui faktor-faktor yang mempengaruhi IPM di Provinsi Jawa Tengah perlu mengakomodasi faktor interaksi spasial tersebut. Pada skripsi ini, pemodelan IPM di Provinsi Jawa Tengah tahun 2012-2017 dilakukan menggunakan model linear panel efek tetap dengan komponen lag spasial pada variabel dependennya yang dikenal juga sebagai model panel Spatial Autoregressive (SAR). Sementara, estimasi parameter berfokus pada metode Maximum Likelihood (ML). Faktor spasial lain berupa daerah pesisir dan bukan pesisir juga turut diikutsertakan dalam analisis karena diduga terjadi kesenjangan kesejahteraan antara daerah tersebut. Dari model akhir yang terbentuk didapat bahwa faktor interaksi spasial antar kabupaten/kota berpengaruh secara positif terhadap IPM di daerah sekitarnya. Sedangkan, kabupaten/kota yang memiliki garis pantai atau daerah pesisir berpengaruh negatif terhadap nilai IPM. Faktor-faktor lainnya adalah persentase kemiskinan, produk domestik regional, tingkat inflasi, jumlah penduduk, dan angka melek huruf.","The Human Development Index (HDI) is an indicator of development performance based on three main dimensions of human development, namely the health dimension, the education dimension, and the decent standard of living. The HDI of Central Java Province increased until 3.31% in 2012-2017. However, when compared to other provinces in Java Island, the Central Java Province's HDI in 2017 only occupied the fifth position out of total of six provinces. The thematic map of Central Java Province's HDI by district from 2012-2017 showed that there was a distribution pattern in which districts that are close, had relatively the same HDI values. This indicated a factor of spatial interaction among the districts. Therefore, to find out the factors that influence HDI in Central Java Province, it is necessary to accommodate these spatial interaction factors. In this thesis, HDI modelling of Central Java Province in 2012-2017 was carried out using a fixed effect linear panel model with spatial lag component in the dependent variable, also known as the Spatial Autoregressive (SAR) panel model. Meanwhile, parameter estimation focused on the Maximum Likelihood (ML) method. Other spatial factor namely coastal and non-coastal area were also included in the analysis, because it was suspected that there was a welfare gap between these areas. From the final model formed, it was found that the spatial interaction between the districts had positive effect on HDI in the surrounding area. Moreover, the district that had a coastline or coastal area had a negative effect on the HDI value. Other factors were poverty percentage, regional domestic product, inflation rate, population size, and literacy rate.","Kata Kunci : Indeks Pembangunan Manusia, Jawa Tengah, model panel efek tetap, Spatial Autoregressive panel, Maximum Likelihood (ML)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/183811,PENENTUAN HARGA OPSI BELI TIPE EROPA MENGGUNAKAN ARTIFICIAL NEURAL NETWORK,"ADDIENA SANISCARA, Dr. Abdurakhman, M.Si.",2020 | Skripsi | S1 STATISTIKA,"Black dan Scholes (1973) menunjukkan bahwa opsi tipe Eropa dari suatu aset dapat dihitung jika harga suatu aset mengikuti proses stokastik tertentu dan gerak geometrik Brown. Namun, harga suatu aset tidak selalu dapat dideskripsikan dengan gerak geometrik Brown. Jaringan syaraf tiruan atau artificial neural network merupakan salah satu pendekatan yang dapat menjelaskan harga opsi tanpa membutuhkan asumsi seperti yang diperlukan pada model Black-Scholes. Artificial neural network dapat mengenali pola dari data pelatihan yang digunakan dan dapat menemukan hubungan antara input dan output dari data yang digunakan. Penelitian ini menggunakan metode pelatihan resilient backpropagation yang memanfaatkan informasi gradien lokal untuk melakukan perubahan bobot dan bias jaringan. Selanjutnya, dilakukan perbandingan antara harga opsi yang diperoleh dari metode Black-Scholes dan artificial neural network pada data pengujian. Dengan menggunakan RMSE (Root Mean Square Error) sebagai kriteria penentuan harga opsi, diperoleh hasil bahwa model artificial neural network lebih baik dibandingkan dengan model Black-Scholes.","Black and Scholes (1973) showed that a European option of an asset can be priced exactly if the price of an asset follows a particular stochastic process and geometric Brownian motion. However, real world stock prices are not generated by geometric Brownian motion. Artificial neural network is an approach that can explain the price of an option without requiring assumptions as needed in the Black-Scholes model. Artificial neural network can recognize training data patterns and can find relationship between the input and output of the data. This study uses a resilient backpropagation training method which utilizes local gradient information for changes in weights and bias. Furthermore, we compare the option price obtained by artificial neural network and the Black-Scholes model with option market price. Using RMSE (Root Mean Square Error) as the criterion of option pricing, the result demonstrates that artificial neural network model performs better than Black-Scholes model.","Kata Kunci : harga opsi, Black-Scholes, artificial neural network, resilient backpropagation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192527,IMPLEMENTASI SUPPORT VECTOR MACHINES DALAM KONSTRUKSI OBLIQUE RANDOM FOREST UNTUK KLASIFIKASI PADA DATA BERDIMENSI TINGGI,"IRFAN WAHYU F, Prof. Drs. Subanar, Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Random forest (RF) telah menjadi salah satu metode klasifikasi gabungan yang banyak menjadi perhatian oleh para peneliti machine learning. Prinsip dari RF menghasilkan suatu metode klasifikasi yang robust terhadap data noise serta tidak overfit. Fungsi pemisah pada tiap simpul RF menggunakan pemisah ortogonal yang tegak lurus terhadap sumbu atribut, yang jika diterapkan pada data dimensi tinggi dimana sangat mungkin terjadi banyaknya dependensi antar atribut akan menghasilkan performa klasifikasi yang kurang baik. Pada data dimensi tinggi sangat mungkin ada begitu banyak kombinasi antar atribut dan sayangnya RF tidak bisa mengeksploitasi situasi ini secara efisien. Pada skripsi ini, diajukan suatu metode oblique random forest (ORF) yang memiliki pemisah miring pada tiap simpulnya dimana digunakan kombinasi dari berbagai atribut secara langsung. Pemisah miring ini diharapkan dapat lebih mampu beradaptasi pada data dimensi tinggi. Metode yang digunakan untuk menemukan pemisah miring yang optimal dalam skripsi ini adalah support vector machines (SVM). Pada skripsi ini dilakukan analisis terhadap 4 data microarray kanker berdimensi tinggi dengan metode ORF serta dengan metode individunya yakni RF dan SVM. Dengan perbandingan nilai akurasi, presisi, sensitifitas, spesivisitas, dan skor F1, dihasilkan kesimpulan bahwa secara umum ORF memiliki performa yang lebih baik dibanding RF dan SVM.","Random forest (RF) has become one of ensemble classification method that has a great concern to machine learning researchers. The principle of RF produces a classification method that is robust against noise and does not overfit. The split function a each node of RF uses an orthogonal split that is perpendicular to the attribute axis. That kind of split, does not have high performance if applied to data with very high dimensions where it is possible that many dependencies between attributes. In high dimensional data it is possible that there are so many combinations between attributes and unfortunately RF can not effectively exploit this situation. This thesis propose an oblique random forest (ORF) method which uses oblique split at each node where a combination of various attributes is used directly. This oblique split is expected to be better adapted to high dimensional data. This thesis uses support vector machines (SVM) to find the optimal oblique split. This thesis analyze 4 microarray high dimensional cancer data using ORF method and the individual methods which are RF and SVM. By comparing the accuracy, precision, sensitivity, spesificity, and F1 score, the conclusion is that, in general, ORF has better performance than RF and SVM.","Kata Kunci : Data Dimensi Tinggi, Random Forest, Support Vector Machines, Oblique Random Forest"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192024,SELEKSI MODEL REGRESI KUANTIL BAYESIAN DENGAN BAYES FACTOR,"ANGELIA T MONEKAKA, Prof. Drs. Subanar, Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Regresi kuantil digunakan untuk mengatasi keterbatasan regresi linear dalam menganalisis asumsi yang tidak terpenuhi pada regresi klasik. Hal ini berfokus pada hubungan antara variabel prediktor dan kuantil dari distribusi variabel dependen yang kontinu.    Regresi kuantil diestimasi dengan metode Bayesian yang berdasarkan pada informasi yang berasal dari sampel dan informasi prior. Gabungan informasi ini disebut posterior. Untuk mencari distribusi posterior digunakan pendekatan Gibbs sampling. Dalam analisis regresi kuantil Bayesian, terdapat ketidakpastian dari banyak kandidat prediktor yang harus dimasukkan dalam model. Untuk mengidentifikasi prediktor penting dan membangun model prediksi yang akurat, metode Bayesian untuk pemilihan variabel dan seleksi model sangat berguna.   Studi kasus yang digunakan dalam skripsi ini bertujuan untuk memprediksi berat badan (Weight) yang sesuai dengan 12 variabel pengukuran lingkar tubur (body girth measurements) dan tinggi badan (Height). Model dari hasil seleksi variabel Bayesian digunakan sebagai kandidat model Bayes factor untuk seleksi model Bayesian. Perbandingan model dengan Bayes factor menunjukkan pengukuran bukti relatif untuk satu model di atas model lainnya. Dengan perbandingan model Bayes factor, didapatkan kesimpulan bahwa model dengan 9 parameter (C, ChestGi, WaistGi, HipGi, ThighGi, ForeaGi, KneeGi, CalfGi, Height) dengan data menjelaskan 43,741 kali lebih baik dibandingkan model alternatif lain.","Quantile regression is used to overcome the limitation of linear regression in analyzing the unfulfilled assumptions of classical regression. The interest focuses on relationship between predictors and the quantile of the distribution of a continuous response.   Quantile regression can be estimated using Bayesian method which is based on information from sample and prior information. Combination of those information is called posterior. Gibbs sampling approach is used to find the posterior distribusion. In Bayesian quantile regression, there is uncertainty in which of the many candidate predictors should be included. In order to identify important predictors and to build accurate predictive model, Bayesian method for variable selection and model selection are very useful.   The case study in this thesis aims to predict body weight according to the 12 variables of body girth measurements and height. Models from Bayesian variable selection are used as candidate models Bayes factor for Bayesian model selection. Model comparison with Bayes factor shows quantify the relative evidence for one over another model. Therefore, model selection with Bayes factor is model with 9 parameters (C, ChestGi, WaistGi, HipGi, ThighGi, ForeaGi, KneeGi, CalfGi, Height) shows that data are about 43,741 times more likely under this model than under the other alternative models.","Kata Kunci : Regresi Kuantil, Bayesian, Gibbs sampling, Seleksi Model Bayesian, Bayes Factor"
http://etd.repository.ugm.ac.id/home/detail_pencarian/191001,Analisis Sentimen Berbasis Aspek Menggunakan Metode Support Vector Machine pada Data Ulasan Restaurant,"EKA SAKINAH OKTAVIANI, Prof. Dr. Sri Haryatmi, M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Opinion mining atau yang juga dikenal sebagai sentiment analysis merupakan metode yang digunakan untuk mengidentifikasi pendapat atau opini seseorang tentang suatu produk, film, berita, dll.   Analisis sentimen terdiri dari 3 tingkatan, yaitu analisis sentimen tingkat kalimat, dokumen dan aspek. Analisis sentimen tingkat kalimat dan tingkat dokumen sudah cukup baik dalam memperoleh orientasi sentimen. Namun, pada kenyataanya kebutuhan terus meningkat dimana informasi yang diperlukan dalam pengembangan produk tidak sebatas memperoleh penilaianya produk secara keseluruhan, tetapi dibutuhkan informasi terkait opini customer pada masing-masing fitur atau aspek-aspek dari produk, entitas ataupun layanan. Oleh karena itu, analisis sentimen tingkat aspek dibutuhkan untuk menjawab kebutuhan informasi yang diperlukan tersebut. Pada analisis sentimen tingkat aspek dapat diketahui bagaimana pandangan seseorang terhadap masing-masing aspek-aspek yang terkandung dalam produk apakah positif atau negatif.  Pada skripsi ini akan dibahas analisis sentimen tingkat aspek menggunakan salah satu metode supervised learning yaitu Support Vector Machine (SVM) pada data ulasan restaurant. Tahapan yang akan dilakukan yaitu dimulai dengan persiapan data ulasan restaurant kemudian melakukan tahap preprocessing ( data cleaning, tokenization, menghapus stopword, lemmatization, feature extraction), terakhir menghitung orientasi sentimen dari masing-masing aspek dengan menggunakan metode SVM. Hasil analisis menunjukkan bahwa akurasi yang dihasilkan yaitu sebesar 88%.","Opinion mining also known as sentiment analysis is a method used to identify someone's opinions about a product, film, news, etc. Levels in sentiment analysis consist of 3 levels : sentence-level, document-level and aspect-levels. Sentiment analysis of sentences and document-level is good enough in obtaining sentiment orientation. However, in fact the needs continue to increase, where the information needed in product development is not limited to obtaining a product assessment as a whole, but information related to customer opinions on the features or aspects of the product is needed. Therefore, aspect-level sentiment analysis is needed to answer the required information needs. In the aspect-level sentiment analysis we can find out how one's view of the aspects contained in the product is positive or negative. This thesis will discuss aspect-level of sentiment analysis on restaurant review using one of the supervised learning methods, Support Vector Machine (SVM). The steps that will be carried out are starting with the preparation of restaurant review data and then doing the preprocessing stage (data cleaning, tokenization, stopword removal, lemmatization, feature extraction), then calculating the sentiment orientation of each aspect using the SVM. The analysis shows that the accuracy produced is equal to 88%.","Kata Kunci : Sentiment Analysis, Support Vector Machine, ABSA, Opinion Mining, Aspect-Based Sentiment Analysis"
http://etd.repository.ugm.ac.id/home/detail_pencarian/196384,PENINGKATAN AKURASI ALGORITMA C4.5 MENGGUNAKAN DISKRITISASI DAN PEMILIHAN FITUR BERBASIS KORELASI,"GITA AYU WULAN SARI, Prof. Drs. Subanar, Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Salah satu teknik dari supervised learning pada machine learning adalah klasifikasi, dan salah satu algoritma klasifikasi adalah C4.5. Tujuan dari penelitian ini adalah untuk meningkatkan akurasi algoritma C4.5 dengan menerapkan diskritisasi dan Correlation-Based Feature Selection (CFS). Akurasi yang lebih baik telah dicapai dengan menerapkan diskritisasi dan CFS. Diskritisasi digunakan untuk menangani nilai kontinu, sedangkan CFS digunakan sebagai pemilihan atribut.             Di bidang kesehatan, teknik klasifikasi dapat digunakan untuk mendiagnosis penyakit dari data rekam medis pasien. Penelitian ini menggunakan empat dataset yang diperoleh dari UCI Machine Learning Repository. Dalam dataset ini, terdiri dari atribut berupa tipe numerik yang kontinu dan nominal. Atribut kontinu dapat menyebabkan akurasi yang rendah karena bentuk data yang tidak terbatas. Sedemikian hingga, atribut perlu diubah menjadi data diskrit dengan diskritisasi. Selain itu, pada kasus tertentu jika semua atribut digunakan dapat menghasilkan tingkat akurasi yang rendah karena tidak relevan dan tidak memiliki korelasi dengan kelas target. Oleh karena itu, setelah didiskritisasi atribut tersebut perlu diseleksi terlebih dahulu untuk mendapatkan hasil yang lebih akurat menggunakan CFS.","One of the techniques of supervised learning in machine learning is classification, and one of the classification algorithms is C4.5. The purpose of this study is to increase the accuracy of the C4.5 algorithm by using discretization and Correlation-Based Feature Selection (CFS). Better accuracy has been achieved by using discretization and CFS. Discretization is used to handle continuous values, while CFS is used as attribute selection.  	In the health sector, classification can be used to diagnose diseases from patient medical records. This study uses four datasets obtained from the UCI Machine Learning Repository. In this dataset, it consists of attributes in the form of numeric and nominal types. The continuous attribute can cause low accuracy due to the infinite form of the data. Thus, attributes need to be converted into discrete data with discretization. In addition, in certain cases if all attributes are used it can result in a low level of accuracy because it is irrelevant and has no correlation with the target class. Therefore, after being discretized these attributes need to be selected first to get more accurate results using CFS method.","Kata Kunci : Klasifikasi, Algoritma C4.5, Diskritisasi, Entropi, Minimal Description Length, Correlation-Based Feature Selection."
http://etd.repository.ugm.ac.id/home/detail_pencarian/191013,ROBUST LIU ESTIMATOR MENGGUNAKAN ESTIMATOR LEAST ABSOLUTE DEVIATION (LAD) PADA REGRESI LINEAR DENGAN MULTIKOLINEARITAS DAN PENCILAN,"ASHIFA RIZKIA R, Prof. Dr. Sri Haryatmi, M.Sc",2020 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan salah satu alat statistik yang banyak digunakan untuk mengetahui pola hubungan antara variabel dependen dengan variabel independen. Dasar analisis regresi klasik adalah metode Ordinary Least Square (OLS), digunakan untuk menduga koefisien regresi yang tepat digunakan ketika semua asumsi klasik regresi linear terpenuhi. Namun ternyata sering terdapat pelanggaran terhadap asumsi tersebut, seperti adanya pencilan dan multikolinearitas yang dapat menyebabkan hasil estimasi menggunakan metode OLS menjadi kurang tepat. Pada skripsi ini akan dibahas metode yang dapat digunakan ketika terjadi masalah pencilan dan multikolinearitas secara bersamaan yaitu metode regresi Robust Liu estimator menggunakan estimator Least Absolute Deviation (LADRLE). Metode regresi LADRLE merupakan penggabungan antara metode liu estimator dengan estimator pada regresi robust LAD. Studi kasus pada skripsi ini menggunakan data PDRB Jawa Tengah tahun 2015 dengan variabel yang mempengaruhinya. Diperoleh kesimpulan bahwa metode LADRLE menghasilkan nilai MSE penduga parameter lebih kecil dibandingkan metode OLS, hal itu menunjukkan bahwa metode LADRLE tepat digunakan untuk data yang memiliki pencilan dan multikolinearitas.","Regression analysis is one of the statistical tools that is widely used to determine the correlations between dependent variables and independent variables. The fundamental of classical regression analysis is an Ordinary Least Square (OLS) method, which is used to estimate the appropriate regression coefficient when all assumptions of classical linear regression are met. However, apparently there are often violations of these assumptions, such as outliers and multicollinearity that cause the estimation results using OLS method be less precise. This study examine a method that can used when outliers and multicollinearity are concurrent, that is Robust Liu estimator regression method using the Least Absolute Deviation (LADRLE) estimator. The LADRLE regression method is a combination of the liu estimator method with the estimator in the robust LAD regression. The case study of this research use Central Java GRDP data in 2015 with variables that affect to that. The conclusion is the LADRLE method produces smaller MSE parameter estimator values than the OLS method, it denotes that the LADRLE method is appropriate for data with outliers and multicollinearity.","Kata Kunci : analisis regresi, pencilan, multikolinearitas, regresi robust, liu estimator, estimator LAD, regresi robust LAD, regresi LADRLE"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192555,ANALISIS CLUSTER DENGAN METODE FUZZY C-MEANS MENGGUNAKAN SOFTWARE MATLAB,"YOHANES P MANIK, Dr. Herni Utami, M.Si.",2020 | Skripsi | S1 STATISTIKA,"Fuzzy C-Means merupakan salah satu metode pengclusteran berbasis Non-Hierarki yang mana keberadaan tiap-tiap titik data dalam suatu cluster ditentukan oleh derajat keanggotaan. Objek (dataset) yang memiliki tingkat kemiripan tinggi dikelompokkan ke dalam suatu segmen / cluster yang sama, sedangkan objek yang memiliki kemiripan rendah berada pada segmen/cluster yang berbeda.  Penelitian ini difokuskan pada pengclusteran tingkat kesejahteraan rakyat Kabupaten/Kota di Jawa Tengah menggunakan software Matlab. Dari hasil penelitian ini dapat dilihat Kabtupaten/Kota mana saja yang tergolong dalam tingkat kesejahteraan rendah, sedang, maupun tinggi.","Fuzzy C-Means is a non-hierarchical clustering method in which the existence of each data point in a cluster is determined by the degree of membership. Objects (datasets) that have a high degree of similarity are grouped into the same segment / cluster, while objects with low similarity are in different segments / clusters. This research is focused on clustering the level of welfare of the people of districts / cities in Central Java using Matlab software.From the results of this study, it can be seen which districts / cities are classified as low, medium and high welfare levels.",Kata Kunci : Fuzzy Clustering
http://etd.repository.ugm.ac.id/home/detail_pencarian/192556,ANALISIS KLASIFIKASI MENGGUNAKAN KOMBINASI METODE NAIVE BAYES DAN POHON KEPUTUSAN (NBTREE),"MUHAMMAD REZA ADI KUSUMA , Drs. Danardono, MPH., Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Pohon Keputusan merupakan metode klasifikasi yang terbukti akurat. Metode ini bekerja dengan mempartisi data latih secara rekursif hingga didapatkan partisi yang hanya memiliki satu kelas. Metode Naive Bayes juga telah terbukti sebagai metode klasifikasi yang akurat. Namun ketika digunakan pada data yang berukuran besar, metode ini sering kali memberikan performa yang kurang baik dibandingkan metode lainnya. Metode NBTree merupakan metode kombinasi Naive Bayes dengan Pohon Keputusan yang bertujuan untuk mengatasi kelemahan ini dengan mempartisi data seperti pada Pohon Keputusan, namun setelah sejumlah partisi, data diklasifikasi dengan metode Naive Bayes. Cara ini lebih baik daripada Pohon Keputusan karena sangat mungkin partisi tersebut berukuran kecil sedangkan metode Naive Bayes unggul dalam melakukan klasifikasi meski dengan ukuran data latih yang kecil. Penelitian ini difokuskan pada metode Pohon Keputusan, Naive Bayes, dan NBTree dalam mengklasifikasi suatu dataset yang berukuran cukup besar. Algoritma C4.5 digunakan untuk membentuk Pohon Keputusan. Dari hasil penelitian ini dapat dilihat bahwa metode NBTree memberikan performa yang lebih baik daripada metode Naive Bayes dan Pohon Keputusan C4.5.","Decision Tree is a classification method that is proven to be accurate. It works by partitioning training data recursively to obtain partitions that only belong to one class. Naive Bayes is also proven as an accurate classification method, however it usually gives lower performance compared to other methods when used to classify large dataset. NBTree is a Naive Bayes and decision tree hybrid classification method made purposely to resolve this weakness by partitioning the training data similar to decision tree, however after several partitionings it classifies these partitions by Naive Bayes method. This method is better than decision tree due to these partitions probably have small size and Naive Bayes method excels in classifying small training data. This research was concerned to classify a quite large dataset using decision tree, Naive Bayes, and NBTree methods. C4.5 algorithm was used to construct decision tree. It could be concluded from the result of this research that NBTree method gave better performance than Naive Bayes and Decision Tree C4.5 methods.","Kata Kunci : klasifikasi, metode kombinasi, hybrid, Pohon Keputusan, decision tree, C4.5, Naive Bayes, NBTree"
http://etd.repository.ugm.ac.id/home/detail_pencarian/188980,KLASIFIKASI DAN PENGELOMPOKAN POLIS ASURANSI KENDARAAN BERMOTOR: UPAYA PERSIAPAN IMPLEMENTASI IFRS 17 PADA PERUSAHAAN ASURANSI UMUM,"ANDIANSYAH PRIMA W, Danang Teguh Qoyyimi, S.Si., M.Sc., Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Standar akuntansi untuk pelaporan kegiatan asuransi akan mengacu kepada IFRS 17 yang semula mengacu kepada IFRS4. IFRS 17 adalah standar akuntansi keuangan yang dikeluarkan oleh International Financial Reporting System yang mengatur perlakuan akuntansi yang disepakati secara internasional untuk kontrak-kontrak asuransi. Dalam upaya meningkatkan akurasi penilaian risiko untuk adaptasi IFRS 17, diperlukan cara yang baik untuk mengelompokkan risiko dari tertanggung. Oleh karena itu, perlu dilakukan penentuan kelompok risiko. Karena data dari suatu perusahaan asuransi memiliki ukuran yang besar, metode CLARA cocok untuk menangani masalah tersebut. CLARA memiliki sifat lebih robust terhadap pencilan dan dapat digunakan untuk menangani data dalam jumlah besar.  Setelah dilakukan pengelompokan, perlu diketahui faktor apa yang menyebabkan seseorang masuk ke dalam kelompok tertentu. Untuk itu perlu dilakukan analisis klasifikasi. Beberapa metode analisis klasifikasi adalah XGBoost, AdaBoost, dan SVM. Extreme Gradient Boosting dan Adaptive Boosting merupakan teknik dalam machine learning untuk masalah regresi dan klasifikasi biner ataupun multikelas yang menghasilkan model prediksi dalam bentuk gabungan model prediksi yang lemah. Support Vector Machine (SVM) adalah suatu teknik untuk melakukan prediksi, baik dalam kasus regresi maupun klasifikasi biner atau multikelas. SVM memiliki prinsip dasar linier classifier.","Accounting standards for reporting insurance activities will refer to IFRS 17, which originally referred to IFRS4. IFRS 17 is a financial accounting standard issued by the International Financial Reporting System that regulates internationally agreed accounting treatment for insurance contracts. In an effort to increase the accuracy of risk assessment for IFRS 17 adaptation, a good way is needed to classify risks from the insured. Therefore, it is necessary to determine the risk group. Because data from an insurance company is large, the CLARA method is suitable for dealing with the problem. CLARA has a more robust nature of outliers and can be used to handle large amounts of data. After grouping, it is important to know what factors cause a person to enter a certain group. For this, classification analysis is needed. Some classification analysis methods are XGBoost, SVM, and AdaBoost. Extreme Gradient Boosting and Adaptive Boosting is a technique in machine learning for binary or multiclass regression and classification problems that results in predictive models in the form of weak predictive models. Support Vector Machine (SVM) is a technique for making predictions, both in the case of regression and binary or multiclass classification. SVM has the basic principle of linear classifier.","Kata Kunci : IFRS 17, CLARA, XGBoost, AdaBoost, SVM, klasifikasi multikelas, boosting, medoid"
http://etd.repository.ugm.ac.id/home/detail_pencarian/194617,Analisis Diskriminan Bayesian dengan Metode Minimum Expected Cost of Misclassification (ECM) dan Decision Tree pada Data Penderita Diabetes,"AWANG PUTRA S R, Prof. Drs. Subanar, Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Terdapat banyak metode pengklasifikasian, salah satunya adalah Analisis Diskriminan Bayesian dengan Minimum Expected Cost of Misclassification (ECM). Metode ini merupakan analisis diskriminan yang dalam analisisnya memperhatikan biaya kesalahan klasifikasi. Namun, sebelum dilakukan analisis perlu memenuhi beberapa asumsi. Asumsi utamanya yaitu kesamaan matriks varian kovarian tiap kelas dan variabel independen berdistribusi normal multivariat tiap kelas. Dalam mendapatkan rumus diskriminan metode Minimum Expected Cost of Misclassification (ECM) dengan bantuan teorema Bayes yang tidak lepas dari probabilitas posterior. Untuk melihat performa dari metode klasifikasi ini dilakukan dengan pembanding metode lainnya yaitu metode decision tree. Data yang digunakan merupakan data penderita diabetes. Berdasarkan beberapa variabel independen suatu individu akan masuk ke kelas negatif atau positif diabetes. Dilakukan transformasi Tukey sebelum analsis diskriminan agar asumsi normalitas multivariat dan kesamaan matriks varian kovarian dapat dipenuhi. Untuk tingkat kesalahan diukur dengan melihat Apparent Error Rate (APER), akurasi, presisi, sensitivitas, dan spesifisitas. Hasil dari analisis menunjukkan diskriminan Bayesian metode minimum Expected Cost of Misclassification (ECM) setelah transformasi Tukey lebih baik dibandingkan dengan metode decision tree berdasarkan APER, akurasi, presisi, dan sensitivitas.","There are many classification methods, one of which is Bayesian Discriminant Analysis with Minimum Expected Cost of Misclassification (ECM). This method is a discriminant analysis which in its analysis considers the cost of misclassification. However, before the analysis is carried out, it is necessary to fulfill several assumptions. The main assumption is the similarity of the variance covariance matrix for each class and the independent variables has a multivariate normal distribution for each class. To get the discriminant formula the Minimum Expected Cost of Misclassification (ECM) method with the help of the Bayes theorem which cannot be separated from posterior probability. To see the performance of this classification method, a comparison method is used, namely the decision tree method. The data used is data on diabetics. The goal is that based on several independent variables an individual will enter the negative or positive class for diabetes. The Tukey transformation is carried out before discriminant analysis so that the assumptions of multivariate normality and variance covariance matrix equality can be fill. The error rate is measured by looking at the Apparent Error Rate (APER), accuracy, precision, sensitivity, and specificity. The results of the analysis show that the Bayesian discriminant method of minimum Expected Cost of Misclassification (ECM) after the Tukey transformation is better than the decision tree method based on APER, accuracy, precision, and sensitivity.","Kata Kunci : Bayesian Discriminant Analysis, Bayes Theorem, Linear Discriminant Analysis, Decision Tree, Tukey Transform, Discriminant Analysis Minimum Expected Cost of Misclasification (ECM) Method"
http://etd.repository.ugm.ac.id/home/detail_pencarian/185406,ANALISIS KINERJA CONVOLUTIONAL NEURAL NETWORK (CNN) DAN HISTOGRAM OF ORIENTED GRADIENTS - SUPPORT VECTOR MACHINE (HOG-SVM) PADA KLASIFIKASI CITRA,"MUH KASIM ASHARDIN, Prof. Dr.rer.nat. Dedi Rosadi. M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Terdapat beragam metode pengklasifikasian citra, paling umum digunakan yaitu Convolutional Neural Network (CNN), tetapi metode ini membutuhkan biaya besar dan waktu lama untuk melakukan pelatihan. Terdapat pula metode klasifikasi lain yang memiliki waktu pelatihan lebih cepat dibandingkan CNN, seperti Support Vector Machine (SVM), akan tetapi metode ini tidak langsung mengklasifikasikan citra secara langsung seperti CNN sehingga membutuhkan metode tambahan untuk melakukan ekstraksi fitur agar dapat diklasifikasi menggunakan SVM, seperti Histogram of Oriented Gradients (HOG). Pada penelitian ini, akan menganalisis performa klasifikasi citra kucing dan anjing beresolusi 128x128 piksel menggunakan CNN dan HOG-SVM pada beragam skenario pembagian dataset. Dataset yang digunakan terdiri dari 100 citra kucing dan 100 citra anjing, dengan menggunakan metode klasifikasi CNN didapatkan parameter yang dilatih sebanyak 7.393.026 parameter dengan tingkat akurasi pada data testing sebesar 88,33%.Sedangkan dengan menggunakan HOG didapatkan 12.288 fitur per citra yang kemudian akan diklasifikasi menggunakan SVM dan menghasilkan tingkat akurasi pada data testing sebesar 96,66%.","There are some various methods of image classification, the most commonly used is Convolutional Neural Network (CNN), but this method requires a large cost and a long time to conduct training data. There are also other classification methods that have faster training time than CNN, such as Support Vector Machine (SVM), but this method does not directly classify images, like CNN, so it requires additional methods to perform feature extraction so it can be classified using SVM, such as Histogram of Oriented Gradients (HOG). In this study case, we will analyze the performance of the classification of cats and dogs images with a resolution of 128x128 pixels using CNN and HOG-SVM with some various scenarios for dataset distribution. The dataset used 100 images of cats and 100 images of dogs, using the CNN classification method, we get the parameters trained were 7,393,026 parameters, an accuracy rate of testing data is 88.33%. While, using HOG, we get 12,288 features per image were obtained. And continous to classified using SVM and we get an accuracy rate of testing data is 96.66% in 70:30 scenario dataset.","Kata Kunci : Convolutional Neural Network, Histogram of Oriented Gradient, Images Classification, Support Vector Machine"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192077,IMPLEMENTASI CLASSIFICATION AND REGRESSION TREE (CART) PADA PERHITUNGAN CADANGAN KLAIM INDIVIDU,"BINTANG APRIANSYAH T, Dr. Adhitya Ronnie E., M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Setiap perusahaan asuransi wajib menyiapkan sejumlah dana untuk memenuhi kewajiban pembayaran klaim yang akan terjadi di masa yang akan datang. Dana inilah disebut dengan cadangan klaim. Perhitungan cadangan klaim yang paling populer digunakan adalah metode Chain Ladder. Metode ini menggunakan data agregat yang diringkas dalam run-off triangle atau model pada tingkat mikro. Sedangkan model pada tingkat makro, digunakan data klaim individu dengan informasi lebih terperinci. Perhitungan cadangan klaim dengan teknik tradisional menggunakan data agregat sering mengabaikan informasi terperinci pada klaim individu. Pada skripsi ini, dikenalkan perhitungan cadangan klaim individu dengan CART (Classification and Regression Tree). Metode ini sangat mudah diinterpretasikan dengan adanya pohon regresi dan pohon klasifikasi serta lebih fleksibel secara penuh dalam perhitungan cadangan klaim karena mempertimbangkan semua fitur atau atribut yang ada. Model yang didapatkan berupa model compound yang terdiri dari bagian frekuensi untuk prediksi banyaknya klaim yang dilaporkan dan bagian severity untuk prediksi jumlah yang dibayar dan dicadangkan. Hasil yang didapatkan perhitungan dengan metode Chain Ladder lebih tinggi dibandingkan CART.","Every insurance company is required to set funds for payment of claims that will occur in the future. The fund is called a reserve claim. The most popular claim calculation used is the textit Chain Ladder method which is used in aggregate data that is summarized in run-off triangle or model at the micro level. While for the model at the macro level, individual claim data is used with more detailed information. Reserve claim calculation with traditional techniques using aggregate data often ignore detailed information on individual claims. In this research, the calculation of individual claim reserves by CART (Classification And Regression Tree) is introduced. This method is very easy to interpret with the regression tree and classification tree and more flexible in the calculation of reserves since it is associated with all the attributes that exist. The model is a compound model consists of a frequency section, for the prediction of the number of reported claims, and a severity section for the prediction of paid and reserved amounts. The results obtained by the calculation by Chain Ladder Method is higher compared to the CART.","Kata Kunci : Cadangan Klaim Individu, CART, Chain Ladder"
http://etd.repository.ugm.ac.id/home/detail_pencarian/186720,Estimasi Value at Risk Menggunakan Model Asymmetric Power ARCH Student-t,"CHAIRUNNISA ELPHIDA, Dr. Abdurakhman, S.Si., M.Si.",2020 | Skripsi | S1 STATISTIKA,"Model ARCH GARCH banyak digunakan pada data finansial untuk mendeskripsikan volatilitasnya. Kedua model tersebut mengasumsikan data return berdistribusi normal serta residual return positif dan residual return negatif memberikan pengaruh yang sama terhadap volatilitasnya. Namun, pada kenyataannya kedua asumsi tersebut seringkali tidak terpenuhi. Untuk mengatasi data yang tidak berdistribusi normal, bersifat heteroskedastisitas, dan mengandung keasimetrisan digunakan model GARCH asimetris yaitu Asymmetric Power ARCH (APARCH) dengan asumsi data berdistribusi student-t. Penelitian ini bertujuan untuk mengestimasi nilai Value at Risk (VaR) dengan pemodelan volatilitas menggunakan model APARCH. Data yang digunakan adalah data saham Perusahaan Aneka Tambang (ANTM) dengan periode 01 Januari 2013 sampai 31 Desember 2017 yang berjumlah 1216 data harga penutupan (Closing Price). 	Penelitian ini diawali dengan memilih model mean terbaik untuk data return. Berdasarkan model mean terbaik yang telah diperoleh, kemudian dibentuk model volatilitas APARCH. Setelah dilakukan perbandingan terhadap model APARCH dan GARCH berdasarkan kriteria pemilihan model, diperoleh model APARCH merupakan model yang paling baik dalam memodelkan volatilitas data return ANTM. Proses akhir yaitu melakukan estimasi perhitungan VaR berdasarkan model volatilitas yang diperoleh. Kemudian dilakukan backtesting meramalkan data penutupan saham selama 10 periode ke depan.","The ARCH GARCH model is widely used in financial data to describe its volatility. Both of these models assume that data returns are normally distributed with the positive and negative residual returns have the same effect on volatility. However, in reality these two assumptions are often not met. To overcome data that are not normally distributed, heteroscedasticity, and contain asymmetricity, the asymmetric GARCH model is used, namely Asymmetric Power ARCH (APARCH) with the assumption that data is student-t distributed. This research aims to estimate Value at Risk (VaR) with volatility modelling using the APARCH model. The data used is the Closing Price of Aneka Tambang Company (ANTM) with period from 01 January 2013 until 31 December 2017, that countain 1216 data. This research started with selecting the best mean model for data return. Based on the best mean model that have been obtained, then forming the APARCH volatility model. After comparing the APARCH and GARCH models based on the model selection criteria, it can be concluded that the APARCH model is the best model for modeling volatility of  ANTM return data. The final process is computing VaR based on the best volatility model. At last, the backtesting and the forecasting of stock closing data for the next 10 periods are conducted.","Kata Kunci : GARCH, GARCH asimetris, APARCH, student-t, Value at Risk (VaR)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/193634,KUALITAS TES UJIAN SEKOLAH BERDASARKAN MODEL TEORI RESPON BUTIR TIGA PARAMETER LOGISTIK,"RESTU BUDI PRABOWO, Dr. Adhitya Ronnie E, M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Dalam bidang Pendidikan diperlukan suatu penilaian untuk mengetahui perkembangan kompetensi siswa selama masa belajar. Penilaian tersebut digunakan sebagai dasar menentukan siswa melanjutkan pendidikan ke jenjang selanjutnya. Oleh karena itu dibutuhkan penilaian dengan yang bermutu tinggi sehingga bisa secara akurat mengetahui kemampuan dari siswa. Model respon teori merupakan salah satu analisis yang dapat digunakan untuk mengetahui kualitas dari butir-butir soal tersebut. Pada skripsi ini digunakan model tiga parameter logistic yaitu daya pembeda, tingkat kesulitan, dan faktor tebakan. Digunakan aplikasi R 4.0.3 menggunakan package â€œirtoysâ€ (Partchev, 2017) untuk menganalisis butir soal. Pada studi kasus Ujian Tengah Semester 1 mata pelajaran matematika SMP N 2 Pleret, dengan peserta sebanyak 204 siswa dan 40 butir soal pilihan ganda. Diperoleh parameter butir soal yang menunjukkan bahwa kualitas soal tersebut bagus dan layak digunakkan untuk ujian tengah semester.","In the field of education, a final assessment is needed to determine the development of student competencies during the learning period. This assessment is used as a basis for determining students to continue their education to the next level. Therefore, an assessment with high quality questions is needed so that it can accurately determine the abilities of students. Item response theory is an analysis that can be used to determine the quality of these items. In this paper, a three-parameter logistic model is used, namely distinguishing power, difficulty level, and guess factor. R 4.0.3 application uses the ""irtoys"" package (Partchev, 2017) to analyze the items. In the Semester 1 Middle Exam case study, mathematics subject at SMP N 2 Pleret, with 204 students and 40 multiple choice questions. Obtained item parameters which indicate that the quality of the questions is good and suitable for use for the midterm exam.","Kata Kunci : teori respon butir, estimasi kemungkinan maksimum, model 3PL, eirt Excel, ujian sekolah"
http://etd.repository.ugm.ac.id/home/detail_pencarian/193643,ESTIMASI CONDITIONAL VALUE AT RISK (CVaR) PORTFOLIO MULTIVARIAT SAHAM-SAHAM LQ45 DENGAN METODE GARCH-STUDENT-T-EVT-VINE COPULA,"MICHAEL, Prof. Drs. Subanar, Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Conditional Value at Risk (CVaR) merupakan ukuran risiko finansial yang baik dalam menunjukkan tingkat risiko pada data return saham yang umumnya bersifat ekor tebal (heavy tail) serta memenuhi sifat ukuran risiko yang koheren yang belum dapat secara umum dipenuhi oleh ukuran risiko VaR (Value at Risk). Para investor umumnya menggunakan ukuran risiko CVaR sebagai penunjang ukuran risiko VaR dalam melakukan manajemen risiko. Selain itu, data return saham mempunyai sifat heteroskedastisitas, heavy tail, dan kebergantungan antar data dalam portfolio yang bersifat non-linear. Oleh karena itu, pemodelan kebergantungan antar data return tersebut perlu menggunakan metode Vine Copula. Metode ini dapat menggabungkan beragam distribusi marginal return univariat untuk menggambarkan kebergantungan non-linear antar data. Kemudian, dalam mengatasi data return saham yang bersifat ekor tebal (heavy tail) yang mengandung nilai-nilai ekstrem, digunakan Extreme Value Theory (EVT) dengan pendekatan GPD (Generalized Pareto Distribution). Dalam mengatasi adanya heteroskedastisitas dalam data return saham, dilakukan pemodelan dengan GARCH (1,1) dengan inovasi Student-t yang berguna untuk menangkap karakteristik data return saham yang cenderung skewed. Dalam studi kasus skripsi ini, digunakan portfolio dari tiga saham LQ45, yaitu ADRO (PT. Adaro Energy Tbk.), GGRM (PT Gudang Garam Tbk.) dan PTBA (PT Bukit Asam Tbk.). Pemodelan Vine Copula menggunakan struktur CD-Vine Copula dari keluarga Gaussian, Student-t, Clayton, Gumbel dan Frank. Copula terbaik dalam memodelkan data dalam studi kasus ini, yaitu struktur C-vine dan D-vine dari keluarga Student-t copula. Selain itu, berdasarkan hasil backtesting, diperoleh bahwa hasil estimasi CVaR dengan model GARCH-Student-t-EVT-Vine Copula secara umum sudah cukup baik untuk digunakan.","Conditional Value at Risk (CVaR) is a common risk measure which is good in examining risk level in stock return data that are commonly heavy tailed and satisfies the properties of coherent risk measure in which Value at Risk (VaR) does not satisfy generally. Investors usually use the CVaR risk measure in supporting the VaR risk measure. Stock return data are often showing the characteristics of heteroscedasticity, heavy tail, and non-linear correlation within the data. Consequently, non-linear correlation can be modeled by using Vine Copula method. This method is able to combine several univariate marginal distributions to show the non-linear correlation. Moreover, Extreme Value Theory (EVT) on GPD (Generalized Pareto Distribution) approach is good in modeling the heavy tailed distribution with extreme values. Because the stock return data are usually showing heteroscedasticity, then the GARCH (1,1) with the Student-t innovation is used to examine the characteristic of the skewed data. In this undergraduate thesis, the multivariate portfolio consists of three LQ45 stocks, which are ADRO (PT. Adaro Energy Tbk.), GGRM (PT Gudang Garam Tbk.) and PTBA (PT Bukit Asam Tbk.). Furthermore, the CD-Vine copula structures from the family of Gaussian, Student-t, Clayton, Gumbel and Frank are used for the analysis. Based on the analysis result, it is found that the best copula for the modelling is Student-t copula both for C-vine and D-vine structures. Eventually, based on the backtesting result, it is concluded that the estimated value of CVaR found through the GARCH-Student-t-EVT-Vine Copula model is overall good to be used.","Kata Kunci : Backtesting,Conditional Value at Risk,GARCH-t(1,1), Generalized Pareto Distribution,Vine Copula"
http://etd.repository.ugm.ac.id/home/detail_pencarian/183660,PARTICLE SWARM OPTIMIZATION (PSO) UNTUK MASALAH OPTIMISASI PORTOFOLIO DIBATASI,"DESI SULISTYOWATI, Prof.Dr.rer.nat.Dedi Rosadi,S.Si.,M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Investasi adalah suatu aktivitas menempatkan dana pada satu periode tertentu dengan harapan mendapatkan keuntungan di masa depan. Dalam berinvestasi, investor selain mendapatkan keuntungan juga akan dihadapkan pada risiko yang harus ditanggung. Salah satu aset investasi yang banyak dipilih oleh investor adalah investasi saham. Untuk memperoleh risiko yang minimal dalam investasi saham tersebut perlu dilakukan disversifikasi portofolio saham. Pembentukan portofolio sendiri memerlukan proporsi bobot yang tepat. Teori portofolio yang terkenal adalah metode optimisasi mean variance. Metode ini memiliki asumsi yakni return saham harus berdistribusi normal. Selain itu metode ini juga memiliki kekurangan yakni dapat menghasilkan bobot yang bernilai negatif. Oleh karena itu, dalam penelitian ini akan digunakan metode lain untuk mengatasi masalah optimisasi portofolio. Penelitian ini menerapkan algoritma particle swarm optimization (PSO) untuk masalah optimisasi portfolio dibatasi. Proses algoritma ini terinspirasi oleh perilaku sosial dari sekumpulan burung dalam suatu swarm, dimana partikel mempunyai kecendrungan untuk bergerak ke area penelusuran yang lebih baik setelah melewati proses penelusuran. Studi kasus pada penelitian ini menggunakan data close price bulanan pada lima saham indeks LQ-45. Kinerja portofolio dihitung dengan menggunakan sharpe ratio yang akan dibandingkan dengan metode mean variance. Kesimpulan yang didapatkan adalah algoritma particle swarm optimization (PSO) untuk masalah optimisasi portofolio dibatasi lebih baik daripada metode mean variance biasa.","Investment is an activity to place funds in a certain period in hopes to making profits in the future. When doing an investment, investors not only gain profits but also faced with many risk that must be borne. One of investment assets that have been chosen by investors is stock investment. To gain minimum risk in stock investment, it is necessary to diversification the portfolio of stock. Formation of portfolio itself requires the rights proportion of weights. The well known portfolio is mean variance optimization method. This method has an assumption that stock returns must be normally distributed. Furthermore, this method has disadvantages that is can produce negative weights. Therefore, this study will use another method to overcome the problem of portfolio optimization. This study applies particle swarm optimization (PSO) for the constrained portfolio optimization problem. This algorithmic process was inspired by the social behavior of birds in a swarm, where particles have a tendency to move to a better search area after passing through the search process. The case study of this research uses monthly close price data on five index stocks LQ-45. Portfolio performance is calculated using the sharpe ratio which will be compared with the mean variance method. The conclusion is the particle swarm optimization (PSO) for the constrained portfolio optimization problems better than the usual mean variance method.","Kata Kunci : Algoritma particle swarm optimization, Mean Variance, Sharpe Ratio, Portofolio Dibatasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/188270,Aplikasi Pembelajaran Mesin menggunakan Model Jaringan Saraf Deep Bidirectional Long Short-Term Memory untuk Pemodelan Runtun Waktu,"RADEN AURELIUS ANDHIKA VIADINUGROHO, Prof. Drs. Subanar, Ph. D.",2020 | Skripsi | S1 STATISTIKA,"Permasalahan yang sering terjadi dalam pemodelan runtun waktu menggunakan metode klasik seperti Autoregressive Integrated Moving Average (ARIMA) adalah ukuran dan kompleksitas pola data. Semakin besar ukuran data dan semakin tidak menentu pola data yang terbentuk, maka persamaan model ARIMA yang terbentuk akan semakin rumit dan ini akan berakibat pada terjadinya overfitting pada model, sehingga performa yang dihasilkan menurun. Oleh karena itu, para peneliti berusaha mengembangkan beberapa model alternatif dengan pendekatan machine learning yang dapat mengatasi permasalahan tersebut, salah satunya adalah model jaringan saraf tiruan. Deep Bidirectional Long Short-Term Memory (DBi-LSTM) adalah salah satu pengembangan dari arsitektur Recurrent Neural Network (RNN) yang menggunakan efek Bidirectional dan penumpukan jaringan saraf Long Short-Term Memory (LSTM) pada arsitekturnya. Pada skripsi ini, digunakan arsitektur DBi-LSTM dengan jumlah neuron sebanyak 100 neuron pada LSTM lapisan pertama dan 50 neuron pada LSTM lapisan kedua. Lebih lanjut, dibentuk empat kombinasi arsitektur DBi-LSTM berdasarkan fungsi aktivasi yang digunakan dan penggunaan dropout. Selanjutnya, untuk mengetahui performa yang dihasilkan dari DBi-LSTM, digunakan dataset berupa nilai penutupan harian Indeks Harga Saham Gabungan (IHSG) dari tahun 2010-2019, dan dilakukan perbandingkan dengan metode ARIMA, LSTM, dan Bi-LSTM. Berdasarkan analisis yang telah dilakukan, didapatkan bahwa model DBi-LSTM menghasilkan performa yang lebih baik dibandingkan dengan metode yang digunakan sebagai pembanding.","The problem that usually occur in time series modelling using classic model like Autoregressive Integrated Moving Average (ARIMA) are the size and pattern complexity. The bigger the data and the more irregular the data pattern, then the ARIMA model equation that created will be more complex and this will results in overfitting of model, so the performance itself will decrease.Therefore, researchers are try to develop some alternative model with machine learning approach that can solve the problem, one of which is artificial neural network model. Deep Bidirectional Long Short-Term Memory (DBi-LSTM) is one of the development from Recurrent Neural Network (RNN) architecture that use Bidirectional effect and stacking of Long Short-Term Memory (LSTM) neural network on its architecture. In this undergraduate thesis, it use DBi-LSTM architecture with 100 neuron in first LSTM layer and 50 neuron in second LSTM layer. Moreover, four combination of DBi-LSTM architecture are created based on the activation function and dropout usage. Next, the Indonesian Composite Stock Price Index (IHSG) closing value data from 2010-2019 are used to find out the DBi-LSTM resulting performance, and compare it with ARIMA, LSTM, and Bi-LSTM method. Based on the analysis that have been conducted, it is found that the DBi-LSTM model produce the better performance than the compared method.","Kata Kunci : pemodelan runtun waktu, LSTM, Bi-LSTM, DBi-LSTM, dropout, fungsi aktivasi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/192890,Penanganan Data Tidak Seimbang Menggunakan Adaptive Neighbor Synthetic Minority Oversampling Technique (ANS) untuk Analisis Klasifikasi,"NAFISHA NUR FATIMA, Dr. Herni Utami, M.Si.",2020 | Skripsi | S1 STATISTIKA,"Perkembangan teknologi memicu berkembangnya ketersediaan data dalam jumlah yang besar dan bentuk yang kompleks. Hal tersebut merupakan suatu alasan penting untuk memajukan pengetahuan dan pemahaman mengenai analisis data. Pada kenyataannya sering ditemui data yang tidak seimbang, yaitu data dengan rasio yang tidak seimbang antara satu kelas dengan kelas lainnya. Dalam analisis klasifikasi, data tidak seimbang menimbulkan masalah karena model yang dihasilkan kurang mampu memprediksi data yang berasal dari kelas yang sedikit (kelas minoritas). Metode ANS, yang merupakan pengembangan dari metode SMOTE, dapat diterapkan untuk menangani permasalahan tersebut. Dengan metode ANS dapat dibuat tupel sintetis yang berasal dari kelas minoritas sehingga dapat menghasilkan data yang lebih seimbang. Pada skripsi ini dilakukan analisis pada data Pima Indians Diabetes dengan menggunakan model klasifikasi Random Forest dan NaÃ¯ve Bayes yang dikombinasikan dengan metode ANS. Dari analisis yang dilakukan diperoleh kesimpulan bahwa model Random Forest pada data dengan ANS menghasilkan performa yang paling baik, terutama dalam akurasi secara keseluruhan dan akurasi kelas positif.","The development in technology initiates the growth of data availability in big quantity and complex form. It is an important reason to improve knowledge and understanding about data analysis. In fact, we usually found imbalanced data, which is data with imbalancedratio between one class with other classes. In classification analysis, imbalanced data causes problem because the output model does not really predict data from fewer class (minority class) accurately. ANS method, the development from SMOTE method, can be used to solve that problem. With ANS method we can make synthetic instance from minority classes so we can get more balanced data. In this thesis, analysis in Pima Indians Diabetes dataset was done using Random Forest and NaÃ¯ve Bayes classification method combined with ANS method. From the analysis, we can conclude that Random Forest method in data with ANS produced the best performance, especially in accuracy and sensitivity.","Kata Kunci : data tidak seimbang, oversampling, ANS, SMOTE, klasifikasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/188540,PREDICTIVE ANALYTICS PADA PENENTUAN PREMI ASURANSI KENDARAAN BERMOTOR,"MATHEW YOSE HANANTA, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Predictive analytics merupakan penerapan analisis lanjutan yang digunakan untuk membuat prediksi mengenai kejadian pada masa yang akan datang. Industri asuransi sebagai pihak yang melakukan manajemen resiko (risk management) men- jadi salah satu industri yang sangat membutuhkan peran penting predictive analytics terutama dalam proses estimasi nilai premi murni asuransi kendaraan bermotor. Se- lama ini perhitungan premi murni asuransi kendaraan bermotor umumnya hanya di- tetapkan berdasarkan sejumlah kecil kriteria seperti harga pertanggungan kendaraan tanpa melihat tingkat resiko yang mungkin dialami. Pada skripsi ini akan dibahas mengenai model estimasi premi murni asuransi kendaraan bermotor menggunakan metode Generalized Linear Models (GLM) sebagai salah satu metode yang digu- nakan dalam predictive analytics. Metode Generalized Linear Models (GLM) akan melibatkan berbagai karakteristik kendaraan bermotor dalam proses estimasi pre- mi murni kendaraan bermotor tersebut. Berdasarkan identifikasi masalah dan hasil uji kecocokan distribusi pada data polis asuransi kendaraan bermotor PT Asuransi Kresna Mitra Tbk periode tahun 2013   2018, regresi Zero Inflated Poisson (ZIP) akan digunakan untuk memodelkan frekuensi klaim dan mengatasi masalah ove- rdispersi pada data cacah sedangkan regresi Gamma akan digunakan untuk memo- delkan besar klaim. Penetapan premi asuransi kendaraan bermotor menggunakan metode Generalized Linear Models (GLM) ini menghasilkan nilai premi menja-  di lebih bervariasi dan adil bagi setiap pemegang polis karena mempertimbangkan beberapa karakteristik kendaraan bermotor seperti status polis (renewal), merk ken- daraan, usia kendaraan, lokasi kendaraan, dan harga pertanggungan.","Predictive Analytics is the application of advanced analysis that is used to make predictions about future events. The insurance industry as a party that carries out risk management becomes one of the industries that really need the important role of predictive analytics, especially in the process of estimating the pure pre- mium value of motor vehicle insurance. So far, the calculation of pure premiums for motor vehicle insurance is generally only determined based on a small number of criteria such as the price of vehicle coverage regardless of the level of risk that may be experienced. This thesis will discuss the model of pure premium estimation of motor vehicle insurance using the Generalized Linear Models (GLM) method as one of the methods used in predictive analytics. The Generalized Linear Models (GLM) method will involve various motor vehicle characteristics in the process of estimating the pure premium of the motor vehicle. Based on the identification of the problems and the results of checking distribution on the motor vehicle insurance policy data of PT Asuransi Kresna Mitra Tbk for the period of 2013    2018, Zero Inflated Poisson (ZIP) regression will be used to model the frequency of claims and overcome the problem of overdispersion in the data counting while Gamma regres- sion will be used to model the amount of claims. Determination of motor vehicle insurance premiums using the Generalized Linear Models (GLM) method results in premium values becoming more varied and fair for each policyholder because it considers several motor vehicle characteristics such as policy status, vehicle brands, vehicle age, vehicle location, and the price of vehicle coverage.","Kata Kunci : predictive analytics, pure premium, Generalized Lienar Models (GLM) method, overdispersion, Zero Inflated Poisson (ZIP) regression, Gamma regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192397,ANALISIS KOMPONEN UTAMA ROBUST SPARSE (ROSPCA) PADA DATA BERDIMENSI TINGGI YANG MEMUAT OUTLIER,"ERIKA EKI OVITASARI, Dr. Herni Utami, M.Si",2020 | Skripsi | S1 STATISTIKA,"Analisis Komponen Utama (AKU) merupakan salah satu analisis multivariat yang berguna untuk mereduksi data berdimensi tinggi yang memiliki korelasi sehingga membentuk data dengan variabel yang lebih sedikit tetapi tetap mampu menjelaskan keragaman dari data. Reduksi dimensi pada PCA klasik didasarkan pada matriks kovarian yang tidak robust terhadap outlier. Oleh karena itu diperlukan Robust PCA pada data yang mengandung outlier. Tujuan dari metode Robust PCA adalah untuk memperoleh komponen utama yang tidak terlalu banyak terpengaruh dengan keberadaan outlier. Telah diketahui juga bahwa Classical PCA dan Robust PCA merupakan kombinasi linear dari seluruh variabel. Hal ini akan mempersulit proses interpretasi komponen utama, karena peneliti tidak dapat mengetahui variabel mana saja yang berperan penting dalam pembentukan komponen utama. Untuk itu, diperlukan Sparse PCA guna mempermudah proses interpretasi komponen utama karena dengan Sparse PCA akan menghasilkan banyak zero loadings pada komponen utama yang terbentuk. Pada skripsi ini akan dibahas metode pembentukan komponen utama Robust Sparse PCA (ROSPCA) pada data berdimensi tinggi yang memuat outlier, yaitu dengan mengintegrasikan Sparse PCA ke dalam ROBPCA yang merupakan gabungan konsep projection-pursuit dan estimator kovarian yang robust (FAST-MCD). Pada skripsi ini akan dilihat performa dari metode Robust PCA (ROBPCA), Sparse Robust PCA (SRPCA), dan Robust Sparse PCA (ROSPCA) dari sisi kemudahan dalam proses interpretasi komponen utama dan ke-robust-an terhadap outlier. Berdasar analisis yang telah dilakuakan diperoleh kesimpulan bahwa metode Robust Sparse PCA (ROSPCA) merupakan metode yang terbaik.","Principal Component Analysis (PCA) is a multivariate analysis that is useful for reducing the high dimensional data of which have a correlation to form the data with less variables but still able to explain the diversity of the data. The dimension reduction of PCA based on covariance matrices that are not robust to outliers. Therefore Robust PCA is required on data containing outliers. The purpose of a Robust PCA method is to obtain the principal components that do not too much influenced by the outlier. It is known that Classical PCA and Robust PCA are linear combinations of all variables. This will inhibit the process of interpretation of the PCs, because researchers canÃ¢ï¿½ï¿½t know which variable plays an important role in the formation of the PCs. For that, Sparse PCA required to simplify the process of interpretation. For this reason, Sparse PCA is needed to simplify the process of interpreting the main components because Sparse PCA will produce a lot of zero loadings on the main components that are formed. This thesis will discuss about the Robust Sparse PCA on high dimensional data with outlier, by integrating Sparse PCA into ROBPCA which consists of the combination of a projection-pursuit concept and robust covariance estimator (FAST-MCD). In this paper we will see the performance of Robust PCA (ROBPCA), Sparse Robust PCA (SRPCA) and Robust Sparse PCA (ROSPCA) methods in terms of convenience in the process of interpretation of the PCs and the goodness of robustness towards outlier. Based on the result of the analysis process, we can conclude that Robust Sparse PCA (ROSPCA) is the best method among the others.","Kata Kunci : Reduksi dimensi, Outlier, Data berdimensi tinggi, FAST-MCD, Projection-Pursuit, Robust, Sparse"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192404,Penerapan Robust Geographically Weighted Regression Menggunakan Least Absolute Deviation,"AGAUSILIA DINDA A, Prof. Dr. Sri Haryatmi, M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Geographically Weighted Regression (GWR) merupakan pengembangan regresi berganda yang memiliki heterogenitas spasial, sehingga estimator GWR akan berbeda pada tiap lokasi. Proses estimasi parameter dalam GWR menggunakan Weighted Least Square (WLS). Tetapi ketika ada outlier dalam data, proses estimasi parameter dengan WLS menghasilkan estimator yang tidak efisien. Oleh karena itu, penelitian ini menggunakan metode robust yang disebut Least Absolute Deviation (LAD) untuk memperkirakan parameter model GWR pada kasus PDRB di Jawa Timur. Hasil penelitian ini menyimpulkan bahwa model Robust GWR dengan metode LAD lebih baik daripada GWR dan prediksi lebih dekat dengan nilai aktual.","Geographically Weighted Regression (GWR) is development of multiple regression that has spatial heterogenity, so that the estimator of GWR is different for each location. Parameter estimation process in GWR uses Weighted Least Square (WLS). But  when there are outliers in the data, the parameter estimation process with WLS produces estimator which are not efficient. Hence, this study uses a robust method called Least Absolute Deviation (LAD), to estimate the parameters of GWR model in the case of GRDP in East Java. The result concludes that Robust GWR model with LAD was better than GWR, and the predictions were closer to the actual values.","Kata Kunci : Robust Geographically Weighted Regression, Least Absolute Deviation, Outlier, Spatial Data"
http://etd.repository.ugm.ac.id/home/detail_pencarian/193430,Model Regresi Logistik untuk Data Uji Hidup Tersensor Interval,"ALFIANTO ASMO D, Drs. Danardono, M.P.H., Ph.D.",2020 | Skripsi | S1 STATISTIKA,"Data tersensor interval sering ditemukan pada studi medis yang memerlukan penanganan lanjut secara periodik. Berbeda dengan data tersensor kiri  atau kanan, diperlukan inferensi statistik yang khusus untuk menanganinya karena memiliki struktur yang lebih rumit. Data observasi yang dihasilkan dari studi medis seringkali memiliki skala diskrit dikarenakan kebutuhan uji klinis meskipun variable tersebut mungkin bersifat kontinu. Model regresi logistik adalah salah satu metode analisis regresi data tersensor interval yang secara khusus dapat digunakan untuk data dengan skala diskrit. Selain  itu digunakan analisis data milestone menggunakan estimasi nonparametrik maksimum likelihood yang dicari menggunakan algoritma self-consistency Turnbull untuk memperoleh estimator. Data milestone merupakan data tahapan perkembangan bayi pertama kali bisa melakukan sesuatu, penelitian mengamati bayi ketika bisa berdiri sendiri tanpa bantuan untuk pertama kali. Pengujian hipotesis dari koefisien regresi nol pada uji log-rank digunakan untuk membandingkan kurva survival berdasarkan metode skor.","Interval-censored data is often found in medical studies that require periodic follow up treatment. In contrast to left or right censored data, special method in statistical inference is needed to handle it because it has a more complex structure. Observed data that arise from medical studies are often given in a discrete scale due to the clinical trials although the underlying variable may be continuous. Logistic regression model is one of the regression analysis method for interval-censored failure time data that can specifically be used for discrete scale data. The analysis of milestone data using nonparametric maximum likelihood estimation can be solved with Turnbull's self-consistency algorithm to obtain the estimator. Milestone data is the stages of baby developing in the first time for doing something, in this research to observe baby can stand up alone without helping. Testing the hypothesis of a zero regression coefficient for log-rank test can be used for comparing survival curves based on score method.","Kata Kunci : data tersensor interval, model logistik, MLE Nonparametrik, Uji Skor / interval-censored data, logistic model, Nonparametric Maximum Likelihood Estimation, Score test"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192412,Distribusi Poisson Weighted-Lindley untuk Menangani Data Cacah dengan Kasus Overdispersi,"ALMAS KURNIA R, Prof. Dr. Sri Haryatmi, M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Pada umumnya distribusi probabilitas yang digunakan untuk mengolah data cacah adalah distribusi Poisson yang memiliki asumsi ekuidispersi, yaitu keadaan dimana mean dan variansi memiliki nilai yang sama. Namun pada praktiknya seringkali ditemukan data cacah dengan kondisi overdispersi, yaitu keadaan dimana nilai mean lebih kecil dibandingkan nilai variansinya. Tidak terpenuhinya asumsi ekuidispersi pada distribusi Poisson akan menghasilkan estimasi parameter yang bias, sehingga distribusi ini tidak efektif untuk digunakan. Sebagai alternatif distribusi Poisson-Weighted Lindley yang merupakan distribusi campuran antara distribusi Poisson dan distribusi Weighted Lindley, dapat digunakan untuk menangani data dengan keadaan overdispersi. Parameter distribusi Poisson-Weighted Lindley akan diestimasi menggunakan metode Maksimum Likelihood dengan iterasi Newton-Raphson. Distribusi Poisson-Weighted Lindley diaplikasikan pada data gizi buruk di kabupaten Pacitan Provinsi Jawa Timur tahun 2012-2018 dan dibandingkan dengan distribusi Poisson dan Poisson-Lindley. Hasil analisis menggunakan Schwart's Bayesian Criterion, -2 loglikelihood, dan Akaike Information Criterion menunjukkan bahwa model distribusi Poisson-Weighted Lindley lebih layak digunakan untuk menangani data cacah dengan keadaan overdispersi.","Commonly distribution that used to process count data is Poisson distribution which has the equidipersion assumptions, condition when the data has the same value of mean and variance. But in fact count data often is found with overdispersion conditions, a condition when the mean is smaller than the variance. The unfulfilled assumption in Poisson distribution can cause a biased parameter estimation so that Poisson distribution is not effective to use. As an alternative Poisson-Weighted Lindley, a Poisson mixture of Weighted Lindley distribution can be used to handle count data with overdispersion. Parameter of Poisson-Weighted Lindley distribution will be estimated using maximum likelihood method with Newton Raphson Iteration. The Poisson-Weighted Lindley is applied to malnutrition data in Pacitan district of East Java in 2012-2018 and compared to the Poisson and Poisson-Lindley distribution. The results of the analysis using Schwart's Bayesian Criterion, -2 loglikelihood, and Akaike Information Criterion show that the Poisson-Weighted Lindley distribution model is more suitable for handling count data with overdispersion.","Kata Kunci : Data Dacah, Distribusi Poisson-Weighted Lindley, Estimasi Maksimum Likelihood, Overdispersi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192671,OPTIMISASI PORTOFOLIO MEAN-VARIANCE DENGAN KLASIFIKASI RANDOM FOREST DAN MODEL GARCH,"TIMOTHY CHRISTIAN, Prof.Dr Dedi Rosadi, M.sc.",2020 | Skripsi | S1 STATISTIKA,"Investasi saham merupakan salah satu instrumen investasi yang menarik, namun sayangnya di Indonesia masih banyak orang belum berinvestasi saham. Ketika seorang investor melakukan investasi saham, investor pada umumnya tidak hanya membeli satu saham saja, untuk mengurangi risiko investor akan melakukan diversifikasi sehingga terbentuklah portofolio saham. Portofolio saham yang dimiliki harusnya merupakan kumpulan dari saham-saham terbaik sehingga dapat memberikan hasil yang baik bagi para investor. Optimisasi portofolio saham adalah proses pemilihan portofolio terbaik dari semua portofolio yang dipertimbangkan, sesuai dengan tujuan investor. Portofolio mean-variance merupakan metode yang paling sering dipakai dalam optimisasi portofolio saham. Teori ini mengasumsikan bahwa investor ingin memaksimalkan return yang bergantung pada jumlah risiko tertentu.  Pada penelitian ingin diketahui portofolio mean-variance dengan implementasi machine learning. Machine learning yang digunakan adalah analisis klasifikasi, sebelum melakukan analisis klasifikasi, akan dilakukan perhitungan beberapa indikator yang sudah ditentukan sebagai input dalam analisis klasifikasi. Metode klasifikasi yang digunakan adalah algoritma random forest untuk memprediksi arah pergerakan harga suatu saham apakah bergerak naik atau turun. Model GARCH dipakai untuk melihat volatilitas serta memprediksi return suatu saham, sehingga diharapkan dapat meningkatkan performa dari optimisasi portofolio mean-variance. Untuk membuktikan apakah portofolio mean-variance dengan implementasi random forest dan model GARCH lebih baik maka akan dibuat perbandingannya dengan portofolio mean-variance biasa, lalu hasil dari kedua portofolio akan diukur kinerjanya melalui sharpe ratio. Kesimpulan yang didapat adalah optimisasi portofolio mean-variance dengan klasifikasi random forest dan model GARCH lebih baik dari pada optimisasi portofolio mean-variance biasa.","Stock investment is an attractive investment instrument. Unfortunately not many Indonesians invest in stocks. When investing, stock investors generally diversify their portfolios to minimize potential risks. A stock portfolio should be comprised of the best that promise the best return for investors. Stock portfolio optimization is the process of selecting the best portfolio among all considered portfolios, according to the investor's goals. The mean-variance portfolio is the most frequently used method of optimizing stock portfolios. This theory assumes that investors want to maximize returns relative to the associated risks.  This research aim to discover the mean-variance portfolio with the implementation of machine learning. The machine learning utilized in this research is the classification analysis. Indicators that have been determined as inputs will be calculated before the analysis. The classification method used is the random forest algorithm, which will predict the direction of the price movement of a stock, whether it is moving up or down. The GARCH model is used to view volatility and predict the return of a stock, which should improve the performance of the mean-variance portfolio optimization. To determine if the implementation of random forest and the GARCH model improves the mean-variance portfolio, the results will be put against that of a normal mean-variance portfolio through the sharpe ratio. The conclusion is that the mean-variance portfolio optimization with random forest classification and the GARCH model is better than the classic mean-variance portfolio optimization.","Kata Kunci : Random Forest, Classic Mean-Variance, Predicted Mean-Variance, GARCH, Portofolio Performance."
http://etd.repository.ugm.ac.id/home/detail_pencarian/186791,Penentuan Harga Opsi Beli Tipe Eropa dengan Adanya Biaya Short-selling,"RIFDAH JENITA HAREFA, Dr. Herni Utami, M.Si.",2020 | Skripsi | S1 STATISTIKA,"Black dan Scholes (1973) mengembangkan suatu model penentuan harga opsi yang telah banyak digunakan baik dalam akademik maupun praktis. Model ini memiliki salah satu asumsi dimana transaksi short selling diizinkan. Aktivitas pembelian saham yang umum dan sederhana adalah membeli di harga rendah dan kemudian menjualnya ketika harganya tinggi. Short-selling adalah kebalikannya, yaitu menjual saham di harga tinggi dan baru kemudian membeli saham tersebut ketika harganya turun. Transaksi short-selling dapat mempengaruhi pergerakan harga saham dimana harga saham juga akan mempengaruhi harga opsi. Sehingga dikembangkanlah model untuk memprediksi harga opsi dimana tedapat faktor biaya short-selling. 	Model perkembangan ini memodifikasi model Black-Scholes pada opsi beli tipe Eropa yang telah ada dan membuat model sesederhana mungkin untuk menentukan harga opsi dimana terdapat biaya short-selling. Model modifikasi Black-Scholes Short-selling memanfaatkan informasi Short Interest dan Cost to Borrow untuk menentukan harga opsi beli tipe Eropa dengan adanya biaya short-selling. Selanjutnya, dilakukan perbandingan antara harga opsi yang diperoleh dari model Black-Scholes dan Black-Scholes Short-selling pada data pengujian. Dengan menggunakan SRPE (Squared Relative Price Error) sebagai kriteria penentuan harga opsi, diperoleh hasil bahwa model Black-Scholes Short-selling lebih baik dibandingkan dengan model Black-Scholes.","Black and Scholes (1973) developed an option pricing model that has been widely used both academically and practically. This model has one of the assumptions where short selling transactions are permitted. A common and simple stock purchase activity is buying at a low price and then selling it when the price is high. Short-selling is the opposite, which is to sell shares at a high price and then buy the stock when the price goes down. Short-selling transactions can affect the movement of stock prices where the stock price will also affect the price of options. So a model is developed to predict the price of an option where there is a short-selling cost factor. This development model modifies the Black-Scholes model for existing European call options and makes the model as simple as possible to determine the price of options where there are short-selling costs. The Black-Scholes Short-selling modification model utilizes the Short Interest and Cost to Borrow information to determine the price of European call options with the cost of short-selling. Furthermore, we compare the option price obtained by the Black-Scholes model and Black-Scholes Short-selling models in the test data. By using SRPE (Squared Relative Price Error) as the criterion of option pricing, the results obtained that the Black-Scholes Short-selling model performs better than the Black-Scholes model.","Kata Kunci : option price, Black-Scholes, Short-selling, Partial Lending, Shorting fee"
http://etd.repository.ugm.ac.id/home/detail_pencarian/193447,Pemodelan Klaim Asuransi Properti Menggunakan Mixture Model dan Pendekatan Bayesian Melalui Simulasi Markov Chain Monte Carlo (MCMC),"INTAN YONIKSA K, Dr. Adhitya Ronnie Effendie, S. Si., M. Sc.",2020 | Skripsi | S1 STATISTIKA,"Pemodelan data klaim merupakan salah satu langkah krusial bagi perusahaan asuransi. Sayangnya, model umum yang banyak digunakan tidak cukup baik untuk mengakomodasi data ekstrem dalam data klaim tersebut. Sehingga diperlukan suatu model khusus untuk dapat menganalisis data klaim dengan lebih baik lagi. Dalam tugas akhir ini akan dibahas salah satu alternatif pemodelan data ekstrem menggunakan metode statistik khusus, yaitu Peaks Over Threshold (POT) dari Extreme Value Theory (EVT). Pemodelan data secara lengkap disajikan dengan membentuk mixture model yang disusun atas distribusi umum, untuk data di bawah nilai ambang, dan Generalized Pareto Distribution dari teorema POT tersebut, untuk data di atas nilai ambang. Estimasi parameter, termasuk ambang, dilakukan berdasarkan metode Bayesian dengan bantuan simulasi Markov Chain Monte Carlo (MCMC). Model terbaik berdasarkan Deviance Information Criterion (DIC), selanjutnya akan digunakan sebagai dasar perhitungan risiko menggunakan VaR dan TVaR yang dapat dijadikan pertimbangan bagi perusahaan untuk menentukan kebijakan.","Claim data modeling is a crucial task in an insurance company. Unfortunately, the commonly used models are not good enough to accommodate extreme data in those claim insurance data. This thesis discusses one of the alternatives extreme data modeling, which is Peaks Over Threshold from Extreme Value Theory, for better claim data analysis. A complete model is provided here by constructing a mixture from the common model and Generalized Pareto Distribution from those theories. Parameters and the threshold are estimated using the Bayesian method through Markov Chain Monte Carlo simulation. The best model will be chosen based on the Deviance Information Criterion and then will be used to calculate VaR and TVaR to give the company some consideration for policymaking.","Kata Kunci : Mixture model, Peaks Over Threshold, MCMC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/186026,LOCALLY COMPENSATED RIDGE GEOGRAPHICALLY WEIGHTED REGRESSION (LCR-GWR) UNTUK MENGATASI MASALAH MULTIKOLINEARITAS PADA MODEL REGRESI SPASIAL,"RIFKA ELSA PRASTIWI, Drs. Zulaela, Dipl.Med.Stats., M.Si",2020 | Skripsi | S1 STATISTIKA,"Geographically Weighted Regression (GWR) adalah metode yang cukup efektif dalam estimasi parameter pada data dengan heterogenitas spasial. Seperti halnya pada regresi linier berganda, masalah multikolinieritas juga dapat ditemui pada regresi spasial, kondisi ini dinamakan multikolinearitas lokal. Jika kondisi ini tidak diselesaikan, estimasi parameter yang diperoleh akan menjadi tidak stabil. Geographically Weighted Ridge Regression (GWRR) dan Locally Compensated Ridge Geographically Weighted Regression (LCR-GWR) merupakan pengembangan dari model GWR untuk menangani adanya kasus multikolinieritas lokal pada data spasial. Studi kasus pada skripsi ini menggunakan data tingkat kemiskinan di 25 kecamatan Kabupaten Wonogiri tahun 2011 dan variabel yang mempengaruhinya. Diperoleh kesimpulan bahwa metode LCR-GWR memberikan performa yang lebih baik dibandingkan dengan metode GWRR.","Geographically Weighted Regression (GWR) is an effective method in estimating parameters in data with spatial heterogeneity. As with multiple linear regression, the problem of multicollinearity can also be found in spatial regression, this condition is called local multicollinearity. If this condition is not resolved, the estimated parameters obtained will become unstable. Geographically Weighted Ridge Regression (GWRR) and Locally Compensated Ridge Geographically Weighted Regression (LCR-GWR) is a development of the GWR model to deal with cases of local multicollinearity in spatial data. The case study in this thesis uses the poverty rate data in 25 districts in Wonogiri Regency in 2011 and the variables that influence it. It was concluded that the LCR-GWR method gives better performance than the GWRR method.","Kata Kunci : heterogenitas spasial, multikolinearitas lokal, ridge, GWR, GWRR, LCR-GWR"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192959,Implementasi Metode Random Forest dan Artificial Neural Network pada Imbalanced dan Balanced Data dalam Mendeteksi Penipuan Kartu Kredit,"DESY AMBARWATI, Dr. Herni Utami, M.Si.",2020 | Skripsi | S1 STATISTIKA,"Perkembangan zaman telah mengantarkan manusia kepada era digital yang semakin memudahkan manusia dalam melakukan pekerjaan, interaksi, dan transaksi. Salah satu inovasi yang sangat bermanfaat adalah adanya kartu kredit terlihat dari semakin bertambahnya pengguna kartu pintar tersebut dari waktu ke waktu. Namun, seiring dengan meningkatnya penggunaan kartu kredit, tindak penipuan kartu kredit pun ikut meningkat sehingga dapat menyebabkan kerugian keuangan yang tidak sedikit. Oleh karena itu, diperlukan metode yang efektif dalam mendeteksi penipuan kartu kredit (credit card fraud detection) sehingga dapat mencegah atau mengurangi kerugian yang akan terjadi akibat adanya penipuan tersebut. Metode analisis yang digunakan dalam penelitian ini adalah Random Forest dan Backpropagation Artificial Neural Network (ANN) sedangkan teknik untuk mengatasi imbalanced training data adalah Synthetic Minority Oversampling Technique (SMOTE). Hasil analisis menunjukkan bahwa pembentukan model dengan imbalanced training data mempunyai performa yang lebih baik dibandingkan dengan balanced training data walau perbedaan keduanya tidak terlalu signifikan, Backpropagation ANN mempunyai performa yang lebih baik dibandingkan dengan Random Forest namun juga dengan perbedaan yang tidak terlalu signifikan, dan penggunaan SMOTE dapat meningkatkan sensitivitas pada performa model. Oleh karena itu, kedua metode sangat baik dalam melakukan klasifikasi atau prediksi terhadap data tidak seimbang maupun data seimbang.","The development of times has led humans to the digital era which makes it easier for people to carry out work, interactions, and transactions. One of the most useful innovations is the existence of a credit card that can be seen from the increasing number of that smart card users from time to time. However, in line with the use of credit cards, the credit card fraud is also increasing so that it can cause significant financial losses. Therefore, an effective method of credit card fraud detection is needed to prevent or reduce losses that will occur due to the fraud. The analytical method used in this research is Random Forest and Backpropagation Artificial Neural Network (ANN), while the technique used for overcoming the imbalanced training data is Synthetic Minority Oversampling Technique (SMOTE). The results of the analysis show that the models that are formed by imbalanced training data has better performance than that are formed by the balanced training data although the difference values between those models are not too significant, Backpropagation ANN has better performance compared to Random Forest but also with less significant difference values, and the use of SMOTE can increase the sensitivity of the performance model. Therefore, both methods are very good at classifying or predicting unbalanced and balanced data.","Kata Kunci : analisis penipuan kartu kredit, SMOTE, Random Forest, Backpropagation, Artificial Neural Network, klasifikasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/184012,"EVALUASI KINERJA BACKPROPAGATION, EXTREME GRADIENT BOOSTING, FEEDFORWARD NETWORK PADA KLASIFIKASI KLIEN BERLANGGANAN DEPOSITO BERJANGKA","TRIS DIANASARI, Prof.Dr.rer.nat. Dedi Rosadi, M.Sc",2020 | Skripsi | S1 STATISTIKA,"Jaringan syaraf adalah metode yang sering digunakan untuk memprediksi. Teknik yang paling popular adalah metode saraf algoritma jaringan backpropagation. Namun algoritma backpropagation memiliki kelemahan yaitu sangat lama untuk konvergen dan permasalahan local minimum yang membuat jaringan syaraf tiruan sering terjebak pada local minimum. Deep Neural Network adalah artificial neural network yang memiliki banyak layer, umumnya lebih dari 3 layers (input layer, N hidden layers,output layer). Mxnet merupakan salah satu pengembangan algoritma dari deep neural networks yang memiliki keunggulan menghasilkan akurasi yang lebih baik. Boosting merupakan salah satu keluarga ensemble yang meliputi banyak algoritma. Xgboost merupakan versi Gradient Boosting Machine yang lebih efisien dan scalable. Dari hasil analisis dengan 3 metode yang berbeda diperoleh kesimpulan bahwa metode yang memiliki akurasi terbaik pada studi kasus data Bank additional adalah Metode Extreme Gradient Boosting, kemudian diikuti dengan metode Deep Learning Feedforward Network, dan terakhir adalah Neural Network.","Neural network is a method often used to predict. The most popular technique is the neural network backpropagation algorithm. However, the backpropagation algorithm has some weaknesses. It took too long to be convergent and it has minimum local problems that make artificial neural networks often get stuck at the local minimum. Deep Neural Network is an artificial neural network that has many layers, generally more than 3 layers (input layer, N hidden layers, output layer). MXnet is one of the developed algorithms from deep neural networks that has the advantage of producing better accuracy.  Boosting is an ensemble family that includes many algorithms. Xgboost is a more efficient and scalable version of the Gradient Boosting Machine. From the results of the analysis with 3 different methods it can be concluded that the method with the best accuracy in the case study of additional Bank data is the Extreme Gradient Boosting Method, followed by the Deep Learning Feedforward Network method, and finally the Neural Network.","Kata Kunci : Backpropagation, Neural Network, Extreme Gradient Boosting, Deep Learning, akurasi, klasifikasi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/186072,IMPLEMENTASI SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE (SMOTE) UNTUK KLASIFIKASI DATA TIDAK SEIMBANG,"I DEWA GEDE NATIH B, Vemmie Nastiti Lestari, S.Si., M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Analisis klasifikasi merupakan salah satu metode supervised learning yang dapat digunakan untuk memprediksi label dari suatu data berdasarkan data latih yang diberikan. Dalam klasifikasi seringkali dijumpai data yang tidak seimbang, yaitu salah satu label memiliki jumlah yang jauh lebih banyak dari label yang lain. Hal ini dapat menimbulkan masalah yaitu kurangnya kemampuan model untuk memprediksi data yang berasal dari label yang sedikit tersebut (kelas minoritas), sehingga walaupun akurasi yang dihasilkan tinggi namun sebagian besar prediksi tepat hanya berasal dari kelas mayoritas saja. Metode SMOTE dapat diterapkan untuk mengatasi permasalahan tersebut, di mana dengan metode ini dibuat instance sintetis yang berasal dari kelas minoritas sehingga menghasilkan data yang lebih seimbang. Pada skripsi ini dilakukan analisis pada data Bank Marketing dengan menggunakan model klasifikasi Naive Bayes dan Random Forest yang dikombinasikan dengan metode SMOTE serta menggunakan rasio yang berbeda-beda. Dari analisis yang dilakukan diperoleh kesimpulan bahwa metode Naive Bayes pada data SMOTE1.0 menghasilkan performa yang paling baik, terutama dilihat dari akurasi kelas positif.","Classification analysis is one of the supervised learning methods that can be used to predict the label of a data based on the training data provided. In classification, it is often found imbalanced data, that is, one label has far more amount than the other label. This can cause the lack of the ability of the model to predict data from the fewer label (minority class), so that even though the accuracy produced is high, most of the correct predictions only come from the majority class. The SMOTE method can be applied to overcome these problems, where from this method synthetic instances are made that come from minority class to produce more balanced data. This undergraduate thesis analyzes the Bank Marketing data using the Naive Bayes and Random Forest classification models combined with the SMOTE method and using different ratios. From the analysis conducted it was concluded that the Naive Bayes method on SMOTE1.0 data produced the best performance, especially viewed from the accuracy of the positive class.","Kata Kunci : klasifikasi, data tidak seimbang, SMOTE, Naive Bayes, Random Forest"
http://etd.repository.ugm.ac.id/home/detail_pencarian/192474,Analisis Obligasi dengan Pemodelan Suku Bunga Vasicek Model Merton,"FUNNIA ALVIONITA I, Drs. Zulaela, Dipl. Med. Stats., M,Si.",2020 | Skripsi | S1 STATISTIKA,"Obligasi adalah suatu instrumen keuangan berupa surat hutang yang disepakati oleh penerbit obligasi dan investor untuk nantinya penerbit obligasi akan membayar kupon dan hutang pokok pada saat jatuh tempo. Kupon merupakan imbal hasil untuk investor dari penerbit obligasi. Dalam pelaksanaannya, berinvestasi berupa obligasi selain mendapatkan keuntungan juga memiliki risiko yang dapat merugikan investor sabagai pemegang obligasi yaitu salah satunya risiko kredit. Pengukuran risiko kredit pertama kali dikembangkan oleh Merton (1974) dengan asumsi yang terdapat pada Model Black-Scholes. Pada model Merton diasumsikan bahwa kondisi obligasi tanpa kupon dan jika perusahaan sebagai penerbit obligasi tidak mampu memenuhi kewajiban pembayaran hutang pada saat jatuh tempo, maka dapat dikatakan perusahaan tersebut default atau mengalami kebangkrutan. Suku bunga bebas risiko yang digunakan dalam perhitungan risiko kredit obligasi ini mengikuti suku bunga Vasicek. Setelah dilakukan pengaplikasian terhadap aset PT Bank Rakyat Indonesia Tbk dan aset PT Bank Bukopin Tbk, hasilnya menunjukkan bahwa kedua perusahaan dianggap masih dapat memenuhi pembayaran kewajibannya pada saat jatuh tempo.","Bonds are financial instruments in the form of debt securities that are approved by bond issuer and investor where the bond issuer required to pay the coupon and the principal value at maturity. Coupons are returns for investors from the bond issuer. In practice, investing in bond instruments will get many profit, but also has risk that can harm investors as the bondholders, one of them is credit risk. Credit risk structural model for the first time was developed by Merton (1974) with the assumptions that exist in the Black-Scholes Model. In the Merton Model assumed bonds condition is without coupon or free coupon and when the firm as the bond issuer cannot pay the debt at the maturity the firm can be said to be default or bankrupt. The free-risk interest rate that used in this credit risk valuation is following Vasicek rate. After applying it to the asset of PT Bank Rakyat Indonesia Tbk and PT Bank Bukopin Tbk, the results show that both companies are considered to be able to full their obligations at the maturity.","Kata Kunci : Risiko Kredit, Model Merton, Suku Bunga Vasicek, Obligasi Berkupon Nol"
http://etd.repository.ugm.ac.id/home/detail_pencarian/186079,MODIFIKASI JACKKNIFE RIDGE DALAM KASUS MULTIKOLINEARITAS  PADA REGRESI POISSON,"HAFILDA BELA F, Prof. rer. nat. Dedi Rosadi, S.Si., M,Sc",2020 | Skripsi | S1 STATISTIKA,"Regresi poisson merupakan salah satu metode regresi yang dapat  menggambarkan hubungan antara variabel independen (X) dan variabel dependen (Y) dimana variabel dependen berbentuk cacaah dan diasumsikan berdistribusi poisson.  Metode estimasi yang umum digunakan untuk menaksir parameter pada model regresi  poisson adalah metode maximum likelihood estimation (MLE). Salah satu asumsi yang  harus di penuhi dalam metode regresi adalah tidak adanya multikolinearitas. Apabila  terdeteksi adanya multikolinearitas, estimasi parameter kuadrat terkecil menjadi  kurang baik. Modified jackknifed poisson ridge regression estimator dapat digunakan untuk  mengatasi masalah multikolinearitas yang terjadi pada regresi poisson, metode ini  dikembangkan oleh Semra Turkan & Gamze Ozel (2015). Modified jackknifed poisson  ridge regression ini merupakan pengembangan dari metode poisson ridge regression yang dikemukakan oleh K. Mansson dan G. Shukur (2011). Studi kasus ini  menggunakan data jumlah kasus Difteri pada tahun 2017 di 34 provinsi di Indonesia.  Diperoleh kesimpulan bahwa metode Modified jackknifed poisson ridge regression lebih efisien digunakan untuk mengatasi masalah multikolinearitas dibandingkan  dengan motode jackknifed poisson ridge regression dan poisson ridge regression dilihat berdasarjan kriteria nilai MSE.","Poisson regression is one of the regression method that can describe the  relationship between independent variable (X) and dependent variable (Y) which is the  dependent variable is in the form of a count and assumed to have poisson distribution.  Maximum-likelihood estimator (MLE) method is commonly used to estimate in  poisson regression model. There are some assumptions shoul be found in classical  regression analysis method, one of them is no multicollinearity. If multicolliearity is  detected, the estimation of least square parameter become less valid and the variance  of error will be large. Modified jackknifed poisson ridge regression estimator is proposed to remedy  the multicollinearity, this method was developed by Semra Turkan and Gamze Ozel  (2015). This Modified jackknifed poisson ridge regression method is a development of  the ridge poisson regression method proposed by K. Mansson dan G. Shukur (2011).  This paper case study is using the number of diphtheria cases in 34 provinces in  Indonesia on 2017. The conclusion is that the modified jackknifed poisson ridge  regression method us more efficient to overcome multicollinearity probles compared  to jackknifed poisson ridge regression and poisson ridge regression method, it seen by  MSE criterion.","Kata Kunci : Multikolinearitas, modified jackknifed poisson ridge regression, MSE."
http://etd.repository.ugm.ac.id/home/detail_pencarian/183776,Analisis Cluster pada Pengukuran Performa Support Vector Regression untuk Pemodelan Harga Saham,"IZZA DINIKAL ARSY, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2020 | Skripsi | S1 STATISTIKA,"Investasi adalah kegiatan membeli suatu aset yang kemudian akan dijual kembali di masa mendatang dengan nilai yang lebih tinggi bertujuan untuk memperoleh keuntungan dengan strategi tertentu. Obyek investasi dapat berupa aset finansial pada pasar modal misalnya saham. Sebelum mengambil keputusan berinvestasi investor perlu mempertimbangkan dana yang akan diinvestasikan dan saham yang akan dipilih. Investor yang risk averse akan mencari jenis investasi saham dengan risiko seminimum mungkin. Salah satu langkah yang dapat ditempuh adalah dengan membuat model dari harga saham dan melakukan prediksi pergerakannya dalam beberapa periode ke depan. Beberapa penelitian mengenai pemodelan atau prediksi pergerakan saham yang telah dilakukan diantaranya menggunakan metode Simple Moving Average, Exponential Moving Average maupun metode ARCH/GARCH. Pada skripsi ini akan dibahas mengenai performa pemodelan saham menggunakan Support Vector Regression (SVR). Performa tersebut diukur menggunakan nilai root mean square error (RMSE) pada dua klaster saham yang dibentuk berdasarkan nilai volatilitas-nya yang didekati dengan nilai variansinya yaitu saham dengan variansi besar dan saham dengan variansi kecil. Studi kasus penelitian ini menggunakan data closing price saham harian periode 12 Oktober 2018 sampai 11 Oktober 2019 dari 10 saham indeks LQ-45. Nilai close price yang diamati dari 10 saham antara lain ADHI, ASII, BBNI, BBRI, BMRI, EXCL, ICBP, TPIA, SMGR dan WIKA. Dari 10 saham tersebut dibentuk nilai estimasi close price menggunakan Simple Moving Average yang nantinya akan menjadi variable dependen pada pemodelan SVR. Kesimpulan yang diperoleh bahwa performa SVR pada saham dengan dengan variansi besar menghasilkan RMSE yang relatif lebih besar dari performa SVR pada saham dengan variansi kecil.","Investment is the activity of buying an asset which will then be resold in the future with a higher value aimed at obtaining profits with certain strategies. The object of investment can be in the form of financial assets in the capital market such as stocks. Before making an investment decision, investors need to consider the funds to be invested and the stocks to be chosen. Investors who are risk averse will look for types of stock investments with minimum risk. One step that can be taken is to create a model of stock prices and predict its movements in the next few periods. Some research on modeling or prediction of stock movements that have been carried out include using the Simple Moving Average method, Exponential Moving Average and the ARCH / GARCH method. This paper will discuss the performance of stock modeling using Support Vector Regression (SVR). The performance is measured using root mean square error (RMSE) value in two stock clusters formed based on its volatility which is approached by its variance value, namely stocks with large variance and stocks with small variance. This case study uses daily closing price data for the period October 12, 2018 to October 11, 2019 from 10 LQ-45 index shares. Close price values observed from 10 stocks included ADHI, ASII, BBNI, BBRI, BMRI, EXCL, ICBP, TPIA, SMGR and WIKA. Out of the 10 stocks, the close price estimation value is formed using Simple Moving Average, which will become the dependent variable in SVR modeling. The conclusion obtained that the performance of SVR on stocks with large variance produces RMSE which is relatively larger than the performance of SVR on stocks with small variance.","Kata Kunci : support vector regression, volaitilias, K-means, RMSE, saham."
http://etd.repository.ugm.ac.id/home/detail_pencarian/194016,PENERAPAN RIDGE MM  UNTUK DATA PENCILAN DAN MULTIKOLINEARITAS PADA MODEL SEEMINGLY UNRELATED REGRESSION,"APRILIA DWI ASTUTI, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2020 | Skripsi | S1 STATISTIKA,"Model Seemingly Unrelated Regression (SUR) merupakan suatu model regresi yang terdiri dari beberapa sistem persamaan dimana antar error pesamaan terdapat contemporaneus correlation. Setiap persamaan dapat diestimasi dengan metode Ordinary Least Square, namun metode OLS memiliki kelemahan dan tidak efisien untuk digunakan karena membuang informasi kemungkinan adanya contemporaneus correlation antar persamaan. Salah satu metode estimasi parameter yang tepat untuk model SUR adalah metode Generalized Least Square (GLS). Namun, metode ini kurang mampu bertahan terhadap kehadiran pencilan dan masalah multikolinearitas pada pengamatan. Metode yang dapat digunakan untuk mengatasi pencilan dan multikolinearitas adalah metode Robust Ridge Regression berdasarkan estimator MM. Estimator MM adalah estimator invariant dari regresi dan dapat mencapai breakdown point setinggi 50% dengan tingkat efisiensi hampir 90%. Uji lagrange Multiplier digunakan untuk menguji ada tidaknya contemporaneus correlation antar persamaan. Dalam penulisan skripsi ini dibahas mengenai contoh penerapan model SUR dengan metode GLS dan metode ridge MM pada studi kasus faktor-faktor yang mempengaruhi Penanaman Modal Asing pada Negara Indonesia, China, dan Filipina.","The Seemingly Unrelated Regression (SUR) model is a regression model that consist of several system od regression equation which between the equation error occurs contemporaneously correlated. Each regression can be estimated by Ordinary Least Square method, but OLS method is not efficient to used, because OLS method is remove information on a possible correlation on system equations. One of the appropriate parameter estimation method for the SUR model is Generalized Least Square (GLS) method. But this method is less able to withstand the presence of multicollinearity and outliers in the observation data. Method that can be used for solving multicollinearity and outliers is Robust Ridge Regression based on MM-estimator. MM-estimator are invariant estimators of regression and can reach breakdown point as high as 50% with an efficiency level of almost 90%. Lagrange Multiplier test is used to test the presence of contemporaneously correlated of error between the equations. In this paper, we discuss the examples of the application of the SUR model using GLS method and ridge MM method in a case study of factors that influence Foreign Direct Investment (FDI) in Indonesia, China, and Phillipine.","Kata Kunci : Seemingly Unrelated Regression, contemporaneously correlated, Generalized Least Square, MM Ridge estimators, Cross Validation Criteria (CV_MM), MM-estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/186850,MODIFIED UNBIASED RIDGE REGRESSION MODEL UNTUK MENGATASI MASALAH MULTIKOLINEARITAS PADA MODEL LINIER,"ANDAR APRILIA W S, Yunita Wulan Sari, S.Si., M.Sc",2020 | Skripsi | S1 STATISTIKA,"Analisis Regresi adalah analisis statistika yang bertujuan untuk mengetahui hubungan antara variabel dependen (Y) dan variabel independen (X). Metode analisis regresi yang biasa digunakan adalah metode Ordinary Least Square (OLS). Metode ini didasarkan pada pemenuhan asumsi regresi klasik, termasuk tidak ada multikolinieritas. Multikolinearitas adalah hubungan antar variabel independen yang menyebabkan estimator OLS menjadi tidak BLUE. Salah satu metode untuk mengatasi masalah multikolinieritas yaitu dengan regresi ridge. Untuk mendapatkan model estimasi regresi ridge salah satunya dengan metode Ordinary Ridge Regression (ORR) yang merupakan pengembangan dari metode Ordinary Least Square (OLS). Pada 1995 muncul pengembangan dari ORR, yaitu Unbiased Ridge Regression (URR). Metode Unbiased Ridge Regression adalah modifikasi ORR dengan mengganti konstanta bias k dengan nilai vektor informasi prior J sehingga diperoleh model regresi ridge yang tidak bias. Pada metode URR terjadi variansi yang meningkat saat menghilangkan bias. Batah dan Gore (2009) mengembangkan penduga penyusutan baru yang disebut Modified Unbiased Ridge Regression (MURR). Dengan pra-kelipatan matriks [I-k(X^' X+kI)^(-1)]untuk mengurangi variansi yang meningkat dalam OLS, juga diharapkan memberikan efek yang sama dengan URR. Data ekonomi bulanan dari Januari 2010 hingga Agustus 2019 yang diperoleh dari Bank Indonesia dalam Statistik Ekonomi Keuangan Indonesia digunakan untuk studi kasus. Jumlah uang yang beredar akan menjadi variabel dependen dan faktor-faktor yang mempengaruhinya adalah variabel independen. Pada hasil akhir dapat disimpulkan bahwa nilai MSE, AIC dan BIC dari metode MURR lebih kecil daripada metode ORR dan URR. Sehingga metode Modified Unbiased Ridge Regression merupakan metode yang paling efisien dalam mengatasi masalah multikolinearitas dibandingkan metode ORR dan URR.","Regression Analysis is a statistical analysis that aims to determine  relationship between dependent variable (Y) and independent variable (X). The usual method of regression analysis is Ordinary Least Square (OLS). This method is based on the fulfillment of classical regression assumptions, which is including there is no multicollinearity. Muticollinearity is the correlation between independent variable which cause the estimator OLS become not BLUE. One of method to solve the problem of mullticolinearity is ridge regression. To get the estimation model on ridge regression one of them is the Ordinary Ridge Regression (ORR) method which is a development of Ordinary Least Square (OLS) method. In 1995 the development of ORR emerged, namely Unbiased Ridge Regression (URR). The Unbiased Ridge Regression method is a modification of Ordinary Ridge Regression by replacing the bias constant k with the value of vector prior information  J so an unbiased ridge regression model is obtained. In the URR method there is an increasing variance when removing bias. Batah and Gore (2009) developed a new shrinkage estimator called Modified Unbiased Ridge Regression (MURR).With pre-multiple the matrix [I-k(X^' X+kI)^(-1)] to reduce the inflated variances in OLS, also give the same effect with URR.  Monthly economic data from January 2010 to August 2019 obtained from Bank Indonesia in Indonesia's Financial Economic Statistics is used to the case study. The amount of money in circulation will be dependent variable and the factors that influence it will be independent variable. The final result concluded that the MSE, AIC and BIC values of the MURR method are smaller than the ORR and URR methods. So, the Modified Unbiased Ridge Regression is the most efficient method to resolve multocollinearity problem than ORR and URR.","Kata Kunci : Multicollinearity, Ordinary Least Square, Ordinary Ridge Regression, Unbiased Ridge Regression, Modified Unbiased Ridge Regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/186868,"ESTIMASI VALUE AT RISK INTRADAY PORTOFOLIO DENGAN METODE CGARCH(1,1)-EVT-COPULA","WILLY RADITYA N, Dr. Abdurakhman, M.Si.",2020 | Skripsi | S1 STATISTIKA,"Transaksi jual beli saham dalam waktu singkat disebut dengan intraday trading. Perhitungan risiko untuk transaksi saham intraday dengan menggunakan data berfrekuensi tinggi menjadi penting untuk beberapa market participant yang aktif seperti intraday trader dan market maker. Data saham intraday memiliki efek musiman yang dikarenakan nilai return akan tinggi pada awal pembukaan harga dan menjelang penutupan harga setiap harinya serta akan berada di titik paling rendah saat tengah hari, sehingga membentuk pola U atau J yang perlu penanganan terlebih dahulu menggunakan rata-rata return kuadrat untuk setiap periode intraday. Selain itu, data saham intraday juga memiliki heteroskedastisitas seperti data runtun waktu keuangan pada umumnya. Heteroskedastisitas akan ditangani dengan model Component-GARCH dengan orde (1,1) yang memisahkan efek short-run dan long-run dari volatilitas. Ekor distribusi data return bersifat fat-tailed yang menunjukkan adanya kejadian ekstrim berupa kerugian, yang ditandai dengan nilai skewness negatif. Penanganan untuk kejadian ekstrim dilakukan dengan salah satu metode dari Extreme Value Theory, yaitu Peak Over Threshold dengan pendekatan Generalized Pareto Distribution. Pemodelan portofolio saham intraday dilakukan dengan copula bivariat dari keluarga Student-t, Clayton dan Gumbel dimana estimasi parameter copula bivariat digunakan metode IFM. Dalam studi kasus digunakan data harga indeks saham PEFINDO25 dan JKMING dalam interval 5 menitan selama 36 hari.","Buying and selling stocks on the same day is called intraday trading. Risk management for the intraday stocks using high frequency data is now important for active market participant such as intraday trader and market maker. Intraday stocks return has a seasonal term since it is typically higher at the opening and towards the closing of trade, and lower around midday, so the curve is U or J shaped. The seasonal term needs to be removed, using average of the squared returns for each intraday period. The intraday stocks also have heteroskedasticity problem just like other financial time series data. The heteroskedasticity is handled using Component-GARCH(1,1) model which decompose volatility into short run and long run component. The fat tailed distribution shows the existence of extreme case or losses. To solve the extreme case, Extreme Value Theory using Peak Over Threshold method with Generalized Pareto Distribution approach is used. The intraday portfolio is modeled by copula from Student-t, Clayton and Gumbel family, and the parameter's estimation using IFM method. For the application, PEFINDO25 and JKMING stock indices with 5 minutes interval for 36 days is used to estimate the Value at Risk of intraday portfolio.","Kata Kunci : Intraday, Component-GARCH, copula bivariat, Generalized Pareto."
http://etd.repository.ugm.ac.id/home/detail_pencarian/183801,Penerapan Robust Jackknife Ridge Regression dengan Estimator Generalized-M untuk Mengatasi Multikolinearitas dan Pencilan,"Farida Dwi Hanifah, Dr. Danardono, MPH",2020 | Skripsi | S1 STATISTIKA,"Model yang didapatkan pada analisis regresi harus memenuhi beberapa asumsi untuk dapat dikatakan model yang baik. Salah satu asumsi tersebut adalah tidak ada multikolinearitas. Multikolinearitas adalah hubungan atau korelasi yang terjadi antar variabel independen pada regresi. Apabila multikolinearitas ini tidak terpenuhi, maka model regresi yang didapat tidak tepat dan memiliki tingkat kesalahan yang tinggi. Selain itu, apabila pada data memiliki pencilan pada variabel independen, maka akan mengakibatkan model regresi yang didapat tidak tepat.  Metode yang sering digunakan untuk mengatasi multikolinearitas adalah dengan menggunakan metode regresi ridge. Regresi ridge memiliki konsep memberikan tetapan k pada matriks Z'Z. Tetapi metode ini memiliki kekurangan yaitu, model regresi yang didapat memiliki nilai bias yang tinggi. Untuk mengatasi hal tersebut, maka dapat digunakan metode regresi jackknife ridge. Namun, metode tersebut tidak dapat digunakan apabila terdapat pencilan pada variabel independen. Metode yang dapat digunakan untuk mengatasi pencilan pada data dapat digunakan metode regresi robust dengan estimator Generalized-M (GM). Sehingga untuk mengatasi multikolinearitas dan pencilan secara bersama-sama dapat digunakan metode regresi robust jackknife ridge dengan estimator GM. Menggunakan nilai Mean Square Error (MSE), Akaike Information Criterion (AIC) dan Bayesian Information Criterion (BIC) dapat disimpulkan bahwa  metode Regresi Robust Jackknife Ridge dengan Estimator GM lebih baik daripada Regresi Robust Ridge degan Estimator GM.","The model that obtained from  regression analysis must fulfill the classical regression assumptions to be a good model. One such assumption is that there is no multicollinearity. Multicollinearity is the relationship or correlation that occurs between independent variables in regression. If there is multicolinearity, then the regression model obtained is incorrect and has a high error rate. In addition, if the data has outliers on independent variables, the model regression that obtained is incorrectly.  The method that often used to overcome multicollinearity is the ridge regression method. Ridge regression has the concept of giving the k constant to the Z'Z matrix. But this method has a disadvantage, that is the regression model obtained has a high bias value. To overcome this, the Jackknife ridge regression method can be used. However, this method cannot be used if there are outliers on the independent variables. Methods that can be used to overcome outliers in data is robust regression methods with Generalized-M (GM) estimators. So, to overcome multicollinearity and outliers simultaneously  can use robust jackknife ridge regression method with a GM estimator. Using the Mean Square Error (MSE), Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) it can be concluded that the Jackknife Ridge Robust Regression method with GM Estimator is better than the Robust Ridge Regression with GM Estimator.","Kata Kunci : multikolinearitas, pencilan, estimator Generalized-M, robust, regresi ridge, regresi jackknife ridge"
http://etd.repository.ugm.ac.id/home/detail_pencarian/173313,Optimisasi Portofolio Berdasarkan Modified Safety First dengan Aset Bebas Risiko,"Diah Ayu Kusumaningrum, Prof. Dr. Sri Haryatmi, M.Sc.",2019 | Skripsi | S1 STATISTIKA,"Dalam dunia investasi, yang menarik minat bagi kebanyakan investor lembaga ataupun perorangan merupakan investasi pada aset finansial baik pada aset berisiko maupun aset bebas risiko. Investasi merupakan komitmen atas sejumlah dana atau sumber daya lainnya yang dilakukan saat ini dengan tujuan memperoleh keuntungan di masa yang akan datang. Keputusan investasi bagi seorang investor tidak dapat dihindarkan dari risiko investasi. Salah satu cara untuk meminimalisir risiko tersebut yaitu dengan membentuk portofolio. Dalam pembentukan portofolio, seorang investor berusaha untuk memaksimalkan return dengan tingkat risiko tertentu. Suatu portofolio secara umum hanya memasukkan aset-aset berisiko ke dalam pembentukan portofolionya, sedangkan aset bebas risiko hanya digunakan untuk menentukan letak dari portofolio optimalnya dan tidak dimasukkan sebagai aset di dalam portofolionya. Pada skripsi ini akan dibahas mengenai pembentukan portofolio optimal berdasarkan modified safety first dengan aset bebas risiko. Dalam membentuk portofolio optimal dengan metode ini digunakan suatu fungsi Lagrange untuk menghitung rumus bobot portofolio, benchmark return, dan mean return yang optimal. Pembentukan portofolio optimal berdasarkan metode ini dilakukan dengan tiga kondisi yang berbeda. Studi kasus dalam skripsi ini menggunakan data saham harian pada periode 04 Juni 2018 sampai 11 Februari 2019 dari 5 saham indeks LQ-45 serta suku bunga SBI sebagai aset bebas risiko periode 18 Januari 2019. Akan dibentuk portofolio dari 5 saham yaitu ASII, BMRI, UNVR, ITMG, BSDE serta sebuah aset bebas risiko yaitu SBI yang diharapkan dapat memberikan keuntungan yang optimal apabila kedua aset tersebut dikombinasikan dengan menggunakan metode modified safety first.","In the world of investment, that interest to most institutional investors or individuals is investment in financial assets either on risky assets and risk-free assets. Investment is the commitment of a number of funds or other resources that is done today with the purpose of gaining profit in the future. The investment decision for an investor can not be avoided from investment risks. One way to minimize these risks is by forming a portfolio. In forming a portfolio, an investor seeks to maximize returns with a certain risk level. A portfolio is generally only put risky assets into the portfolio formation, while the risk-free asset is only used to determine the location of optimal portfolios and not included as assets in the portfolio. In this paper, we will discuss the formation of an optimal portfolio based on modified safety first with risk-free asset. In forming an optimal portfolio using this method, a Lagrange function is used to calculate the portfolio weighting formula, benchmark return, and optimal mean return. The formation of optimal portfolio based on this method is carried out with three different conditions. The case studies in this paper uses daily stock data during the period of June 4, 2018 until February 11, 2019 from 5 stocks LQ-45 index and SBI interest rate as the risk-free asset period January 18, 2019. There shall be established a portfolio of five stocks that ASII, BMRI, UNVR, ITMG, BSDE as well as a risk-free asset, namely SBI is expected to provide optimal benefit if both of these assets are combined using the modified safety first method.","Kata Kunci : portofolio, aset bebas risiko, benchmark return, mean return, modified safety first"
http://etd.repository.ugm.ac.id/home/detail_pencarian/175107,PENERAPAN ALGORITMA GENETIKA DALAM PEMILIHAN VARIABEL PADA MODEL REGRESI LOGISTIK,"LABIBAH ALYA HUWAIDA, Drs. Danardono, M.P.H., Ph.D",2019 | Skripsi | S1 STATISTIKA,"Pemilihan fitur atau variabel merupakan hal yang sangat penting dalam membangun sebuah model prediksi. Metode pemilihan variabel ada berbagai macam seperti purposeful selection, best subset, dan stepwise. Namun, metode-metode tersebut memiliki banyak kekurangan khususnya pada era big data. Pada penelitian ini digunakan metode Algoritma Genetika untuk memilih variabel yang relevan dalam model regresi logistik. Metode seleksi backward, forward, dan stepwise digunakan sebagai pembanding untuk mengetahui metode mana yang memiliki performa lebih baik. Dari hasil analisis dengan menggunakan dua buah data set dengan ukuran variabel yang berbeda diperoleh kesimpulan bahwa metode Algoritma Genetika memberikan nilai akurasi, sensitivitas, dan spesifisitas yang lebih baik dibandingkan dengan metode seleksi backward, forward, dan stepwise.","Feature selection is very important to consturct a predictive model. There are several methods of feature selection such as purposeful selection, best subset, and stepwise. Nevertheless, those methods have many drawbacks especially in big data era. In this study, genetic algorithm will be used to select variables which are relevant in logistic regression model. Backward, forward, and stepwise selection method are used to compare which method gives the better performance. In this study, two datasets with different dimensions are used. The conclusion from the analysis is genetic algorithm gives higher accuracy, sensitivity, and specificity than backward, forward, and stepwise selection.","Kata Kunci : Pemilihan Variabel, Algoritma Genetika, Seleksi Backward, Seleksi Forward, Seleksi Stepwise/ Feature Selection, Genetic Algortihm, Backward Selection, Forward Selection, Stepwise Selection"
http://etd.repository.ugm.ac.id/home/detail_pencarian/168208,KOEFISIEN KORELASI REGRESI TERMODIFIKASI UNTUK MENGATASI MASALAH MULTIKOLINEARITAS PADA MODEL REGRESI POISSON,"NUR ANISA YUNIARTI, Drs. Danardono, M.P.H., Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Ukuran kekuatan prediksi memiliki peranan penting dalam pemilihan model terbaik untuk analisis regresi, tak terkecuali untuk model regresi Poisson. Apabila salah menggunakan metode pengukuran kekuatan prediksi memungkinkan untuk salah memilih model terbaiknya. Pada model regresi Poisson terdapat asumsi yang diberlakukan yaitu ketiadaan multikolinearitas antarvariabel prediktor. Pada penelitian  Yunitaningtyas (2016) sebelumnya Regression Correlation Coefficient (RCC) atau koefisien korelasi regresi telah diusulkan sebagai ukuran kekuatan prediksi yang efektif untuk model regresi Poisson, namun ternyata belum mampu untuk menangani penyimpangan multikolinearitas. Untuk mengatasi permasalahan tersebut maka digunakan Modified Regression Correlation Coefficient atau koefisien korelasi regresi termodifikasi. Metode ini terinspirasi dari adjusted R^2 yang mampu menurunkan nilai RMSE dari estimatornya. Koefisien korelasi regresi termodifikasi ini memberikan nilai RMSE yang lebih kecil jika dibandingan dengan koefisien korelasi regresi. Dalam skripsi ini, metode koefisien korelasi regresi termodifikasi diaplikasikan untuk mencari model terbaik dari kasus gizi buruk balita di Indonesia di masing-masing provinsi tahun 2012.","Predictive power measure has an important role in choosing the best model of regression, no exception for Poisson regression models. Incorrect use of the predictive power measure method make it possible to incorrectly choose the best model. In Poisson regression model there is an assumption required, no multicollinearity between predictor variables. In the previous research Yunitaningtyas (2016), Regression Correlation Coefficient (RCC) has been proposed as an effective predictive power measure for Poisson regression model, but not yet able to handle the multicollinearity observations. To overcome these problems, Modified Regression Correlation Coefficient is used.This method is inspired by adjustedÃƒÂ£Ã¯Â¿Â½Ã¯Â¿Â½ RÃƒÂ£Ã¯Â¿Â½Ã¯Â¿Â½^2 which can reduce the RMSE value from the estimator. The modified regression correlation coefficient of this term gives a smaller RMSE value when compared to the regression correlation coefficient. In this thesis, the modified regression correlationt coefficient method was applied to find the best model of cases of under-five malnutrition in Indonesia in each province in 2012.","Kata Kunci : Regresi Poisson, Adjusted R^2, Multikolinearitas, Koefisien Korelasi Regresi, Koefisien Korelasi Regresi Termodifikasi, Gizi Buruk Balita, RMSE."
http://etd.repository.ugm.ac.id/home/detail_pencarian/168214,Pemodelan Data Runtun Waktu Nonlinier Menggunakan Pendekatan Algoritma Regresi Spline Adaptif Multivariat,"QUDHROTUL ZAHRO KH, Drs. Zulaela, Dipl.Med.Stats., M.Si",2019 | Skripsi | S1 STATISTIKA,"Regresi spline adaptif multivariat atau multivariate adaptive regression splines (MARS) adalah salah satu algoritma yang merupakan perbaikan dari algoritma recursive partitioning dengan mengganti fungsi langkah oleh fungsi spline terpotong. Algoritma MARS digunakan untuk memodelkan hubungan antara variabel respon dan prediktor dengan cara nonparametrik. Cara nonparametrik digunakan pada data yang mempunyai dimensi tinggi, yaitu data dengan variabel prediktor lebih dari 3 dan jumlah sampel lebih dari 50. Dalam pemodelan data runtun waktu, algoritma MARS digunakan untuk memodelkan data runtun waktu nonlinier sebagai perbaikan dari pemodelan Threshold Autoregressive (TAR) dengan tujuan untuk menghasilkan model yang bersifat kontinu pada batas subregion. Salah satu alasan dilakukan pemodelan nonlinier adalah jika terdapat data pencilan dalam periode data runtun waktu. Model terbaik diperoleh dari kriteria pemilihan model dengan membandingkan nilai GCV* (modifikasi Generalized Cross Validation) berdasarkan jumlah minimum observasi (MO) dan maksimum interaksi (MI) yang digunakan dalam mengestimasi model. Pada studi kasus, dilakukan pemodelan data runtun waktu pada data inflasi Indonesia periode Januari 2005 hingga Oktober 2018. Setelah dilakukan analisis, diperoleh model nonlinier terbaik dengan jumlah MO sebanyak 2 dan jumlah MI sebanyak 3 dengan nilai GCV* sebesar 0,1734099.","Multivariate adaptive regression splines (MARS) is one algorithm that is an improvement of the recursive partitioning algorithm by replacing the step function by truncated spline functions. MARS algorithm is used to model the relationship between response and predictors variables in nonparametric method. Nonparametric method are used in data that has high dimensions, which has the number of predictor variables is more than 3 and the number of samples is more than 50. In time series data modeling, the MARS algorithm is used to model nonlinear time series data as an improvement of the Threshold Autoregressive (TAR) model with the goal is to produce a model that is continuous at the subregion boundary. One reason for nonlinear modeling is if there are outlier data in the time series data period. The best model is obtained of the model selection criteria by comparing the value of GCV* (modified Generalized Cross Validation) based on the number of minimum observation (MO) and maximum interaction (MI) used in estimating the model. In the case study, time series data modeling was carried out on Indonesian inflation data for the period January 2005 to October 2018. After analysis, the best nonlinear model is obtained with the number of MO is 2 and the number of MI is 3 with a value of  GCV* is 0.1734099.","Kata Kunci :  Data Runtun Waktu, MARS untuk model runtun waktu, modifikasi Generalized Cross Validation (GCV*), Data Pencilan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/175382,ALGORITMA  KLASTER K-MODES UNTUK DATA KATEGORIK DENGAN TAMBAHAN INFORMASI ANTAR KLASTER,"FARRAH GITA NURMALA, Drs. Danardono, M.P.H., Ph.D.",2019 | Skripsi | S1 STATISTIKA,"K-modes merupakan teknik analisis klaster untuk mengelompokkan data kategorik. Performa dari hasil akhir analisis bergantung pada pusat klaster awal. Pada umumnya pusat klaster awal dipilih secara random dan hanya akan baik jika pusat klaster yang dipilih dekat dengan nilai klaster yang sesungguhnya. Maka diusulkan inisialisasi pusat klaster awal dengan memperhatikan jarak dan densitas dari data. Dalam algoritma k-modes, pada umumnya hanya memperhatikan informasi dalam klaster dan mengabaikan informasi antar klaster yang mungkin akan mengakibatkan lemahnya pemisahan antar klaster. Penelitian ini akan memperhatikan informasi antar klaster dalam algoritma k-modes. Metode yang disulkan akan diaplikasikan pada dataset Soybean (Small) . Analisis klaster yang menggunakan inisialisasi pusat klaster awal dan memperhatikan informasi antar klaster memiliki hasil yang lebih baik dan lebih akurat.","K-modes algorithm is a cluster analysis technique used to cluster categorical data. The performance of the algorithm depends on initial cluster center. Usually, initial cluster center is choosen randomly from the data and it will work well only if that initial choice is close to a good solution. This research aims to initialization initial cluster center which the distance and the density between the object considered. In the k-modes algorithm, generally computed based on within-cluster information only and the between-cluster information is not considered, which maybe result in the clustering result with weak separation among different cluster. This research aims to adding the between-cluster information to the k-modes algorithm. The proposed method will be applied to the Soybean (Small) dataset. Cluster analysis using initial cluster center initialization and betwen-cluster information has better and more accurate result.","Kata Kunci : k-modes, inisialisasi pusat klaster awal, analisis klaster, data kategorik"
http://etd.repository.ugm.ac.id/home/detail_pencarian/175127,ANALISIS VALUASI OBLIGASI MODEL FIRST PASSAGE TIME,"FARIDA NUR DADARI, Dr. Abdurakhman, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Obligasi adalah surat hutang yang telah disepakati oleh perusahaan pihak penerbit obligasi dan pihak pemegang obligasi untuk membayar hutang beserta bunganya pada tanggal pembayaran yang sudah ditentukan. Obligasi menjadi instrumen pasar modal yang banyak diminati oleh investor karena memiliki banyak keuntungan. Obligasi juga memiliki risiko yang dapat merugikan investor yaitu risiko kredit. Model struktural risiko kredit pertama kali diperkenalkan oleh Black-Scholes pada tahun 1973, kemudian dikembangkan oleh Merton pada tahun 1974 yang membuat model risiko kebangkrutan suatu perusahaan dengan menggunakan modifikasi model Black-Scholes. Pada tahun 1976, model Merton ini dikembangkan lagi oleh Black & Cox dengan menambahkan perjanjian pola waktu kebangkrutan yang dikenal dengan model First Passage Time. Waktu kebangkrutan perusahaan diasumsikan dapat terjadi kapan saja sebelum waktu jatuh tempo ketika nilai aset lebih kecil daripada nilai hutang. Setelah dilakukan pengujian terhadap aset PT Bank Central Asia Tbk dan PT Bank Bukopin Tbk, hasilnya menunjukkan bahwa model First passage Time dengan kupon satu periode lebih tepat untuk kondisi return aset yang berdistribusi normal.","Bonds are debt instruments that have been agreed upon by the bond issuers and the bondholders to pay the debt and interest on the specified payment date. Bonds become capital market instruments that are much in demand by investors because they have many advantages. Bonds also have risks that can harm investors, namely credit risk. The credit risk structural model was first introduced by Black-Scholes in 1973, then developed by Merton in 1974 which modeled the risk of bankruptcy of a company using a modification of the Black-Scholes model. In 1976, the Merton model was developed again by Black & Cox by adding a bankruptcy time pattern agreement known as the First Passage Time model. The time of the bankruptcy of a company is assumed to occur anytime before the due date when the asset value is smaller than the value of the debt. After testing the assets of PT Bank Central Asia Tbk and PT Bank Bukopin Tbk, the results show that the First passage Time model with one period coupons is more appropriate for the condition of asset returns that are normally distributed.","Kata Kunci : Valuasi Obligasi, First Passage Time, Peluang Kebangkrutan, Obligasi Satu Kupon. / Bond Valuation, First Passage Time, Probability Default, One Coupon Bond."
http://etd.repository.ugm.ac.id/home/detail_pencarian/175641,GRAFIK PENGENDALI FUZZY MULTIVARIATE EXPONENTIALLY WEIGHTED MOVING AVERAGE (FMEWMA),"MARTHA FEBRI JESICA PANGARIBUAN, Dr. Herni Utami, S.Si, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Grafik pengendali kualitas adalah suatu alat yang digunakan untuk mendeteksi penyimpangan pada suatu proses produksi. Kualitas produk menjadi faktor yang berpengaruh terhadap kepuasan konsumen. Salah satu grafik pengendali yang efisien digunakan untuk mendeteksi pergeseran yang kecil serta untuk mengendalikan proses dari dua atau lebih variabel yang berhubungan secara bersamasama adalah grafik pengendali Multivariate Exponentially Weighted Moving Average (MEWMA). Terkadang, terdapat ketidakpastian pada data apabila pada proses pengukuran ada campur tangan manusia. Data yang tidak pasti tersebut disebut data fuzzy. Apabila terdapat data fuzzy pada suatu proses pengendalian kualitas dan ingin dideteksi pergeseran kecil pada mean maka dibutuhkan grafik pengendali Fuzzy Multivariate Exponentially Weighted Moving Average (FMEWMA). Grafik pengendali FMEWMA mengurangi kesalahan dalam pengambilan keputusan. PT. Pioneer Flour Industries telah melakukan pengontrolan terhadap proses produksi tepung terigu Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¢Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½PalapaÃƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¢Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½. Variabel karakteristik kualitas yang dikontrol adalah moisture (X1), glutten (X2), dan ash (X3). Fungsi keanggotaan untuk X1, X2, dan X3 adalah fungsi keanggotaan gabungan antara kurva segitiga dan linear. Dari hasil analisis dan pembahasan didapatkan kesimpulan bahwa dengan besar pembobot r= 0,15 dan r= 0,2 terlihat proses terkendali secara statistik.","Quality control chart is a tool used to detect deviations in a production process. Product quality is a factor that influences consumer satisfaction. One of the control charts that efficiently used to detect small shift and to control the process of two or more variables are Fuzzy Multivariate Exponentially Weighted Moving Average (FMEWMA) control chart. Sometimes, there is a data that classified as ""uncertain"" or ""vagueness"" due to human subjectivity at measurement process. That kind of data is called as fuzzy data. If there is a small shifted fuzzy data at quality control process then it needs a fuzzy MEWMA control chart to handle such things. PT. Pioneer Flour Industries has controlled the productivity process of the ""Palapa"" white flour. The controlled characteristic quality variables were moisture (X1), glutten (X2), and ash(X3). The membership function for X1, X2, X3 is the combined membership function between triangle and linear curve. From the results of the analysis and discussion it was concluded that the weighting r = 0.15 and r = 0.2, the process can be said to be statistically controlled.","Kata Kunci : Grafik Pengendali Multivariat, MEWMA, Himpunan Fuzzy, Fungsi Keanggotaan."
http://etd.repository.ugm.ac.id/home/detail_pencarian/179748,Grafik Pengendali Nonparametric Mixed Exponentially Weighted Moving Average-Cummulative Sum (NPMEC),"Aprilia Puspitasari , Dr. Herni Utami, M.Si.",2019 | Skripsi | S1 STATISTIKA,Grafik pengendali nonparametric mixed Exponentially Weighted Moving Average-Cummulative Sum (NPMEC) merupakan salah satu grafik pengendali yang dapat digunakan dalam pengendalian kualitas statistik tanpa asumsi normalitas. Grafik ini menggunakan statistik EWMA nonparametrik sebagai input CUSUM. Performa grafik dievalusi dengan average run length (ARL) dan diperoleh hasil bahwa grafik ini lebih efisien dalam mendeteksi pergeseran proses yang sedang hingga besar dan sedikit terlambat dalam pergeseran yang kecil dibandingkan grafik pengendali EWMA nonparametrik. Selanjutnya dilakukan penaksiran kemampuan proses dengan analisis kapabilitas proses secara nonparametrik berdasarkan pendekatan fungsi densitas kernel menggunakan kernel Epanechnikov dan unbiased cross validation (UCV) dalam pemilihan bandwith.,Nonparametric mixed Exponentially Weighted Moving Average-Cummulative Sum (NPMEC) control chart is one of the control charts that can be used in statistical quality control without the assumption of normality. This graph uses nonparametric EWMA statistics as CUSUM inputs. Graphical performance is evaluated with average run length (ARL) and the results show that this graph is more efficient in detecting medium to large process shifts and is slightly late in small shifts compared to nonparametric EWMA control charts. Then the process capability is estimated by nonparametric capability process analysis based on the kernel density function approach using kernel Epanechnikov and unbiased cross validation (UCV) in bandwidth selection.,"Kata Kunci : Grafik pengendali NPMEC, ARL, EWMA nonparametrik, Kapabilitas proses, Fungsi densitas kernel, UCV."
http://etd.repository.ugm.ac.id/home/detail_pencarian/182052,Grafik Pengendali Double Exponentially Weighted Moving Average Nonparametrik (NPDEWMA),"RENA AYUDANA R, Dr. Herni Utami, M.Si ; Dr. Abdurkhman, M.Si ; Drs. Zulaela, Dipl.Med.Stats., M.Si",2019 | Skripsi | S1 STATISTIKA,"Kualitas produk merupakan faktor yang dapat mempengaruhi seseorang untuk menggunakan suatu produk. Salah satu metode upaya untuk meningkatkan kualitas dan produktivitas adalah grafik pengendali. Grafik pengendali dapat mendeteksi permasalahan kualitas dalam proses produksi seperti pergeseran proses yang kecil. Grafik pengendali variabel yang dapat mendeteksi pergeseran proses yang kecil pada distribusi normal adalah grafik pengendali Exponentially Weighted Moving Average (EWMA). Grafik pengendali Double Exponentially Weighted Moving Average (DEWMA) merupakan hasil pengembangan dari grafik pengendali EWMA dengan asumsi normalitas. Pada kenyataannya, tidak semua data berdistribusi normal. Oleh karena itu, akan digunakan grafik pengendali DEWMA nonparametrik (NPDEWMA) dan EWMA nonparametrik (NPEWMA) untuk mendeteksi pergeseran rata-rata yang tidak berdistribusi normal. 	 Pada skripsi ini, dibahas mengenai grafik pengendali DEWMA nonparametrik dan akan dibandingkan menggunakan grafik pengendali EWMA nonparametrik. Untuk membandingkan perfoma grafik NPDEWMA dan NPEWMA digunakan metode Average Run Length (ARL) dan Extra Quadratic Loss (EQL). Dari studi kasus, diperoleh kesimpulan bahwa grafik pengendali DEWMA nonparametrik lebih sensitif dalam mendeteksi pergeseran rata-rata  yang kecil.","Product quality is a factor that can influence people to use a product. One method of efforts to improve quality and productivity is control charts. Control charts can detect quality problems in the production process such as small process shifts. Variable control charts that can detect small process shifts in the normal distribution is known as Exponentially Weighted Moving Average (EWMA) control chart. The Double Exponentially Weighted Moving Average (DEWMA) control chart is the result of the development of the EWMA controlling chart with the assumption of normality. In fact, not all data are normally distributed. Therefore, the nonparametric DEWMA controller (NPDEWMA) and nonparametric EWMA controller (NPEWMA) charts will be used to detect averages that are not normally distributed. In this thesis, the nonparametric DEWMA controller chart is discussed and will be compared using a nonparametric EWMA controller chart. To compare the performance of the NPDEWMA and NPEWMA charts, the Average Run Length (ARL) and Extra Quadratic Loss (EQL) methods are used. From the case study, it was concluded that the nonparametric DEWMA controller chart was more sensitive in detecting small shifts in the mean than the NPEWMA controller chart.","Kata Kunci : Grafik Pengendali, Pergeseran rata-rata proses, EWMA nonparametrik, DEWMA nonparametrik, ARL, EQL"
http://etd.repository.ugm.ac.id/home/detail_pencarian/182053,Grafik Pengendali Exponentially Weighted Moving Average Nonparametrik Menggunakan Sampling Berulang,"Muslimah, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2019 | Skripsi | S1 STATISTIKA,"Grafik pengendali Exponentially Weighted Moving Average (EWMA) nonparametrik menggunakan sampling berulang merupakan grafik pengendali yang digunakan untuk mendeteksi pergeseran proses yang kecil dan untuk data yang asumsi normalitasnya tidak terpenuhi. Grafik ini akan dilakukan dengan teknik sampling berulang. Kinerja grafik akan dievaluasi dengan nilai ARL. Selanjutnya, akan dibandingkan dengan grafik pengendali EWMA nonparametrik menggunakan sampling tunggal. Diperoleh hasil bahwa grafik pengendali EWMA nonparametrik menggunakan sampling berulang lebih sensitif dan lebih efisien dibandingkan dengan menggunakan sampling tunggal, karena sampling berulang memberikan indikasi yang cepat ketika proses akan di luar kendali. Sehingga dapat mengurangi kesalahan dalam pengambilan keputusan suatu produksi.","Nonparametric control chart based on the Exponentially Weighted Moving Average (EWMA) using repetitive sampling is a control chart used to detect small shifts in the average process of the target for data where the assumption of normality is not fulfilled using repetitive sampling. This graph will be evaluated by ARL value. Furthermore, it will be compared with nonparametric EWMA control chart using  single sampling. The results obtained that nonparametric EWMA control chart using  repetitive sampling is more sensitive and more efficient compared using single sampling, because repetitive sampling provides a quick indication when the process will be out of control. So, it can reduce errors in making  production decision.","Kata Kunci : Grafik pengendali, EWMA, Repetitive Sampling, Nonparametrik, ARL"
http://etd.repository.ugm.ac.id/home/detail_pencarian/183847,PENERAPAN METODE CHI-SQUARE AUTOMATIC INTERACTION DETECTOR (CHAID)  PADA ANALISIS REGRESI LOGISTIK UNTUK MENINGKATKAN AKURASI PADA DATA LOYALITAS PEMEGANG POLIS ASURANSI KENDARAAN,"MAYANG PUSPITASARI, Dr. Gunardi, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan ilmu yang mempelajari tentang suatu hubungan fungsional antara variabel-variabel yang dinyatakan dalam bentuk persamaan matematika. Pada analisis regresi, variabel tersebut dibedakan dalam dua jenis variabel yaitu variabel respon (Y) dan variabel prediktor (X). Namun untuk beberapa kasus variabel responnya tidak bersifat berkuantitatif melainkan kualitatif yang bersifat biner misalnya dalam mengukur loyalitas pemegang polis (lanjut atau tidak) oleh sebab itulah perlu menggunakan analisis regresi logistik. 	Dalam pengambilan keputusan tentunya mengharapkan akurasi yang terbaik. Apabila menggunakan regresi logistik maka ada kemungkinan terdapat heterogenitas akurasi klasifikasi antar segmen sehingga dapat membentuk model yang sama untuk semua pengamatan yang berakibat ukuran akurasi kurang baik. Hal ini dapat terjadi jika kinerja klassifier bervariasi secara signifikan diberbagai segmen pengamatan. Oleh sebab itu kita perlu membagi sampel menjadi beberapa kelompok homogen dan membangun model terpisah untuk masing-masing segmen menggunakan variabel dummy yang dapat dillakukan dengan metode Chi-Square Automatic Interaction Detector (CHAID). Dalam skripsi ini, metode Chi-Square Automatic Interaction Detector (CHAID) diaplikasikan untuk memodelkan faktor-faktor yang mempengaruhi keputusan pemegang polis asuransi kendaraan. Diperoleh akurasi model yang terbentuk meningkat.","Regression analysis is the study of relationship between variables expressed in the form of mathematical equations. In regression analysis, these variables are divided into two types of variables, the response variable (Y) and prediktor variable (X). But for some cases the response variable is not quantitative but qualitative which is binary for example in making decision (yes or no) because of that we use logistik regression analysis. In making decisions, we want have the best accuracy. If using logistik regression, there is a possibility that there is heterogeneity in classification accuracy between segments so that it can form the same model for all observations so the accuracy isnÃ¢ï¿½ï¿½t good. This can be done if the classifier's performance varies significantly in the various observation segments. Therefore we need to divide the sample into several homogeneous groups and build separate models for each segment using dummy variables that can be done using the Chi-Square Automatic Interaction Detector (CHAID) method. In this thesis, the Chi-Square Automatic Interaction Detector (CHAID) method was applied to model the factors that influence the decisions of vehicle insurance policyholders.","Kata Kunci : Regresi Logistik, Peningkatan akurasi, Maksimum Likelihood, CHAID."
http://etd.repository.ugm.ac.id/home/detail_pencarian/173363,APLIKASI BAYESIAN ROBUST DALAM PERHITUNGAN PREMI ASURANSI KEBAKARAN,"PRISKA KHARISMA, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Metode penghitungan premi berdasarkan data klaim asuransi adalah membandingkan besar kerugian yang dialami oleh pihak tertanggung dengan besar kerugian tertanggung lainnya yang mempunyai karakteristik yang sama. Metode ini membantu perusahaan asuransi menentukan kemungkinan bahwa pemegang polis tertentu akan mengajukan klaim. Dalam hal ini, pengalaman kerugian masa lalu dari pemegang polis digunakan untuk menentukan perubahan di masa depan terhadap premi yang dibebankan untuk polis. Sehingga besar kenaikan premi tergantung riwayat klaim yang diajukan oleh pemegang polis. Pada skripsi ini akan dibahas metode penghitungan premi yang memungkinkan adanya klaim yang cukup besar. Bayesian robust diharapkan dapat mengatasi data yang skewed dan tak-negatif yaitu data klaim asuransi, khususnya distribusi gamma. Metode Bayesian robust digunakan untuk mencari mean posterior yang dianggap sebagai estimator premi risiko. Pada studi kasus digunakan data klaim asuransi kebakaran untuk menghitung premi dari tiap tipe bangunan hingga didapatkan hasil bahwa mean posterior dari Bayesian robust dapat digunakan sebagai salah satu metode perhitungan premi yang dapat meminimalisir pengaruh adanya klaim yang cukup besar.","The premium calculation method based on insurance claim data is a method of insurance pricing by comparing the amount of losses experienced by the insured and the others of similar characteristics. This method helps insurance companies determine the likelihood of certain policyholders for making claims. In this case, the experience of past losses from policyholders is used to determine future changes to the premium. Therefore, the increase in premiums depends on the history of claims submitted by policy holders. This undergraduate thesis will discuss the method of calculating premiums that flexible on the existence of large claims. Bayesian robust is expected to overcome the data that is skewed and non-negative, namely insurance claim data, especially the gamma distribution. The Bayesian robust method is used to find the posterior mean which is considered a risk premium estimator. In the case study, we use fire insurance claim data to calculate premiums for each build types until the results obtained that the posterior mean of Bayesian robust can be used as one of the premium calculation methods that can minimize the effect of large claims.","Kata Kunci : penghitungan premi,distribusi gamma,statistik bayesian,premium calculation,gamma distribution,bayesian statistics,bayesian robust,markov chain monte carlo"
http://etd.repository.ugm.ac.id/home/detail_pencarian/168763,GRAFIK PENGENDALI  PROGRESSIVE MEAN NONPARAMETRIK UNTUK MENGAWASI PROSES PRODUKSI,"HUSNUL ARIS HAIKAL, Prof. Dr. Sri Haryatmi, M.Sc",2019 | Skripsi | S1 STATISTIKA,Faktor yang mempengaruhi tingkat kepuasan konsumen adalah kualitas dari poduk. Dalam proses produksi kualitas dari produk perlu dikontrol agar selalu sesuai dengan target yang telah ditetapkan oleh perusahaan. Grafik pengendali merupakan salah satu alat yang sering digunakan untuk mengontrol kualitas dari produk. Permasalahan kualitas statistik seperti pergeseran proses yang kecil atau oleh sebab - sebab yang tak terduga dapat di deteksi oleh grafik pengendali. Salah satu metode grafik pengendali yang digunakan dalam mengontrol kualitas produk adalah metode Progressive Mean yang berasumsi normal. Namun tidak semua kasus berasal dari data yang berdistribusi normal maka untuk menangani data yang tidak berdistribusi normal salah satunya digunakan metode grafik pengendali Progressive Mean Nonparametrik.  Pada penelitian ini akan dilihat performa dari grafik pengendali Progressive Mean Nonparametrik dan dibandingkan dengan grafik pengendali Progressive Mean. Metode average run length ( ARL ) digunakan untuk membandingkan kedua metode grafik pengendali tersebut. Setelah dibandingkan kedua grafik didapat kesimpulan bahwa Progressive Mean Nonparametrik merupakan grafik pengendali yang lebih baik dalam mendeteksi pergeseran proses yang kecil untuk data yang tidak berdistribusi normal.,"The factors that effects of customer satisfaction is the quality of the product. In the production process the quality of the product needs to be controlled so that it is always in accordance with the targets set by the company. Control chart is one of tools that can control the quality of the product. Statistical quality problems such as small process shifts or unexpected causes can be detected by control charts. One method of controlling graphics used in controlling product quality is the Progressive Mean method with the assumption of normality. In facts, not all data is normally distributed, therefore, a method called a Nonparametric Progressive Mean will be used to handle data that is not normally distributed. In this study, the performance of the control chart of the Progressive Mean Nonparametric and the Progressive Mean control will be seen. The average run length (ARL) method is used to compare the two control chart methods. After the comparison the two control chart, it can be concluded that Progressive Mean Nonparametric control chart is more sensitive in detecting small average process shifts for data with not normally distributed.","Kata Kunci : Grafik Pengendali, Pergeseran rata-rata proses, Control Charts, Average Run Length, ARL, Pogressive Mean, Nonparametric Pogressive Mean."
http://etd.repository.ugm.ac.id/home/detail_pencarian/175168,OPTIMISASI PORTOFOLIO MEAN VARIANCE DENGAN ANALISIS KLASTER AFFINITY PROPAGATION,"Sarah Fitri Fajriaty, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc",2019 | Skripsi | S1 STATISTIKA,"Ilmu statistika pada bidang keuangan dapat diterapkan dalam investasi saham. Investasi merupakan suatu kegiatan menempatkan sejumlah dana pada suatu aset di masa kini dengan harapan akan memperoleh keuntungan di masa mendatang. Dalam berinvestasi, investor cenderung menghindari risiko. Terdapat suatu cara dalam investasi yang bertujuan untuk meminimumkan risiko yaitu diversifikasi. Diverfikasi adalah usaha penganekaragaman yang dilakukan untuk memaksimalkan keuntungan sehingga investasi dapat lebih stabil. Salah satu bentuk dari diversifikasi adalah portofolio saham yaitu kombinasi dari beberapa saham. Optimisasi portofolio merupakan usaha untuk membentuk portofolio saham yang optimal. Berbagai macam penelitian untuk optimisasi portofolio telah banyak dilakukan, salah satunya adalah dengan menggabungkan analisis klaster dan metode mean variance. Pada skripsi ini akan dibahas mengenai optimisasi portofolio mean variance dengan menggunakan analisis klaster affinity propagation. Saham-saham dikelompokkan terlebih dahulu, kemudian dicari bobotnya menggunakan metode mean variance. Studi kasus penelitian ini menggunakan data harga saham bulanan dari 10 saham LQ-45 pada periode Maret 2014 sampai Februari 2019, yaitu ADRO, AKRA, ASII, BBNI, BBRI, BMRI, INDF, KLBF, PTBA, dan UNTR. Portofolio yang dibentuk menggunakan mean variance dengan affinity propagation. akan dibandingkan dengan portofolio yang dibentuk menggunakan mean variance tanpa affinity propagation. Perbandingan kinerja kedua portofolio tersebut dilakukan dengan menggunakan rasio Sharpe. Diperoleh kesimpulan bahwa portofolio yang dibentuk menggunakan mean variance dengan affinity propagation lebih baik dari portofolio yang dibentuk menggunakan mean variance tanpa affinity propagation.","Statistics in the field of finance can be applied in stock investment. Investment is an activity by involving funds in an assets today with the hope that it will gain advantage in the future. In investment, investors choose to avoid risk. There is a way in investment that aim to minimize risk, namely diversification. Diversification is an effort undertaken to maximixe profits so that investment can be more stable. One form of diversification is portfolio that is a combination of several stocks. Portfolio optimization is an effort to form an optimal stock portfolio. Many types of research for portfolio optimization have been carried out, one of them is portfolio optimization by combining cluster analysis and mean variance method. In this paper we will discuss portfolio optimization mean variance using cluster analysis affinity propagation.. Stocks are grouped at first, then we search the portfolio weights using mean variance method. This case study uses monthly stock price data in the period of March 2014 to February 2019, the name stocks are ADRO, AKRA, ASII, BBNI, BBRI, BMRI, INDF, KLBF, PTBA, and UNTR. Portfolio that formed by mean variance with affinity propagation is compared to portfolio that formed by mean variance without affinity propagation. The comparison of two portfolios was carried out using sharpe ratio. The conclusion is portfolio that formed by mean variance with affinity propagation is better that portfolio that formed by mean variance without affinity propagation.","Kata Kunci : optimisasi portofolio, affinity propagation, mean variance, rasio Sharpe"
http://etd.repository.ugm.ac.id/home/detail_pencarian/180549,Analisis Klaster Hierarki Menggunakan Matriks Ensemble Dissimilarity untuk Data Kategorik,"Zulfa Romadhoni, Dr. Adhitya Ronnie Effendie, M.Sc.",2019 | Skripsi | S1 STATISTIKA,"Analisis klaster merupakan analisis yang digunakan untuk mengelompokkan objek-objek ke dalam beberapa kelompok sehingga objek yang berada pada klaster yang sama memiliki tingkat kemiripan yang tinggi dan objek yang berada pada klaster yang berbeda memiliki tingkat kemiripan yang rendah. Pada penelitian ini, akan dipaparkan suatu metode analisis klaster untuk data kategorik, yaitu algoritma analisis klaster hierarki agglomerative menggunakan matriks ensemble dissimilarity. Matriks ketidaksamaan ini dibentuk dengan melakukan analisis klaster hierarki agglomerative menggunakan hamming distance beberapa kali, yang kemudian hasilnya digabungkan ke dalam satu matriks. Dari matriks baru ini dihitung jarak antar objek menggunakan rata-rata hamming distance, sehingga diperoleh matriks ensemble dissimilarity. Metode ini juga dapat diterapkan pada data kategorik berdimensi tinggi, dengan menerapkan teknik subspace clustering. Metode ini diaplikasikan untuk melakukan segmentasi terhadap data klaim BPJS Kesehatan, data ekspor Provinsi Jawa Tengah dan data rangkaian gen Rhabdoviridae. Diperoleh bahwa metode ini memberikan kualitas hasil clustering yang lebih baik dibandingkan dengan metode basenya.","Clustering analysis is a technique for grouping objects into some groups so that objects which are in the same cluster have a high similarity and objects which are in the different cluster have a low similarity. We present a new clustering method for categorical data, that is a hierarchical clustering algorithm using ensemble dissimilarity matrix. The ensemble dissimilarity matrix is form by generating some clusterings using hierarchical clustering algorithm and hamming distance. These clusterings are then combined into a matrix and based on this new matrix, we calculate the distance between objects using hamming distance, and so we get the ensemble dissimilarity matrix. This method can be applied to a high dimensional categorical data by applying subspace clustering technique. The method is applied to do a segmentation of BPJS Kesehatan claim data, Jawa Tengah export data, and Rhabdoviridae gene sequence data. We get that this method give a better quality of clustering than its base method.","Kata Kunci : Analisis klaster hierarki, Data kategorik, Metode ensemble, Hamming distance, Segmentasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/167504,EFISIENSI MODIFIED JACKKNIFED LIU ESTIMATOR UNTUK MENGATASI MASALAH MULTIKOLINEARITAS PADA MODEL LINEAR,"Widia Mansur, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2019 | Skripsi | S1 STATISTIKA,"Analisis Regresi adalah suatu kajian yang bertujuan untuk mengetahui hubungan satu variabel yang disebut sebagai variabel dependen dengan satu atau lebih vaiabel independen. Dalam asumsi yang terdapat pada analisis regresi klasik diantaranya adalah tidak adanya multikolinearitas. Hal itu dapat menyebabkan hasil estimasi dengan menggunakan kuadrat terkecil menjadi tidak valid. Pada skripsi ini akan dibahas mengenai Efisiensi Modified Jackknifed Liu Estimator Untuk Mengatasi Masalah Multikolinearitas pada Model Linear yang dikembangkan oleh Ezra dan Fikri (2010). Metode Modified Jackknifed Liu Estimator ini merupakan pengembangan dari metode Jackknife Liu Estimator yang sebelumnya dikembangkan dari Liu Estimator oleh Kejian (1993), dan juga merupakan gabungan dari metode Generalized Liu Estimator dan Jackknifed Liu Estimator. Studi kasus ini menggunakan data Jumlah Uang Beredar di Indonesia dan Faktor yang mempengaruhinya dari bulan Januari 2004 sampai bulan Agustus 2018. Diperoleh kesimpulan bahwa metode Modified Jackknifed Liu Estimator lebih efisien digunakan untuk mengatasi masalah multikolinearitas dibandingkan dengan metode Generalized Liu Estimator dan Jackknifed Liu Estimator dilihat berdasarkan kriteria MSEnya.","Regression Analysis is research that aims to find out the relationship of one variable called a dependent variable by one or more independent variables. The most profound of classical regression analysis is that there is no multicollinearity. That can cause the results using the OLS to be invalid. In this paper, we discuss the Modified Jackknife Liu Estimator Efficiency to Overcome Multicollinearity Problems in the Linear Model developed by Ezra and Fikri (2010). Liu Estimator's Modified Jackknifed method is a development of the Jackknifed Liu Estimator method which was previously developed from Liu Estimator by Kejian (1993), and is also a combination of the Generalized Liu Estimator and Jackknifed Liu Estimator methods. This case study uses data on the amount of money circulating in Indonesia and the factors that influenced it from January 2004 to August 2018. It was concluded that the Modified Jackknife Liu Estimator method was more efficiently used to overcome multicollinearity problems compared to the Generalized Liu Estimator and Jackknifed Liu Estimator methods seen based on the MSE criteria.","Kata Kunci : Multikolinearitas, Liu Estimator, Generalized Liu Estimator, Jackknifed Liu Estimator, Modified Jackknifed Liu Estimator, MSE."
http://etd.repository.ugm.ac.id/home/detail_pencarian/171089,ESTIMASI VALUE AT RISK (VaR) PORTOFOLIO MULTIVARIAT MENGGUNAKAN METODE GJR GARCH-EVT-VINE COPULA,"YULIANA TRIASTUTI, Prof. Dr. Sri Haryatmi, M.Sc",2019 | Skripsi | S1 STATISTIKA,"Value at Risk (VaR) merupakan salah satu cara untuk menganalisis atau mengukur risiko. Perhitungan VaR dengan metode konvensional mengasumsikan data berdistribusi normal dan mengukur kebergantungannya dengan korelasi linear. Pada kenyataannya, data return finansial seringkali tidak berdistribusi normal. Kebergantungan antar saham yang tidak linear juga tidak sesuai jika diukur dengan korelasi linear sehingga metode konvensional tidak akurat lagi. Vine Copula adalah suatu fungsi yang menggabungkan beberapa distribusi marginal serta dapat menggambarkan kebergantungan tidak liniernya. Ekor distribusi yang tebal memiliki kemungkinan lebih besar menunjukan adanya kejadian ekstrim dalam data sehingga diperlukan permodelan Extreme Value Theory (EVT) dengan distribusi GPD. Data return finansial pada umumnya juga mengandung unsur heterokedastik yang dapat diatasi menggunakan model GARCH(1,1) dengan inovasi Student-t. Apabila terdapat leverage effect, model GJR GARCH(1,1) dapat dijadikan solusi untuk mengatasi permasalahan tersebut. Studi kasus skripsi ini menggunakan indeks saham JKSE, N225, dan HSI yang dimodelkan dengan CD-Vine Copula dari keluarga Gaussian, Student-t serta Clayton. Copula terbaik yang memodelkan data dalam skripsi ini adalah struktur C-Vine dan D-Vine dari keluarga Gaussian. Hasil backtesting menunjukkan bahwa estimasi VaR menggunakan metode GJR GARCH - EVT - Vine Copula secara umum baik untuk digunakan.","Value at Risk (VaR) is one way to analyze or measure risk. Conventional method for calculating VaR assuming normally distributed returns and the dependence between stocks are linear. In fact, financial data returns usually have fat tailed and the dependence between stocks are not linear so the conventional method is not accurate anymore. Vine Copula is a function that combines several marginal distributions and study about non-linear dependence between events in multivariate cases. Fat tail in return shows extreme movement in data which need to be modeled by Extreme Value Theory (EVT) with GPD distribution. Returns which generally heterokedastic have to be modeled by GARCH(1,1) with Student-t distribution innovation. If there is leverage effect, GJR GARCH(1,1) can be the solution for this problem. This paper uses a portfolio of JKSE, N225, and HSI  stock indexes that are modeled with CD-Vine Copula from the Gaussian, Student-t and Clayton family. A maximum likelihood-based IFM method is used to estimating the copula parameter. The best copula that fit the data is Gaussian D-Vine Copula. Backtesting result shows that the VaR estimation by using GJR GARCH - EVT - Vine Copula is generally good for use.","Kata Kunci : Value at Risk, Vine Copula, GJR GARCH, Generalized Pareto Distribution, Backtesting"
http://etd.repository.ugm.ac.id/home/detail_pencarian/180821,OPTIMISASI PORTOFOLIO BERDASARKAN STANDARDISASI DANA DAN ALGORITMA GENETIKA,"Alvian Zia Irsyad, Prof. Drs. Subanar, Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Diversifikasi portofolio dapat diartikan sebagai pembentukan portofolio sedemikian rupa sehingga dapat mengurangi risiko portofolio tanpa meminimumkan return. Hal ini merupakan tujuan yang ingin dicapai oleh investor. Algoritma Genetika adalah algoritma yang memanfaatkan proses seleksi alamiah yang dikenal dengan proses evolusi. Dalam proses evolusi, individu secara terus-menerus mengalami perubahan gen untuk menyesuaikan dengan lingkungan hidupnya. Hanya individu-individu yang bisa menyesuaikan dengan keadaan sekitar yang mampu bertahan. Dengan optimisasi portofolio menggunakan algoritma genetika berdasarkan standardisasi dana diharapkan investor bisa mendapatkan bobot yang optimal dan bisa memperkirakan seluruh biaya dalam transaksi saham. Dalam skripsi ini dibahas optimisasi portofolio menggunakan algoritma genetika berdasarkan standardisasi dana pada studi kasus lima saham di indeks LQ-45.","Portfolio diversification can be interpreted as a portfolio created so as to reduce the risk of portfolio without minimize return. This is the goal to be achieved by investors. 	Genetic Algorithms are algorithms that utilize the natural selection process known as the evolution process. In the process of evolution, each individual continually fixes gene changes to adapt to the environment passed. ""Only individuals that can adapted with the surroundings can be survive"". By optimizing portfolios using genetic algorithms based on fund standardization, it is expected that investors can obtain optimal weights and can estimate all costs in stock transactions. This thesis discusses portfolio optimization using genetic algorithms based on fund standardization in a case study of five stocks in the LQ-45 index.","Kata Kunci : Algoritma Genetika, Mean Variance, Portofolio, Sharpe Ratio."
http://etd.repository.ugm.ac.id/home/detail_pencarian/171102,Perbandingan Improved Ridge Estimator dan Jackknifed Ridge M-Estimator pada Regresi Linear dengan Multikolinearitas dan Pencilan,"Faizah Dayu Upitria, Prof. Dr. Sri Haryatmi, M.Sc",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan salah satu alat statistik yang digunakan dalam berbagai bidang. Tujuan dari analisis regresi adalah untuk mengetahui pola hubungan antara variabel dependen dengan variabel independen. Berdasarkan asumsi klasik pada regresi linear diantaranya tidak adanya multikolinearitas dan tidak adanya pencilan pada data regresi maka apabila dalam model regresi terdapat multikolinearitas dan pencilan akan menyebabkan hasil estimasi menggunakan metode least square  menjadi tidak valid. Pada skripsi ini akan dibahas mengenai penduga regresi linear ganda dengan menggunakan metode improved ridge estimator dengan adanya multikolinearitas dan pencilan pada variabel respon yang dikembangkan oleh Dorugade (2016). Kemudian metode tersebut akan dibandingkan dengan metode jackknifed ridge M-Estimator yang sebelumnya telah diperkenalkan oleh Jadhav dan Kashid (2011). Studi kasus pada skripsi ini menggunakan data pendapatan asli daerah di 35 kabupaten/kota provinsi Jawa Tengah tahun 2013 dan variabel yang mempengaruhinya. Diperoleh kesimpulan bahwa metode improved ridge estimator menghasilkan nilai MSE, AIC, dan BIC lebih kecil dibandingkan dengan metode jackknifed ridge M-Estimator.","Regression analysis is one of the statistical tools used in various fields. The purpose of regression analysis is to find out the pattern of the relationship between the dependent variable (response) and the independent variable (predictor). Based on the classical assumptions in linear regression among others are no multicollinearity and outliers in the regression model. If there are multicollinearity and outliers present in a regression model, the estimated result predicted by the least square method will be invalidate. 	In this paper is aimed to discussing the estimation of multiple linear regression using improved ridge estimator method in the presence of multicollinearity and outliers in the response variables which was developed by Dorugade (2016). Then the method will be compared with jackknifed ridge M-Estimator method that was previously introduced by Jadhav and Kashid (2011). The data of regional original income in 35 Central Java Districts/Cities on 2013 and their factors become the case study in this paper. Based on the value of  Mean Square Error, Akaike Information Criterion, And Bayesian Information Criterion, Improved Ridge Estimator is better than Jackknifed Ridge M-estimator.","Kata Kunci : multiple linear regression, multicollinearity, outliers, improved ridge estimator, jackkifed rideg m-estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/168543,Sparse Ridge Fusion Untuk Mengatasi Multikolinearitas Dalam Regresi Linear Pada Data Berdimensi Tinggi,"PUTRI AZIZATUN HIDAYATI, Prof. Drs. Subanar, Ph.D",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi banyak digunakan diberbagai bidang, bahkan akhir-akhir ini analisis regresi sangat berguna untuk bidang kesehatan dan farmasi. Namun, sekarang ini banyak data yang jumlah observasinya lebih sedikit daripada jumlah prediktornya (p>n) seperti pada data NIRS dan microarray. Data tersebut tentunya memiliki masalah multikolinearitas yang tinggi yang mana analisis regresi dengan Ordinary Least Square (OLS) tidak dapat menanganinya. Oleh karena itu, regresi terpenalti seperti ridge, lasso, smooth lasso, dan elastic-net mulai diperkenalkan. Regresi terpenalti yang disebut Ridge Sparse Fusion juga mulai diperkenalkan untuk mengatasi masalah tersebut. Skripsi ini membandingkan performasi dalam bentuk Mean Squared Error antara Sparse Ridge Fusion dengan analisis lasso dan smooth lasso pada data yang berdimensi tinggi. Hasil yang  diperoleh adalah Mean Squared Error dari metode Sparse Ridge Fusion lebih rendah daripada Mean Squared Error dari metode lasso dan smooth lasso. Hal tersebut menandakan bahwa metode Sparse Ridge Fusion dapat dikatakan lebih akurat daripada metode lasso dan smooth lasso.","Regression analysis is widely used in various fields, even recently regression analysis is very useful for the health and pharmaceutical fields. However, at present there is a lot of data whose number of observations is less than the number of predictors (p>n) as in the NIRS and microarray data. The data certainly has a high multicollinearity problem in which Ordinary Least Square (OLS) regression analysis cannot handle it. Therefore, penalized regression such as ridge, lasso, smooth lasso, and elastic-net was introduced. Penalized regression called Sparse Ridge Fusion was also introduced to overcome this problem. 	This thesis compares performance in the form of Mean Squared Error between Sparse Ridge Fusion with lasso and smooth lasso analysis on high dimensional data. The results obtained are the Mean Squared Error of the Sparse Ridge Fusion method lower than the Mean Squared Error from the lasso and smooth lasso method. This indicates that the Sparse Ridge Fusion method can be said to be more accurate than the lasso and smooth lasso method.","Kata Kunci : analisis regresi, ridge, elastic-net, lasso, smooth lasso, sparse ridge fusion, regression analysis, ridge, elastic-net, lasso, smooth lasso, sparse ridge fusion, high dimensionhigh dimension ,"
http://etd.repository.ugm.ac.id/home/detail_pencarian/181602,"Estimasi Value at Risk (VaR) dengan Menggunakan Model GTLGARCH (1,1)","NURUL HIDAYATI, Prof. Dr. Sri Haryatmi Kartiko, M.Sc.,",2019 | Skripsi | S1 STATISTIKA,"Value at Risk (VaR) merupakan salah satu ukuran risiko finansial dalam manajemen risiko. Terdapat beberapa metode dalam mengestimasi VaR yang biasa digunakan dalam tataran praktis. Namun, sebagian besar metode perhitungan VaR tersebut mengasumsikan data berdistribusi normal, sedangkan dalam kenyataannya data return finansial sering memiliki ekor yang gemuk atau bersifat fat tails dan skewed. Distribusi Generalized Tukey Lambda merupakan distribusi serbaguna yang mampu menangkap berbagai tingkat kemiringan dalam data yang menunjukkan perilaku thin or thick tailed. Distribusi ini juga tidak terbatas pada kesimetrisan dari bentuk kurva atau dapat dikatakan bahwa distribusi ini juga bisa digunakan untuk data yang asimetris. Selain bersifat fat tails, data return finansial juga umumnya mangandung efek heteroskedastisitas, sehingga sebelum perhitungan VaR, data dimodelkan terlebih dahulu dengan model GARCH (Generalized Autoregressive Conditional Heteroskedasticity). Pada penulisan skripsi ini akan dikombinasikan distribusi GTL dengan model GARCH atau disebut juga dengan model GTLGARCH. Model ini akan digunakan dalam perhitungan Value at Risk. Hasil yang diperoleh kemudian dibandingkan dengan estimasi VaR yang dihasilkan dengan model GARCH dengan residual yang berdistribusi normal. Estimasi VaR menggunakan model GTLGARCH (1,1) memiliki performa yang lebih baik dibandingkan dengan estimasi VaR menggunakan model GARCH (1,1) untuk studi kasus dalam skripsi ini.","Value at Risk (VaR) is one of the measurement of financial risk in risk management. There are many method for estimating VaR that usually used in practical field. But most of them assume that data must be normally distributed, whereas financial return data usually have fat tails and skewed shape. Generalized Tukey Lambda distribution is versatile distribution that able to capture varying degrees of skewness in thin or thick tailed data. This distribution is also not limited to the symmetry of the shape of the curve or it can be said that this distribution can also be used for asymmetric data. Besides being fat tails, financial return data also has heteroscedasticity effect, so the data must be modelled with GARCH (Generalized Autoregressive Conditional Heteroskedasticity) model. In this thesis, the GTL distribution will be combined with GARCH model and it is called GTLGARCH model. This model will be used in estimasting Value at Risk. Then the result will be compared with VaR generated by GARCH model with normal distributed residuals. VaR estimastion using GTLGARCH (1,1) model has better performance than using GARCH (1,1) model for the case in this thesis.","Kata Kunci : Value at Risk, GTL, GARCH, GTLGARCH"
http://etd.repository.ugm.ac.id/home/detail_pencarian/181350,PENGEMBANGAN METODE DISSOLVED GAS ANALYSIS UNTUK TRANSFORMATOR DI INDONESIA DENGAN ALGORITMA GENETIK K-MEANS,"Agung Berliano Wibowo, Prof. Drs. Subanar, Ph.D.",2019 | Skripsi | S1 STATISTIKA,"K-means merupakan metode analisis klaster yang sering digunakan, dengan objek-objek pada data dikelompokkan menjadi klaster berdasarkan karakteristiknya dengan banyaknya klaster ditentukan oleh peneliti. Karena kesederhanaannya, metode K-means memiliki kelemahan bahwa metode ini hanya dapat konvergen ke optimum lokal. Algoritma genetik K-means (GKA) merupakan hibridisasi yang memadukan kesederhanaan dari metode K-means, dengan keunggulan algoritma genetika yang mampu mencari solusi optimal. Dengan GKA, solusi optimum global dapat dicapai dengan lebih cepat dan efisien. Transformator merupakan komponen yang memiliki peranan penting dalam penyaluran listrik. Keberadaan transformator tak lepas dari ancaman gangguan mula, sehingga perlu dilakukan pemeriksaan kondisi secara berkala. Salah satu metode yang digunakan adalah DGA (Dissolved Gas Analysis). Dalam skripsi ini dikembangkan metode DGA untuk mendiagnosis gangguan transformator di Indonesia. Diagnosis dilakukan dengan memanfaatkan hasil analisis klaster menggunakan GKA. Hasil tersebut digunakan sebagai dasar dalam merancang grafik segitiga untuk mendiagnosis dan mengelompokkan gangguan mula pada transformator di Indonesia sesuai karakteristik gangguan mula yang dialami.","K-means is a popular clustering analysis, where the objects is being grouped into clusters based on their characteristics and the number of clusters is determined. Because of its simplicity, K-means method has a weakness that it is only converge to a local optimum. Genetic K-means algorithm (GKA) is a hybrid that combines the simplicity of K-means and the robust nature of GAs. Globally optimal solution can be found faster and more efficient using GKA. Transformers is an equipment that holds an important role in distribution of electricity. Incipient fault is a threat to transformers performance, therefore, periodical transformer check up is needed. One of the method is dissloved gas analysis (DGA). In this thesis, DGA is developed to diagnose the Indonesian transformers fault data. Clustering analysis result using GKA is used as a base to design a triangular graph to diagnose and classify incipient fault of Indonesian transformers according to incipient faults characteristics.","Kata Kunci : klastering, K-means, algoritma genetika, optimum global, transformator, gangguan mula, dissolved gas analysis"
http://etd.repository.ugm.ac.id/home/detail_pencarian/175212,ROBUST LIU ESTIMATOR MENGGUNAKAN ESTIMATOR MM DAN ESTIMATOR LEAST TRIMMED SQUARE (LTS) DENGAN PENGARUH MULTIKOLINEARITAS DAN PENCILAN (Studi Kasus : PDRB beserta Faktor-Faktor yang Mempengaruhi di Provinsi Jawa Tengah Tahun 2013),"Fikra Ar Razi, Dr. Abdurakhman, M.Si",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan suatu analisis yang sering digunakan dalam statistika yang bertujuan untuk memodelkan dan menyelidiki pola hubungan antara variabel independen dengan variabel dependen. Metode estimasi koefisien regresi yang biasanya digunakan adalah Ordinary Least Square (OLS), namun metode ini tidak tepat digunakan apabila asumsi klasik pada regresi linear tidak terpenuhi. Pada kenyataannya seringkali terdapat pelanggaran terhadap asumsi klasik tersebut, beberapa asumsi yang sering tidak terpenuhi diantaranya terdapat data pencilan yang dapat mengakibatkan residual tidak berdistribusi normal dan terjadi multikolinearitas yang tinggi antara variabel independen. Pada skripsi ini akan dibahas dua metode yang dapat mengatasi masalah multikolinearitas dan pencilan pada data secara bersamaan, metode ini merupakan penggabungan metode liu estimator dengan menggunakan estimator pada regresi robust, estimator robust yang digunakan adalah estimator yang memiliki breakdown point yang tinggi sebesar 50%, yaitu estimator MM dan estimator LTS. Kemudian dilakukan perbandingan antara kedua metode tersebut dengan studi kasus data produk domestik regional bruto (PDRB) beserta faktor-faktor yang mempengaruhinya di 35 kabupaten/kota yang ada di provinsi Jawa Tengah. Selanjutnya diperoleh kesimpulan yang menunjukkan bahwa Robust liu Estimator menggunakan estimator LTS memberikan nilai MSE, AIC dan BIC yang lebih kecil dibandingkan Robust liu Estimator menggunakan estimator MM.","Regression analysis is an analysis that is often used in statistics which aims to modelling and find out about the pattern of the relationship between the independent variable and the dependent variable. The estimation method for regression coefficients that are usually used is Ordinary Least Square (OLS), but this method is not appropriate to use if the classical assumptions in linear regression are not fulfilled. In fact, there are often violations of these classic assumptions, some of the assumptions that are often not fulfill are there is outlier data that can make the residual not normally distributed and high multicollinearity between independent variables. 	This paper will discussed two methods that can overcome multicollinearity and outliers on data simultaneously, this method is a combination of liu estimator method using robust regression estimator, the robust estimator will be used is an estimator which has a breakdown point 50%, that is the MM estimator and the LTS estimator. Then a comparison is made between these methods with a case study of gross domestic regional product (GDRP) data and their factors in 35 districts/cities in the province of Central Java. Furthermore, conclusions are obtained show that the Robust liu Estimator using LTS estimator gives the values of MSE, AIC and BIC that are smaller than the Robust liu Estimator using MM estimator.","Kata Kunci : analisis regresi, pencilan, multikolinearitas, regresi robust, liu estimator, estimator MM, estimator LTS, robust liu estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/175988,"NONLINEAR GREY BERNOULLI MODEL(1,1) UNTUK MENGANALISIS POTENSI RUTE BARU KERETA API WIJAYAKUSUMA","Retno Anjarwati, Dr. Gunardi, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Grey Model sangat berguna dalam peramalan data yang langka atau data terbatas dengan ukuran kecil dalam menentukan parameter model untuk peramalan dengan tingkat akurasi yang baik. Tak jarang data yang ditemukan dalam kehidupan sehari-hari banyak yang bersifat terbatas dan memiliki pola nonlinear terutama pada data peluncuran produk terbaru. Salah satu fungsi peramalan digunakan untuk mengendalikan jumlah permintaan, kapasitas, serta sistem penjadwalan bagi perencanaan keuangan dan pemasaran. Pada skripsi ini akan membahas tentang data jumlah penumpang Kereta Api Wijayakusuma relasi Yogyakarta-Banyuwangi yang merupakan perpanjangan jalur relasi Yogyakarta-Solo menggunakan Nonlinear Grey Bernoulli Model disingkat NGBM(1,1). Nonlinear Grey Bernoulli Model(1,1) merupakan salah satu perluasan penting dari Grey Model(1,1) tradisional menggunakan diferensial orde pertama persamaan Bernoulli untuk mengestimasi parameter maupun mengestimasi nilai ramalan agar didapatkan hasil peramalan jangka pendek yang lebih akurat untuk jumlah data ukuran kecil dan terbatas. Tingkat kesalahan yang diperoleh pada metode NGBM(1,1) akan dibandingkan dengan Grey Model(1,1) dan diperoleh kesimpulan bahwa metode NGBM(1,1) cenderung memberikan tingkat kesalahan yang relatif lebih kecil dibandingkan dengan Grey Model(1,1). Perpanjangan rute Kereta Api Wijayakusuma berpotensi cukup baik untuk menambah jumlah penumpang.","Grey Model is very useful in forecasting for rare data or limited data with small size in determining model parameters for forecasting with good accuracy. Not infrequently the data found in everyday life is limited and has a nonlinear pattern, especially in the latest product launch data. One of the forecasting functions is used to control the number of requests, capacity, and scheduling systems for financial planning and marketing. In this undergraduate thesis, we will discuss about the number of passengers on the Wijayakusuma Train in Yogyakarta-Banyuwangi relation, which is an extension of the Yogyakarta-Solo relation using the Grey Bernoulli Nonlinear Model(1,1) abbreviated as NGBM(1,1). NGBM(1,1) is one of the important extensions of the traditional Grey Model(1,1) using the first-order differential Bernoulli equation to estimate parameters and estimate forecast values so that short-term forecasting results are more accurate for small and limited size data. The error rate obtained in the NGBM(1,1) method will be compared with the Grey Model(1,1) and the conclusion that the NGBM(1,1) method tends to provide a relatively smaller error rate compared to the Grey Model(1,1). The extension of the Wijayakusuma train route has the potential to be high enough to increase the number of passengers.","Kata Kunci : Grey Model, Nonlinear Grey Bernoulli Model, peramalan dan data terbatas."
http://etd.repository.ugm.ac.id/home/detail_pencarian/177789,EXACT TEST  TABEL KONTINGENSI 2 ARAH DENGAN  STRUCTURAL ZERO,"DIMAS PERDANA WC, Dr. Abdurakhman, M.Si;Yunita Wulansari, S.Si,M.Sc",2019 | Skripsi | S1 STATISTIKA,"Salah satu model yang paling umum dalam tabel kontingensi adalah model independensi. Uji Chi-square digunakan pada distribusi dengan jumlah sampel yang besar. Distribusi sampel yang besar tidak memberikan perkiraan yang baik ketika ukuran sampel kecil. Untuk itu, uji eksak Fisher adalah metode alternatif yang memungkinkan menghitung nilai p yang tepat untuk menguji hipotesis independensi sampel kecil. Masalahnya terjadi ketika ada nol struktural dalam tabel kontingensi, kasus ini dapat menggunakan generalisasi uji eksak Fisher untuk kasus di mana beberapa entri tabel dibatasi menjadi nol. Untuk uji eksak Fisher pada sampel besar, penghitungan lengkap distribusi ini sering tidak layak secara komputasi dan uji eksak Monte Carlo digunakan. Studi kasus yang digunakan adala data caretta hatchling. Data zoologi dikumpulkan dari studi lapangan di Pantai Dalyan di Turki. Ingin diketahui apakah ada hubungan antara jarak sarang dari laut dan daerah bersarang Caretta Hatchling.","One of the most common models to be investigated in contingency tables is the independence model. Chi-square test rely on large sample distributions. The large sample distribution does not provide a good approximation when the sample size is small. The Fisher exact test is an alternative method which enables us to compute the exact p-value for testing the independence hypothesis. The problem occurs when there is a structural zero in the contingency table, we can use generalization of Fisher's exact test for the case where some of the table entries are constrained to be zero. For Fisher exact test on large sample, a complete enumeration of this distribution is often computationally infeasible and a Monte Carlo exact test is used  The case studies were used that the caretta hatchlings. The zoological data were collected from field studies on Dalyan Beach in Turkey. We want to know if there is a connection between The distances of the nests from the sea and Caretta Hatchling nesting area.","Kata Kunci : Fisher's exact test,markov chain monte carlo,caretta hatchling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/183683,IMPLEMENTASI NAIVE BAYES DAN RANDOM FOREST UNTUK ANALISIS SENTIMEN TERHADAP DATA IMBALANCED REVIEW PRODUK KOSMETIK PADA PLATFORM ONLINE SOCIOLLA,"LELY NUR AISAH, Dr. Herni Utami, M.Si",2019 | Skripsi | S1 STATISTIKA,"Perkembangan teknologi yang terus meningkat dalam era Big Data ini menjadi salah satu perintis munculnya situs jual beli online di Indonesia. Salah satu situs jual beli online yang terkenal di kalangan anak muda adalah Sociolla, situs jual beli produk kosmetik terlengkap dan terpercaya. Tidak hanya membeli produk kosmetik, pada situs tersebut konsumen juga dapat memberikan ulasan terhadap produk yang mereka beli. Tidak bisa dipungkiri bahwa ulasan yang muncul dapat mempengaruhi citra dari perusahaan. Penting bagi perusahaan untuk mengetahui tanggapan publik mengenai produk yang mereka tawarkan. Tanggapan publik tersebut berjumlah terlalu banyak untuk diproses secara manual. Oleh sebab itu, diperlukan metode yang mampu mengklasifikasikan ulasan ke dalam snetimen positif dan negatif secara otomatis. Metode klasifikasi yang dapat digunakan adalah NaÃƒÂ¯ve Bayes dan Random Forest. Dalam proses klasifikasi data ulasan secara real, tentunya akan menemui data imbalanced, dimana jumlah data pada kelas sentimen positif berjumlah lebih banyak dibandingkan dengan jumlah data pada kelas sentimen negatif. Maka diperlukan metode yang dapat mengatasi hal tersebut, yaitu dengan teknik undersampling. Dari perbandingan nilai akurasi, sensitivitas, dan spesifisitas diperoleh kesimpulan bahwa metode Random Forest Undersampling merupakan metode terbaik untuk klasifikasi pada kasus ini.","The growth of technology in the Big Data era has become one of the pioneers of the rise of e-commerce in Indonesia. One of the most popular e-commerce platforms among young people is Sociolla, which is the most complete and trustworthy e-commerce for buying cosmetic products. On this site, aside from buying products, consumers can also write reviews of product they have bought. Review can affect companyÃ¢ï¿½ï¿½s image in public eye. It is important for company to know about how public responds about their product. The amount of reviews is too much to be processed manually. Therefore, a special method is needed to classify the reviews automatically, whether it is poitive or negative. We can analyze it using the NaÃƒÂ¯ve Bayes and Random Forest method to classify review data. However, because the dataset that is being used is an imbalanced data, where the amount of positive class is greater than the amount of negative class, it is necessary to approach by resampling on the original data using the imbalanced undersampling techniques. After using the undersampling technique, we can conclude that Random Forest method is the best method for this case.","Kata Kunci : Analisis sentimen, review, web scraping, data imbalanced, NaÃƒÂ¯ve Bayes, Random Forest, teknik undersampling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/174981,Implimentasi Analisis Regresi Ridge Tak Bias Untuk Mengatasi Masalah Multikolinieritas,"ULIL KHAIR M, Dr. Herni Utami, M.Si",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen (Y) dan variabel independen (X). Secara umum, metode Ordinary Least Square (OLS) digunakan untuk mengestimasi koefisien regresi. Dalam metode tersebut ada asumsi klasik yang harus dipenuhi, diantaranya adalah tidak adanya multikolinieritas, menyebabkan hasil estimasi menjadi tidak valid.  Metode yang sering digunakan untuk menyelesaikan masalah multikolinieritas adalah regresi ridge. Konsep dari regresi ridge adalah menambah tetapan bias k dalam matriks korelasi XTX. Namun, dalam pemilihan tetapan bias k pada regresi ridge, banyak peneliti yang memberikan penentuan nilai k yang berbeda. Hal ini menjadikan tidak ada rumus tunggal dari metode regresi ridge. Pada skripsi ini akan dibahas mengenai unbiased ridge regression yang dikembangkanoleh Ozkale dan Gore (2009) dan Batah (2009). Metode ini merupakan modifikasi dari metode regresi ridge yang dikenalkan oleh Hoerl dan Kennard (1970). Konsep regrsi ridge ini yaitu menggandi tetapan bias k dengan memanfaatkan nilai dari vektor informasi prior J. Studi kasus ini menggunakan data produksi minyak kayu putih di Gunung Kidul dan faktor yang mempengaruhi pada September 2012 sampai Desember 2015. Diperoleh kesimpulan bahwa nilai Mean Square Error (MSE) yang lebih kecil daripada regresi ridge.","Regression is a statistical anlysis used to model a relationship between dependent variable (Y) and independent variables (X). Ordinary least square method is used generally to obtain the estimation of regression coefficient. This method is based on the fullfilment of classical regression assumptions, which is including there is no multicoliniearity will cause the parameter estimation of ordinary least square method become unvalid.  One of the popular method to solve multikoliniearity problem is the ridge regression. The concept of the ridge regression is adding a constant k to correlation matrix XTX. But on selecting the constant k for ridge regression, many researcher proposed different approximation for it. There is no explicit formula for ridge regression method. This paper will discuss about unbiased ridge regression developed by Ozkale and Gore (2009) and Batah (2009). This method is a modification method of ridge regression estimator proposed by Hoerl and Kennard (1970). The concept of the unbiased ridge regression is replacing the constant k with the value of vector prior information J. This paper case study is using eucalyptus oil in Gunung Kidul and the factors that affecting it from September 2012 until Desember 2015. The conclusion is the unbiased ridge regression estimator gives a smaller MSE than the ridge regression estimator.","Kata Kunci : multikolinieritas, OLS, regresi ridge, unbiased ridge regression, MSE"
http://etd.repository.ugm.ac.id/home/detail_pencarian/183685,"Perbandingan Kinerja Portofolio Mean Variance Menggunakan Klaster K-Means, Klaster Hierarki, dan Model Based Clustering","DELANEIRA RACHMITA P, Dr. Abdurakhman, S.Si., M.Si.",2019 | Skripsi | S1 STATISTIKA,"Investasi merupakan kegiatan menempatkan dana pada suatu aset dalam periode waktu tertentu dengan harapan bisa memperoleh keuntungan. Aset yang dapat diinvestasikan ada beragam, salah satunya ialah aset finansial berupa saham. Kegiatan investasi tidak dapat dihindarkan dari risiko investasi. Untuk meminimalkan risiko dalam investasi saham diperlukan manajemen investasi dengan cara membentuk portofolio. Portofolio merupakan gabungan dari beberapa aset atau saham. Investor dapat mengurangi tingkat risiko dengan membentuk portofolio yang efisien dan optimal. Berbagai macam metode dan penelitian telah dilakukan mengenai optimisasi portofolio. Salah satunya yaitu optimisasi portofolio dengan menggabungkan metode mean variance dengan analisis klaster. Sebelumnya telah dilakukan penelitian tentang optimisasi portofolio menggunakan metode mean variance dengan analisis klaster hierarki dan K-Means. Akan tetapi, kedua metode klaster tersebut tidak didasari oleh model statistik. Salah satu metode pengelompokkan data yang didasari oleh model statistik adalah model based clustering. Pada skripsi ini akan dibahas mengenai kinerja portofolio menggunakan metode mean variance dengan analisis klaster K-Means, klaster hierarki, dan model based clustering. Asumsi yang harus dipenuhi sesuai dengan asumsi metode mean variance yakni asumsi normalitas return. Studi kasus pada skripsi ini menggunakan data closing price saham bulanan periode Agustus 2014 hingga Juli 2019 dari 8 saham indeks LQ-45. Nilai return yang diamati dari 8 saham antara lain ADRO, ASII, BBCA, BBTN, INCO, INDF, KLBF, dan PTPP, kemudian dibentuk portofolio menggunakan metode mean variance dengan analisis klaster K-Means, klaster hierarki, dan model based clustering. Kinerja portofolio ketiga analisis tersebut kemudian dibandingkan dengan menggunakan rasio Sharpe dan berdasarkan keadaan di pasar saham. Kesimpulan yang diperoleh bahwa kinerja portofolio menggunakan metode mean variance dengan analisis model based clustering lebih baik daripada menggunakan analisis klaster hierarki dan K-Means.","Investment is an activity of putting funds in some assets for certain period of time and expecting to gain profit. Assets that can be invested are varied, one of them is financial assets, for example stocks. Investment activities can not be avoided from investment risk. To minimize risks in stock investments, investment management is needed by forming a portfolio. A portfolio is a combination of several assets or stocks. Investor can reduce the level of risk by forming an efficient and optimal portfolio. Many research on portfolio optimization have been carried out. One of them is portfolio optimization by combining the variance method with the cluster analysis. Previously, research on portfolio optimization has been carried out using the mean variance method with hierarchical and K-Means clustering analysis.However, the two cluster method are not based on statistical models. One of the data clustering method which is based on a statistical model is a model based clustering. In this paper, we will discuss about performance of portfolio using the mean variance method with K-means cluster, hierarchical cluster, and model based clustering anlysis. The assumptions that must be fulfilled are in accordance with assumption of the mean variance method, that is the assumption of normality of return. This research case study uses monthly stock closing price data for periode of August 2014 to July 2019 from 8 stocks of the LQ-45 index. The observed return value of 8 stocks including ADRO, ASII, BBCA, BBTN, INCO, INDF, KLBF, and PTPP then formed a portfolio using the mean variance method with K-Means clsuter analysis, hierarchical cluster, and model based clustering. Portfolio's performance from those three analyses then compared by using sharpe ratio and based on stock market situation. The conclusion is that performance of portfolio using the mean variance method with model based clustering analysis is better than using hierarchical and K-Means clustering analysis.","Kata Kunci : analisis klaster, model based clustering, portofolio, mean variance, rasio Sharpe"
http://etd.repository.ugm.ac.id/home/detail_pencarian/180103,Penerapan Regresi Beta Terboboti Geografis,"Yessi Oktavia, Prof. Dr Sri Haryatmi, M.Sc.",2019 | Skripsi | S1 STATISTIKA,"Model regresi beta terboboti geografis merupakan pengembangan dari regresi beta ke regresi terboboti geografis dengan variabel respon berdistribusi beta yaitu dalam interval (0,1) dan data mengandung faktor spasial. Estimasi parameter menggunakan metode Maximum Likelihood Estimation terboboti, yang mana letak geografis digunakan sebagai faktor pembobotnya yang menyebabkan setiap lokasi pengamatan mempunyai bobot yang berbeda sehingga menghasilkan model sejumlah lokasi pengamatan. Fungsi pembobot yang digunakan adalah fungsi kernel gaussian. Studi kasus dalam skripsi ini menggunakan data proporsi gizi buruk pada balita di kabupaten/kota Jawa Barat tahun 2011 yang diduga dipengaruhi oleh faktor berat badan bayi lahir rendah, kesehatan, usia ibu dan faktor kemiskinan. Berdasarkan analisis regresi beta terboboti geografis dihasilkan 26 model yang membagi wilayah Provinsi Jawa Barat ke dalam kelompok berdasarkan faktor yang mempengaruhi proporsi gizi buruk pada balita tahun 2011. Sebagai pembanding dalam pemilihan model digunakan regresi beta yang mana berdasarkan model yang dihasilkannya proporsi gizi buruk balita di kabupaten/kota Jawa Barat secara global dipengaruhi oleh persentase berat badan bayi lahir rendah dan persentase ibu usia kurang dari atau sama dengan 15 tahun.","The geographically weighted beta regression model is a development from beta regression to geographically weighted regression with beta distribution response variable, in the interval (0,1) and the data is influenced by spatial factors. Weighted Maximum Likelihood Estimation method used for measure parameter estimation, where the geographical location is used as a weighting factor which causes each observation location has different weights, resulting model for each observation location. The gaussian kernel function is used as the weighting function. The data used in this research is the proportion of under-five malnutrition in regencies/cities in West Java in 2011 which assumed to be affected by some factors such as low birth weight, health, maternal age, and poverty. From the analysis generated 26 models and divided the area of West Java Province into groups based on the factors that influenced the proportion of under-five malnutrition in regencies/cities in West Java in 2011. Beta regression is used as a comparison in the selection of models, according to the model, the proportion of under-five malnutrition in regencies/cities in West Java globally influenced by the percentage of low birth weight babies and the percentage of mothers under the age of 15.","Kata Kunci : Regresi Beta Terboboti Geografis, Bandwidth, Data Spasial, Fisher Scoring."
http://etd.repository.ugm.ac.id/home/detail_pencarian/179853,Penyusutan Koefisien dan Seleksi Variabel Regresi Menggunakan Metode Modifikasi Bootstrap LASSO,"Anisa Nur Firdaus, Drs. Danardono, M.P.H., Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan analisis statistika yang mempelajari hubungan antara satu atau lebih variabel independen dengan variabel dependen. Untuk kasus dimana variabel dependen memiliki dua kemungkinan hasil (dikotomus), analisis regresi yang digunakan adalah analisis regresi logistik. Dalam analisis regresi terdapat asumsi yang harus dipenuhi salah satunya adalah tidak ada multikolinearitas pada variabel independen. Apabila asumsi ini tidak terpenuhi menyebabkan model regresi yang terbentuk tidak lagi efisien karena nilai standar eror koefisien regresi menjadi sangat besar (overestimate). Pada skripsi ini akan dibahas mengenai Penyusutan Koefisien dan Seleksi Variabel Regresi Menggunakan Metode Modifikasi Bootstrap LASSO yang dikembangkan oleh A. Chatterjee dan S.N. Lahiri (2011). Metode Modifikasi Bootstrap LASSO merupakan pengembangan dari metode Bootstrap LASSO yang sebelumnya dikembangkan dari LASSO oleh Tibshirani (1996). Studi kasus ini menggunakan data tipe punggung manusia dan faktor-faktor yang mempengaruhinya. Diperoleh kesimpulan bahwa dengan nilai threshold sebesar 0,0053 dan perulangan sebanyak 130 kali, metode Modifikasi Bootstrap LASSO lebih efisien untuk mengatasi masalah multikolinearitas dengan memberikan nilai akurasi prediksi model regresi yang lebih baik dibandingkan dengan metode LASSO maupun Bootstrap LASSO. Hal tersebut dapat dilihat semakin meningkatnya nilai proporsi konkordansi.","Regression analysis is statistical analysis that studies the relationship between one or more independent variables with the dependent variable. For cases where the dependent variable has two possible outcomes (dichotomous), the regression analysis used is logistic regression analysis. In the regression analysis there are assumptions that must be fulfilled one of which is that there is no multicollinearity in the independent variable. If this assumption is not fulfilled, causing the regression model obtained becomes inefficient because the standard error value of the regression coefficient becomes overestimate. 	In this paper, will be discused about Coefficient Shrinkage and Variable Selection Using Modified Bootstrap LASSO method developed by Chatterjee A. and Lahiri S.N. (2011). Modified Bootstrap LASSO method is development of the Bootstrap LASSO method which was previously developed by Tibshirani (1996). This case study uses data on human back type and factors that influence it. It was conclude that using a threshold value of 0,0053 and repetition of 130 times (BM=130),  Modified Bootstrap LASSO method is more efficient to overcome multicollinearity problem by giving a better accuration prediction value of regression model  than the LASSO and Bootstrap LASSO methods. It can be seen that the concordance proportion value increases.","Kata Kunci : Regresi logistik, multikolinearitas, Modifikasi Bootstrap LASSO, Bootstrap LASSO, LASSO, nilai proporsi konkordansi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/175250,KARAKTERISTIK STATISTIK DAN PREDIKTABILITAS BITCOIN DENGAN MODEL AUTOREGRESI,"Olivia Monalisa, Prof. Dr. rer. nat. Dedi Rosadi, M.Sc",2019 | Skripsi | S1 STATISTIKA,"Penelitian ini secara empiris menyelidiki karakteristik statistik dan prediktabilitas return dan volatilitas Bitcoin. Distribusi dari return dan volatilitas Bitcoin menunjukkan kegemukan ekor kanan dan bagian tengah yang tinggi. Bitcoin tidak memperlihatkan properti dinamis dari persistensi volatilitas, bertentangan dengan fakta yang ada pada runtun waktu keuangan. Juga, model autoregresi dengan nilai volatilitas masa lalu tidak signifikan dalam memprediksi perubahan volatillitas Bitcoin pada masa yang yanga akan datang. Sentimen investor terhadap Bitcoin memiliki nilai informasi yang tidak signifikan untuk menjelaskan perubahan volatilitas Bitcoin untuk masa yang akan datang.","This study empirically investigates the statistical characteristics and predictability of Bitcoin return and volatility. The distribution of Bitcoin returns and volatility display a fat right tail and high central parts. Bitcoin does not show the dynamic property of volatility persistence, contrary to stylized facts in financial time series. Also, the autoregressive model using past volatility does not well work in predicting changes in Bitcoin volatility for future periods. Investor sentiment regarding Bitcoin has not significant information value for explaining changes in Bitcoin volatility for future periods.","Kata Kunci : Virtual Currency, Bitcoin, Properti Statistika, Model Autoregresi AR(1), Long Memory, GPH Hurst, ARIMAX"
http://etd.repository.ugm.ac.id/home/detail_pencarian/167828,ALTERNATIF METODE WARD HIERARCHICAL CLUSTERING MENGGUNAKAN UKURAN JARAK MANHATTAN,"QYRATURRAHMANI, Dr. Herni Utami, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Analisis klaster merupakan metode statistika multivariat yang bertujuan untuk mengelompokkan objek-objek yang memiliki kemiripan karakteristik ke dalam suatu klaster. Klaster Hirarki merupakan salah satu teknik analisis klaster yang dilakukan tanpa mengetahui jumlah klaster yang diinginkan. Metode Ward dikenal sebagai metode terbaik dalam analisis klaster hirarki. Namun, metode ini hanya dapat digunakan untuk satu ukuran jarak yaitu jarak squared euclidean. Sementara ada beberapa ukuran jarak yang memiliki kelebihan, salah satunya seperti jarak Manhattan yang cenderung memiliki sensitivitas yang rendah terhadap data outlier. Oleh karena itu, dilakukan perkembangan metode Ward ini dengan ukuran jarak Manhattan dengan ketentuan tidak melanggar sifat-sifat dari metode Ward Hierarchical Clustering.  	Melalui rekursif formula e-distance dengan jarak Manhattan diperoleh nilai parameter Lance-Williams yang sama dengan metode Ward menggunakan jarak squared euclidean. Hal ini menunjukkan metode Ward dengan jarak Manhattan tidak melanggar sifat dari metode Ward. Dalam aplikasinya, untuk kasus yang memiliki beberapa data outlier metode Ward dengan jarak Manhattan lebih unggul dibanding dengan jarak squared euclidean melalui validitas dunn index.","Cluster analysis is a multivariate statistics method aiming at agglomerating objects which have similar characteristics into one cluster. Hierarchical clustering is one of the cluster analysis performed without knowing the amount of the desired clusters. The Ward method is known as the best method in hierarchical clustering analysis. However, this method can only be used for one distance measure: squared euclidean. Meanwhile, there are several distance measures that have more advantages, one of them is Manhattan distance that has a low sensitivity to outlier data. Therefore, the Ward method development was performed using Manhattan distance measure without violate the properties of Ward Hierarchical Clustering method. Through recursive formula e-distance using Manhattan distance, it was obtained the same Lance-Williams parameter value using Ward method with squared euclidean distance. This shows that Ward method development using Manhattan distance does not contravene the properties of it. In application, some cases that have several outlier data of the Ward method are more advanced compared to the squared euclidean distance through dunn index validity.","Kata Kunci : Ward Hierarchical Clustering, e-distance, Manhattan, Dunn Index."
http://etd.repository.ugm.ac.id/home/detail_pencarian/175259,ANALISIS KLASTER MENGGUNAKAN ALGORITMA I-CLARANS UNTUK DATASET BESAR DENGAN PENCILAN,"KD DEWI YULIANINGSIH, Dr. Herni Utami, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Analisis klaster merupakan metode statistika multivariat yang bertujuan untuk mengelompokkan objek-objek yang memiliki kesamaan karakteristik ke dalam suatu klaster. K-medoids adalah metode analisis klaster dengan medoid sebagai pusat klasternya, di mana medoid merupakan objek yang letaknya terpusat di dalam suatu klaster, sehingga robust terhadap pencilan. Algoritma k-medoids yang digunakan dalam penelitian ini adalah Improved Clustering Large Applications based on Randomized Search (I-CLARANS), di mana I-CLARANS merupakan pengembangan dari algoritma Clustering Large Applications based on Randomized Search (CLARANS) dalam meningkatkan kualitas klastering. Metode I-CLARANS menggunakan dua batasan, yaitu numlocal, untuk membatasi iterasi dan maxneighbour, untuk membatasi neighbour pada suatu node. Pengelompokan didasarkan pada ukuran jarak Euclidean dan Manhattan. Selanjutnya, untuk mengetahui tingkat validasi digunakan silhouette width.  Metode analisis klaster terbaik untuk mengelompokkan beton-beton berdasarkan komponen yang digunakan adalah metode I-CLARANS dengan jarak Manhattan, k=6, numlocal = 2, dan maxneighbour = 80. Pada studi kasus, dapat diketahui bahwa berdasarkan nilai overall average silhouette width metode I-CLARANS lebih baik dibandingkan metode CLARANS.","Cluster analysis is a multivariate statistical method to classify objects that have similar characteristics into a cluster. K-medoids is a clustering method with the medoid as its center cluster, where medoid is the most centrally located object in a cluster, which is robust to outliers. The k-medoids algorithm that used in this study is Improved Clustering Large Applications based on Randomized Search (I-CLARANS), where I-CLARANS is development of Clustering Large Applications based one Randomized Search (CLARANS) algorithm in improving quality of clusters. I-CLARANS method uses two parameters, named as numlocal, to limit the iteration and maxneighbour, to limit neighbours to a node. Clustering is based on Euclidean distance and Manhattan distance. Then, to determine the validation level used silhouette width as evaluation method.  The best clustering method for classifying concretes based on components used is I-CLARANS method with Manhattan distance, k=6, numlocal = 2, and maxneighbour = 80. In the case study, can be known based on the overall average silhouette width value that I-CLARANS method is better than CLARANS method.","Kata Kunci : K-medoids, Improved Clustering Large Applications based on Randomized Search, pencilan, dataset besar, silhouette width, Clustering Large Applications based on Randomized Search"
http://etd.repository.ugm.ac.id/home/detail_pencarian/180636,REGRESI ROBUST RIDGE DENGAN ESTIMASI LEAST MEDIAN SQUARE (LMS) PADA REGRESI LINEAR UNTUK MENGATASI MULTIKOLINEARITAS DAN PENCILAN (Studi Kasus: Belanja Modal beserta Faktor-Faktor yang Mempengaruhi di Jawa Timur pada Tahun 2017),"DAFFA ULHAQ PRADANA, Prof. Dr. Sri Haryatmi, M.Sc.",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi klasik mengandung beberapa asumsi yang harus dipenuhi agar model yang terbentuk bisa dikatakan valid salah satunya adalah tidak adanya multikolinearitas dan tidak adanya pencilan pada data. Apabila terdapat multikolinearitas dan pencilan, maka hasil estimasi model regresi menggunakan metode ordinary least square (OLS) menjadi tidak valid. Pada skripsi ini akan dibahas mengenai penggabungan metode regresi ridge dengan metode regresi robust dengan estimasi least median square (LMS) untuk mengatasi adanya multikolinearitas dan pencilan secara bersama-sama. Studi kasus yang digunakan adalah data belanja modal dan variabel yang mempengaruhi di setiap kabupaten/kota di Jawa Timur tahun 2017. Diperoleh kesimpulan bahwa metode regresi robust ridge dengan estimasi LMS dapat mengatasi adanya multikolinearitas dan pencilan.","Classical regression analysis contains several assumptions that must be fulfilled so that the model formed can be said to be valid, one of which is the absence of multicollinearity and the absence of outliers in the data. If there is multicollinearity and outliers, the estimation of the regression model using the ordinary least square (OLS) method becomes invalid. In this paper we will discuss the merging of ridge regression methods with robust regression methods with least median square (LMS) estimation to overcome the presence of multicollinearity and outliers together. The case study used is capital expenditure and variable data that affect each district / city in East Java in 2017. The conclusion is that the robust ridge regression method with LMS estimation can overcome the presence of multicollinearity and outliers.","Kata Kunci : analsis regresi, multikolinearitas, pencilan, regresi ridge, regresi robust, least median square."
http://etd.repository.ugm.ac.id/home/detail_pencarian/181923,VALUASI OBLIGASI DENGAN KUPON SATU PERIODE BERDASARKAN REVISED FIRST PASSAGE TIME,"NURPATMA HERLI APRIANI, Dr. Abdurakhman, S.Si, M.Si",2019 | Skripsi | S1 STATISTIKA,"Obligasi merupakan salah satu instrumen keuangan yang merupakan suatu pernyataan utang dari penerbit obligasi kepada pemegang obligasi beserta janji untuk membayar kembali pokok utang beserta bunganya pada saat jatuh tempo.  Sekuritas obligasi merupakan investasi berpendapatan tetap (fixed income securities) karena keuntungan yang diberikan kepada investor obligasi didasarkan pada tingkat suku bunga yang telah ditentukan sebelumnya. Investasi obligasi selain menghasilkan pendapatan juga memberikan potensi risiko investasi. Salah satu risiko investasi obligasi adalah risiko kredit. Risiko kredit (credit risk) adalah risiko kerugian yang disebabkan suatu perusahaan gagal membayar hutangnya pada saat jatuh tempo sehingga dapat dinyatakan bangkrut (default). Berdasarkan beberapa permasalahan di atas, pada penelitian ini dikembangkan valuasi obligasi dengan kupon satu periode Revised First Passage Time. Hasil dari penelitian ini adalah perumusan nilai ekuitas, liabilitas, dan probabilitas kebangkrutan perusahaan saat jatuh tempo obligasi. Studi empiris dilakukan pada data obligasi korporasi PT Maybank Indonesia Tahun 2019 Seri A dan Seri B dengan nilai face value sebesar 650 Milyar dan 350 Milyar. Nilai probabilitas kebangkrutan sebesar 4,439718E-39 dan 2,545973E-28 mengindikasikan bahwa PT Maybank Indonesia Tbk dianggap masih dapat memenuhi pembayaran kewajibannya pada April 2022 dan April 2024.","Bond is one of the financial instrument that consist of the debt letter from issuer to bondholder including the date for paying the debt value and coupon as they mature. Bond investment is fixed-income securities because the profit given to bond investors is depended on interest rate level which has been set earlier. Not only generating income, bond investment also brings investment risk potential. One of bond investment risks is credit risk. Credit risk is risk of loss which is caused by company failure to pay debt in the maturity time thus it is considered bankrupt. Based on a couple of previously stated problems, valuation of coupon bond based on Revised First Passage Time. The result of this research is the formulation of equity value, liability value, and probability of company default in the bond maturity time. An empirical study using corporate bond of PT Maybank Indonesia Tbk Series A and Series B with 650 billion and 350 billion face values. Values of Probability of Default 4,439718E-39  and 2,545973E-28 prove that PT Maybank Indonesia still can pay the principal of the bond at April 2022 and April 2024.","Kata Kunci : Revised First Passage Time, ekuitas, liabilitas, dan probabilitas kebangkrutan."
http://etd.repository.ugm.ac.id/home/detail_pencarian/172725,Grafik Pengendali Poisson Progressive Mean,"AYU FITRI NUR H, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2019 | Skripsi | S1 STATISTIKA,Kualitas produk merupakan hal yang sangat penting bagi perusahaan. Kualitas produk dapat dipengaruhi saat proses produksi maupun distribusi. Analisis yang digunakan untuk mengetahui kualitas produk adalah pengendalian kualitas statistik. Salah satu alat yang dapat digunakan dalam pengendalian kualitas statistik adalah grafik pengendali. Grafik pengendali c biasa digunakan untuk memonitor ketidaksesuaian dari proses produksi yang berdasarkan distribusi Poisson. Terdapat alternatif grafik pengendali lain untuk memonitor banyaknya ketidaksesuaian dari proses produksi yang berdasarkan distribusi Poisson yang lebih efisien dari grafik pengendali c yaitu grafik pengendali Poisson Progressive Mean. Grafik pengendali Poisson Progressive Mean akan menjadi fokus pembahasan pada skripsi ini. Tingkat sensitivitas grafik pengendali c dan Poisson Progressive Mean dapat dilihat pada hasil grafik pengendali dan Average Run Length (ARL). Hasil perbandingan tersebut diperoleh bahwa grafik pengendali Poisson Progressive Mean lebih sensitif dibandingkan dengan grafik pengendali u dalam memonitor rata-rata banyak ketidaksesuaian per unit pada proses produksi.,Product quality is very important for the company. Product quality can be affected during the production and distribution process. The analysis used to find out the quality product is statistical quality control. A tool that can be used in statictical quality control is control chart. U-chart usually used to monitor the number of nonconformities on a unit of a process based on Poisson distribution. There is an alternative control chart that used to monitor the number of nonconformities on a unit of a process based on Poisson distribution that more efficient than u control chart which is Poisson Progressive Mean control chart. Poisson Progressive Mean control chart will be the focus of discussion on this thesis. The sensitivity level of the control chart u and Poisson Progressive Mean can be seen in the results of the control chart and Average Run Length (ARL). The comparison results are obtained that the Progressive Mean Poisson controller chart is more sensitive than the control chart u in monitoring the average number of incompatibilities per unit of the production process.,"Kata Kunci : control chart, Poisson Progressive Mean, Average Run Length, ARL"
http://etd.repository.ugm.ac.id/home/detail_pencarian/171449,Estimasi LTS dan Estimasi LMS Pada Regresi Robust Linear,"SIFAULU DUHA, Drs. Zulaela, Dipl.Med.Stats., M.Si",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi linear merupakan analisis statistik yang bertujuan untuk mengetahui hubungan antar variabel. Salah satu metode untuk mengestimasi parameter regresi linear adalah metode kuadrat terkecil. Diketahui bahwa estimasi metode kuadrat terkecil merupakan metode yang relatif lebih mudah, metode ini memiliki asumsi klasik yang harus dipenuhi, salah satunya residual yang berdistribusi normal. Pada kenyataannya masih terdapat data yang tidak memenuhi asumsi klasik tersebut, sehingga estimasi metode kuadrat terkecil tidak tepat digunakan. Skripsi ini membahas perbandingan dua estimasi robust pada regresi linear sebagai estimasi alternatif dari metode kuadrat terkecil saat residual tidak berdistribusi normal yakni saat data terkontaminasi dengan outlier. Metode estimasi robust dapat mengatasi outlier dengan mencocokkan model regresi dengan sebagian besar data, sehingga analisis regresi dengan estimasi robust dapat menghasilkan estimasi yang adaptif terhadap outlier. Metode estimasi yang digunakan dalam skripsi ini adalah estimasi least trimmed square (LTS) dan estimasi least median square (LMS). Data yang digunakan adalah data kadar kolesterol seseorang di Balai Laboratorium Kesehatan Yogyakarta tahun 2016 hubungannya dengan variabel HDL, LDL dan Trigliserida. Diperoleh hasil bahwa kedua estimasi robust lebih baik dari estimasi kuadrat terkecil berdasarka standard error yang lebih kecil. Di antara kedua estimasi robust diperoleh bahwa estimasi LTS lebih baik dari estimasi LMS berdasarkan efisiensi variansi parameter.","Linear regression analysis is a statistical analysis that aims to determine the relationship between variables. One method for estimating linear regression parameters is the least squares method. It is known that the estimation of the least squares method is a relatively easier method, this method has a classic assumption that must be fulfilled, one of which is a residual that is normally distributed. In fact, there are still data that do not fulfill the classical assumptions, so the estimation of the least squares method is not appropriate to use. This paper discusses the comparison of two robust estimates of linear regression as an alternative estimate of the least squares method when the residual is not normally distributed, i.e when the data is contaminated with outliers. Robust estimation method can overcome outliers by matching regression models with most data, so that regression analysis with robust estimation can produce estimates that are adaptive to outliers. The estimation method used in this paper is the least trimmed square (LTS) estimate and least median square (LMS) estimation. The data used is a person's cholesterol level data at the Yogyakarta Health Laboratory Center in 2016 in relation to the variables HDL, LDL and Triglycerides. The results obtained by the two robust estimates are better than the least squares estimation based on a smaller standard error. Between the two robust estimates, it is obtained that the LTS estimation is better than the LMS estimation based on the efficiency of the variance parameters.","Kata Kunci : outlier, robust, estimasi LTS, estimasi LMS"
http://etd.repository.ugm.ac.id/home/detail_pencarian/174532,Valuasi Obligasi Berbasis Transformasi Fast-Fourier,"HENI PUSPITASARI, Dr. Abdurakhman, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Obligasi merupakan salah satu bentuk investasi yang memberikan pendapatan tetap kepada investor sesuai dengan waktu yang disepakati. Akan tetapi, dalam obligasi terdapat resiko kredit. Resiko kredit adalah resiko berupa perusahaan penerbit obligasi mengalami kesulitan dalam pembayaran kewajiban saat jatuh tempo. Untuk mengatasi hal tersebut, Merton pada tahun 1974 memperkenalkan valuasi obligasi dengan menerapkan metode perhitungan opsi Black-Scholes, dimana return aset diasumsikan berdistribusi normal. Dalam investasi nyata, terdapat return aset yang tidak berdistribusi normal. Model Transformasi Fast-Fourier dikembangkan sebagai solusi dari masalah tersebut. Model Transformasi Fast-Fourier merupakan transformasi yang memiliki fungsi karakteristik. Dalam model ini digunakan fungsi karakteristik Gamma Varians yang mampu menangkap perilaku logreturn yang tidak berdistribusi normal.","Bonds are a form of investment that provides fixed income to investors according to the contract time. There are credit risks in bonds. Credit risk is a risk of issuing company having difficulties in paying debt at maturity. To anticipate that, Merton in 1963 introduced bond valuation by applying the Black- Scholes option formula, where asset returns are assumed to be normally distributed. In real investment, not all of the asset returns are normally distributed. The Fast-Fourier Transformation Model was developed as a solution to this problem. The Fast-Fourier Transformation Model is an transformation that has a characteristic function. In this model, the characteristic function of Gamma Variance is used which is able to capture logreturn behavior that do not follow a normal distribution.","Kata Kunci : Valuasi Obligasi, Transformasi Fast-Fourier, Gamma Varians"
http://etd.repository.ugm.ac.id/home/detail_pencarian/175305,FUSI SKOR BIOMETRIK MENGGUNAKAN REGRESI ISOTONIK,"Muhammad Rizky Mangkuwidjaya, Dr. Nanang Susyanto, S.Si., M.Sc.",2019 | Skripsi | S1 STATISTIKA,"Verifikasi biometrik adalah suatu sistem yang akan memutuskan apakah skor kemiripan antara dua sampel biometrik adalah skor yang diperoleh dari orang yang sama (genuine score) atau diperoleh dari orang yang berbeda (impostor score). Sistem uni biometrik yang memberikan kesimpulan hanya berdasarkan dari satu sumber informasi biometric memiliki keterbatasan yaitu kurangnya keunikan dari sifat biometric yang dipilih, noisy data dan spoof attack. Sistem multi biometric memadukan beberapa informasi biometrik sehingga dapat memberikan kinerja pengenalan yang lebih baik dan mengatasi keterbatasan dari system uni biometric.  Regresi isotonik adalah suatu model regresi yang digunakan untuk fungsi regresi yang berbentuk monoton naik. Regresi isotonic dapat memperkirakan peluang posterior untuk menerima H_1 sehingga dapat diganakan untuk menentukan likelihood ratio yang digunakan dalam pengambilan keputusan. Pengukuran performa metode ini dilakukan dengan membandingkan True Positive Rate (TPR) pada saat ditetapkan False Acceptance Rate (FAR) tertentu. Studi kasus menggunakan data publik NIST dan XM2VTS memberikan hasil metode ini unggul ketika digunakan pada clasiffier yang independen.","Biometric verification is a system that will determine whether the similarity score between two biometric samples is a score obtained from the same person (genuine score) or obtained from a different person (impostor score). The uni-biometric system that provides conclusions only from one biometric source that has limitations is the uniqueness of the selected biometric properties, noisy data and spoof attacks. Multi-biometric systems combine several biometric information that can provide better improvements and overcome the difficulties of uni-biometric system. Isotonic regression is a regression model used for regression function in the form of monotonically rising. Isotonic regression can estimate the posterior ratio to receive H_1 so that it can be applied to determine the likelihood ratio used in decision making. Measuring the performance of this method is done by comparing the True Positive Rate (TPR) when certain False Acceptance Rate (FAR) is set. Case studies using NIST and XM2VTS public data give the results of this method superior when used on independent clasiffier.","Kata Kunci : Fusi Biometrik, Likelihhod Ratio, Pooled Adjacent Violator Algorthm (PAVA), Regresi Isotonik."
http://etd.repository.ugm.ac.id/home/detail_pencarian/182729,APLIKASI REGRESI BINOMIAL NEGATIF DAN REGRESI POISSON INVERSE GAUSSIAN UNTUK MENANGANI OVERDISPERSI PADA DATA CACAH (Studi Kasus : Jumlah Kematian Ibu di Jawa Tengah Tahun 2017),"Windha Maytarizal, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2019 | Skripsi | S1 STATISTIKA,"Angka kematian ibu merupakan salah satu target dalam program Sustainable Development Goals (SDGs) untuk mencapai tujuan menjamin kehidupan yang sehat dan mendorong kesejahteraan bagi semua orang di segala usia. Hal ini dikarenakan belum tercapainya target dalam pembangunan Millenium Development Goals (MDGs) yang berakhir pada tahun 2015. Di Jawa Tengah, angka kematian ibu sebesar 88,05 per 100.000 kelahiran hidup pada tahun 2017. Berbagai penelitian telah dilakukan untuk mengetahui faktor utama yang mempengaruhi jumlah kematian ibu sebagai upaya penanggulangan. Penelitian ini bertujuan untuk mendapatkan faktor-faktor apa saja yang mempengaruhi banyaknya jumlah kematian ibu di Jawa Tengah dengan membandingkan metode Poisson Inverse Gaussian Regression (PIGR) dan Negative Binomial Regression (NBR). PIGR dan NBR adalah metode yang digunakan ketika terjadi overdispersion pada regresi Poisson. Data yang digunakan dalam penelitian ini diperoleh dari Dinas Kesehatan dan Badan Pusat Statistik Provinsi Jawa Tengah yang meliputi jumlah kematian ibu di Jawa Tengah menurut Kabupaten/Kota beserta faktor-faktor yang diduga mempengaruhinya. Berdasarkan hasil analisis, model dari PIGR adalah model yang terbaik dibandingkan model dari regresi poisson dan NBR. Variabel yang mempengaruhi jumlah kematian ibu secara signifikan menurut model tersebut adalah persentase pelayanan kesehatan nifas, persentase ibu hamil melaksanakan program K1, dan persentase tenaga kesehatan.","The maternal mortality rate is one of the targets in the Sustainable Development Goals (SDGs) program to achieve the goal of ensuring a healthy life and encourage welfare for all people of all ages. This is due to the fact that targets in the Millennium Development Goals (MDGs) have not been completed in 2015. In Central Java, the maternal mortality rate was 88.05 per 100,000 live births in 2017. Various studies have previously been conducted to determine the main factors affecting the number of maternal death. This study aimed to obtain what factors that influence the number of maternal deaths in Central Java by comparing the Poisson Inverse Gaussian Regression (PIGR) and Negative Binomial Regression (NBR). PIGR and NBR are methods that have been used when the assumptions on Poisson Regression are not met due to overdispersion. The data used in this study was obtained from the Provincial Health Office and Central Bureau of Statistics of Central Java . Data contains the number of maternal deaths in Central Java by Regency / City and the factors that allegedly affect them. Based on the analysis, the model of PIGR is the best model compared to the model of Poisson regression and NBR. Variables that significantly affect the number of maternal deaths according to the model are the percentage of puerperal health services, the percentage of pregnant women implementing the K1 program, and the percentage of health workers.","Kata Kunci : Overdispersi, AIC, PIGR, NBR, Angka Kematian Ibu, dan Regresi Poisson"
http://etd.repository.ugm.ac.id/home/detail_pencarian/171723,Sistem Bonus Malus Tergeneralisasi dengan Frekuensi Klaim Berdistribusi Binomial Negatif dan Besar Klaim Berdistribusi Pareto,"Fransiska Eygenio, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Sistem Bonus-Malus adalah suatu sistem perhitungan premi yang disesuaikan atas dasar pengalaman klaim dari setiap pemegang polis. Sistem Bonus-Malus optimal merupakan sistem dimana pemegang polis yang telah mengajukan satu klaim atau lebih akan dikenakan hukuman (malus) berupa kenaikan premi, sedangkan bagi pemegang polis yang tidak mengajukan klaim akan diberikan penghargaan berupa penurunan premi di periode berikutnya. Selain mendorong pemegang polis asuransi kendaraan bermotor untuk mengemudi dengan hati-hati, sistem ini bertujuan untuk menilai risiko individu dengan lebih baik. Pada skripsi ini akan dibahas mengenai desain sistem Bonus-Malus tergeneralisasi dengan frekuensi klaim berdistribusi binomial negatif dan besar klaim berdistribusi Pareto pada asuransi kendaraan bermotor. Estimasi parameter untuk komponen frekuensi klaim diperoleh menggunakan metode maksimum likelihood dan estimasi parameter komponen besar klaim diperoleh menggunakan metode quasi-likelihood. Dalam pembentukan tabel premi, digunakan metode Bayesian. Berdasarkan studi kasus dengan data polis asuransi kendaraan bermotor tahun 2014 perusahaan asuransi Jasindo, sistem Bonus Malus tergeneralisasi dengan frekuensi klaim berdistribusi binomial negatif dan besar klaim berdistribusi Pareto ini lebih adil karena sistem ini mempertimbangkan frekuensi klaim, besar klaim, dan variabel lokasi dalam mengestimasi risiko terjadinya klaim dari setiap pemegang polis.","Bonus-Malus system is a premium calculation system that is adjusted based on the claim experience of each policyholder. The optimal Bonus-Malus system is a system where policyholders who have submitted one or more claims will be subject to a malus penalty in the form of an increase in premiums, while for policyholders who do not claim will be given a reward in the form of a decrease in premium in the following period. In addition to encourage automobile insurance policyholders to drive carefully, this system aims to better assess individual risk. This thesis will discuss the design of generalized Bonus-Malus system with the claim frequency distribution is negative binomial and the claim severity distribution is Pareto on automobile insurance. Parameter estimation for claim frequency component is obtained using the maximum likelihood estimation and parameter estimation for the claim severity component is obtained using the quasi-likelihood estimation. Bayesian method is used to create premium tables. Based on case study using automobile insurance data of Jasindo company in 2014, the generalized Bonus Malus system with the claim frequency distribution is negative binomial and the claim severity distribution is Pareto, is more fair because this system considers the claim frequency, claim severity and variable of location in estimating the claim risk from each policy holder.","Kata Kunci : premi, sistem Bonus-Malus, distribusi binomial negatif, distribusi Pareto, model linear tergeneralisasi (GLM)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/174542,Algoritma KAMILA untuk Analisis Clustering pada Data Tipe Campuran,"HANNY NUR HANIFAH, Prof. Dr. rer. nat. Dedi Rosadi, M. Sc.",2019 | Skripsi | S1 STATISTIKA,"Data yang tersedia untuk diolah dapat terdiri dari campuran data bertipe kontinu dan kategorik. Pengelompokkan data tipe campuran tersebut dapat dilakukan dengan beberapa metode. Namun beberapa metode memerlukan pembobotan yang tepat antara variabel kontinu dan variabel kategorik, serta perlunya asumsi parametrik yang kuat.  Algoritma KAMILA merupakan gabungan dari metode algoritma K-means dan Gaussian-multinomial mixture models, dimana nantinya algoritma KAMILA dapat menangani kelemahan dari dua metode tersebut. Selanjutnya dengan menggunakan nilai Adjusted Rand Index akan ditunjukkan bahwa algoritma KAMILA lebih baik dalam mengelompokkan data dibandingkan metode weighted K-means.","Datasets that available to process consist of mixed continuous and categorical data. There are several methods can be used for mixed data clustering analysis. We show that current clustering method need appropriate weighting between continuous and categorical data, also need strong parametric assumptions. KAMILA algorithm combines two popular methods, K-means algorithm and Gaussian-multinomial mixture models, that can handle their weakness. We show that KAMILA algorithm is better than weighted K-means method from their Adjusted Rand Index value.","Kata Kunci : Clustering, KAMILA, mixed data, data tipe campuran, K-means, Gaussian-multinomial mixture models"
http://etd.repository.ugm.ac.id/home/detail_pencarian/168152,PENERAPAN METODE KOMBINASI NAIVE BAYES DAN K NEAREST NEIGHBOR (cNK) DALAM ANALISIS KLASIFIKASI,"Diah Ayu Setyaningsih, Prof. Dr. rer. nat. Dedi Rosadi, M.Sc",2019 | Skripsi | S1 STATISTIKA,"Metode Naive Bayes dan K Nearest Neighbor memiliki kelemahan yang berlawanan. Metode Naive Bayes memiliki kelemahan dalam mengatasi atribut dengan tipe data numerik. Sedangkan K Nearest Neighbor memiliki kelemahan dalam mengatasi atribut dengan tipe data kategorik. Metode kombinasi Naive Bayes dan K Nearest Neighbor (cNK) adalah sebuah metode yang bertujuan meningkatkan performa dari kedua metode penyusunnya, yakni Naive Bayes dan K Nearest Neighbor, dengan menutupi kelemahan dari masing-masing metode. Dari hasil analisis dengan menggunakan tiga buah data set yang berbeda diperoleh kesimpulan bahwa metode cNK memiliki akurasi yang lebih tinggi dibandingkan dengan metode Naive Bayes dan K Nearest Neighbor.","Naive Bayes and K Nearest Neighbor method have opposite weakness. The weakness of Naive Bayes method is to deal with numerical attributes. While the weakness of K Nearest Neighbor is to deal with categorical attributes. Combination Naive Bayes and K Nearest Neighbor (cNK) method is a method that has a goal to improve the performance of both Naive Bayes and K Nearest Neighbor method by covering both method's weakness. In the case study, three different datasets are used. The conclusion from the analysis is cNK method has greather accuracy than Naive Bayes and K Nearest Neighbor method.","Kata Kunci : Naive Bayes, K Nearest Neighbor, cNK, akurasi, klasifikasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/171737,Penerapan Modified Restricted Liu Estimator dalam Menangani Masalah Multikolinearitas pada Model Regresi Logistik,"AFIFAH INDAH FLUORIANA, Drs. Zulaela, Dipl.Med.Stats.,M.Si",2019 | Skripsi | S1 STATISTIKA,"Regresi logistik merupakan analisis yang digunakan untuk mengetahui hubungan antara variabel dependen yang berdistribusi tak normal dengan variabel independen, dimana variabel dependen mempunyai dua atau lebih kategori. Koefisien parameter regresi logistik dapat diestimasi dengan metode maksimum likelihood estimator. Terdapat alternatif lain yang dapat mengestimasi koefisien parameter regresi logistik yaitu metode restricted maximum likelihood estimator dengan mempertimbangkan batasan linear. Namun ketika terdapat masalah multikolinearitas, metode modified restricted liu estimator lebih tepat digunakan.  Pada skripsi ini, modified restricted liu estimator pada analisis regresi logistik diaplikasikan untuk memodelkan faktor-faktor yang mempengaruhi risiko kredit macet di BPRS Khasanah Ummat Purwokerto, dimana terdapat masalah multikolinearitas. Rata-rata kuadrat galat dari restricted maximum likelihood estimator dan modified restricted liu estimator akan dibandingkan. Kemudian diperoleh hasil yang menunjukkan bahwa modified restricted liu estimator memberikan nilai rata-rata kuadrat galat yang lebih kecil dari restricted maximum likelihood estimator.","Logistic regression is an analysis used to determine the relationship between the dependent variable which is abnormally distributed and independent variables, where the dependent variable has two or more categories. The logistic regression parameter coefficient can be estimated using the maximum likelihood estimator method. There are other alternatives that can estimate the logistic regression parameter coefficient, namely the restricted method of maximum likelihood estimator by considering linear constraints. But when there are multicollinearity problems, the modified restricted liu estimator method is more appropriate to use. 	In this paper, modified restricted liu estimator in logistic regression analysis was applied to analyze the factors that affect the credit risk in BPRS Khasanah Ummat Purwokerto, where there are multicollinearity problems. The MSE of the restricted maximum likelihood estimator and modified restricted liu estimator will also be compared. Then the results obtained showed that modified restricted liu estimator gives a mean squared error is smaller than the restricted maximum likelihood estimator.","Kata Kunci :  regresi logistik, MLE, restricted MLE, multikolinearitas, liu estimator, modified restricted liu estimator, MSE"
http://etd.repository.ugm.ac.id/home/detail_pencarian/168164,GRAFIK PENGENDALI RATA-RATA BERGERAK TERTIMBANG EKSPONENSIAL DENGAN BATAS PENGENDALI ESTIMATOR ROBUST Qn DAN MEDIAN ABSOLUTE DEVIATION,"RAHMADINA ROSADI, Prof. Dr. rer.nat. Dedi Rosadi, S.Si., M.Sc",2019 | Skripsi | S1 STATISTIKA,"Untuk menjaga target kepuasan konsumen terhadap suatu produk, dalam proses produksi diperlukan teknik dan manajemen untuk memantau hasil produksi. Grafik pengendali merupakan salah satu alat yang digunakan untuk mendeteksi perilaku menyimpang dalam proses produksi. Grafik Pengendali Rata-rata Bergerak Tertimbang Eksponensial adalah salah satu teknik pengendalian proses statistik yang sering digunakan untuk mendeteksi pergeseran kecil dalam proses mean dan variansi, dengan asumsi dasar distribusi dari karakteristik kualitas adalah normal.Untuk proses non-normal, pendekatanrobust dapat diimplementasikan karena sifatnya yang kurang sensitive terhadap asumsi normalitas. Metode grafik yang akan di bahas kali ini adalah grafik pengendali rata-rata bergerak tertimbang eksponensial dengan batas pengendali menggunakan estimator robust Qn dan MAD.","To maintain the customer satisfaction of a product, in the process of production, technical and management are needed to monitor outputâ€™s production. The control chart is one of the tools used for detect deviant behavior in the production process. The Exponentially Weighted Moving Average (EWMA) chart is very popular in statistical process controlfor detecting the small shifts in process mean and variance. This chart performs well under the assumptionof normality but when data violate the assumption of normality, the robust approaches are needed.Methods that will be discussed is EWMA control chart with control limits using robust estimator Qn and MAD","Kata Kunci : grafik pengendali, exponentially weighted moving average, EWMA, robust, estimator, Qn, MAD, Interval Width, Expected Points Out"
http://etd.repository.ugm.ac.id/home/detail_pencarian/180964,Metode Robust MM Untuk Data Pencilan Dalam Model Seemingly Unrelated Regression,"ANNISA MUFIDAH, Prof. Drs. Subanar, Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Model Seemingly Unrelated Regression (SUR) merupakan model regresi yang terdiri dari beberapa sistem persamaan regresi dimana terdapat korelasi error antar persamaan regresi tersebut. Masing-masing persamaan regresi dapat diestimasi dengan metode Ordinary Least Square (OLS), tetapi metode ini tidak efisien karena membuang informasi bahwa error antar persamaan saling berkorelasi. Salah satu metode estimasi parameter yang tepat untuk model SUR adalah metode Generalized Least Square (GLS). Namun, metode ini kurang mampu bertahan terhadap kehadiran outlier di dalam data pengamatan. Estimator MM adalah salah satu metode estimasi robust. Metode ini memiliki kemampuan bertahan terhadap kehadiran outlier. Estimator MM adalah estimator invariant dari regresi dan dapat mencapai breakdown point setinggi 50% dengan tingkat efisiensi hampir 90%, yang berarti dapat menangani hampir separuh dari observasi buruk dan memberikan pengaruh yang baik. Uji Lagrange Multiplier digunakan untuk menguji adanya korelasi dari error antar persamaan. Dalam penulisan skripsi ini dibahas mengenai contoh penerapan model SUR metode GLS dan metode robust MM pada studi kasus mengenai faktor-faktor yang mempengaruhi Penanaman Modal Asing pada Negara Indonesia dan China.","The Seemingly Unrelated Regression (SUR) model is a regression model that consists of several systems of regresson equations where errors between equations are contemporaneously correlated. Each regression equation can be estimated by Ordinary Least Square (OLS) method, but this method is inefficient because it discards information that the errors between the equations are correlated. One of the appropriate parameter estimation methods for the SUR model is Generalized Least Square (GLS) method. However, this method is less able to withstand the presence of outliers in the observation data. MM estimator is one of the robust estimation methods. This method has the ability to withstand the presence of outliers. MM estimators are invariant estimators of regression and can reach breakdown points as high as 50% with an efficiency level of almost 90%, which means they can handle almost half of bad observations and have good leverages. Lagrange Multiplier test is used to test the correlation of errors between the equations. In writing this paper, we discuss the examples of the application of the SUR model using GLS method and robust MM method in a case study of the factors that influence Foreign Direct Investment (FDI) in Indonesia and China.","Kata Kunci : seemingly unrelated regression, korelasi contemporaneous, generalized least square, robust MM, uji lagrange multiplier."
http://etd.repository.ugm.ac.id/home/detail_pencarian/183781,DAERAH KEPERCAYAAN UNTUK LINTASAN RIDGE RESPON GANDA,"INAYATUL KHOIRIYAH, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Analisis ridge mempunyai peranan penting dalam metode permukaan respon, yang mana dapat mengeksplorasi kondisi operasi optimal dari faktor-faktor percobaan. Model seemingly unrelated regression (SUR) diaplikasikan untuk mendapatkan model permukaan respon ganda. Model SUR dapat menentukan model sesuai untuk tiap respon sebagai suatu sistem persamaan yang mempunyai korelasi satu sama lain. Respon-respon hasil estimasi ditrasnformasi melalui fungsi desirability yang kemudian dioptimasi dengan kendala-kendala tertentu. Lintasan ridge dihasilkan dari plot antara desirability maksimum dan radius, yaitu nilai maksimum overall desirability dan jarak faktor-faktor optimal dari pusat daerah percobaan. Plot berupa lintasan ridge tersebut beserta interval kepercayaan disebut sebagai daerah kepercayaan untuk lintasan ridge repon ganda.","Ridge analysis has an important role in the response surface methodology, which can explore the optimal operating conditions of the experimental factors. The seemingly unrelated regression (SUR) model was applied to fit a multi-response surface model. The SUR model could fit the model of each response as a system of equations that have correlation with each other. The estimated responses are transformed via desirability function, then optimized with certain constraints. The ridge path is defined as a plot between the maximum desirability and radius, that are the maximum value of overall desirability and the distance of the optimal factors from the center of the experiment region. The plot of the ridge path along with the confidence interval is called as the confidence region for a multi-response ridge path.","Kata Kunci : multi-response surface methodology, seemingly unrelated regression, desirability function, generalized reducent gradient."
http://etd.repository.ugm.ac.id/home/detail_pencarian/180970,OPTIMISASI PORTOFOLIO SAHAM INDEKS LQ-45 MENGGUNAKAN ALGORITMA GENETIKA BERDASARKAN KLASTER K-MEANS,"Theresia Gabriela Mosey, Dr. Gunardi, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Investasi merupakan suatu kegiatan yang akhir-akhir ini sangat diminati oleh banyak orang. Aset yang dapat diinvestasikan ada beragam, salah satunya ialah aset finansial berupa saham. Kegiatan investasi saham selalu berkaitan dengan keuntungan dan risiko. Untuk meminimalkan risiko dalam investasi saham dapat dengan membentuk portofolio. Portofolio yaitu gabungan dari beberapa aset atau saham, portofolio saham yang terbentuk harus dioptimisasi agar menghasilkan portofolio yang efisien dan optimal. Berbagai macam metode dan penelitian telah dilakukan mengenai optimisasi portofolio. Salah satunya yang populer adalah optimisasi portofolio mean variance. Tetapi dalam beberapa kasus, metode tersebut kurang baik digunakan karena dapat menghasilkan bobot yang bernilai negatif dan nilai returnnya harus berdistribusi normal. Sehingga banyak terdapat pengembangan yang telah dilakukan guna memberikan portofolio yang lebih optimal salah satunya yaitu dengan menggabungkan analisis klaster dan metode mean variance. Sebelumnya telah dilakukan penelitian tentang optimisasi portofolio menggunakan metode mean variance dengan analisis klaster non-hierarki, Kmeans. Pada skripsi ini akan dibahas mengenai optimisasi portofolio menggunakan algoritma genetika berdasarkan klaster non-hirarki, K-means. Metode algoritma genetika tidak memiliki asumsi yang harus dipenuhi, tetapi dikarena nantinya pada studi kasus akan dibandingkan dengan optimisasi portofolio mean variance yang memerlukan asumsi normalitas return, maka perlu dilakukan uji asumsi normalitas terlebih dahulu. Studi kasus pada skripsi ini menggunakan data closing price bulanan beberapa periode dari 44 saham indeks LQ-45. Nilai return yang diamati dari 44 saham dibentuk portofolio menggunakan algoritma genetika berdasarkan klaster K-means. Kinerja portofolio metode tersebut kemudian dibandingkan dengan kinerja portofolio mean variance biasa menggunakan Sharpe ratio. Kesimpulan yang diperoleh bahwa optimisasi portofolio menggunakan algoritma genetika berdasarkan K-means klaster lebih baik daripada metode mean variance biasa.","Investment is an activity which lately is very popular with many people. Assets that can be invested are varied, one of them is financial assets, for example stocks. Stock investment activities are always related to profits (returns) and risks. To minimize the risk in stock investment can form a portfolio. A portfolio is a combination of several assets or stocks, a stock portfolio that is formed must be optimized to produce an efficient and optimal portofolio. Various methods and research have been conducted on portofolio optimization. One of the most popular is mean variance portofolio optimization. But in some cases, this method is not well used because it can produce negative values and the return value must be distributed normally. So that, there is a lot of development that has been done to provide a more optimal portfolio, one of them is by combining cluster analysis and the mean variance method. Previous research on portfolio optimization has been done using the mean variance method with non-hierarchical cluster analysis, K-means. In this undergraduate thesis, we will discuss about portfolio optimization using genetic algorithm based on non-hieararchical clusters, K-means. Genetic algorithm method does not have assumptions that must be fulfilled, but because later in the case study it will be compared with mean variance method which requires the assumption of normality of return, it is necessary to test the assumption of normality first. The case study in this undergraduate thesis uses monthly stock closing price data for the various period from 44 stocks of the LQ-45 index. The observed return values of 44 stocks formed a portfolio using genetic algorithms based on the K-means cluster. The portfolio performance of the method is then compared with the mean variance portfolio performance using Sharpe ratio. The conclusion is that portfolio optimization using genetic algorithm based on K-means cluster is better that the usual means variance method.","Kata Kunci : algoritma genetika, mean variance, analisis klaster, K-means, portofolio."
http://etd.repository.ugm.ac.id/home/detail_pencarian/167915,WELCH ANOVA & UJI GAMES-HOWELL SEBAGAI ALTERNATIF KASUS HETEROGENITAS VARIANS PADA ANOVA,"HILMAN ADI PRASETYA, Herni Utami, Dr., M.Si..",2019 | Skripsi | S1 STATISTIKA,"Analisis variansi (ANOVA) merupakan analisis yang digunakan untuk mengetahui ada tidaknya perbedaan rata rata yang signifikan antar lenih dari 2 level faktor, yang kemudian dilanjutkan dengan uji perbandingan ganda untuk melihat urutan rata rata antar level faktor tersebut. ANOVA ini baik digunakan apabila asumsi klasiknya terpenuhi, salah satunya adalah asumsi kesamaan variansi. Apabila asumsi dalam ANOVA tidak terpenuhi, maka dapat menyebabkan kesalahan pengambilan keputusan, oleh karena itu perlu dilakukan transformasi data. Transformasi data dilakukan cukup sekali agar tidak mengubah nilai keaslian dari data tersebut. Transformasi data tidak menjamin membuat data menjadi langsung memenuhi asumsi klasik pada ANOVA.  Untuk mengatasi masalah heterogenitas variansi ini, digunakan metode Welch ANOVA sebagai alternatif untuk uji ANOVA tradisional dengan cara melakukan pembobotan banyaknya data terhadap variansi setiap level faktor. Analisis perbandingan ganda juga dapat dilakukan dengan Games-Howell test sebagai alternatif uji Tukey-Kramer yang berbeda cara penentuan nilai standard error (SE) dan nilai degree of freedom (df).","Analysis of variance (ANOVA) is a statistical technique that assesses potential differences in a scale-level dependent variable by a nominal-level variable having 2 or more categories, which is then followed by a multiple comparison analysis to see the order of mean between these level-factors. ANOVA can be used when the classical assumptions are met, one of them is the homogeneity of variance. If the assumptions in ANOVA are not fulfilled, it can lead to decision making errors, therefore data transformation needs to be done. But data transformation only can be used once because it can change the authenticity of the data. Data transformation does not always make the data meet the classic assumptions in ANOVA. To overcome this heterogeneity in variance problem, Welch ANOVA method is an alternative to traditional ANOVA test by weighting the amount of data on the variance of each level factors to reduce the effect of heterogeneity. Multiple comparison analysis can also be done with Games-Howell test as an alternative of Tukey-Kramer test, that the difference is in determining the standard error (SE) value and the degree of freedom (df).","Kata Kunci : Analisis Variansi, Welch Anova, Games-Howell test"
http://etd.repository.ugm.ac.id/home/detail_pencarian/171756,ANALISIS VALUASI OBLIGASI DATA RETURN NON-NORMAL,"EKO YULI KURNIAWAN, Dr. Abdurakhman, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Obligasi adalah suatu surat berharga yang dikeluarkan oleh penerbit obligasi kepada investor, dimana nantinya penerbit obligasi akan memberikan suatu imbal hasil dalam bentuk kupon serta nilai pokok ketika obligasi yang diterbitkan telah jatuh tempo. Namun, berinvestasi dalam obligasi juga memiliki risiko yang harus diperhatikan, dimana salah satunya adalah risiko kredit. Risiko kredit adalah kondisi ketika perusahaan tidak dapat memenuhi kewajiban pembayaran hutangnya pada saat jatuh tempo sehingga perusahaan dapat dikatakan mengalami kebangkrutan (default). Salah satu cara untuk mengantisipasi risiko kredit adalah dengan melakukan suatu valuasi obligasi untuk melihat kemampuan perusahaan penerbit obligasi dalam memenuhi kewajiban pembayaran hutangnya. Selain itu, dengan valuasi obligasi juga dapat menghitung peluang kebangkrutan dari perusahaan tersebut. Pemodelan valuasi obligasi untuk pertama kalinya dikembangkan oleh Merton (1974) pada kondisi obligasi tanpa kupon dan dengan asumsi return aset mengikuti distribusi normal. Namun pada kenyataannya, banyak return aset dari suatu obligasi tidak mengikuti distribusi normal dan merupakan obligasi dengan kupon. Oleh karena itu, dalam skripsi ini akan digunakan metode Transformasi -Fast-Fourier untuk mengantisipasi kondisi tersebut dengan menggunakan fungsi karakteristik Normal Inverse Gaussian karena dianggap mampu untuk menangkap perilaku return aset. Kemudian, dilakukan perbandingan valuasi obligasi dengan metode Transformasi Fast-Fourier dengan model Merton, hasilnya menunjukkan bahwa metode Transformasi Fast-Fourier lebih tepat digunakan untuk kondisi return aset yang tidak mengikuti distribusi normal.","Bonds are securities that is issued by bond issuer to investor, where the issuer is required to pay a fix sum annually called coupon and the principal value until the maturity of the bonds. However, investing in bond instruments also has risks that should be noted, one of them is credit risk. Credit risk is a condition when the company is unable to pay its obligation during the maturity date so it is declared bankrupt (default). One of the way to anticipate the credit risk is by doing the bond valuation to see the ability of the issuer to fulfill its obligations.  On the other hand, bond valuation also can calculate the default probability for the company as the bond issuer. Bond Valuation Model for the first time was developed by Merton (1974) on the condition of free-coupon bond and assuming that asset returns has the normal distribution. By the fact, some of the asset return of the bond are a non-normally distributed and has a coupon. Therefore, in this paper adapts Fast-Fourier Transformations method to anticipate this condition and using the Normal Inverse Gaussian as the characteristics function because this characteristics function has the advantage to capture the asset returns behaviour. Afterwards, the bond valuation comparation for the Fast-Fourier Transform method and Merton model shows that Fast-Fourier Transform method is more precise to calculate the bond valuation for the non-normally distributed asset return condition.","Kata Kunci : Valuasi Obligasi, Transformasi Fast-Fourier, Fungsi Karakteristik Normal Inverse Gaussian"
http://etd.repository.ugm.ac.id/home/detail_pencarian/168173,GRAFIK PENGENDALI NONPARAMETRIK CHANGE POINT BERDASARKAN UJI MANN-WHITNEY,"MENGHAYATI NAKHE, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2019 | Skripsi | S1 STATISTIKA,"Kualitas merupakan faktor penting yang mempengaruhi seseorang untuk menggunakan suatu produk. Dalam proses produksi akan ada gangguan yang mempengaruhi kualitas yang kemudian menyebabkan pergeseran rata-rata proses. Pergeseran rata-rata dapat dideteksi dengan Grafik Pengendali. Grafik pengendali yang dapat mendeteksi pergeseran rata-rata proses yang kecil salah satunya adalah grafik pengendali change point parametrik untuk data berdistribusi normal. Namun pada kenyataanya tidak semua data yang ditemukan di lapangan dapat memenuhi asumsi normal. Oleh karena itu salah satu grafik pengendali alternatif yang dapat digunakan adalah Grafik Pengendali Nonparametrik Change Point.  Grafik Pengendali Nonparametrik Change Point adalah grafik yang dapat mendeteksi pergeseran rata-rata proses yang kecil pada data yang berdistribusi tidak normal. Mendeteksi ada tidaknya change point diuji dengan menggunakan uji Mann-Whitney. Berdasarkan studi kasus, performa grafik pengendali Nonparametrik change point dengan uji Mann-Whitney menghasilkan skema terbaik pada saat alarm rate 0,02 atau ARL0 50","Quality is an important factor that influences people to use a product. In the production process there will be a disruptionthat affects the quality then cause the process mean shift. The process mean shift can detected by the control chart. The control charts that can detect small process mean shifts in normally distributed data are the Parametric change point control chart. In fact, not all data is normally distributed. Therefore, one of the control charts that can be used is the Nonparametric Change Point Control Chart. Nonparametric Change Point Control Chart is a graph that can detect small shifts in process averages on data that are not normally distributed. Detecting the presence or absence of change points was tested using the Mann-Whitney test. Based on the case study, the performance of the Nonparametric change point control chart with the Mann-Whitney test produced the best scheme when alarm rate of 0,02 or ARL0 50","Kata Kunci : grafik pengendali, nonparametrik, change point, ARL, Uji Mann-Whitney"
http://etd.repository.ugm.ac.id/home/detail_pencarian/177136,GRAFIK PENGENDALI INDIVIDUAL BERBASIS DISTRIBUSI BURR 3-PARAMETER TIPE XII,"IMANDINI PURBOWATI, Dr. Adhitya Ronnie E., M.Sc.",2019 | Skripsi | S1 STATISTIKA,"Grafik pengendali individual dapat digunakan  sebagai alat pengendalian kualitas suatu produk secara statistik apabila nilai sample tunggal. Grafik pengendali ini membutuhkan dua asumsi klasik dimana data harus berdistribusi Normal dan sampel yang diambil berasal dari populasi acak. Namun sering dijumpai data yang tidak berdistribusi Normal, sehingga perlu adanya grafik pengendali non parametrik berdasarkan fungsi distribusi empirik yang dapat digunakan untuk mempresentasikan data tersebut. 	Penelitian ini akan mengkaji tentang grafik pengendali individual untuk data berdistribusi Burr 3 Parameter Tipe XII. Batas-batas pengendali dapat diperoleh dengan bantuan nilai estimator dari masing-masing parameter distribusi Burr. Sampel data yang digunakan adalah data kadar fosfat pada nira perahan pertama produksi PG Krebet Baru II Bululawang Malang pada tahun 2010. Berdasarkan hasil studi kasus, grafik pengendali individual berbasis distribusi Burr 3-P Tipe XII memiliki tingkat kesensitifan yang lebih baik dibandingkan grafik pengendali yang dibuat dengan mengasumsikan data berdistribusi Normal.","Individual control chart can be used as statistical process control tool using a single sample. This control chart requires two classical assumptions in which data should be Normally distributed and samples drawn from random populations. However, not all the obtained data have Normal distribution, so there is a need for Non-parametric control charts based on empirical distribution function. 	This thesis will examine the individual control charts for the Burr 3 Parameter Type XII distributed data. Control limits can be obtained using 3 parameters of the Burr distribution. Data used on the study case are phosphate concentration data on the first pulp production of PG Krebet Baru II Bululawang Malang in 2010. Based on the study case, individual control chart based on the Burr 3-P Type XII distribution has better sensitivity level than individual control chart based on assuming Normal distributed data.","Kata Kunci : individual control chart, Burr 3-P Type XII"
http://etd.repository.ugm.ac.id/home/detail_pencarian/180987,ANALISIS KLASTER MENGGUNAKAN ALGORITMA CHAMELEON,"Fasya Nabila Ramadhani, Prof. Drs. Subanar, Ph.D.",2019 | Skripsi | S1 STATISTIKA,"Analisis klaster metode hierarki memulai pengelompokan dengan dua atau lebih objek yang mempunyai kesamaan paling dekat hingga klaster akan membentuk semacam pohon dimana terdapat tingkatan (hierarki) yang jelas antar objek, dari yang paling mirip hingga yang paling tidak mirip. Namun, metode hierarki sebelumnya seperti CURE dan ROCK memiliki kekurangan utama dalam hal penggabungan pasangan klaster yaitu hanya mempertimbangkan salah satu di antara informasi interconnectivity atau closeness, Oleh karena itu, dalam penelitian ini akan dibahas mengenai algoritma CHAMELEON yaitu metode analisis klaster hierarki berdasarkan model dinamis yang menggunakan kedua informasi tersebut. 	Algoritma CHAMELEON menggunakan pendekatan algoritma k-nearest neighbor graph yang kemudian graf dipartisi menggunakan library METIS untuk mendapatkan initial sub-cluster. Selanjutnya digunakan algoritma analisis klaster hierarki agglomerative untuk menemukan klaster akhir dengan berulang kali menggabungkankan nilai relative interconnectivity dan relative closeness dari klaster-klaster yang sebelumnya terbentuk. Untuk mengetahui kualitas dan validitas dari klaster, digunakan validasi internal silhouette width.","Hierarchical clustering starts grouping with two or more objects that have the closest similarity so that the cluster will form a tree where there are clear levels (hierarchy) between objects, from the most similar to the least similar. But the existing hierarchical clustering algorithms like CURE and ROCK have a major disadvantage when it comes to combining cluster pairs, they only consider one of the interconnectivity or closeness information. Therefore, in this work we will discuss the CHAMELEON algorithm which is a hierarchical clustering using dynamic modeling that uses both information. CHAMELEON algorithm uses the k-nearest neighbor graph algorithm approach which is then partitioned using the METIS library to get the initial sub-cluster. Then the agglomerative hierarchical clustering algorithm is used to find the final cluster by repeatedly combining the value of relative interconnectivity and relative closeness of the clusters that were previously formed. To determine the quality and validity of the cluster, internal validation silhouette width was used.","Kata Kunci : analisis klaster, klaster hierarki, model dinamis, algoritma chameleon, k-nearest neighbor graph, METIS, interconnectivity,  closeness , silhouette width"
http://etd.repository.ugm.ac.id/home/detail_pencarian/168188,Penentuan Harga Opsi Beli Tipe Eropa Menggunakan Model Transformasi Fast-Fourier,"NOVIKA PURBA, Dr. Herni Utami, M.Si.",2019 | Skripsi | S1 STATISTIKA,"Produk derivatif yang laris diperdagangkan saat ini ialah opsi. Investasi dalam bentuk opsi memberikan fungsi lindung nilai (hedging) terhadap aset induk (saham), sehinggga memberi peluang investor untuk mendapatkan keuntungan yang tinggi dengan risiko yang rendah. Pada 1973 diperkenalkannya model penentuan harga opsi Black-Scholes yang mengasumsikan bahwa pergerakan return saham berdistribusi normal dan volatilitas konstan. Namun, pada perkembangan dalam dunia keuangan terjadi perubahan pergerakan harga saham secara fluktuasi sehinggga model harga saham tidak selalu mengikuti distribusi normal.  Selain itu, terdapat return saham yang tidak berdistribusi normal yang mengimplikasikan perlunya mempertimbangkan skewness dan kurtosis pada aset saham yang mendasari. Model Transformasi Fast-Fourier dikembangkan sebagai solusi dari masalah tersebut.  Model Transformasi Fast-Fourier merupakan teknik transformasi Fourier dengan akurasi yang tinggi dan lebih efektif karena menggunakan fungsi karakteristik. Fungsi karakteristik yang digunakan ialah Normal Inverse Gaussian, yang mana memiliki keunggulan yakni mampu menangkap perilaku logreturn dari suatu harga saham dan juga dianggap mampu dengan baik menangani pergerakan fluktuasi.","The best-selling derrivative financial instrument currently traded is an option. Options provide hedging of underlying asset and gives investors the opportunity to earn high profits with low risk. In 1973 Black-Scholes developed an option pricing model  which impractical assumptions that constant volatility of stock return and normal distributin of return. However, in developments in the financial world there has been a fluctuating change in stock price prices so that the stock price model does not always follow a normal distribution. In addition, there are stock returns that are not normally distributed which implies the need to consider skewness and kurtosis on the underlying stock assets. The Fast-Fourier Transformation Model was developed as a solution solving. The Fast-Fourier Transformation Model is a Fourier transformation technique with high accuracy and more effective because it uses characteristic functions. The characteristic function used is Normal Inverse Gaussian, which has the advantage of being able to capture the logreturn behavior of a stock price and is also considered capable of handling fluctuating movements well.","Kata Kunci : Harga Opsi, Black-Scholes, Transformasi Fast-Fourier, Normal Inverse Gaussian"
http://etd.repository.ugm.ac.id/home/detail_pencarian/180988,ROBUST LIU ESTIMATOR MENGGUNAKAN ESTIMATOR LEAST MEDIAN SQUARE (LMS) DENGAN PENGARUH MULTIKOLINEARITAS DAN PENCILAN,"AFIFAH ARNINGTYAS, Prof. Dr. Sri Haryatmi, M.Sc.",2019 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan analisis yang digunakan untuk mengetahui hubungan antara variabel dependen dan independen. Analisis regresi dapat memodelkan dan menyelidiki pola hubungan dengan cara memprediksi nilai variabel dependen melalui variabel independennya. Metode yang sering digunakan adalah metode  Ordinary Least Square (OLS). Namun metode tersebut memperlukan beberapa asumsi yang harus dipenuhi, seperti tidak ada multikolinearitas dan pencilan. Pada skripsi ini akan dibahas mengenai metode estimasi parameter pada regresi linear menggunakan metode robust Liu estimator menggunakan estimator Least Median Square (LMS). Metode ini merupakan gabungan dari metode Liu estimator dan estimator LMS yang merupakan estimator pada regresi robust. Metode tersebut dapat mengatasi masalah multikolinearitas dan pencilan pada data secara bersamaan. Setelah dilakukan pengujian terhadap Angka Kematian Bayi (AKB) di Jawa Timur pada Tahun 2009 dan faktor-faktor yang mempengaruhinya, hasilnya menunjukkan metode robust Liu estimator menggunakan estimator LMS lebih tepat digunakan untuk data yang memiliki multikolinearitas dan pencilan yang dibandingkan dengan metode OLS.","Regression analysis is an analysis that used to find out about the relationship between the independent variable and the dependent variable. Regression analysis can issue the model and find out the pattern of the relationship by predicting the dependent variable through their independent variable. The method is usually used is Ordinary Least Square method (OLS). However, that method needs some assumptions that have to be fulfilled, like there is no multicollinearity and outliers on data. This paper will be discussed about the estimation parameter method for linear regression by robust Liu estimator using least median square (LMS). This method is a combination of Liu estimator method and LMS estimator, that is a robust regression estimator. This method can overcome multicollinearity and outliers on data simultaneously. After testing the infant mortality in Jawa Timur 2009 and their effect factors, the results show that robust Liu estimator using LMS is a more appropriate method for data that have multicollinearity and outliers comparing with OLS method.","Kata Kunci : analisis regresi, multikolinearitas, pencilan, regresi robust, estimator Liu, estimator LMS, robust Liu estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/156416,Valuasi Obligasi Berbasis Ekspansi Gram-Charlier,"TRI MULYANINGSIH, Dr. Abdurakhman, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Investasi pada hakikatnya adalah penempatan sejumlah dana pada saat ini dengan harapan mendapatkan keuntungan dimasa yang akan datang. Obligasi merupakan salah satu instrumen investasi yang cukup diminati oleh investor karena memberikan pendapatan tetap (fixed income). Disisi lain, berinvestasi pada instrumen obligasi juga memiliki risiko salah satunya adalah risiko kredit. Risiko kredit terjadi apabila pada saat jatuh tempo perusahaan penerbit obligasi tidak mampu membayarkan kewajibannya sehingga dinyatakan bangkrut (default). Untuk mengantisipasi risiko tersebut diperlukan suatu valuasi obligasi guna melihat kemampuan perusahaan penerbit obligasi dalam memenuhi kewajibannya. Valuasi obligasi juga meliputi perhitungan peluang kebangkrutan perusahaan penerbit obligasi. Merton (1974) memodelkan valuasi obligasi tanpa kupon yang mengasumsikan bahwa return aset mengikuti distribusi normal. Fakta bahwa terdapat return aset yang tidak berdistribusi normal mengimplikasikan bahwa model valuasi obligasi harus mempertimbangkan skewness dan kurtosis. Model valuasi obligasi yang dikembangkan dalam skripsi ini mengadaptasi ekspansi Gram-Charlier. Kemudian dilakukan perbandingan valuasi obligasi dengan ekspansi Gram-Charlier dan model Merton, hasilnya menunjukkan bahwa model dengan ekspansi Gram-Charlier lebih konsisten untuk diterapkan dalam valuasi obligasi dibandingkan dengan model Merton.","Investment is essentially the placement of the current amount of funds in the hope of making a profit in the future. Bonds are one of the investment instruments that investors are interested in because they provide fixed income. On the other hand, investing in bond instruments also has one of the risks being credit risk. Credit risk occurs when at maturity the bond issuing company is unable to pay its obligations so it is declared bankrupt (default). To anticipate those risks, a bond valuation is needed to see the ability of the issuer to fulfill its obligations. Bond valuations also include the calculation of bankruptcy opportunities for bond issuers. Merton (1974) modeled the asset-free bonds assuming that asset returns follow a normal distribution. The fact that there is a non-distributed asset return implies that the bond valuation model should consider skewness and kurtosis. The bond valuation model developed in this paper adapts the Gram-Charlier expansion. Then the comparison of bond valuation with Gram-Charlier expansion and Merton model shows that the model with Gram-Charlier expansion is more consistent to be applied in bond valuation compared with Merton model.","Kata Kunci : valuasi obligasi, ekpansi Gram-Charlier, peluang kebangkrutan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132610,ESTIMASI VALUE AT RISK (VaR) PORTOFOLIO BERDASARKAN MODEL TERBAIK DEKOMPOSISI VINE COPULA,"I. A. A SURI ASMANI, Prof.Dr.rer.nat.Dedi Rosadi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Value at Risk (VaR) merupakan salah satu cara untuk menganalisis atau mengukur risiko. Salah satu metode perhitungan VaR yang sering digunakan yaitu metode Variansi-Kovariansi. Metode ini menghampiri fungsi distribusi portofolio dengan distribusi normal multivariat dan mengukur kebergantungannya dengan korelasi linear. Pada kenyataannya, data return finansial seringkali tidak mengikuti sifat distribusi normal. Kebergantungan antar saham yang non-linear juga tidak sesuai jika diukur dengan korelasi linear, sehingga estimasi VaR dengan metode Variansi-Kovariansi tidak akurat lagi. Copula adalah suatu fungsi yang menggabungkan beberapa distribusi marginal serta dapat mempelajari kebergantungan tak linier antar kejadian dalam kasus multivariat. Konstruksi dari fungsi distribusi multi-dimensi menjadi semakin rumit ketika dimensinya semakin bertambah, sehingga digunakan Vine Copula. Studi kasus skripsi ini menggunakan portofolio dari indeks saham HSI, N225, JKSE, dan PSEI yang dimodelkan dengan CD-Vine Copula dari keluarga Gaussian serta keluarga Frank. Dalam mengestimasi parameter copula, digunakan metode Inference Function for Margin (IFM) berbasis maksimum likelihood. Skripsi ini, menekankan pada pemilihan model terbaik berdasarkan dekomposisi Vine Copula. Dekomposisi terbaik yang memodelkan data dalam skripsi ini berasal dari struktur D-Vine keluarga Gaussian. Hasil backtesting menunjukkan bahwa estimasi VaR dengan menggunakan model terbaik dari dekomposisi Vine Copula secara umum baik untuk digunakan.","Value at Risk (VaR) is one way to analyze or measure risk. One method of VaR calculation which often used is the method of Variance-Covariance. This method approaches the portfolio distribution function with a multivariate normal distribution and measures its dependence with linear correlation. In fact, financial  data return often do not follow the characteristic of normal distribution. Non-linear interdependence of stocks is also unsuitable if measured by linear correlation, so the VaR estimation by the Variance-Covariance method is not accurate anymore. Copula is a function that combines several marginal distributions and study about non-linear dependence between events in multivariate cases. The construction of the multi-dimensional distribution function becomes more complicated when the dimension increase, so that Vine Copula is used. This paper uses a portfolio of HSI, N225, JKSE, and PSEI stock indexes modeled on CD-Vine Copula from the Gaussian family and the Frank family. A maximum likelihood-based IFM method is used to estimating the copula parameter. This paper is to determine the best model based on Vine Copula decomposition. The best model of Vine Copula decomposition in this paper came from the D-Vine structure of Gaussian family. The bactesting results show that the VaR estimation by using the best decomposition of Vine Copula is generally good for use.","Kata Kunci : Value at Risk, Vine Copula, Dekomposisi, Gaussian, Frank, Backtesting."
http://etd.repository.ugm.ac.id/home/detail_pencarian/132611,ESTIMASI VALUE AT RISK (VaR) MENGGUNAKAN METODE WAVELET  BERDASARKAN GARCH STUDENT-T EVT,"FIAN SETIYANINGSIH, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Value at Risk (VaR) merupakan salah satu ukuran standar risiko finansial dalam manajemen risiko. Terdapat banyak metode untuk mengestimasi VaR yang berkembang di dunia finansial. Sebagian besar metode mengasumsikan bahwa data berdistribusi normal. Akan tetapi pada kenyataanya, data return finansial seringkali berekor gemuk dan skewed. Ekor distribusi yang gemuk menunjukkan bahwa terdapat pergerakan ekstrim dalam data sehingga diperlukan pemodelan menggunakan pendekatan Generalized Pareto Distribution (GPD). Karena GPD memerlukan asumsi data i.i.d, maka data return finansial yang umumnya mengandung unsur heteroskedastik dimodelkan terlebih dahulu menggunakan GARCH (Generalized Autoregressive Conditional Heteroskedasticity). Pada penulisan skripsi ini, model tersebut akan dikombinasikan dengan metode wavelet untuk mengetahui informasi yang relevan dalam mengestimasi VaR harian aset tunggal. Adapun saham yang digunakan pada studi kasus skripsi ini adalah saham KO, INTC, AXP dan TRV. Pertama, data log return didekomposisi menggunakan Haar MODWT (Maximal Overlap Discrete Wavelet Transformation). Kemudian, setiap skala hasil dekomposisi wavelet dimodelkan dengan GARCH dan GPD. Untuk menggabungkan skala waktu hasil dekomposisi wavelet, digunakan copula dari keluarga Gaussian dan Student-t. Berdasarkan backtesting diperoleh bahwa VaR harian berdasarkan tiga skala pertama menunjukkan performa yang baik. Dengan demikian, tiga skala pertama hasil dekomposisi merupakan informasi yang relevan untuk mengestimasi VaR harian.","Value at Risk (VaR) is a standard measurement of financial risk in risk management. There are many method for estimating VaR in the financial field. Most method assume that data must be normally distributed. Whereas, financial return series usually have fat tail and skewed shape. Fat tailed distribution indicates extreme movement in the data which is needed to be modeled with Generalized Pareto Distribution (GPD) approximation. Because GPD needs i.i.d assumption, financial return series whose heteroskedasticity property is modeled with GARCH (Generalized Autoregressive Conditional Heteroskedasticity). In this undergraduate thesis, they will be combined with wavelet method to get relevant information in estimating daily VaR. Case investigation in this undergraduate thesis use several stocks, they are KO, INTC, AXP and TRV.  First, return series is to be decomposed using Haar MODWT  (Maximal Overlap Discrete Wavelet Transformation). Then, every time scale is to be modeled with GARCH and GPD. Gaussian and Student-t copula is used to link time scale of wavelet decomposition. Based on backtesting, daily VaR based the first three scale decomposition has good performance. Thereby, the first three scale is relevant information to estimate daily VaR.","Kata Kunci : Value at Risk, Wavelet, GARCH, Generalized Pareto Distribution, Copula"
http://etd.repository.ugm.ac.id/home/detail_pencarian/167427,Analisis Klasterisasi K-means Untuk Optimisasi Portofolio,"Adinda Bidari Afifah, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Investasi adalah kegiatan membeli suatu aset yang kemudian akan dijual kembali di masa mendatang dengan nilai yang lebih tinggi bertujuan untuk memperoleh keuntungan dengan strategi tertentu. Obyek investasi dapat berupa aset finansial pada pasar modal misalnya saham. Dalam investasi saham, perlu adanya manajemen investasi yang baik agar tujuan investasi tercapai. Kegiatan investasi tidak dapat dihindarkan dari risiko investasi. Investor hanya dapat melakukan optimisasi portofolio untuk mengurangi tingkat risiko yang mungkin muncul. Optimisasi portofolio merupakan metode untuk membentuk portofolio (gabungan beberapa aset atau saham) secara efisien dan optimal. Berbagai penelitian tentang optimisasi portofolio telah banyak dilakukan. Salah satunya yaitu optimisasi portofolio dengan menggabungkan metode Mean Variance dengan analisis klaster. Sebelumnya telah dilakukan penelitiaan tentang optimisasi portofolio menggunakan metode Mean Variance dengan analisis klaster hierarki. 	Pada skripsi ini akan dibahas mengenai optimisasi portofolio menggunakan metode Mean Variance dengan analisis klaster non-hierarki, K-means. Asumsi yang harus dipenuhi sesuai dengan asumsi metode Mean Variance yakni asumsi normalitas return. Studi kasus penelitian ini menggunakan data closing price saham bulanan periode Agustus 2015 sampai Juli 2018 dari 8 saham indeks LQ-45. Nilai return yang diamati dari 8 saham antara lain ADRO, ASII, BBNI, BBRI, INDF, KLBF, PTBA dan UNTR kemudian dibentuk portofolio dengan menggunakan metode Mean Variance dengan analisis klasterisasi K-means. Kinerja portofolio metode tersebut kemudian dibandingkan dengan kinerja portofolio metode Mean Variance biasa menggunakan rasio Sharpe. Kesimpulan yang diperoleh bahwa optimisasi portofolio menggunakan metode Mean Variance dengan analisis klasterisasi K-means lebih baik daripada metode Mean Variance biasa.","Investment is the activity of buying an asset which will be resold in the future with a higher value aimed at gaining profit with certain strategies. The object of investment can be financial assets in the capital market for example stocks. In stock investment, it is necessary to have good investment management so that the investment goal is reached. Investment activities canâ€™t be avoided from investment risk. Invertors can only do portfolio optimization to reduce the level of risk that might arise. Portfolio optimization is a method to form a portofolio (a combination of several assets) efficiently and optimally. Many researches on portfolio optimization have been carried out. One of them is portfolio optimization by combining the Mean Variance method with cluster analysis. Previously, research on portfolio optimization has been carried out using the Mean Variance method with hierarchical cluster analysis. 	In this paper, we will discuss portfolio optimization using the Mean Variance method with non-hierarchical cluster analysis, K-means. The assumptions that must be fulfilled are in accordance with assumption of the Mean Variance method, that is the assumption of normality of return. This research case study uses monthly stock closing price data for the period of August 2015 to July 2018 from 8 stocks of the LQ-45 index. The observed return value of 8 stocks including ADRO, ASII, BBNI, BBRI, INDF, KLBF, PTBA and UNTR then formed a potfolio using the Mean Variance method with K-means clustering analysis. The portfolio performance of that method is compared with the portfolio performance of the ordinary Mean Variance method using the Sharpe ratio. The conclusion is that portfolio optimization using the Mean Variance method with K-means clustering analysis is better than the usual Mean Variance method.","Kata Kunci : portofolio, mean variance, analisis klaster, K-means, rasio Sharpe."
http://etd.repository.ugm.ac.id/home/detail_pencarian/162057,Analisis Komponen Utama Robust Sparse Dengan Pendekatan Projection-Pursuit Pada Data Berdimensi Tinggi,"DWI RESTI INDAH PUSPITAWATI ,  Dr. Herni Utami, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Pada era digital ini, fenomena big data mulai bermunculan. Hal yang paling menonjol dari sebuah big data yaitu dimensi datanya yang tinggi, dan karenanya perlu dilakukan reduksi dimensi. Principal Component Analysis merupakan salah satu metode reduksi dimensi yang didasarkan pada matriks kovarians yang tidak robust terhadap outlier. Oleh karena itu diperlukan Robust PCA pada data yang mengandung outlier. Salah satu metode Robust PCA yaitu dengan menggunakan pendekatan Projection-pursuit, yaitu mencari arah robust PCA yang mampu memaksimalkan projection index, dan dalam hal ini digunakan Qn kuadrat sebagai projection index. Telah diketahui bahwa Classical PCA dan Robust PCA merupakan kombinasi linear dari seluruh variabel. Hal ini akan menghambat proses interpretasi komponen utama, karena peneliti tidak dapat mengetahui variabel mana yang berperan penting dalam pembentukan komponen utama. Untuk itu, diperlukan Sparse PCA guna mempermudah proses interpretasi.  Pada skripsi ini akan dibahas metode pembentukan komponen utama Robust Sparse PCA dengan menggunakan pendekatan Projection-pursuit, yang merupakan kombinasi dari Robust PCA dan Sparse PCA. Pada skripsi ini akan dilihat performa dari metode Classical PCA, Robust PCA, Sparse PCA, dan Robust Sparse PCA dari sisi kemudahan dalam proses interpretasi komponen utama, persentase variabilitas data yang dapat dijelaskan oleh k komponen utama, dan kemampuan dalam mendeteksi serta membedakan jenis outlier.","In this digital era, big data phenomenon began to emerge. The most prominent thing of a big data is its high data dimension, and hence need a dimension reduction. Principal Component Analysis is one of the dimension reduction methods based on covariance matrices that are not robust to outliers. Therefore Robust PCA is required on data containing outliers. One of the Robust PCA methods is to use the Projection-pursuit approach, which is looking for a robust PCA direction that maximizes the projection index, and in this case squared Qn is used as the projection index. It is known that Classical PCA and Robust PCA are linear combinations of all variables. This will inhibit the process of interpretation of the PCs, because researchers can't know which variable plays an important role in the formation of the PCs. For that, Sparse PCA required to simplify the process of interpretation.  In this thesis will be discussed the Robust Sparse PCA using the Projection-pursuit approach, which is a combination of Robust PCA and Sparse PCA. In this paper we will see the performance of Classical PCA, Robust PCA, Sparse PCA, and Robust Sparse PCA methods in terms of convenience in the process of interpretation of the PCs, the percentage of data variability that can be explained by the k PCs, and the ability to detect and differentiate the types of outliers.","Kata Kunci : reduksi dimensi, outlier, Qn kuadrat, projection-pursuit, Robust PCA, Robust Sparse PCA"
http://etd.repository.ugm.ac.id/home/detail_pencarian/162572,SELEKSI VARIABEL REGRESI MENGGUNAKAN BOOTSTRAP LASSO,"ILLUMINATA WYNNIE, Prof. Drs. Subanar, Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Regresi logistik digunakan untuk memprediksi variabel respon yang biner dengan satu set variabel prediktor. Dalam regresi logistik, asumsi yang harus dipenuhi adalah tidak ada multikolinearitas dan jika tidak terpenuhi maka estimasi parameternya akan jauh dari nilai seharusnya dan membuat model regresi yang diperoleh menjadi tidak efisien lagi karena nilai standar error koefisien regresi menjadi sangat besar (overestimate).  Metode Least Absolute Shrinkage and Selection Operator (LASSO) dikenal sebagai metode yang digunakan untuk mengatasi data yang memiliki multikolinearitas. LASSO akan menyusutkan koefisien (parameter  ) yang berkorelasi, menjadi nol atau mendekati nol, dan sekaligus melakukan seleksi variabel regresi. Namun, LASSO cenderung akan memilih variabel yang relevan dan tidak relevan. Pada skripsi ini, diusulkan metode alternatif yaitu Bootstrap LASSO untuk menyeleksi variabel regresi yang relevan sehingga dapat digunakan untuk mengatasi multikolinearitas. Pada akhirnya, performa model regresi logistik Bootstrap LASSO akan dibandingkan dengan model regresi logistik LASSO. Dengan melihat nilai proporsi konkordansi, metode Bootstrap LASSO dianggap lebih baik daripada regresi logistik LASSO karena memiliki nilai proporsi konkordansi yang lebih tinggi.","Logistic regression is used to predict binary response variables with a set of predictor variables. In logistic regression, the assumption that must be met is that there is no multicolinearity and if not met then the parameter estimation will be far from the value and make the regression model obtained becomes inefficient because the standard error value of the regression coefficient becomes overestimate.  The Least Absolute Shrinkage and Selection Operator (LASSO) method is known as the method used to overcome data that has multicolinearity. LASSO will shrink the correlated coefficients (parameters), to zero or near zero, and simultaneously perform the selection of regression variables. Therefore, LASSO will produce a more representative end model. In this thesis, it is proposed an alternative method of Bootstrap method applied to LASSO estimation which is equally used to overcome multicolinearity in regression variable selection. Ultimately, the performance of the LASSO Bootstrap logistic regression model will be compared with the LASSO logistic regression model. By looking at the concordance proportion value, the LASSO Bootstrap method is considered better than the LASSO logistic regression because it has a higher concordance proportion value.","Kata Kunci : Bootstrap LASSO, LASSO, multicollinearity, logistic regression, Coordinate Descent, Generalized Linear Model"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132621,METODE EGARCH-EVT-VINE COPULA UNTUK MENGESTIMASI VALUE AT RISK (VaR) PORTOFOLIO MULTIVARIAT,"RISQIA FADHILAH SYAHRIR, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Value at Risk (VaR) adalah salah satu ukuran standar risiko keuangan dalam manajemen risiko. Estimasi VaR didapatkan dengan metode statistik. Metode perhitungan konvensional VaR portofolio mengasumsikan bahwa data portofolio berdistribusi normal dan kebergantungannya adalah linear. Padahal pada praktik langsung, data return portofolio biasanya berekor gemuk, skewed, leptokurtic dan kebergantungannya tidak linear. Dengan demikian, metode konvensional tidak lagi akurat. Vine Copula adalah fungsi distribusi multivariat yang menggabungkan distribusi marginal return univariat dalam portofolio, sekaligus menggambarkan struktur kebergantungan non-linearnya. Ekor distribusi return yang gemuk menunjukkan adanya kejadian ekstrim dalam data yang perlu dimodelkan dengan Extreme Value Theory (EVT) dengan distribusi GPD atau GEV untuk menghindari kejadian underestimate pada risiko. EVT memerlukan asumsi data i.i.d, sehingga data return finansial yang umumnya mengandung unsur heteroskedastik akan dimodelkan terlebih dahulu dengan model GARCH (1,1) dengan inovasi Student-t. Namun, jika terdapat leverage effect yang umumnya terjadi pada data finansial maka akan digunakan pengembangan dari model GARCH(1,1) yakni EGARCH(1,1). Studi kasus skripsi ini menggunakan indeks saham HSI, JKSE, dan N225 yang tergabung dalam suatu portofolio yang dimodelkan dengan CD-Vine Copula dari kelas Eliptik dan Archimedean. Estimasi parameter copula menggunakan metode IFM berbasis maximum likelihood. Copula terbaik untuk memodelkan data dalam studi kasus ini adalah Gaussian D-Vine baik itu pada distribusi GPD maupun GEV. Hasil backtesting menunjukkan metode EGARCH-EVT-Vine Copula valid digunakan.","Value at Risk (VaR) is a standard measurement of financial risk in risk management. VaR can be computed with statistical method. Conventional method for calculating VaR assuming normally distributed returns and the dependence between stocks in portfolio are linear. In fact, returns are generally fat tailed, skewed, leptokurtic and the dependence between stocks are not linear. Thus, conventional method is no longer accurate. Vine Copula is a multivariate distribution function that combines distribution of univariate marginal returns in portfolio, as well as to describe the structure of non-linear dependence . Fat tail in return distribution shows extreme events in data which need to be modeled by Extreme Value Theory (EVT) with GPD or GEV distributions for avoid risk underestimation. EVT need i.i.d assumption in the data, so returns which generally heteroskedastic have to be modeled by GARCH (1,1) with Student-t distribution innovation. But,  if there is leverage effect in the data, we will use EGARCH(1,1) as the development from GARCH(1,1). The case study in this undergraduate thesis is using HSI, JKSE, and N225 indexes in a portfolio that are modeled with CD-Vine Copula of Elliptic and Archimedean classes. Copula parameters are estimated using IFM that based on maximum likelihood estimation. The best copula that fit the data in this case study is Gaussian D-Vine Copula from GPD distribution, also for GEV distribution. Backtesting result shows that EGARCH-EVT-Vine Copula method is valid for estimating VaR.","Kata Kunci : Value at Risk, GARCH, EGARCH, Generalized Pareto Distribution, Generalized Extreme Value,  Vine Copula"
http://etd.repository.ugm.ac.id/home/detail_pencarian/159758,ALGORITMA WEIGHTED ROBUST SPARSE K-MEANS (WRSK) UNTUK ALTERNATIF ANALISIS KLASTER ROBUST DAN SPARSE YANG EFISIEN PADA DATA BERDIMENSI TINGGI,"DITA DWI APRILLIANI AYU LESTARI, Prof. Drs. Subanar, Ph.D",2018 | Skripsi | S1 STATISTIKA,"Minat yang semakin tinggi pada analisis data multivariat didorong oleh kebutuhan penelitian saat ini, yang mengarah pada sekumpulan peristiwa kompleks atau data berdimensi tinggi. Salah satu metode analisis data multivariat yang cukup populer adalah K-Means. Namun ketika dihadapkan pada data berdimensi tinggi, metode K-Means menjadi kurang optimal, mengingat semakin banyak permasalahan data yang muncul, diantaranya keberadaan outliers dan variabel mubazir (noise). Metode K-Means standar bersifat tidak robust karena menggunakan mean sebagai ukuran pusat klaster. Di samping itu, metode K-Means standar melibatkan keseluruhan variabel dalam proses clustering, sehingga sangat rentan terhadap pengaruh variabel mubazir (noise) yang mampu menyamarkan struktur klaster sesungguhnya. Data real world juga sering kali tidak memberikan informasi awal (prior), sehingga sulit melakukan estimasi terhadap parameter yang dibutuhkan dalam analisis klaster.  Untuk itu, pada skripsi ini membahas algoritma Weighted Robust Sparse K-Means (WRSK) sebagai metode analisis klaster untuk data berdimensi tinggi yang mampu melakukan identifikasi klaster, mendeteksi outliers serta menemukan variabel informatif secara bersamaan. Algoritma WRSK memuat tiga tahapan secara iterasi, yakni inisialisasi pusat klaster melalui metode ROBIN (Robust Initialization), deteksi outliers dengan pendekatan bobot (weighted) berdasarkan LOF (Local Outliers Factor), serta seleksi variabel berbasis algoritma Sparse K-Means dengan kendala jumlahan nilai mutlak bobot w kurang dari sama dengan s dan jumlahan nilai mutlak kuadrat bobot w kurang dari sama dengan 1. Dalam hal ini, estimasi terhadap parameter pusat awal maupun outliers dilakukan secara otomatis, sehingga WRSK dapat menjadi alternatif yang efisien bagi RSK (Robust Sparse K-Means). Selain itu, berdasarkan studi kasus yang dilakukan, diperoleh bahwa algoritma WRSK lebih unggul dalam melakukan analisis klaster pada data berdimensi tinggi.","Increasing of interest in multivariate data analysis is driven by current research needs, leading to a set of complex events or high-dimensional data. One of the most popular methods of multivariate data analysis is K-Means. However, when high dimensional data, K-Means method becomes less optimal, facing the number of data problems, containing both outliers and variable redundant (noise). The standard K-Means method is not robust because it uses the mean as the seed of the cluster. In addition, the standard K-Means method involves the entire variables in the clustering process, so it is particularly susceptible to the effect of redundant variables that can easily mask a real cluster structures. Real world data also often does not provide preliminary information, which make it difficult to estimate the parameters required in cluster analysis. Therefore, this thesis discusses the Weighted Robust Sparse K-Means (WRSK) algorithm as a cluster analysis method for high-dimensional data that is able to identify clusters, detect outliers and find informative variables simultaneously. The WRSK algorithm contains three stages in iteration, i.e. cluster seed initialization through ROBIN (Robust Initialization) method, outliers detection with weighted approach based on LOF (Local Outlier Factor), and variable selection based on the Sparse K-Means with the constraint of the sum of the absolute value of w weights less than equal to s and the sum of the absolute value of the square of w weights less than equal to 1. In this case, estimation of the initial seed parameters and outliers are done automatically, so WRSK can be an efficient alternative to RSK (Robust Sparse K-Means). In addition, based on case studies, it was found that WRSK algorithm is superior in conducting cluster analysis on high-dimensional data.","Kata Kunci : K-Means, Robust Initialization, Local Outliers Factor, Seleksi Variabel, Weighted Robust Sparse K-Means, Data Berdimensi Tinggi/K-Means, Robust Initialization, Local Outliers Factor, Variable Selection, Weighted Robust Sparse K-Means, High Dimensional Dat"
http://etd.repository.ugm.ac.id/home/detail_pencarian/156689,PERHITUNGAN PREMI TOTAL DENGAN  FREKUENSI KLAIM SANTUNAN KEMATIAN BERDISTRIBUSI BINOMIAL NEGATIF -CRACK (NB-CR),"INKA APRILIA SAPUTRI, Dr. Adhitya Ronnie Effendie, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Tujuan dari skripsi ini adalah memperkenalkan distribusi alternatif yang dapat memodelkan data cacah yang overdispersi dan memiliki excess zeros, yaitu distribusi Binomial Negatif-Crack (NB-CR) yang terbentuk dari distribusi campuran Binomial Negatif dengan distribusi Crack. Distribusi ini digunakan untuk memodelkan frekuensi klaim dari santunan kematian. Frekuensi klaim yang dimodelkan dengan distribusi Negative Binomial-Crack dan ada pula besar klaim (severity) yang dimodelkan dengan distribusi Lognormal, menjadi kunci utama dalam perhitungan premi total pada sebuah perusahaan asuransi. Perhitungan premi yang tepat perlu dilakukan oleh perusahaan asuransi agar dapat memenuhi klaim yang diajukan pemegang polis. Prinsip perhitungan premi dapat dilakukan dengan menghitung ekspektasi risiko asuransi. Risiko asuransi yang dimaksud adalah klaim, meliputi frekuensi klaim dan besar klaim. Oleh karena itu, diperlukan perhitungan ekspektasi dari frekuensi klaim dan besar klaim untuk mendapatkan nilai premi total.","The objective of this thesis is to provide an alternative distribution for modelling overdispersed count data and excess zeros in the data, itâ€™s called Negative Binomial-Crack (NB-CR) distribution which is obtained by mixing the NB distribution with a Crack distribution. This distribution can be used for modelling frequency of claims. Frequency of claims which modelled by Negative Binomial-Crack distribution and the amount of claims which modelled by Lognormal distribution are important to calculate the total premium. The effective calculation of total premium needs to done by the insurance company in order to cover all claims of the policyholder. The premium principle calculation can be done by calculating the expected value of insurance risk. The risks referred to claims, including the frequency of claims and amount of claims (severity). Therefore, calculation of the expected value of frequency of claims and the expected value of amount of claims is needed to get the total premium.","Kata Kunci : distribusi campuran Negative Binomial, distribusi Negative Binomial-Crack, data cacah, overdispersi, excess zeros, frekuensi klaim, besar klaim (severity), premi total"
http://etd.repository.ugm.ac.id/home/detail_pencarian/157975,METODE DUA LANGKAH DALAM PENGELOMPOKKAN DATA UNTUK PENGELOMPOKAN DATA CAMPURAN BERJENIS KATEGORIK DAN NUMERIK,"NAUFAL IBNU AMZANI, Dr. Herni Utami, S.Si, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Data dengan jenis atribut campuran numerik dan kategorik semakin sering kita temukan dalam kehidupan sehari-hari. Sementara itu, algoritma analisis pengelompokkan data yang sering dipakai dalam kehidupan sehari-hari kebanyakan hanya mampu menganalisis data yang berjenis kategorik saja atau berjenis numerik saja. Metode dua langkah dalam pengelompokkan data untuk pengelompokkan data campuran berjenis kategorik dan numerik ini merupakan salah satu metode pengelompokkan data berjenis campuran tersebut dengan menggunakan co-occurrence sebagai dasar untuk memberikan nilai numerik pada atribut kategorik. Penggunaan co-occurrence dimaksudkan untuk menemukan kesamaan hubungan antar masing-masing atribut kategorik. Sehingga, semua data berjenis kategorik akan memiliki nilai numerik. Kemudian, metode ini menggunakan dua langkah pengelompokkan data. Pertama metode ini mengaplikasikan analisis cluster hirarki aglomeratif yang kemudian hasilny diintegrasikan dengan jumlahan item kategorik sebagai item tambahan. Metode ini menggunakan analisis cluster k-means sebagai algoritma pengelompokkan data terakhir. Kemudian, cluster yang terbentuk kemudian dianalisis kualitasnya menggunakan entropi. Metode ini menghasilkan hasil yang baik sebagai analisis cluster dengan data berjenis campuran.","Data with mixed attributes of numerical and categorical types are increasingly common in real life. Meanwhile, clustering analysis algorithm that is often used in mostly only able to analyze data with pure categorical and numeric only. Clustering mixed categorical and numeric data with two-step method of clustering is one of the methods of clustering mixed data by using co-occurrence as a basis for assigning numerical values into categorical attributes. The use of co-occurrence is intended to find similarity of relationships between each categorical attributes. Thus, all categorical data will have numeric values. Then, this method uses two-step of clustering. First, this method applied hierarchial agglomerative clustering which is then the results integrated with the sum of categorical items as additional items. This methods then uses k-means clustering analysis as the final clustering algorithm. Then, the formed clusters will be tested for its quality with entropy. This method produced good results as cluster analysis for mixed data.","Kata Kunci : clustering analysis, k-means clustering, hierarchial aggomerative clustering, mixed atrributes, co-occurrence, entropy"
http://etd.repository.ugm.ac.id/home/detail_pencarian/162329,MODEL REGRESI LINEAR FUZZY BERDASARKAN PENDEKATAN ALPHA-CUTS PADA DATA DENGAN SAMPEL BERULANG,"BAGAS KRISNA S, Prof. Drs. Subanar, Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Analisis regresi dalam statistika adalah salah satu metode untuk menentukan hubungan sebab-akibat antara suatu variabel atau beberapa variabel bebas atau independen dengan sebuah variabel tak bebas atau dependen. Terkadang, terdapat ketidakpastian pada data apabila pada proses pengukuran ada campur tangan manusia. Ketidakpastian ini berupa adanya perulangan dalam pengambilan data tiap sampel. Untuk mengatasi ketidakpastian ini, maka dibentuk model alternatif dari regresi yaitu model regresi linear fuzzy berdasarkan pendekatan alpha-cuts. Dari hasil penelitian didapatkan bahwa model regresi linear fuzzy dengan pendekatan alpha-cuts lebih baik digunakan pada sampel yang dalam pengambilan data dengan sampel yang terdapat perulangan karena dapat mengurangi kesalahan pada pengambilan keputusan.","Regression analysis in statistics is one method to determine the causal relationship between a variable or some independent variable with a dependent variable. Sometimes, there is uncertainty in the data if the measurement process there is human intervention. The uncertainty contained in this paper is the existence of iterations in taking data from each sample. Therefore, an alternative regression model was developed to overcome this uncertainty, a linear fuzzy regression model based on the alpha-cuts approach. The experiment shows that linear fuzzy regression model based on alpha-cuts approach is more properly used in samples which in sampling data is repetitive because it can reduce errors in decision making.","Kata Kunci : Model Regresi, Regresi Fuzzy, Alpha-cuts, Bilangan fuzzy segitiga"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132378,BRUTO DAN MARS UNTUK ANALISIS DISKRIMINAN FLEKSIBEL,"RAHMANINGRUM K, Dr.rer.nat Dedi Rosadi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Analisis diskriminan fleksibel adalah pengembangan dari analisis diskriminan linier yang digunakan untuk menyelesaikan masalah klasifikasi pada nonparametrik. Analisis diskriminan fleksibel menggantikan langkah regresi linier dengan regresi nonparametrik. Metode yang dibahas adalah metode dengan pendekatan spline yaitu MARS (Multivariate Adaptive Regression Spline) dan Bruto (Adaptive Additive Spline). Analisis dikatakan baik ketika diperoleh nilai GCV (Generalized Cross Validation) yang minimum sehingga memberikan nilai APER (Apparent Error Rate) yang juga minimum. Dilakukan pengujian untuk mengklasifikasikan sinar gamma dari latarnya dengan kedua metode tersebut, diperoleh kesalahan klasifikasi dengan metode Bruto 14.17% sedangkan 15.3% dengan metode MARS.","Flexible Discriminant Analysis (FDA) is a development from linear discriminant analysis that being used to solve classification problems in nonparametric area. FDA replaces parametric regression with a nonparametric regression. Methods explained here are the spline approach, MARS (Multivariate Additive Regression Spline) and Bruto (Adaptive Additive Spline). The analysis is said well-performed if the value of GCV (Generalized Cross Validation) and APER (Apparent Error Rate) are minimum. The two methods are conducted to classify gamma rays from its background, and the error of classification results are 14.17% error by Bruto method and 15.3% error by MARS method.","Kata Kunci : Klasifikasi, Analisis Diskriminan, Spline, MARS (Multivariate Adaptive Regression Splines), GCV (Generalized Cross Validation), APER (Aparent Error Rate)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/162852,PEMBENTUKAN GRAFIK PENGENDALI UNTUK DATA RUNTUN WAKTU DENGAN METODE ROBUST HOLT-WINTERS,"ILHAM FAJAR PRAMADHI, Herni Utami, S.Si., M.Si., Dr.",2018 | Skripsi | S1 STATISTIKA,"\textit{Statistical Process Control} (SPC) merupakan sebuah alat dengan kapabilitas yang tinggi dalam menilai, mengevaluasi dan mengembangkan strategi bisnis untuk organisasi, perusahaan manufaktur, penyedia pelayanan kesehatan, dan badan-badan usaha milik pemerintah. Proses pengendalian kualitas membutuhkan metode statiska untuk mendeteksi sebuah anomali. Grafik Pengendali umumnya digunakan pada data univariat namun kenyataannya terdapat data runtun waktu yang perlu dilakukan pengendalian.  	 	Metode Grafik Pengendali yang dapat digunakan untuk data runtun waktu adalah metode Holt-Winters dan Robust Holt-Winters. Kedua metode ini mengandalkan \textit{Forecast error} dalam pembentukan grafik pengendali. \textit{forecast error} didapatkan dengan cara meramalkan yang kemudian dihitung selisih dari hasil peramalan dengan data aktual. Berdasarkan studi kasus grafik pengendali yang dibentuk dengan metode Robust Holt-Winters memiliki performa yang lebih baik jika tepdapat data pencilan pada periode \textit{training} karena batas kendali yang dihasilkan lebih rapat.","Statistical Process Control is a tool with high capability for scoring, evaluating ang developing business strategy in organization, company, health provider and government-owned enterprises. Quality control process needs statistical method to detect an anomali. Control chart is usually used in univariat data but actually there is a time series data that should be controlled.   Control chart method which can be used fo time series data is hol-winters method and robust holt-winters. Both of those method use forecast error in creating control chart. Forecast error is obtained by forecasting data and calculated the differences between forecast data and actual data. According to a case study the control chart generated by robust holt-winters has a better result if there is an outlier data because has a tighter control limits.","Kata Kunci : grafik pengendali, runtun waktu, data pencilan, \textit{Robust}, \textit{Holt-Winters}, peramalan, \textit{forecast error}."
http://etd.repository.ugm.ac.id/home/detail_pencarian/165157,DISTRIBUSI TWEEDIE DALAM CADANGAN KLAIM IBNR DENGAN DOUBLE GENERALIZED LINEAR MODEL,"DESSI ARIYANI, Drs. Zulaela, Dipl.Med.Stats, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Incurred but not reported (IBNR) adalah jenis klaim pada asuransi umum yang sudah terjadi namun belum dilaporkan kepada perusahaan asuransi. Hal ini dapat terjadi karena prosedur administratif atau prosedur hukum lainnya.  Dalam sebuah analisa mengenai perhitungan cadangan klaim IBNR, diasumsikan bahwa besarnya klaim individual berdistribusi Gamma sedangkan jumlah klaimnya berdistribusi Poison, keduanya digabungkan menjadi sebuah distribusi baru bernama Tweedie.  Ketika memodelkan besarnya mean dari klaim, sangat penting juga untuk memodelkan variansinya. Dengan tujuan untuk memodelkan variansi (dispersi), digunakanlah metode DGLM (Double Generalized Linear Model) untuk menaksir kedua parameternya.  Studi kasus yang digunakan adalah data klaim IBNR asuransi kendaraan Third Party Liability (TPL) dari Royal & Sun Alliance selama 9 periode kejadian. Hasil dari estimasi DGLM dengan ML diperoleh total cadangan klaim sebesar  Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â£1.917.471 dengan MSE sebesar 42887,82.","Incurred but not reported (IBNR) is one of the types of claim in general insurance which already incurred but not reported yet to the insurance company. This maybe happen because of some of the administrative procedure or others.  One analyze about IBNR claim reserving assumed that the amount of individual claim amount is Gamma distribute while the amount of claim count is Poisson distribute, both of them was joined to be a new distribution called Tweedie. When modelling the mean of the claim cost, itÃƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¢Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â½s necesarry to modele the variance as well. With purpose to model the variance (dispersion), we use Double Generalized Linear Model (DGLM) to estimate both of the parameters.  The case study used in this thesis is IBNR claim amount of motor Third Party Liability (TPL) from Royal Sun alliance with 9 years of accident periods. The result of this DGLM is total claim reserve IBNR by DGLM with ML Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã‚Â¯Ãƒï¿½Ã‚Â¿Ãƒï¿½Ã‚Â½Ãƒï¿½Ã¯Â¿Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½Ãƒï¿½Ã¯Â¿Â½Ãƒï¿½Ã‚Â£1.917.471 and MSE 42887,82.","Kata Kunci : cadangan klaim, IBNR, Distribusi Tweedie, generalized linear model, double generelized linear model, REML"
http://etd.repository.ugm.ac.id/home/detail_pencarian/159017,Grafik Pengendali Robust Berdasarkan Estimator Qn,"RANI SYAFRIMA PUTRI, Dr. Herni Utami, M.Si",2018 | Skripsi | S1 STATISTIKA,Kualitas produk merupakan faktor yang berpengaruh terhadap kepuasan konsumen. Pengendalian kualitas diperlukan guna menjaga kestabilan suatu proses produksi agar menghasilkan produk yang memenuhi standar yang telah ditetapkan dan memenuhi syarat agar dapat digunakan konsumen. Grafik pengendali merupakan salah satu alat yang digunakan untuk mendeteksi perilaku menyimpang dalam proses produksi. Grafik pengendali Shewart Xbar dan S adalah salah satu yang banyak digunakan sebagai teknik pengendalian proses statistik untuk mengontrol rata-rata dan variabilitas proses dengan asumsi dasar distribusi dari karakteristik kualitas adalah normal. Metode robust adalah salah satu metode statistik yang sering digunakan sebagai alternatif ketika asumsi normalitas yang mendasari tidak terpenuhi. Grafik pengendali robust berdasarkan estimator Qn merupakan salah satu alternatif pengganti grafik pengendali Xbar dan S ketika asumsi normalitas yang mendasari tidak terpenuhi dan nilai standar deviasi tidak diberikan. Untuk mengetahui dan membandingkan performa grafik pengendali Shewart (Xbar dan S) dan grafik pengendali robust berdasarkan estimator Qn digunakan metode Average Run Length (ARL).,"Product quality is one of many factor that affect to the consumer satisfaction. Quality control are needed to ensure process stability so that the products and services meet requirements . Control chart is one of the tools that are used to detect the deviant behavior in the production process. Xbar and S control chart is one of the most widely used as statististical quality control techniques developed to control the average and process variability based on the basic assumption that the underlying distribution is normal. A robust method is one of the statistical methods often as an option when the underlying assumptions of normality were not met. Robust control chart based on Qn estimator provides an alternative Xbar and S control chart when the underlying assumptions of normality were not met or and the standard valuesare unknown or not given. To know and compare the performance of Shewart control chart (Xbar and S) and robust control chart based on Qn estimator, in this study used the method Average Run Length (ARL).","Kata Kunci : grafik pengendali, Xbar dan S, robust, estimator Qn, Average Run Length (ARL)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/159793,ESTIMASI PARAMETER REGRESI COX BERDASARKAN FUNGSI FULL LIKELIHOOD DENGAN PENDEKATAN EMPIRIK,"NOFITA IKA UTAMI, Drs. Danardono, M.P.H.,Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Regresi Cox merupakan model regresi yang sering digunakan untuk memodelkan data survival dengan fungsi partial likelihood sebagai dasar estimasi parameter. Estimasi parameter regresi Cox klasik tersebut tidak mendemonstrasikan keuntungan untuk ukuran sampel kecil. Pada kenyataannya, dalam bidang kesehatan, sampel untuk data survival dengan sensor sering didapatkan dalam ukuran kecil. Skripsi ini membahas estimasi parameter regresi Cox berdasarkan fungsi full likelihood untuk sampel kecil. Terdapat dua parameter yang diestimasi dalam model regresi Cox ini, yaitu baseline survival dan parameter regresi.  Baseline survival diestimasi berdasarkan pendekatan empirical likelihood. Selanjutnya estimator baseline survival digunakan untuk membentuk fungsi estimasi parameter regresi yang akan diestimasi menggunakan maximum likelihood estimator. Studi simulasi dalam skripsi ini menunjukkan bahwa estimasi parameter regresi Cox berdasarkan fungsi full likelihood lebih baik dibandingkan dengan estimasi parameter regresi klasik untuk ukuran sampel kecil.","Cox regression is a well-known regression to model survival data using partial likelihood function as the basis of parameter estimation. Parameter estimation of classical Cox regression did not demonstrate the advantage for small-samples. In fact, in medical cinical trials, the sample size of survival data with censored is often obtained in small amounts. This undergraduate thesis discusses parameter estimation of Cox regression based on full likelihood function for small-sample. There are two parameters estimated in this Cox regression model, that are baseline survival and regression parameter. Baseline survival is estimated by empirical likelihood parameterization. Furthermore, baseline survival estimator is used to form the estimation function of regression parameter and it is estimated using maximum likelihood estimator method. Simulation study in this undergraduate thesis shows that parameter estimation of Cox regression based on full likelihood function is better than parameter estimation of classical regression for small-sample size.","Kata Kunci : survival analysis, Cox regression, right-censored data, empirical likelihood"
http://etd.repository.ugm.ac.id/home/detail_pencarian/165940,PERAMALAN DATA COMPOSITIONAL TIME SERIES DENGAN METODE RUNTUN WAKTU MULTIVARIAT MODEL VECTOR AUTOREGRESSIVE (VAR) (Studi Kasus: Penerapan pada Peramalan Distribusi Populasi Penduduk Amerika Serikat Berdasarkan Rentang Usia),"HERDIAN DENI PRAYOGA, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Aitchison (1986) mengatakan bahwa kebanyakan metode statistika standar mengasumsikan data yang dianalisis berada di ruang riil dengan struktur geometri Euclidean, sedangkan ruang sampel natural dari data komposisional adalah simpleks. Berdasarkan hal tersebut, salah satu jenis dari data komposisional yaitu data Compositional Time Series (CTS) juga memiliki ruang sampel simpleks. Hal tersebut menyebabkan penggunaan metode peramalan data runtun waktu multivariat standar misalkan peramalan dengan model Vector Autoregressive (VAR) tidak sesuai digunakan pada data CTS. Akan tetapi, metode tersebut sebenarnya masih tetap dapat digunakan pada data CTS dengan catatan yang pertama yaitu harus dilakukan transformasi terlebih dahulu pada sampel data CTS menggunakan transformasi isometric log-ratio (ilr) berdasarkan dari Kynclova dkk. (2015), sehingga sampel data CTS tersebut berada dalam ruang riil berstruktur Euclidean dan setelah itu data hasil transformasi pada setiap variabel yang terbentuk harus stasioner. Sampel data hasil transformasi tersebut yang digunakan dalam pemodelan VAR, yang mana model VAR tersebut digunakan untuk meramalkan data yang berada di dalam ruang riil. Data hasil ramalan yang masih berada di dalam ruang riil tersebut, ditransformasikan kembali ke dalam ruang simpleks menggunakan transformasi invers ilr.  Studi kasus peramalan data CTS dengan metode runtun waktu multivariat model VAR diterapkan pada data distribusi penduduk Amerika Serikat per tahun berdasarkan rentang usia. Jika data hasil peramalan yang dibangkitkan dengan model VAR mendekati data aktualnya, maka penggunaan metode tersebut dapat dikatakan sesuai, sehingga model VAR dapat digunakan untuk melakukan peramalan data CTS berdasarkan studi kasus peramalan distribusi penduduk Amerika Serikat berdasarkan rentang usia.","Aitchison (1986) said that the most standard statistical methods assume the analyzed data are in a real space with Euclidean geometric structures, whereas the natural sample space of the compositional data is a simplex space. Based on this case, one of the compositional data types is the Compositional Time Series (CTS) data which is also have a simplex sample space. That the matter causes the use of a standard multivariate time series forecasting method such as forecasting with Vector Autoregressive (VAR) model is not appropriate for the CTS data. However, the method is proper to be used for the CTS data under some conditions. Firstly, the CTS sample data must be transformed using isometric log-ratio (ilr) transformation based on Kynclova et al. (2015), so the CTS sample data are in a real space with a Euclidean structure, and after that the transformed sample data on each variable must be stationary. The transformed sample data are used for VAR modeling, which are used to forecast data in a real space. The forecast data in the real space, are transformed back into the simplex space using inverse ilr transformation.  The case study for CTS data forecasting with the multivariate time series method under VAR model, are applied to the population distribution data of United States of America per year by age group. If the forecast data, which are generated with the VAR model, approach the actual data, then the use of the method can be said to be appropriate, so that the VAR model can be used to forecast the CTS data based on case studies of the population distribution forecasting of United States of America by age group.","Kata Kunci : Data Komposisional, Compositional Time Series, isometric log-ratio, stasioner, Vector Autoregressive."
http://etd.repository.ugm.ac.id/home/detail_pencarian/162873,Penentuan Harga Opsi Beli Tipe Eropa dengan Model Eksponensial untuk Data Perdagangan Frekuensi Tinggi,"DWI ANI HASTININGRUM, Dr. Gunardi, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Opsi merupakan instrumen finansial derivatif dari saham yang memberikan peluang investor untuk mendapatkan keuntungan yang tinggi dengan risiko yang rendah. Black dan Scholes (1973) mengembangkan suatu model penentuan opsi yang menggunakan asumsi praktis return saham berdistribusi normal dan volatilitas konstan. Akan tetapi, penelitian pada data finansial dengan frekuensi data yang lebih tinggi memperlihatkan bahwa fluktuasi harga cenderung mengikuti proses non-Gaussian dengan fungsi distribusi probabilitas empiris (Empirical Distribution Fumction) dari return membentuk  eksponensial. Perbedaan distribusi return saham tersebut akan berakibat pada kurang tepatnya metode Black-Scholes, sehingga dikembangkan suatu metode baru yaitu model Eksponensial. Selanjutnya, dilakukan perbandingan antara harga opsi yang diperoleh dengan Model Eksponensial dan metode Black-Scholes terhadap harga opsi di pasar. Dengan menggunakan Downhill Simplex Method  untuk memperoleh parameter model Eksponensial dan menggunakan SRPE (Squared Relative Price Error) sebagai kriteria penentuan harga opsi, hasil menunjukkan bahwa model Eksponensial lebih baik dibandingkan dengan model Black-Scholes.","Option is a derivative financial instrument of stock that gives investors the opportunity to earn high profits with low risk. Black and Scholes (1973) developed an option pricing model which impractical assumptions including constant volatility of stock return and normal distributin of return. However, studies on high frequency financial data have shown that price fluctuations behave as non-Gaussian processes, with the empirical probability distribution function (EDF) of returns exhibiting exponential model..  With the diferentiation of the distribution, Black-Scholes method is considered less precise. Therefore, a new method called Exponential Model is developed. Furthermore, we compare the option price obtained by Exponential model and the Black-Scholes method with option market price. Using Downhill Simplex Method to get the parameters of exponential model and  SRPE (Squared Relative Pricing Error) as the criterion of option pricing, the result shows that Exponential model  performs better than Black- Scholes method.","Kata Kunci : Harga Opsi, Model Eksponensial, Black-Scholes, Downhill Simplex Method"
http://etd.repository.ugm.ac.id/home/detail_pencarian/166979,Metode Ranked Set Sampling untuk Estimasi Mean,"EUGENIA SEKAR RISANT, Dr. Abdurrakhman, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Dalam kehidupan sehari-hari diperlukan data untuk mengambil sebuah keputusan atau inferensi tertentu. Namun, pada praktiknya pengumpulan suatu data dalam populasi akan memakan waktu, tenaga, dan biaya yang lebih. Maka digunakanlah metode sampling untuk meminimalkan keluarnya waktu, tenaga, dan biaya. Pemilihan metode sampling yang tepat akan memberikan hasil estimasi yang tepat. Selain itu pemilihan estimator yang baik juga diperlukan agar hasil penelitian lebih akurat. Namun, banyak orang yang menggunakan cara yang paling mendasar untuk pengambilan sampel, yaitu sampel random sederhana. Dalam kehidupan nyata terdapat banyak asumsi dari sampel random sederhana yang tidak dapat dipenuhi. Pada tahun 1952, McIntyre memperkenalkan konsep ranked set sampling dan mengatakan dapat lebih efisien dibanding metode sampel random sederhana. Pada skripsi ini akan dibuktikan bahwa estimasi mean dari metode ranked set sampling merupakan estimator yang tak bias dan lebih efisien dibandingkan metode sampel random sederhana dengan cara membandingkan variansi estimator mean pada masing-masing metode.","In real life, data is needed to take certain decisions or inferences. However, in practice the collection of data in the population will take more time, effort, and costs. So that, a sampling method is used to minimise the discharge of time, effort, and costs. Choosing the right sampling method will give the right estimation result. In addition, the selection of a good estimator is also needed so that the result of the research are more accurate. Many people use the very basic way to do sampling, which is simple random sampling. In real life there are so many assumption of simple random sampling method that can not be fulfilled. In 1952, McIntyre introduced the concept of ranked set sampling and said it could be more efficient than a standard simple random sampling method. This thesis will prove that the mean estimation of ranked set sampling method is an unbiased and act as a more efficient estimator than the estimator of simple random sampling method by comparing the mean estimator variance in each method.","Kata Kunci : sampling, estimasi mean, sampel random sederhana, ranked set sampling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/162117,Penentuan Bobot Portofolio berdasarkan Mean-Variance dan Mental Accounts,"REZZA ABDURRAHMAN IB, Dr. Abdurakhman, M.Si., Drs. Zulaela, Dipl.Med.Stats.,M.Si., Yunita Wulan Sari,S.Si.,M.Si., Prof. Dr. Sri Haryatmi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Dunia Investasi saham mengalami perkembangan yang cepat dan tidak pernah kehilangan peminat. Risiko yang dihadapi berbanding lurus dengan imbal balik yang dijanjikan. Diversifikasi dalam portofolio merupakan salah satu cara manajemen risiko investasi. Portofolio yang paling sering digunakan adalah mean-variance Markowitz. Dasar dari portofolio Markowitz adalah meminimalkan risiko dalam bentuk variansi dan memaksimalkan imbal balik dalam bentuk rerata. Seiring perkembangan ilmu pengetahuan, ukuran variansi dirasa kurang cukup  mewakili potensi kerugian. Ukuran risiko dalam bentuk Value at Risk sudah lebih lazim digunakan. Permasalahan mental accounts adalah usaha meminimalkan  Value at Risk dalam pembentukan portofolio. Karya ini membahas optimasi portofolio untuk menghasilkan bobot portofolio yang dapat memenuhi kondisi mean-variance Markowitz dan permasalahan mental accounts. Karya ini juga akan mensimulasikan metode tersebut pada saham-saham yang dianggap menjanjikan untuk investasi pada 2018.","Stock investment has always improved last time and didn't ever lost people's interests. The investment risk that should be handled usually as great as its return. Diversification in the portfolio is one of investment risk management method. The most used portfolio is Markowitz's mean-variance (1952). The basic method of Markowitz's portfolio is to minimize risk in the form of variance and maximize return in the form of mean. Along science's development, the variance is assumed not enough to representatively measure investments loss potential. Value at Risk is recently used as the measurement of loss potential. Mental accounts problem is an attempt to minimize value at risk in portfolio. This paper will discuss the portfolio optimization to generate portfolio weight that could fulfill Markowitz mean-variance condition and mental accounts problem. This paper will also simulate this method to 2018 recommended stocks for investment.","Kata Kunci : Portfolio, Mean-variance, Value at Risk, Mental accounts"
http://etd.repository.ugm.ac.id/home/detail_pencarian/159560,Perhitungan Premi Asuransi Jiwa berdasarkan Transformasi Hazard Linear dengan Asumsi Alfa-power Approximation,"NOFFALINDA MUSTIKA, Dr. Adhitya Ronnie Effendie, M.Sc",2018 | Skripsi | S1 STATISTIKA,"Asuransi jiwa adalah perjanjian antara dua pihak dengan pihak yang satu berkewajiban membayar iuran dan pihak yang lain memberikan jaminan sepenuhnya kepada pihak tertanggung apabila terjadi risiko kematian pada seseorang. Untuk mengurangi risiko dan mengendalikan masalah-masalah perilaku pemegang polis, perusahaan asuransi melakukan metode memindahkan bobot ke kerugian yang lebih besar dengan transformasi hazard linear. Pada skripsi ini akan dibahas mengenai transformasi hazard linear dan aplikasinya dalam peluang hidup seseorang. Dengan menggunakan transformasi hazard linear, fungsi survival dari suatu risiko menyimpang dari biasanya, dimana memberikan safety margin dalam kalkulasi harga produk asuransi jiwa. Asumsi alfa-power approximation merupakan asumsi usia pecahan yang merupakan penggeneralisasian dan perluasan dari asumsi usia pecahan sebelumnya. Dengan tambahan asumsi usia pecahan yaitu alfa-power approximation dikombinasikan dengan transformasi hazard linear, didapatkan nilai asuransi tunggal pada polis asuransi jiwa kontinu dapat didekati dalam nilai asuransi tunggal diskrit. Perhitungan premi asuransi jiwa dengan menggunakan transformasi hazard linear dan asumsi alfa-power approximation diharapkan perusahaan asuransi dapat memenuhi klaim-klaim yang diajukan.","Life insurance is an agreement between insured and insurer, with the insured is obliged to pay premiums and the insurer provides full guarantee to the insured in case of risk of death. In this undergraduate thesis aims to discuss the linear hazard transform and its applications in life contingencies. To reduce risks and control the behavioral problems of policy holders, insurance companies resort to methods of moving weights to greater losses with linear hazard transform. Under the linear hazard transform, the survival function of a risk is distorted, which provides a safety margin for pricing insurance products. alfa-power approximation assumption is the assumption of fractional age which is the generalization and extension of the previous fractional age assumptions. In addition to the fractional age, alfa-power approximation assumption combined with linear hazard transforms, the net single premium of a continuous life insurance policy can be approximated in terms of the net single premiums of discrete ones. The calculation of life insurance premiums under the linear hazard transform with alfa-power approximation assumption is expected by the insurance company to fulfill the claims filed.","Kata Kunci : premi, transformasi hazard linear, alfa-power approximation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/167500,OPTIMISASI PORTOFOLIO MULTI OBJEKTIF MENGGUNAKAN ANALISIS KLASTER FUZZY C-MEANS,"Dipalia Br Tarigan, Prof. Dr.rer.nat Dedi Rosadi, S.Si., M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Investasi pada hakekatnya merupakan penempatan sejumlah dana yang dimiliki pada saat ini dengan harapan akan memperoleh keuntungan dimasa mendatang. Setiap investasi yang ditanamkan pastilah mempunyai risiko yang harus ditanggung oleh investor. Oleh sebab itu, pemilihan portofolio merupakan hal yang sangat penting untuk menghasilkan tingkat pengembalian yang diharapkan dengan risiko yang masih dapat ditoleransi. Pada penelitian ini akan dilakukan suatu teknik yang dapat secara efisien menyarankan portofolio yang bernilai investasi. Pertama, analisis fuzzy c-means clustering digunakan untuk mengkategorikan sejumlah saham ke dalam beberapa kelompok berdasarkan rata-rata return dan risikonya. Beberapa indeks validitas digunakan untuk memilih jumlah klaster yang optimal. Dipilih perwakilan dari masing-masing klaster yang sesuai dengan karakterisik kelompok berdasarkan pusat klaster. Selanjutnya akan dilakukan pembobotan dengan menggunakan portofolio multi objektif.  Data yang digunakan yakni data closing price saham bulanan dari 21 saham LQ-45. Data tersebut dikelompokan dan dipilih perwakilan dari masingmasing klaster. Terdapat 3 saham perwakilan yang akan dimasukkan ke dalam portofolio. Dilakukan kombinasi koefisien pembobot (k) sebagai pilihan mean return yang diharapkan dan risiko yang diinginkan oleh investor. Teknik klaster terbukti dapat menghemat waktu dalam memilih portofolio yang optimal. Dari 21 saham awal terpilih 3 saham yang mempunyai risiko rendah dan mean return yang tinggi dibandingkan saham lainnya.","Investment is essentially the placement of a number of funds held at the present time in the hope that will get benefit in the future. Every investment invested must have risks that must be borne by investors. Therefore, portfolio selection is very important to produce the high expected rate with risks that can still be tolerated. In this study, a technique will be carried out that can efficiently suggest a portfolio that is worth investing. First, fuzzy c-means clustering analysis is used to categorize a huge amount of stock data into several groups based on their associated average return and risk. Several validity indexes are used to select the optimal cluster number. Representatives from each cluster were selected according to the group characteristics based on the cluster center. Furthermore, weighting will be carried out using a multi-objective portfolio. The data used is 21 monthly stock closing price data from LQ-45 stock. The data is grouped and selected representatives from each cluster. There are 3 representative stocks that will be included in the portfolio. A combination of weighting coefficients (k) is used as the choice of the mean return expected and the risk desired by the investor. Cluster techniques are proven to save time in choosing an optimal portfolio. Of the 21 initial stocks selected 3 stocks that have a low risk and high return on investment compared to other stocks.","Kata Kunci : optimisasi portofolio, analisis klaster, fuzzy c-means clustering, portofolio multi objektif"
http://etd.repository.ugm.ac.id/home/detail_pencarian/163919,GRAFIK PENGENDALI FUZZY CUMULATIVE SUM (CUSUM) (Studi Kasus: Pengendalian Kualitas Sosis pada PT So Good Manufacturing Cikupa),"ARYA PAMBUDI BAYUAJI, Dr. Abdurakhman, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Kualitas produk merupakan salah satu faktor yang mempengaruhi tingkat kepuasan konsumen. Grafik pengendali digunakan untuk mengkontrol kualitas produk tersebut. Salah satunya yaitu grafik pengendali Cumulative Sum (CUSUM). Grafik pengendali CUSUM adalah grafik pengendali yang digunakan untuk mengontrol proses dengan pergeseran kecil. Terkadang, terdapat ketidakpastian pada data apabila pada proses pengukuran ada campur tangan manusia. Data yang tidak pasti tersebut disebut data fuzzy. Apabila terdapat data fuzzy pada suatu proses pengendalian kualitas dan ingin dideteksi pergeseran kecil pada mean maka dibutuhkan grafik pengendali Fuzzy Cumulative Sum (FCUSUM). Grafik pengendali FCUSUM mengurangi kesalahan dalam pengambilan keputusan.  Dari hasil penelitian didapatkan bahwa grafik pengendali FCUSUM lebih baik digunakan untuk data yang tidak pasti karena dapat mengurangi kesalahan pada pengambilan keputusan.","Product quality is one of many factor that affect the level of consumer satisfaction. Control chart is widely use to control product quality. One of them is cumulative sum (CUSUM) control chart. CUSUM is a control chart that handling small shifted data. Sometimes, there is a data that classified as ""uncertain"" or ""vagueness"" due to human subjectivity at measurement process. That kind of data is called as fuzzy data. If there is a small shifted fuzzy data at quality control process then it needs a fuzzy CUSUM control chart to handle such things.   The experiment shows that FCUSUM control chart is more properly used for small shifted fuzzy data than CUSUM control chart because it can reduce false alarm at decision making.","Kata Kunci : Grafik Pengendali, CUSUM, Fuzzy CUSUM, Pergeseran proses"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132689,IMPLEMENTASI SISTEM INFERENSI FUZZY SEBAGAI INDIKATOR TEKNIKAL SAHAM DAN MANAJEMEN DANA MENGGUNAKAN MODIFIKASI OPTIMAL F,"BIMO PRASETYO, Dr. Abdurakhman, S.Si., M.Si.",2018 | Skripsi | S1 STATISTIKA,"Tiga fungsi sistem trading yaitu trading rules, manajemen risiko, dan manajemen dana. Trading rules menentukan bagaimana cara seorang trader bisa masuk ke pasar. Indikator teknikal adalah salah satu cara membentuk trading rules. Ada berbagai pendapat yang subjektif dalam menginterpretasikan indikator ini, tergantung apakah trader berani menanggung risiko tinggi atau malah menghindari risiko. Output dari indikator teknikal ini berupa nilai kontinu sehingga kadang muncul nilai yang menyebabkan interpretasi yang rancu dan subjektif. Kerancuan dan kesubjektifan inilah yang membuat konsep fuzzy bisa diterapkan untuk meningkatkan kemampuan indikator tersebut. Sistem Inferensi Fuzzy adalah salah satu alternatif untuk mendapatkan hasil tegas dari data yang tidak pasti. Setelah trader menemukan cara untuk masuk ke pasar, manajemen dana dengan optimal f dapat digunakan oleh trader untuk mengetahui seberapa besar alokasi dana yang harus digunakan untuk mendapatkan hasil trading terbaik. Dari hasil penelitian diperoleh bukti bahwa sistem inferensi fuzzy mampu meningkatkan kemampuan indikator teknikal dengan input data yang rancu. Manajemen dana juga terbukti mampu meningkatkan keuntungan yang didapat jika dibandingkan dengan trading rules tanpa manajemen dana.","Trading system consist of 3 function: trading rules, risk management, and capital management. Trading rules determines how trader can enter the market. Technical indicator is one of the ways to contructs a trading rules. However, the ways to interpret the output of tecnical indicators depend on the preference of the trader whether he is a risk seeker or a risk averse trader. Moreover, the output of the indicator is a continuous value which mean that sometimes the value could lead to a vague and subjective interpretation. Fuzzy Inference System could be applied to enhance the capability of the technical indicator to produce a distinct output from a fuzzy or uncertain data. When entering the market, capital management using optimal f could be used to know how much the capital allocation needed to trade for the best result.  The experiment shows that fuzzy inference system could enhance the capability of the technical indicator to process a fuzzy data. The experiment also shows that using optimal f as capital management could increase the profit of the trading rules.","Kata Kunci : Indikator Teknikal, Relative Strength Index, Average Directional Movement Index, Moving Average Convergence Divergence, Sistem Inferensi Fuzzy, Manajemen Dana, Optimal f, Sistem Trading."
http://etd.repository.ugm.ac.id/home/detail_pencarian/132691,IMPLEMENTASI METODE GENERALIZED TWO STAGE RIDGE REGRESSION UNTUK MENGATASI PERMASALAHAN AUTOKORELASI DAN MULTIKOLINEARITAS,"AGUS PURWADI, Prof. Subanar, Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Dalam analisis regresi klasik yang menggunakan metode kuadrat terkecil terdapat beberapa asumsi klasik yang harus terpenuhi, diantaranya adalah tidak adanya autokorelasi dan multikolinearitas. Jika asumsi tersebut tidak terpenuhi, estimasi parameter dengan menggunakan metode kuadrat terkecil menjadi kurang valid serta akan memiliki variansi dan galat yang besar. Terdapat berbagai macam metode untuk menangani pelanggaran asumsi klasik pada analisis regresi. Metode kuadrat terkecil dua tahap digunakan untuk mengatasi autokorelasi dengan transformasi data oleh koefisien autokorelasi ( ), dan regresi ridge digunakan untuk mengatasi multikolinearitas dengan menambahkan tetapan bias   ke dalam matriks korelasi  . Dalam skripsi ini akan dibahas mengenai penduga parameter baru untuk model linear ganda yang mengalami permasalahan autkorelasi (AR(1)) dan multikolinearitas yaitu metode Generalized Two Stage Ridge Regression. Metode ini merupakan penggabungan antara metode kuadrat terkecil dua tahap dan metode Generalized Ridge Regression, yang memiliki kelebihan yaitu akan diperoleh tetapan bias   yang meminimalkan Mean Square Error dari penduga parameter dalam menangani permasalahan autokorelasi dan multikolinearitas.","Regression analysis is a statistical analysis that used to perform model relationship between dependent variable and independent variable. There are some classical assumptions should be met in classical regression analysis using Least Squares Method, two of them is no autocorrelation and no multicollinearity. If the assumption is not met, parameter estimation using Least Squares method become less valid and the variance and error will be large. There are a lot of variety method for solving violation of classical assumption in regression analysis. The Two Stage Least Squares method is used to deal with autocorrelation by transform the data by coefficient of autocorrelation ( ) and the Ridge Regression method is used to deal with multicollinearity by adding a   biased constant to the correlation matrix  . In this paper we will discuss about new estimator for the multiple linear model which suffers from both problem autocorrelation (AR(1)) and multicollinearity, it is Generalized Two Stage Ridge Regression method. This method is a mixed method between Two Stage Least Squares method and Generalized Ridge Regression method, which has advantage that the resulting   biased constant is minimize the Mean Square Error of estimator for solving autocorrelation and multicollinearity problem.","Kata Kunci : Autokorelasi, Multikolinearitas, Metode Kuadrat Terkecil Dua Tahap, Generalized Ridge Regression, Generalized Two Stage Ridge Regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/159832,MODEL REGRESI HYPER-POISSON SEBAGAI MODEL ALTERNATIF UNTUK DATA OVERDISPERSI,"HANIFAH MARWA SETIANI, Prof. Drs. Subanar, Ph.D",2018 | Skripsi | S1 STATISTIKA,"Regresi Poisson merupakan salah satu model yang sering digunakan untuk data cacah. Dalam regresi Poisson terdapat asumsi yang harus terpenuhi, yaitu asumsi equidispersi atau suatu keadaan ketika nilai mean dan variansi sama. Namun pada kenyataannya, seringkali asumsi equidispersi tidak terpenuhi yaitu variabel respon memiliki variansi lebih besar daripada nilai mean atau disebut dengan keadaan overdispersi. Untuk data yang mengalami overdispersi, dapat digunakan model alternatif dari regresi Poisson yaitu regresi hyper-Poisson yang akan dibahas pada skripsi ini. Model regresi hyper-Poisson ini memungkinkan adanya campuran dispersi tingkat observasi, maka dari itu akan dibentuk model hyper-Poisson dengan dispersi konstan dan model regresi hyper-Poisson dengan campuran dispersi tingkat observasi. Estimasi model yang digunakan adalah metode Maximum Likelihood Estimation. Dalam studi kasus, kedua model regresi hyper-Poisson akan dibandingkan dengan model regresi Poisson dengan kriteria AIC dan BIC untuk menentukan apakah model regresi hyper-Poisson lebih baik daripada model regresi Poisson atau tidak.","Poisson regression is one of the most commonly used models for count data. In the Poisson regression there is an assumption that must be fulfilled, the assumption of equidispersion or a state when the mean and variance are equal. But in fact, often the assumption of equidispersion is not met ie the response variable has a variance greater than the mean value or called the overdispersive state. For data that overdispersion, can be used alternative model of Poisson regression is hyper-Poisson regression which will be discussed in this thesis. This hyper-Poisson regression model allows for a mixture of dispersion of the observational level, hence a hyper-Poisson model with constant dispersion and hyper-Poisson regression model with mixed dispersion level of observation. Estimated model used is Maximum Likelihood Estimation method. In the case study, both hyper-Poisson regression models will be compared with the Poisson regression model with the AIC and BIC criteria to determine whether the hyper-Poisson regression model is better than the Poisson regression model or not.","Kata Kunci : data cacah, regresi Poisson, overdispersi, regresi hyper-Poisson, distribusi hyper-Poisson, Crow-Bardwell"
http://etd.repository.ugm.ac.id/home/detail_pencarian/165976,PERBANDINGAN OPTIMASI PORTOFOLIO MENGGUNAKAN MEAN-VARIANCE DAN MEAN-LOWER PARTIAL MOMENT DERAJAT 2,"AYUN P, Dr. Gunardi, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Lower Partial Moment (LPM) khususnya pada derajat 2 merupakan ukuran risiko dalam optimasi portofolio. Semivariance merupakan kasus khusus LPM derajat n dimana n = 2. Dalam skripsi ini akan dibandingkan mean-LPM derajat 2 dengan metode mean-variance menggunakan fungsi Lagrange. LPM merupakan alat pengukuran risiko downside, yaitu risiko yang terjadi di bawah terget/ rata-rata return. Variansi merupakan alat pengukuran return di atas dan di bawah target return sebagai risiko. Optimasi dengan menggunakan mean-LPM tidak memerlukan asumsi distribusi apapun, sehingga lebih mudah penggunaannya dibandingkan dengan mean-variance. Pada studi kasus, kedua metode tersebut diaplikasikan pada data saham yang tergabung dalam kelompok Indeks LQ-45 dari berbagai sektor pada periode  3 April 2017 Ã¢ï¿½ï¿½ 3 April 2018. Hasil yang diperoleh menunjukkan bahwa portofolio meanÃ‚Â¬-LPM derajat 2 lebih baik dibandingkan portofolio mean-variance. Baik untuk data return yang berdistribusi normal maupun tidak","Lower Partial Moment (LPM), especially in degree 2 as a measure of risk in portfolio optimization. Semivariance is a special case of the n-degree LPM where n=2. In this thesis will be compared the mean-LPM 2-degree with the mean-variance method using the Lagrange function. LPM becomes a downside risk measurement tool, which is an asset that occurs below the target/ average return. Variance is a return measurement tool and is below the target return as a risk. Optimization using the mean LPM does not require any distribution assumptions, therefore it is easier to use than mean-variance. In the case study, the two methods are applied to the stock data which is incorporated in the LQ-45 index group from various sectors in the period of April 3rd 2017 - April 3rd 2018. The results obtained show that the mean-LPM 2-degree portfolio is better than the mean-variance portfolio. Either for data returns that are normally distributed or not.","Kata Kunci : portofolio, mean-Lower Partial Moment derajat 2, mean-variance,  Lagrange"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132700,Analisis Klaster Dengan Metode Robust Sparse K-means,"YOSINTA MEGA MUSTIKA, Dr. Herni Utami, M. Si.",2018 | Skripsi | S1 STATISTIKA,"Witten and Tibshirani  (2010) mengusulkan algoritma untuk menemukan klaster dan memilih variabel yang tepat, bernama Sparse K-means. Sparse K-means sangat berguna apabila terdapat data dengan noise variable (variabel tanpa informasi yang berguna untuk membedakan klaster). Sparse K-means bekerja sangat baik pada data yang bersih tanpa noise variable dan data yang komplit tapi tidak dapat menanggani data yang mengandung pencilan ataupun missing value. Untuk mengatasi masalah ini, Kondo (2012) mengajukan robustification dari algoritma Sparse K-means berdasarkan algoritma Trimmed K-means dari Cuesta-Albertos et al (1997).  Dengan menggabungkan antara algoritma Sparse K-means dan Trimmed K-means, Robust Sparse K-means dapat mengatasi dataset yang berukuran besar dan mengandung pencilan.","Witten and Tibshirani  (2010) proposed an algorithm to simultaneously find clusters and select clustering variables, called sparse K-means. Sparse K-means is particularly useful when the dataset has large fraction of noise variable (that is, variables without useful information to separate the cluster). Sparse K-means works very well on clean and complete data but cannot handle outliers nor missing data. To remedy this problems, Kondo (2012) propose a robustification of their sparse K-means algorithm based on the trimmed K-means algorithm of Cuesta-Albertos et al (1997). By combining algorithm from sparse K-means and trimmed K-means, robust sparse K-means can handle high dimension data with outlier on it.","Kata Kunci : K-means, robust sparse K-means, sparse K-means, trimmed K-means, pencilan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/160353,Perbandingan Jackknifed Liu Estimator dan Jackknifed Ridge Regression Pada Model Regresi Linear dengan Autokorelasi Pada Error,"MUHAMMAD SURYO S, Prof.Subanar, Ph.D",2018 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis yang digunakan untuk mengetahui ada tidaknya pengaruh variabel independen terhadap variabel dependen sehingga variabel dependen dapat ditaksir atau diramalkan berdasarkan variabel independen. Metode yang sering digunakan adalah Metode OLS (Ordinary Least Square), dengan beberapa asumsi klasik yang harus terpenuhi diantaranya tidak adanya multikolinearitas dan No Autokorelasi. Metode OLS menjadi kurang tepat apabila ada asumsi klasik yang tidak terpenuhi, yaitu adanya multikolinearitas dan Autokorelasi pada error. Pada skripsi ini akan dibahas mengenai metode estimasi parameter pada regresi linear dengan menggunakan Metode Jackknifed Liu Estimator dan Jackknifed Ridge Regression. Kedua metode ini merupakan Metode pengembangan dari metode OLS yang dapat dikatakan efektif untuk mengatasi adanya masalah multikolinearitas dan autokorelasi pada error. Pada skripsi perbandingan metode Jackknifed Liu Estimator dan Jackknifed Ridge Regression diaplikasikan pada data uang primer di Indonesia dan faktor-faktor yang mempengaruhinya dari Desember 2010 hingga Februari 2018. Kedua metode ini kemudian saling dibandingkan kemudian diperoleh kesimpulan estimator dengan Jackknifed Liu Estimator lebih baik dibandingkan dengan estimator dengan metode Jackknifed Ridge Regression.","Regression analysis is the analysis used to determine whether there is influence of independent variable to dependent variable so that dependent variable can be estimated or predicted based on independent variable. The most commonly used method is the OLS (Ordinary Least Square) Method, with some classic assumptions that must be met such as the absence of multicolinearity and No Autocorrelation. OLS method becomes less precise if there are unquestionable classic assumption, that is existence of multikolinearitas and autocorrelation in error. In this thesis will be discussed about parameter estimation method in linear regression by using Jackknifed Liu Estimator Method and Jackknifed Ridge Regression. Both of these methods are the method of development of OLS method which can be said effective to overcome the problem of multicollinearity and autocorrelation in error. The comparison of Jackknifed Liu Estimator and Jackknifed Ridge Regression methods was applied to the base money data in Indonesia and the factors that influenced it from December 2010 to February 2018. Both methods are then compared to each other then obtained conclusion estimator with Jackknifed Liu Estimator better than estimator with method Jackknifed Ridge Regression.","Kata Kunci : multicolinearity, correlated error, ordinary least square, generalized least square, metode Jackknifed, liu estimator, jackknifed liu estimator, ridge regression, Jackknifed Ridge Regression, mean square error."
http://etd.repository.ugm.ac.id/home/detail_pencarian/154725,Pemodelan Topik untuk Media Sosial Menggunakan Correlated Topic Model,"RIZQI HARYASTUTI, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Media sosial saat ini menjadi sumber dan tempat bertukar informasi yang paling diminati oleh masyarakat. Hal ini menyebabkan banyak opini yang menumpuk dan mengharuskan kita untuk dapat menarik informasi utama yang masih tersembunyi di dalamnya. Pemodelan topik atau topic models merupakan analisis teks yang bermanfaat dalam pemodelan data tekstual dengan tujuan menemukan topik yang tersembunyi di dalamnya. Hanya saja dari setiap topik yang dihasilkan, pada umumnya dimungkinkan terjadi adanya korelasi atau hubungan antar topik tersebut. Sehingga dikembangkan sebuah pemodelan topik yang akan akan dibahas dalam skripsi ini, yaitu Correlated Topic Model (CTM). CTM merupakan sebuah model probabilitas dari data tekstual dimana dapat menjelaskan korelasi antar topik yang tersembunyi tersebut. CTM merupakan suatu metode pengembangan yang sudah pernah dibahas sebelumnya, yakni LDA. Estimasi parameter yang digunakan dalam model adalah metode Bayesian, dimana metode memberikan nilai estimasi melalui distribusi posterior. Metode Bayesian yang digunakan untuk menghitung estimasi distribusi posterior tersebut adalah Variational Expectation Maximization (VEM). Dalam skripsi ini diterapkan model probabilitas Correlated Topic Model (CTM) untuk data tekstual yang bersumber dari platform Twitter dalam periode tertentu. Hasil dari pemodelan topik ini adalah topik dominan dari data tekstual yang merupakan estimator parameter mean dan relasi antar topik yang merupakan estimator parameter kovariansi.","Social media now is the most interesting source and place of information exchange for the people. This leads to a lot of opinion that accumulates and requires us to be able to pull the key information that is still hidden in it. Topic models is a useful text analysis in textual data modeling in order to find the hidden topics in it. It's just that from each topic generated, there is possible a correlation or relationship between the topic generally. So, developed a topic models that will be discussed in this thesis, the Correlated Topic Model (CTM). CTM is a probability model of textual data which can explain the correlation between the hidden topics. CTM is a method development that has been discussed previously, the Latent Dirichlet Allocation (LDA). Parameter estimation used in the model is the Bayesian method, where the method provides an estimation value through the posterior distribution. The Bayesian method used to calculate the estimation of the posterior distribution is Variational Expectation Maximization (VEM). In this thesis applied probability model, Correlated Topic Model (CTM) for textual data sourced from Twitter platform in certain period. The result of this topic models is the dominant topic of the trending news which is the estimator of the mean parameters and the relation between topics which is the estimator of the covariance parameter.","Kata Kunci : pemodelan topik, Twitter, Correlated Topic Model, analisis teks, Variational Expectation Maximitation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/156774,ESTIMASI RESTRICTED MAXIMUM LIKELIHOOD MODEL FAY-HERRIOT PADA KASUS SMALL AREA ESTIMATION BERDASARKAN METODE EMPIRICAL BEST LINEAR UNBIASED PREDICTION,"RIFAI NUR WIDYANARA, Dr. Herni Utami, M.Si",2018 | Skripsi | S1 STATISTIKA,"Konsep small area estimation bertujuan untuk meningkatkan keakuratan penduga suatu parameter, yaitu dengan menggunakan metode pendugaan secara tidak langsung. Pendugaan tidak langsung dapat dilakukan dengan cara meminjam kekuatan atau memanfaatkan variabel-variabel tambahan (auxiliary variables) dalam menduga parameter. Pada kasus ini, estimasi parameter efek tetap Beta diestimasi dengan menggunakan metode EBLUP Maximum Likelihood, sedangkan Komponen Variansi nya diestimasi dengan menggunakan metode EBLUP Maximum Likelihood dan metode Restricted Maximum Likelihood. Kemudian, dilakukan metode evaluasi estimator dengan menggunakan Mean Squared Error. Aplikasi pada kasus ini adalah dengan mencari estimasi tidak langsung model EBLUP MLE dan model EBLUP RMLE pada data pengeluaran per kapita per bulan di tiap Kecamatan di Kabupaten Kebumen. Didapatkan hasil estimasi tidak langsung pengeluaran per kapita dengan metode EBLUP RMLE mempunyai Standard Error yang paling kecil.","The concept of small area estimation aims to improve the accuracy of the estimator of a parameter by using the indirect estimation method. Indirect estimates can be done by borrowing strength or utilizing auxiliary variables in estimating parameters. These additional variables can be relevant variables related to the variables that are of concern to the small area. 	In this case, estimation of fixed effect parameter Beta is estimated using EBLUP Maximum Likelihood method, while its Variance component is estimated using EBLUP Maximum Likelihood method and Restricted Maximum Likelihood method. Then, the estimator method is evaluated using Mean Squared Error. 	The application in this case is to look for indirect estimation of EBLUP MLE model and EBLUP RMLE model on per capita expenditure data per month in each District in Kebumen District. Indirect estimates of per capita expenditure by EBLUP RMLE method have the smallest Standard Error.","Kata Kunci : Small Area Estimation, Empirical Best Linear Unbiased Prediction, Variance Component, Restricted Maximum Likelihood, Mean Squared Error, Per Capita Expenditure"
http://etd.repository.ugm.ac.id/home/detail_pencarian/164200,Estimasi Interval Konfidensi untuk Mean Absolute Deviation dengan Metode Jackknife Empirical Likelihood,"IKA NURHAYATI, Prof. Drs. Subanar, Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Jackknife merupakan salah satu metode estimasi inferensi statistika yang bekerja membangkitkan data dari sampel asli yang berukuran kecil untuk mendapatkan sampel bayangan. Sampel bayangan ini diperoleh dengan cara menghapus suatu observasi dari sampel asli yang kemudian digunakan untuk menghitung nilai estimator. Salah satu kelebihan Jackknife ialah tidak diperlukannya asumsi distribusi dari sampel yang dimiliki.  Dalam skripsi ini, metode Jackknife diterapkan untuk mengestimasi nilai suatu parameter dari fungsi kemungkinan empiris dimana proses penaksiran parameter tersebut didahului dengan pembentukan fungsi likelihood yang kemudian disebut dengan metode Jackknife Empirical Likelihood (JEL). Metode ini nantinya digunakan untuk mengestimasi interval konfidensi pada Mean Absolute Deviation. Metode JEL dalam skripsi ini diilustrasikan dalam penentuan interval konfidensi pada suatu kandungan Sodium yang terdapat pada beberapa merk Hotdogs Hasil Analisis Laboratorium Kalori di Carneige Mellon University. Metode ini diterapkan untuk memperkecil bias dari variabel tersebut yang dikarenakan kecilnya nilai sampel yang diambil oleh peneliti. Berdasarkan hasil analisis yang diperoleh, metode JEL mampu memperkecil standar deviasi dan memperkecil lebar interval konfidensi.","Jackknife is one of the estimation methods, computer-based statistical inference estimation that works to generate data from a small original sample to get a pseudo-sample. Pseudo-sample is obtained by removing an observation from the original sample that can be used to calculate the value of the estimator. One of the advantages of Jackknife is no need for a distribution assumption from the sample. 	In this paper, a Jackknife method is applied to estimate the value of a  parameter from the Empirical Likelihood function where the parameter estimation process is preceded by the formation of a likelihood function which is called the Jackknife Empirical Likelihood (JEL) method. In this paper, this method will be used to estimate the confidence interval for the Mean Absolute Deviation. The JEL method in this paper is illustrated in determining the confidence interval in a Sodium content that contained in several brands of Hotdogs Results of Calorie Laboratory Analysis at Carneige Mellon University. This method is applied to minimize the bias of the variable which is due to the small sample value taken by the researcher. Based on the results of the analysis, the JEL method is able to reduce the Standard Deviation and reduce the length of the confidence interval.","Kata Kunci : Metode Jackknife Empirical Likelihood, Maximum Likelihood Estimation (MLE), Mean Absolute Deviation (MAD), Interval Konfidensi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/156011,ANALISIS KORELASI KANONIK (STUDI KASUS PADA SISWA KELAS 10 SMA KRISTEN SATYA WACANA),"IDEAL MENPATIEN, Dr. Herni Utami, M.Si",2018 | Skripsi | S1 STATISTIKA,"Bimbingan dan penyuluhan dalam rangka membantu para siswa mengatasi kesulitan belajar dan perencanaan penjurusan pada kelas sebelas merupakan salah satu hal yang umum dilakukan oleh pihak sekolah. Variabel prestasi belajar dan variabel psikologis siswa menjadi acuan dasar dalam membantu para siswa. Untuk mengetahui korelasi antara prestasi belajar dan psikologis siswa kelas sepuluh di SMA Kristen Satya Wacana digunakan analisis korelasi kanonik. Indikator yang digunakan dalam prestasi belajar dan psikologis siswa masing-masing berjumlah sepuluh dan empat belas indikator. Beberapa asumsi seperti linearitas, normalitas multivariat, dan tidak terjadi multikolinieritas harus dipenuhi. Setelah asumsi sudah terpenuhi, pengolahan data bisa dilakukan sehingga diperoleh suatu kesimpulan. Hasil analisis korelasi kanonik menunjukkan bahwa ada korelasi yang signifikan antara variabel prestasi belajar dan psikologis siswa. Dari sepuluh indikator yang membentuk variabel psikologis, variabel indikator Berpikir dengan Bilangan, Berhitung Soal, Berpikir dengan Kata-kata, Kecepatan dan Ketelitian Administrasi dan Hubungan Kata paling dominan kontribusinya. Sedangkan dari empat belas indikator yang membentuk variabel prestasi belajar, variabel indikator Bahasa dan Sastra Indonesia, Ekonomi, Pendidikan Seni, Matematika, Fisika, Kimia, dan Bahasa Inggris memberi kontribusi paling dominan.","Guidance and counseling in order to help students overcome learning difficulties and planning majors in the eleventh grade is one thing commonly done by the school. Variables of student achievement and variables of student psychology become the basic reference in helping the students. To know the relation between the variabel of student achievement and variables of student psychology of the tenth grade in Satya Wacana Christian High School is used canonical correlation analysis. The indicators used in student achievement and student psychology are ten and fourteen indicators respectively. Some assumptions like linearity, normal multivariate, and do not multicolinearity should be fulfilled. After the assumption have fulfilled, data processing can be done so that obtatined a conclusion. The result of canonical correlation analysis indicate that there is a significant correlation between student achievement and student psychology. From the ten indicators that make up the student psychology variables, the indicator variables such as Thinking with Numbers, Counting Problems, Thinking with Words, Speed and Accuracy of Administration and Word Relationships are the most dominant contributors. While from the fourteen indicators that make up the student achievement variables, the variables of Indonesian Language and Literature, Economics, Art Education, Mathematics, Physics, Chemistry and English are the most dominant contributors.","Kata Kunci : prestasi belajar, psikologis siswa, analisis korelasi kanonik."
http://etd.repository.ugm.ac.id/home/detail_pencarian/155245,GRAFIK PENGENDALI EXPONENTIALLY WEIGHTED MOVING AVERAGE NONPARAMETRIK BERDASARKAN ARL DENGAN PENDEKATAN RANTAI MARKOV,"PUTRI ARISTIANI, Dr. Danardono, M.PH, Ph.D",2018 | Skripsi | S1 STATISTIKA,"Kualitas produk merupakan faktor yang sangat berpengaruh terhadap kepuasan seseorang. Salah satu alat untuk mengontrol kualitas produk agar sesuai dengan yang ditargetkan adalah grafik pengendali. Grafik pengendali dapat mendeteksi dengan cepat permasalahan kualitas dalam proses produksi seperti sebab-sebab tak terduga atau pergeseran proses yang kecil. Pergeseran proses tersebut akan menjadi acuan dalam pengawasan proses.  Salah satu metode grafik pengendali yang digunakan untuk mendeteksi pergeseran kecil adalah Exponentially Weighted Moving Average (EWMA)  dengan asumsi normalitas. Namun pada kasus tertentu terdapat data yang asumsi distribusi normalitas tidak terpenuhi. Sehingga untuk menangani pergeseran proses dengan kasus tersebut akan digunakan metode  Exponentially Weighted Moving Average (EWMA) nonparametrik. Metode EWMA nonparametrik akan menjadi acuan pembahasan. Pada skripsi ini, dibahas mengenai grafik pengendali EWMA nonparametrik berdasarkan Average Run Length (ARL) dengan pendekatan rantai Markov untuk memperoleh tingkat kesensitifitasan dari grafik pengendali tersebut. Kemudian metode tersebut dibandingkan dengan grafik pengendali EWMA. Setelah membandingkan kedua grafik diperoleh kesimpulan bahwa grafik pengendali EWMA nonparamterik lebih sensitif dalam mendeteksi pergeseran rata-rata proses yang kecil.","Product quality is the factor that affect someone satisfaction. One of tools that can  control the product quality which met the target is Control Chart. Control charts can quickly detect quality problems in production processes such as unexpected causes or small process shifts. The shift in the process will be a reference in process control. One of control chart used to detect small shifts is Exponentially Weighted Moving Average (EWMA) with the assumption of normality. However, in certain case there is data that the assumption of normality is not fulfilled. Thus, to handle the process shift with that case, Nonparametric Exponentially Weighted Moving Average (EWMA) method will be applied. Nonparametric EWMA  method will be the focus discussion. This thesis will discuss about Nonparametric EWMA method based on Average Run Length (ARL) with Markov Chain approach to get sensitive level of the control chart. Then, that method compared with the EWMA control chart. After the comparison, it can be concluded that nonparametric EWMA control chart is more sensitive in detecting small average process shift.","Kata Kunci : Grafik Pengendali, Pergeseran rata-rata proses, ARL, Rantai Markov, EWMA, EWMA Nonparametrik"
http://etd.repository.ugm.ac.id/home/detail_pencarian/167535,I PERBANDINGAN ROBUST LIU ESTIMATOR DAN ROBUST RIDGE REGRESSION MENGGUNAKAN ESTIMATOR LEAST TRIMMED SQUARE UNTUK MENGATASI MULTIKOLINEARITAS DAN HIGH LEVERAGE POINT,"NANDA NOOR H A, Dr. Herni Utami, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan metode yang digunakan untuk mengetahui ada tidaknya pengaruh hubungan antara variabel independen (predictor) dengan variabel dependen (penjelas) sehingga nilai dari variabel dependen bisa diprediksi. Metode yang sering digunakan untuk mengestimasi koefisien regresi adalah Ordinary Least Square. Analisis dengan metode ini baik digunakan apabila asumsi klasiknya terpenuhi, salah satunya tidak ada multikolinearitas. Adanya multikolinearitas menyebabkan estimasi parameter menggunakan Ordinary Least Sqaure menjadi kurang baik. Selain itu, adanya high leverage point menyebabkan variansi pada data menjadi lebih besar sehingga mengakibatkan kesalahan pengambilan keputusan dan kesimpulan.  Untuk mengatasi masalah multikolinearitas dan high leverage point secara bersamaan, digunakan penggabungan metode antara regresi ridge maupun estimator Liu dengan menggunakan estimator pada regresi robust, yaitu Least Trimmed Square (LTS). Selanjutnya, akan dilakukan perbandingan metode antara Robust Liu Estimator dan Robust Ridge Regression dengan estimator LTS menggunakan Mean Square Error, Akaike Information Criterion, dan Bayesian Information Criterion.","Regression analysis is a method used to determine whether there is an effect of the relationship between the independent variable (predictor) and the dependent variable (explanatory) so that the value of the dependent variable can be predicted. The method often used to estimate the regression coefficient is Ordinary Least Square. Multicollinearity causes parameter estimation using Ordinary Least Square becomes less accurate. In addition, high leverage points cause greater variance in the data resulting in error in decision making and conclusions.  To simultaneously overcome the problem of multicollinearity and high leverage points, a combination between ridge regression and Liu estimator is used by adding a robust regression estimator, namely Least Trimmed Square (LTS). Furthermore, a comparison of methods between Robust Liu Estimator and Robust Ridge Regression will be made with the LTS estimator using the Mean Square Error, Akaike Information Criterion, and Bayesian Information Criterion.","Kata Kunci : Multicollinearity, high leverage point, ridge regression, Liu estimator, robust, LTS estimator."
http://etd.repository.ugm.ac.id/home/detail_pencarian/132465,GRAFIK PENGENDALI CUMULATIVE-SUM NONPARAMETRIK (CUSUMNP) : ANALISIS PADA PERUBAHAN PROPORSI PROSES,"FADILLA MUKTIAWATI, Yunita Wulan Sari, S.Si., M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Kualitas merupakan faktor penting yang mempengaruhi seseorang untuk menggunakan suatu produk. Dalam proses produksi akan ada gangguan yang mempengaruhi kualitas yang kemudian menyebabkan pergeseran rata-rata proses. Pergeseran rata-rata proses dapat dideteksi dengan grafik pengendali. Grafik pengendali variabel yang dapat mendeteksi pergeseran rata-rata proses yang kecil pada data berdistribusi normal adalah grafik pengendali cumulative-sum (CUSUM). Pada kenyataannya, tidak semua data berdistribusi normal. Oleh karena itu, akan digunakan metode yang disebut grafik pengendali CUSUM nonparametrik (CUSUMNP). Grafik pengendali CUSUMNP adalah metode untuk mendeteksi pergeseran rata-rata proses yang kecil pada data yang tidak berdistribusi normal. Pada grafik pengendali CUSUMNP memantau pergeseran rata-rata proses ekuivalen dengan memantau perubahan proporsi proses. Untuk mengetahui dan membandingkan performa grafik pengendali CUSUM dan CUSUMNP, pada penelitian ini digunakan metode Average Run Length (ARL) dan Extra Quadratic Loss (EQL). Kemudian dibandingkan antara ARL1 dan EQL kedua grafik. Dari studi kasus, diperoleh ARL1 dan EQL dari grafik pengendali CUSUMNP nilainya lebih kecil dibandingkan CUSUM. Sehingga disimpulkan bahwa grafik pengendali CUSUMNP lebih sensitif dan efisien dalam mendeteksi pergeseran rata-rata proses yang kecil dibandingkan CUSUM.","Quality is an important factor that influences people to use a product. In the production process there will be a disruption that affects the quality then cause the process mean shift. The process mean shift can be detected by the control chart.The variable control charts that can detect small process mean shifts in normally distributed data are the cumulative-sum (CUSUM) control charts. In fact, not all data is normally distributed. Therefore, a method called a nonparametric CUSUM control chart (CUSUMNP) will be used. CUSUMNP control chart is a method for detecting a small process mean shift to the not normally distributed data. In the CUSUMNP control chart monitoring small process mean shifts is equivalent to monitoring small changes in process proportion. To know and compare the performance of control charts CUSUM and CUSUMNP, in this study used the method Average Run Length (ARL) and Extra Quadratic Loss (EQL). Then compared between ARL1 and EQL both graphs. From the case study, ARL1 and EQL obtained from the CUSUMNP control charts are smaller than CUSUM. It is concluded that the CUSUMNP control charts are more sensitive and efficient in detecting small mean shifts process than CUSUM.","Kata Kunci : Grafik pengendali, pergeseran rata-rata proses, perubahan proporsi proses, CUSUM, CUSUMNP, ARL, EQL, control chart, mean shifts process, changes in process proportion."
http://etd.repository.ugm.ac.id/home/detail_pencarian/159622,PENERAPAN REGRESI LOGISTIK MULTINOMIAL TERBOBOTI GEOGRAFIS,"RULLY FITRIA NURMALITA SARI, Prof. Dr. Sri Haryatmi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Model regresi logistik multinomial terboboti geografis merupakan pengembangan dari model regresi terboboti geografis yang mana variabel dependennya mempunyai kategori lebih dari dua. Model ini digunakan untuk menangani data yang dipengaruhi faktor spasial khususnya keragaman spasial sehingga estimasi parameter dilakukan pada setiap lokasi observasi. Fungsi kernel yang digunakan dalam pemilihan bandwidth optimum diantaranya fungsi bisquare, gaussian, dan tricube. Sebuah studi kasus menggunakan satu set variabel independen yang diambil dari faktor demografis dianggap mempunyai pengaruh terhadap tingkat pertumbuhan penduduk kabupaten/kota di Provinsi Jawa Barat. Dari analisis regresi logistik multinomial terboboti geografis dihasilkan 26 model di setiap lokasi observasi yang masing-masing terdiri dari model logit 1 dan model logit 2. Selain itu diketahui bahwa variabel independen yang signifikan berbeda-beda di setiap lokasi. Hal tersebut menunjukkan sifat lokal pada model regresi logistik multinomial terboboti geografis. Model regresi logistik multinomial klasik dikalibrasi untuk dijadikan pembanding. Variabel yang berpengaruh terhadap tingkat pertumbuhan penduduk secara global yaitu angka migrasi masuk.","The geographically weighted multinomial logistic regression model is the development of the geographically weighted regression model in which the dependent variable has more than two categories. This model is used to handle data that is influenced by spatial factor especially spatial variation so that parameter estimation is calibrated at each location of observation. The kernel function that is used in optimum bandwidth selection is bisquare, gaussian, and tricube function. The case study using a set of independent variables derived from a demographic factor is considered to have an influence on population growth rates at region/city of West Java Province. From the geographically weighted multinomial logistic regression analysis generated 26 pairs of models in each location of observation, each consisting of logit 1 and logit 2. Other than that, it is known that significant independent variables differed in each location. It indicates local properties in the geographically weighted multinomial logistic regression model. Classical multinomial logistic regression models are calibrated for comparison. Variables that affect the population growth rate globally is the number of incoming migration.","Kata Kunci : Regresi Logistik Multinomial Terboboti Geografis, Bandwidth, Data Spasial / Geographically Weighted Multinomial Logistic Regression, Bandwidth, Spatial"
http://etd.repository.ugm.ac.id/home/detail_pencarian/164487,INFERENSI BERDASARKAN ESTIMATOR HC5M UNTUK MODEL REGRESI LINEAR DENGAN HETEROSKEDASTISITAS DAN TITIK LEVERAGE TINGGI,"SITI LISNAYANTI, Prof. Dr. Sri Haryatmi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Metode estimasi Ordinary Least Square (OLS) sering digunakan pada analisis regresi. Dalam analisis regresi sering dijumpai adanya pencilan pada variabel independennya, pencilan pada variabel independen disebut high leverage poin atau titik leverage tinggi. Heteroskedastisitas merupakan salah satu penyimpangan asumsi OLS yang dapat menyebabkan inferensi dalam analisis menjadi menyesatkan. Jika bentuk heteroskedastisitas tidak diketahui, salah satu cara mengatasi penyimpangan tersebut yaitu dengan menggunakan White Heteroskedasticity-Consistent Covariance Matrix Estimator yang disebut dengan HC0. Estimator HC0 mengganti variansi eror dengan residual kuadrat. Namun jika secara bersamaan terdapat titik leverage tinggi, estimator HC0 tidak bisa diandalkan karena bersifat liberal, liberal artinya kecenderungan untuk menolak hipotesis nol. Oleh karena itu estimator matriks kovariansi HC5m dapat digunakan sebagai estimator alternatif lain dari estimator HC0. Dalam studi simulasi dan implementasi data APBD di Indonesia tahun 2016, estimator HC5m terbukti lebih andal dalam inferensi menggunakan uji quasi-t ketika heteroskedastisitas dan terdapat titik leverage tinggi.","Ordinary Least Square (OLS) is often used in regression analysis. In regression analysis, there are often found outliers in the independent variables, this outliers are called high leverage point. Heteroskedasticity is one of violation classic OLS assumption that can cause inference in the analysis to be misleading. If the form of heteroskedasticity is not known, one way to overcom these violation is by using the White Heteroskedasticity-Consistent Covariance Matrix Estimator, called HC0. HC0 estimator replaces the error variance with squared residual of OLS. However, if simultaneously there is a high leverage point, HC0 estimator cannot be reliable because it is liberal, liberal means the tendency to reject null hypothesis. Therefore, HC5m covariance matrix estimator can be used as another alternative estimator of HC0 estimator. In the simulation study and implementation of Regional Revenue and Expenditure Budget data in Indonesia in 2016, HC5m estimator proved to be more reliable in inference using quasi-t test when heteroskedasticity and there are high leverage points.","Kata Kunci : regresi linear, Ordinary Least Square (OLS), heteroskedastisitas, uji quasi-t, titik leverage tinggi, Heteroskedasticity-Consistent Covariance Matrix Estimator (HCCME)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/157576,MODEL REGRESI POISSON-WEIGHTED EXPONENTIAL UNTUK MENANGANI OVERDISPERSI PADA DATA CACAH,"ANNAZ TRIO WARDHANA, Prof. Dr. Sri Haryatmi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Regresi Poisson sering digunakan untuk memodelkan data cacah, dan memiliki asumsi ekudispersi pada variabel respon, atau kesamaan antara mean dengan variansi. Pada praktiknya, sering terjadi pelanggaran terhadap asumsi ekuidispersi, yaitu variabel respon menunjukkan variansi yang lebih besar daripada mean, keadaan ini disebut overdispersi. Untuk mengatasi overdispersi, dibentuk model alternatif dari regresi Poisson yaitu regresi Poisson-Weighted Exponential (P-WE). Pada skripsi akan dibahas pembentukan model regresi P-WE serta penanganan overdispersi dengan menggunakan model ini. Model regresi P-WE akan dibentuk, lalu dibandingkan dengan model regresi Poisson untuk melihat model mana yang lebih baik dalam menangani data cacah dengan overdispersi. Estimasi parameter dari kedua model regresi ini akan dihasilkan melalui metode Maximum Likelihood Estimation (MLE). Dari studi kasus diketahui bahwa model regresi P-WE lebih baik dalam menangani data cacah dengan overdispersi daripada model regresi Poisson. Hal ini dapat dilihat melalui nilai AIC dan BIC dari model regresi P-WE yang lebih kecil daripada model regresi Poisson. Penanganan overdispersi dapat dilakukan dengan berbagai cara, salah satunya dengan model regresi P-WE. Oleh karena itu, perlu adanya penelitian yang membahas penanganan overdispersi dengan metode atau model lain.","Poisson regression is often used for modelling the count data, and has an assumption of equidispersion of the response variable, in the other words, the similarity between mean and variance. In practice, there is often a violation of the assumption of equidispersion, i.e. the response variable shows variance greater than the mean, this is called overdispersion. To handle overdispersion, there is an alternative model of Poisson regression that is Poisson-Weighted Exponential (P-WE) regression. In this thesis, will be discussed forming the P-WE regression model and overdispersion handling by using this model. The P-WE regression model will be formed and then compared with the Poisson regression model to see which model is better handling count data with overdispersion. The parameter estimation of these two regression models will be generated by Maximum Likelihood Estimation (MLE). From the case study, it is known that the P-WE regression model is better at handling the count data with overdispersion than Poisson regression models. This can be seen through the values of AIC and BIC on the P-WE regression model that is smaller than Poisson regression model. Handling overdispersion can be done in various ways, one of them is the P-WE regression model. Therefore, a research about handling overdispersion with other models or other methods is needed.","Kata Kunci : Distribusi Campuran, Mixed Poisson, Overdispersi, Regresi Poisson, Regresi Poisson-Weighted Exponential"
http://etd.repository.ugm.ac.id/home/detail_pencarian/161162,ANALISIS KLASTER MENGGUNAKAN METODE CLARA PADA DATA YANG MENGANDUNG PENCILAN,"AHMAD BUKHARI MUSLIM, Dr. Gunardi, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Analisis klaster adalah metode statistika multivariat yang bertujuan untuk mengelompokkan objek-objek yang memiliki kemiripan karakteristik ke dalam suatu klaster. K-Means merupakan metode analisis klaster dengan menggunakan mean sebagai pusat klasternya. Namun, mean tidak robust terhadap adanya pencilan, sehingga algoritma K-Means sangat sensitif terhadap data yang mengandung pencilan. Untuk mengatasi hal tersebut, dapat digunakan metode k-medoids untuk mengelompokkan data yang mengandung pencilan. Medoid merupakan objek yang letaknya terpusat di dalam suatu klaster, sehingga robust terhadap adanya pencilan. Salah satu metode k-medoids yang popular digunakan adalah Clustering Large Application (Clara). Metode Clara juga baik digunakan pada data dalam jumlah besar. Pada analisis klaster, objek-objek dikelompokkan berdasarkan kemiripannya. Untuk mengukur tingkat kemiripan tersebut digunakan ukuran jarak, yaitu jarak Euclidean dan jarak Manhattan. Selanjutnya, untuk mengetahui kualitas hasil analisis klaster dilakukan uji validasi dengan silhouette width.  Metode analisis klaster terbaik untuk mengelompokkan film-film berdasarkan popularitas pada tahun 2009 sampai 2016 adalah metode Clara dengan jarak Manhattan. Dapat diketahui pula bahwa metode Clara dengan jarak Manhattan lebih robust dibandingkan metode K-Means dengan jarak Euclidean dan metode Pam dengan jarrak Manhattan untuk mengelompokkan data dengan pencilan dan data dalam jumlah besar.","Cluster analysis is a multivariate statistical methods to classify objects that have similar characteristics into a cluster. K-means is a clustering method using mean as its cluster center. However, mean is not robust to the presence of outliers, so k-means algorithm is sensitive for data with outliers. To overcome this problem, k-medoids methods can be used to classify data with outliers. Medoid is the most centrally located object in a cluster, so it's robust to outliers. One of the popular methods for k-medoids is Clustering Large Application (CLARA). CLARA method is also good to use for data with large quantities. In cluster analysis, the objects are grouped by the similarity. To measure the similarity, it can be used distance measures, Euclidean distance and Manhattan distance. Then, to determine the quality of the clustering results can be used validity index with silhouette width.    The best clustering method to classify the film based on popularity in 2009 until 2016 is CLARA with Manhattan distance. Furthermore, it can be concluded that CLARA with Manhattan distance is more robust than k-means with Manhattan distance and PAM method with Manhattan distance to classify data with outliers and data with large quantities.","Kata Kunci : k-medoids, Clustering Large Application, pencilan, silhouette width"
http://etd.repository.ugm.ac.id/home/detail_pencarian/164754,Kolektibilitas Kredit Menggunakan Algoritma Fast K-prototypes,"SHUFI ATIQOH SADIAH, Drs. Danardono, MPH., Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Kredit bermasalah atau macet dapat diselesaikan dengan beberapa strategi penyelesaian yang diberikan kepada debitur tergantung pada kolektibilitas kredit. Kolektibilitas kredit adalah penggolongan keadaan pembayaran pokok atau angsuran pokok dan bunga kredit oleh debitur yang dikelompokkan berdasarkan kelancaran atau ketidaklancaran pembayaran. Bank melakukan penggolongan kualitas kredit menjadi beberapa kategori, yaitu kredit lancar, dalam perhatian khusus, kurang lancar, diragukan, dan macet. Kondisi yang terjadi saat ini adalah untuk mengetahui kolektibilitas debitur yang melakukan kredit macet, pihak bank melakukan pengecekan satu persatu data debitur secara manual, sehingga tidak efisien dan bisa terjadi kesilapan. Oleh karena itu, dibutuhkan metode untuk kolektibilitas kredit menggunakan teknik pemodelan berdasarkan data kredit pada masa lalu, seperti analisis klaster. Data pembayaran kredit nasabah biasanya terdiri dari atribut numerik dan kategorik, sehingga diperlukan metode analisis klaster untuk data campuran numerik dan kategorik. Salah satu metode analisis klaster untuk data campuran adalah algoritma fast k-prototypes.  Algoritma fast k-prototypes memodifikasi algoritma k-prototypes dalam perhitungan jarak, karena tidak menghitung jarak antara objek dengan pusat klaster untuk seluruh atribut dalam data. Pada penelitian ini, terlebih dahulu ditentukan jumlah klaster optimal menggunakan silhouette width dengan membandingkan beberapa jumlah klaster. Selanjutnya dilakukan analisis klaster untuk kolektibilitas kredit menggunakan kedua algoritma tersebut. Untuk mengukur efisiensi dan performa dari algoritma k-prototypes dan fast k-prototypes, digunakan perbandingan kompleksitas waktu dan validasi internal.","Impaired credit or loss debt can be solved with various strategies given to the debtor depending on the credit collectibility. Credit collectibility is a grouping of the basic payment or basic installment and the credit interest from the debtor which grouped based on the payment. Credit quality classified by bank into categories which are, pass, special mention, substandard, doubtful, and loss. The recent condition is to get to know the credit freeze by debtor collectibility, bank will check each data of the debtor manually which will make it inefficient. Therefore, the credit collectibility method is needed by using modeling technique based on the former data credit such as cluster analysis. The data of client credit payment usually consist of numerical and categorical attributes that needed cluster analysis method for mixed data of numerical and categorical. One of the cluster analysis method for mixed data is fast k-prototypes algorithm.  Fast k-prototypes algorithm modified the k-prototypes algorithm on distance calculation because it did not calculate the distance between object and the main cluster for the whole attributes of the data. In this research, first, the amount of optimal cluster is determined using silhouette width and compared with some other amount cluster. Next, cluster analysis is done to credit collectibility using both algorithms. To calculate the efficiency and performance from k-prototypes algorithm and fast k-prototypes algorithm, time complexity and internal validation comparison were used.","Kata Kunci : kolektibilitas kredit, analisis klaster, fast k-prototypes, validasi internal, kompleksitas waktu"
http://etd.repository.ugm.ac.id/home/detail_pencarian/164755,Optimisasi Portofolio Dengan Metode Kataoka Safety First Dengan Kostrain Mean Return,"FIDELIS RESTU W, Dr. Abdurakhman, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Dalam pembentukan portofolio, investor berusaha untuk memaksimalkan mean return dengan tingkat risiko tertentu. Karya ini membahas pembentukan portofolio dengan metode Kataoka Safety-First dengan konstrain mean return. Kelebihan dari metode ini adalah investor dapat menentukan sendiri tingkat resiko yang ingin ia tanggung dan juga dapat menentukan sendiri tingkat mean return yang ingin ia dapatkan. Dalam pembentukan bobot portofolio ini digunakan suatu fungsi Lagrange dengan beberapa kendala yang selanjutnya akan diminimumkan untuk membentuk rumus bobot, benchmark return, dan mean return yang optimal. Terdapat dua kondisi dalam menentukan bobot, yaitu ketika nilai mean return yang dipilih investor kurang dari atau sama dengan dengan nilai B/C, atau dapat dikatakan investor hanya dapat menentukan sendiri tingkat resiko yang ingin dia tanggung, dan untuk kondisi kedua yaitu ketika nilai mean return yang dipilih investor lebih dari B/C, atau dapat dikatakan investor dapat menentukan tingkat resiko sekaligus mean return yang ingin dia dapatkan. Dalam studi kasus ini digunakan data harian dari 4 saham pada periode 1 Januari 2018 sampai 20 Juli 2018. Dari perhitungan didapatkan hasil bahwa nilai benchmark return pada kondisi kedua lebih besar daripada kondisi pertama, namun hal itu sejalan dengan tingkat resiko yang lebih besar juga yaitu 40%, sedangkan pada kondisi pertama tingkat resiko sebesar 30%. Setelah didapatkan bobot portofolio, selanjutnya di aplikasikan dengan membeli saham pada tanggal 20 Juli 2018, dan menjual kembali pada tanggal 30 Juli 2018, dan didapatkan keuntungan masing-masing sebesar    Rp 8.022.886,2 dan Rp 11.476.009,7.","In the formation of a portfolio, investors try to maximize the mean return with a certain level of risk. This work discusses portfolio formation with the Kataoka Safety-First method with the mean return constraint. The advantage of this method is that investors can determine their own level of risk they want to bear and can also determine their own level of return that they want to get. In forming this portfolio we use a Lagrange function with some constraints which will then be minimized to form the weight formula, benchmark return, and optimal mean return. There are two conditions in determining the weight, namely when the mean return chosen by the investor is less than or equal to the B / C value, or it can be said that the investor can only determine the level of risk he wants to bear, and for the second condition that is when the mean return the investor chooses more than B / C, or it can be said that the investor can determine the level of risk as well as the mean return he wants to get. In this case study, daily data is used from 4 shares in the period of January 1, 2018 to July 20, 2018. From the calculation, the result shows that the benchmark return value in the second condition is greater than the first condition, but it is in line with the greater risk level of 40 %, while in the first condition the risk level is 30%. After obtaining portfolio weight, then it is applied by buying shares on July 20, 2018, and reselling on July 30, 2018, and obtained profits of Rp. 8,022,886.2 and Rp. 11,476,009.7 respectively.","Kata Kunci : portofolio, downside risk, Kataoka Safety-First, mean return"
http://etd.repository.ugm.ac.id/home/detail_pencarian/161943,ALGORITMA MODIFIKASI NIPALS UNTUK REGRESI PARTIAL  LEAST SQUARE,"HIKMAH RAHMADANI, Prof. Drs. Subanar, Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Statistika  sering  digunakan  untuk  menentukan  hubungan  antara  sebuah  variabel  dependen  dengan  sebuah  variabel  independen  melalui  suatu  model   regresi.   Metode estimasi yang sering digunakan dalam model regresi linear adala metode  kuadrat terkecil. Multikolinearitas dalam model dengan estimasi kuadrat terkecil  menghasilkan parameter yang tidak efisien, sehingga diperlukan metode alternatif  lain yaitu regresi Partial least square  (PLS-R).     Model  Linear  PLS-R  dibangun  dari  hasil  reduksi  variabel  independen  X  yang  menghasilkan  variabel  baru  yang  disebut  komponen  utama.  Estimasi   parameter-parameter    yang    ada    pada    model  regresi  PLS  menggunakan   Algoritma  Modifikasi  Nonlinear  Iterative  Partial  least  squares  (NIPALS).  Algoritma  ini  merupakan  algoritma  modifikasi  dari  algoritma  standar  dengan  hanya  melakukan  deflasi  pada  variabel  dependen  Y  sehingga  proses  estimasi  parameter menjadi lebih cepat.  Algoritma  modifikasi  Nonlinear  Iterative  Partial  least  squares  (NIPALS)   digunakan  untuk  menentukan  parameter  dalam  regresi  Partial  least  square  .  Keefektifan  dari  metode  ini  akan  ditunjukkan  melalui  simulasi  dan  analisis  menggunakan data nyata yaitu jumlah uang beredar di Indonesia dan faktor-faktor  yang  mempengaruhinya  yaitu  aktiva    luar  negeri  bersih,  tagihan  bersih  kepada  pemerintah pusat, tagihan kepada pemerintah daerah, tagihan  kepada  lembaga   keuangan  lainnya,  tagihan  kepada  perusahaan  bukan  keuangan BUMN,  dan   tagihan  kepada  sektor  swasta.","Statistics is often used  to determine the relationship  between a dependent   variable  and  a  independent  variable  through  a  regression  models. Ordinary  least  squares  method  is  commonly  used  for  parameter  estimation  in  linear  regression. The presence of multicolinearity in ordinary least squares estimation  yields  inefficient  parameters,  so  the  other  method  such  as  Partial  least  square   Regression (PLS-R) proposed.   Partial  least  square    Regression  model    is    build    by    reducing    predictor   variable  X  that produces a new variable called the main component. Estimation  of parameters in the PLS regression model using the Modified Nonlinear Iterative  Partial  least  squares  Algorithm  (NIPALS).  This  algorithm  is  a  modification  algorithm of the standard algorithm by only deflating the dependense variable Y  so that the parameter estimation process becomes faster.  Modified NIPALS algorithm is used to determine the parameter in PLS-R.  Bayesian  procedure  is  used  by  adapting  currently  available  procedures.  The  effectiveness of this method will be demonstrated through simulation and analysis  using real data, i.e the money supply in Indonesia and the factors that influence  net assets, net bills to the central government, bills to local governments, bills to  other financial institutions, non-state finance companies, and bills to the private  sector.","Kata Kunci : Partial least square , Regresi Partial least square , Regresi ganda,  Multikolinearitas, Modifikasi NIPALS, Jumlah uang beredar."
http://etd.repository.ugm.ac.id/home/detail_pencarian/161944,GRAFIK PENGENDALI PROGRESSIVE MEAN (Studi Kasus : Pengendalian Kualitas Persen Massa Unsur Fe pada PT. Krakatau Steel),"DWI AJI WIDIANTORO, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2018 | Skripsi | S1 STATISTIKA,"Kualitas produk merupakan hal yang sangat penting bagi suatu perusahaan. Perlu adanya suatu teknik dan manajemen untuk memantau proses produksi agar tetap sesuai dengan target standar, yaitu pengendalian kualitas statistik. Salah satu alat yang dapat digunakan dalam pengendalian kualitas statistik adalah grafik pengendali. Grafik pengendali Shewhart biasa digunakan pada data dengan pergeseran rata-rata yang besar. Sedangkan pada data pegeseran rata-rata yang kecil, biasa digunakan grafik pengendali Cumulative-Sum (CUSUM) atau Exponentially Weighted Moving Average (EWMA). Terdapat alternatif grafik pengendali lain untuk menganalisis data dengan pergeseran yang kecil, salah satunya grafik pengendali Progressive Mean. Metode Progressive Mean akan menjadi fokus pembahasan pada skripsi ini. Untuk membandingkan tingkat kesensitifitasan grafik pengendali CUSUM, EWMA, dan Progressive Mean dapat dilihat pada hasil grafik pengendali dan digunakan Average Run Length (ARL). Dari perbandingan tersebut dan didukung oleh studi kasus diperoleh bahwa grafik pengendali Progressive Mean lebih sensitif dibandingkan grafik pengendali CUSUM dan EWMA.","Product quality is very important for a company. It needs a technique and management to monitor the production process in order to stay in accordance with the standard targets, namely statistical quality control. One tool that can be used in statistical quality control is the control chart. Shewhart control charts are used on data with large average shifts. While on the average data of the small averages, commonly used graphs controlling Cumulative-Sum (CUSUM) or Exponentially Weighted Moving Average (EWMA). There are alternate charts of other controllers to analyze data with small shifts, one of which is the Progressive Mean control chart. Progressive Mean method will be the focus of discussion on this thesis. To compare the sensitivity of the control charts CUSUM, EWMA, and Progressive Mean can be seen in the graph of the controller and used Average Run Length (ARL). From the comparison and supported by case study it is found that Progressive Mean control chart is more sensitive than CUSUM and EWMA control charts.","Kata Kunci : grafik pengendali, Progressive Mean, Average Run Length, ARL"
http://etd.repository.ugm.ac.id/home/detail_pencarian/166050,PERAMALAN DATA MENGGUNAKAN SINGULAR SPECTRUM ANALYSIS DENGAN METODE PERAMALAN LINEAR RECURRENT FORMULA,"SENANG UKURTA TARIGAN, Dr. Abdurakhman, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Statistika sering digunakan untuk meramalkan suatu data pada periode selanjutnya. Peramalan yang sudah dikenal selama ini adalah peramalan dengan menggunakan data stasioner terhadap mean dan variansi. Metode yang sudah banyak dibahas adalah metode ARIMA untuk data yang mengandung trend dan SARIMA untuk data yang mengandung trend dan musiman. Jika data tidak memenuhi asumsi maka akan dilakukan differencing data. Pada tulisan ini akan membahas metode singular spectrum analysis, metode ini adalah salah satu metode peramalan data nonparametrik, artinya adalah tidak memiliki asumsi stasioner terhadap mean dan variansi. Metode ini melibatkan Singular Values Decomposition dalam melihat karekteristik dari data yang ada. Dalam peramalannya akan menggunakan linear recurrent formula (LRF). LRF ini sendiri memiliki kinerja pemanggilan fungsi sebelumnya sehingga akan selalu melibatkan data sebelumnya. Studi kasus yang di angkat pada tulisan ini menggunakan data total penggunaan listrik di Yogyakarta periode Januari 2015 hingga Juli 2018 (dalam bulanan). Menggunakan metode SSA, MAPE yang diperoleh adalah sebesar 2,20% yang mana kurang dari 10%, artinya bahwa metode SSA baik digunakan untuk data total penggunaan listrik di Yogyakarta.","Statistical methods are commonly used to predict data for the next period. The forecasting technique which has been widely known so far is forecasting by stationary data on mean and variance. The methods which have been generally discussed are the ARIMA method for data containing trends and the SARIMA method for seasonal data and trend data. If those assumptions are not fulfilled, the differencing method will be demonstrated. This paper described the methodology of singular spectrum analysis (SSA). This method is known as one of the nonparametric forecasting methods meaning that there is no stationary assumption in mean and variance. This method involves Singular Values Decomposition in viewing characteristics of the existing data. linear recurrent formula (LRF) will be demonstrated in the forecasting method. The LRF method itself features the previous function calls performance so that it always involves the use of prior data. The case study raised in this paper related to the data on total electricity consumption in Yogyakarta from the period of January 2015 to July 2018 (in monthly terms). By using the SSA method, MAPE is obtained the value of 2.20% which is less than 10% value. In summary, the ARIMA method is the best forecasting method for total electricity consumption data in Yogyakarta.","Kata Kunci : Singular Spectrum Analysis, peramalan, Singular Values, Decomposition, Linear Recurrent Formula"
http://etd.repository.ugm.ac.id/home/detail_pencarian/155043,POHON REGRESI DENGAN PENDEKATAN GENERALIZED UNBIASED INTERACTION DETECTION ESTIMATION (GUIDE) UNTUK DATA MULTIRESPON,"MUHAMMAD ILHAM MUBAROK, Drs. Danardono, M.PH., Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Pohon regresi dapat digunakan untuk memprediksi objek atau sampel berdasarkan beberapa variabel prediktor. Pohon regresi merupakan salah satu teknik decision tree yang dibentuk melalui algoritma penyekatan secara rekursif. Perbedaannya dengan regresi klasik adalah pada metode pohon regresi pengaruh variabel bebas serta pendugaan responnya dilakukan pada kelompok-kelompok pengamatan yang ditentukan berdasarkan variabel-variabel bebas, sehingga interpretasi hasil dari metode ini lebih mudah. CART merupakan algoritma yang paling umum dalam membentuk pohon regresi. Namun algoritma ini memiliki kelemahan yaitu adanya bias dalam seleksi variabel. Bias dalam seleksi variabel ini dapat menghasilkan struktur pohon yang terlalu besar atau terlalu kecil serta dapat mengaburkan pentingnya variabel. GUIDE merupakan algoritma yang berusaha menangani bias dalam seleksi variabel dengan mengganti metode satu langkah CART menjadi dua langkah yaitu pemilihan variabel kemudian menemukan titik split dari variabel yang terpilih. GUIDE dapat juga diterapkan pada data multirespon. Pada studi kasus digunakan data tentang energy performance of building (EPB) yang akan digunakan untuk membentuk pohon regresi multirespon. Diperoleh pohon regresi multirespon dengan 15 simpul dan 8 simpul terminal serta variabel bebas yang membentuk pohon adalah Overall Height, Glazing Area, Wall Area, dan Relative Compactness.","Regression trees can be used to predict objects or samples based on several predictor variables. The regression tree is one of the decision tree techniques that is formed through recursive reconciling algorithms. The difference with the classical regression is the regression tree method of the influence of independent variables and the estimation of the response is done to the observation groups determined based on the independent variables, so interpretation of the results of this method is easier. CART is the most common algorithm in forming regression trees. But this algorithm has a weakness that is the bias in the selection of variables. Bias in this variable selection can result the tree structures that are too large or too small and may obscure the importance of variables. GUIDE is an algorithm that tries to handle bias in variable selection by substituting one step CART method into two steps ie variable selection then find split point of selected variable. GUIDE can also be applied to multirespon data. In the case study, we use data on energy performance of building (EPB) to form multirespon regression tree. So that, provided multirespon regression tree with 15 node and 8 terminal nodes and predictor variables that make up the tree are Overall Height, Glazing Area, Wall Area, and Relative Compactness.","Kata Kunci : Pohon Keputusan, Pohon Regresi, GUIDE, Uji Chi-square, Multirespon"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132267,PENENTUAN HARGA OPSI BELI TIPE EROPA MENGGUNAKAN MODEL TRUNCATED GRAM-CHARLIER EXPANSION,"ANDRIANTO MAULANA, Dr. Abdurakhman, M.Si",2018 | Skripsi | S1 STATISTIKA,"Opsi merupakan instrumen finansial derivatif dari saham yang memberikan peluang investor untuk mendapatkan keuntungan yang tinggi dengan risiko yang rendah. Black dan Scholes (1973) mengembangkan suatu model penentuan opsi yang menggunakan asumsi praktis return saham berdistribusi normal dan volatilitas konstan. Pada kenyataannya, bursa saham seringkali melakukan pembatasan terhadap pergerakkan perubahan harga harian asetnya yang menyebabkan return saham tidak lagi berada dalam interval (-infinity, infinity), akan tetapi terpotong di atas dan di bawah. Selain itu, terdapat return saham yang tidak berdistribusi normal yang mengimplikasikan perlunya mempertimbangkan skewness dan kurtosis pada aset saham yang mendasari. Model truncated Gram-Charlier expansion dikembangkan sebagai solusi dari masalah tersebut. Metode aproksimasi yang digunakan adalah pendekatan alternatif dengan polinomial hermite. Model truncated Gram-Charlier expansion merupakan pengembangan dari model dasar Black-Scholes, Truncated Black-Scholes dan ekspansi Gram-Charlier. Selanjutnya, dilakukan perbandingan antara harga opsi yang diperoleh dengan model truncated Gram-Charlier expansion dan ketiga model dasarnya tersebut terhadap harga opsi di pasar. Dengan menggunakan SRPE (Squared Relative Price Error) sebagai kriteria penentuan harga opsi. Hasilnya menunjukkan bahwa model truncated Gram-Charlier expansion lebih baik dibandingkan dengan ketiga model dasarnya.","Option is a derrivative financial instrument of stock that gives investors the opportunity to earn high profits with low risk. Black and Scholes (1973) developed an option pricing model which impractical assumptions including constant volatility of stock return and normal distributin of return. In fact, the stock market often restricts the movement of the daily price change of its assets, causing the stock return to be no longer within the (-infinity, infinity) interval, but truncated above and below. Besides that, there is a stock return that is not normally distributed which implies the need to consider skewness and kurtosis on the underlying asset return. Truncated Gram-Charlier expansion model was developed as a problem solving. Approximation method of Gram-Charlier expansion uses alternative approach of hermite polynomial. Truncated Gram-Charlier expansion model is the development of the basic model Black-ScholesÂ¸ Truncated Black-Scholes and Gram-Charlier expansion. Futhermore, we compare the option price obtained by truncated Gram-Charlier expansion and these three basic models with option market price. Using SRPE (Squared Relative Price Error) as criterion of option pricing, the result demonstrates that truncated Gram-Charlier expansion model performs better than these three basic models.","Kata Kunci : Option Pricing, Black-Scholes, Truncated Distribution, Gram-Charlier Expansion, Hermite Polynomial, Truncated Gram-Charlier Expansion"
http://etd.repository.ugm.ac.id/home/detail_pencarian/164789,Estimasi Value at Risk dengan Pemodelan GARCH-GEV Menggunakan Metode Estimasi Probability Weighted Moments,"PINASTHI RENANINGTIAS, Dr.Gunardi, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Saham merupakan salah satu investasi yang banyak diminati investor, namun seringkali memiliki resiko kerugian yang tinggi. Dalam upaya untuk meminimalisir kerugian dalam investasi, perlu dilakukan pengukuran risiko. Value at Risk (VaR) merupakan salah satu ukuran risiko finansial yang sering digunakan dalam pengukuran risiko khususnya pada perdagangan saham. Distribusi data return saham umumnya memiliki sifat fat tailed dan leptokurtic, dimana memiliki kecenderungan lebih besar terjadinya kejadian ekstrim dan kasus heteroskedastisitas. Pada penelitian ini, sebelum menghitung nilai VaR, data return yang umumnya bersifat heteroskedastik terlebih dahulu dimodelkan dengan GARCH (1,1) dengan inovasi distribusi Student-t. Selanjutnya, kejadian ekstrim dimodelkan dengan pendekatan Generelized Extreme Value Distribution (GEV).  Dalam penelitian ini, dilakukan fitting distribution atau melakukan estimasi parameter distribusi GEV. Metode estimasi parameter yang populer adalah Maximum Likelihood, namun karena tidak terbentuk closed form, maka dibahas metode lain yakni metode Probability Weighted Moments (PWM). Data yang digunakan dalam studi kasus adalah saham harian PT.Pembangunan Perumahan (Persero) Tbk pada periode 26 Februari 2018 hingga 13 Juli 2018. Diperoleh kesimpulan nilai VaR yang dihasilkan dari model GARCH-GEV dengan metode estimasi PWM memiliki validitas yang lebih baik daripada model GARCH-GEV metode estimasi MLE pada tingkat signifikansi 1%, 5% dan 10%.","Stock is one of investments that used by investor but often has high risk. As an attempt to minimize losses in investment, we need to calculate risk measurement. Value at Risk (VaR) is one of financial risk measures that is often used in risk measurement, especially in stock trading. Distribution of stock returns data are generally fat tailed and leptokurtic, which has tendency to have extreme events and cases of heteroscedasticity. In this study, the heteroskedastic return data is modeled with GARCH (1,1) with Student-t distribution innovation before calculating VaR. Furthermore, extreme events are modeled with the Generalized Extreme Value Distribution (GEV) approach. This study requires fitting distribution to estimate the GEV distribution parameters. The popular parameter estimation method is Maximum Likelihood, yet it is not formed closed form, so another method of Probability Weighted Moments (PWM) is discussed.  Data that used in this case study is a daily closing of PT.Pembangunan Perumahan (Persero) Tbk in the period of February 26, 2018 to July 13, 2018. In conclusion, the VaR value that calculated from the GARCH-GEV model with the PWM estimation method has better validity than the GARCH-GEV model with the MLE estimation method at a significance level of 1%, 5% and 10%.","Kata Kunci : Probability Weighted Moments, Generalized Extreme Value Distribution, Value at Risk, Fitting Distribution, GARCH."
http://etd.repository.ugm.ac.id/home/detail_pencarian/164026,Valuasi Program Dana Pensiun Imbalan Pasti dengan Pengembangan Iuran Dana berdasarkan PSAK 24,"MICHAEL VIDIANTO DWI, Dr. Adhitya Ronnie Effendie, M. Sc.",2018 | Skripsi | S1 STATISTIKA,"Program dana pensiun sudah menjadi hal yang diwajibkan bagi seluruh perusahaan untuk semua karyawannya yang merupakan hak dari seorang karyawan. Hal ini terbukti dari bahwa peraturan program dana pensiun sudah diatur secara hukum dan memilki undang-undangnya sendiri di berbagai negara. Program Dana Pensiun sendiri dibagi menjadi imbalan pasti dan iuran pasti. Pada skripsi ini, secara khusus akan dibahas mengenai dana pensiun untuk imbalan pasti yang sebagian didanai. Dana yang ada yaitu kontribusi pada periodenya dikelola oleh lembaga independen yakni Dana Lembaga Pensiun Keuangan. Selanjutnya akan dibahas mengenai valuasi pada masa sekarang terkait dengan manfaat dana pensiun di masa depan berdasarkan perhitungan serta asumsi aktuarial. Dalam pembahasan ini, akan dibentuk tabel multiple decrement berdasarkan associated single table decrement yakni meninggal, pensiun, cacat, dan mengundurkan diri. Secara akuntansi, valuasi ini diatur dalam Pernyataan Standar Akuntansi Keuangan no 24 tentang imbalan kerja dari Ikatan Akuntan Indonesia. Dalam perhitungan aktuarial terkait dengan dana pensiun akan dicari nilai sekarang manfaat masa depan (present value future benefit), nilai jasa kini (current service cost), dan biaya jasa lalu (past benefit oblogation). Untuk metode valuasi digunakan metode Projected Unit Credit (PUC) yang mengakru secara prorata nilai sekarang manfaat masa depan menggunakan tahun jasa.","Pension plan is a responsibility for a company to their employees for their prosperity in their old age and as rights for their service. Pension plan also already has its legal law in Indonesia and also many countries. Pension plan is divided into defined benefit pension plan and defined contribution pension plan. In this undergraduate thesis, there will be a study specially about defined benefit pension plan with an accumulated contribution fund from their employee or companies. This accumulated contribution will be managed by an independent pension plan institution. After that, there will be a study about present value of future benefit based on actuarial calculation and assumptions. The multiple decrement tables will be formed by associated single decrement table. The decrements that pension plan used are death, disable, withdrawal, and pension. This valuation is based on Pernyataan Standar Akuntansi Keuangan number 24 about employee benefit from Ikatan Akuntan Indonesia. From PSAK 24, an accountant should find some values such as present value future benefit, current service cost, and past benefit obligation. Usually, those values are calculated by actuarial researcher. The method that researcher used is projected unit credit that accrued present value of future benefit according to years of service or total years of service.","Kata Kunci : projected unit credit, dana pensiun, imbalan pasti, PSAK 24"
http://etd.repository.ugm.ac.id/home/detail_pencarian/162491,Estimasi Expected Shortfall (ES) dengan Menggunakan Ekspansi Gram Charlier,"FARAH ADIBAH M, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Expected shortfall (ES) merupakan salah satu alternatif dalam pengukuran risiko finansial dalam manajemen risiko. Terdapat banyak metode untuk mengestimasi ES yang berkembang di dalam dunia finansial. Namun, sebagian besar metode mengasumsikan bahwa data yang digunakan mengikuti distribusi normal. Padahal dalam praktik, sangat sulit untuk menemukan data return finansial  yang berdistribusi normal, sehingga penggunaan metode dengan asumsi mengikuti distribusi normal tidak lagi akurat untuk digunakan. Pada skripsi ini akan dibahas mengenai ukuran risiko ES yang mengikuti distribusi Gram-Charlier dengan model yang digunakan adalah model ARMA(1,1)-EGARCH(1,1). ARMA(1,1) digunakan untuk memodelkan rata-rata kondisional sedangkan EGARCH(1,1) digunakan untuk mengatasi masalah data return yang leptokurtik, memiliki skewness yang menceng ke kiri dan masalah volatilitas clustering yang dapat ditemukan pada data return saham. Adapun saham yang digunakan dalam studi kasus sripsi ini adalah saham DBC, IWM, MTZ dan NOC. Kemudian hasil estimasi ES dengan pendekatan Gram-Charlier yang sudah didapatkan dibandingkan dengan hasil estimasi ES dengan pendekatan normal standar, diperoleh hasil bahwa memang estimasi ES dengan pendekatan Gram-Charlier mempunyai performa lebih baik dibandingkan dengan hasil estimasi ES dengan pendekatan normal standar untuk data finansial yang tidak berdistribusi normal.","Expected shortfall (ES) is one of the alternatives of finansial risk in risk management. There are many method for estimating ES in the finansial field. However, most of the method assume that data must be normally distributed. Whereas in practice, it is very difficult to find return finansial data that following normal distribution, so the use of that methods is no longer accurate to use. This undergaduate thesis will explain about estimating ES with Gram-Charlier expansion. The model that will be used is ARMA(1,1)-EGARCH(1,1), where  ARMA(1,1) is used for modelling conditional mean and EGARCH (1,1) is used for capturing leptocurtic, negative skewness and volatility clustering that can be found in stoack return data. Case investigation in this undergraduate thesis use several stocks, they are DBC, IWM, MTZ dan NOC. After estimating ES with Gram-Charlier expansion, then it will be compared with ES with Gaussian expansion. The result is proved that ES with Gram-Charlier expansion has better performance than ES with Gaussian expansion, for finansial data that not following normal distribution.","Kata Kunci : Expected shortfall, ARMA, EGARCH, Polinomial Herite, Gram-Charlier."
http://etd.repository.ugm.ac.id/home/detail_pencarian/162253,AKURASI UJI DIAGNOSTIK MENGGUNAKAN LUASAN BAWAH KURVA ROC SMOOTHED EMPIRICAL,"ZAKY NUR KUSMANTORO, Dr. Danardono, M.P.H., Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Kurva ROC adalah representasi grafis dari hubungan antara sensitivitas dan 1-spesifisitas. Dalam penelitian medis kurva ROC banyak digunakan untuk menggambarkan keakuratan diagnostik dan menentukan nilai cut-off yang optimal. Keakuratan diagnosis berasal dari area di bawah kurva ROC dan optimal cut-off digunakan untuk mengidentifikasi kondisi positif dan negatif dalam diagnosis. Banyak penelitian telah menggunakan kurva ROC dengan metode empiris untuk menggambarkan akurasinya. Kurva ROC empiris mempertahankan sifat-sifat dari fungsi distribusi empiris yang tidak bias terhadap distribusi teoritisnya, tetapi tidak sangat akurat ketika ukuran sampel kecil karena variabilitas dari luasan bawah kurva ROC. Metode lain diperlukan untuk mendapatkan kurva ROC yang cukup akurat untuk ukuran sampel yang kecil. Kemudian, kurva ROC smoothed empirical diusulkan untuk menangani akurasi dari kurva ROC dalam menggambarkan akurasi diagnostik, terutama untuk ukuran sampel yang kecil. Selanjutnya, dengan estimator smoothed empirical dapat memperoleh nilai sensitivitas dan 1-spesifisitas dalam membangun kurva untuk membentuk daerah di bawah kurva ROC menggunakan metode rotate ordinal graph. Setelah itu, area di bawah kurva ROC dapat diukur dengan menggunakan aturan trapesium.","The ROC curve is a graphical representation of the relationship between sensitivity and 1-specificity. In medical research widely used it for describing the accuracy of a diagnostic and determining the value of optimal cut-off. Accuracy of a diagnostic derived from area under ROC curve and optimal cut-off used for identifying positive and negative condition in a diagnostic . Many research had used ROC curve with empirical method for describing the accuracy. The empirical ROC curve retains many properties of the empirical distribution function that unbiased to the theoritical one, but it is not very accurate when small sample size due to the variability.  Other method are needed to obtain ROC curve that accurate enough for small sample size. Then, smoothed empirical ROC curve was proposed for handling  the accuracy regarding ROC curve in describing the accuracy of diagnostic, especially for small sample size. Furthermore, smoothed empirical estimator can obtain sensitivity and 1-specificity  to build the curve for shaping the area under ROC curve using rotate ordinal graph method. Afterwards, area under ROC curve can be measured by trapezoid rule.","Kata Kunci : kurva ROC, sensitivitas, spesifisitas, luasan bawah kurva, AUC, grafik ordinal dominance, smoothed empirical"
http://etd.repository.ugm.ac.id/home/detail_pencarian/159442,Optimisasi Portofolio dengan Metode Median Variance pada Data Terbatas,"ANINDYA FAUZIANIZAHR, Yunita Wulan Sari, S.Si., M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Pembentukan portofolio yang terdiri dari beberapa saham atau diversifikasi adalah salah satu cara meminimalisir risiko. Portofolio dapat dibentuk sebanyak-banyaknya dengan berbagai kombinasi saham yang terlibat, namun jika menginginkan keuntungan yang optimal investor harus memilih portofolio yang optimal. Markowitz memperkenalkan salah satu cara memperoleh portofolio yang optimal, yang dikenal dengan metode Mean Variance. Pada metode ini data yang digunakan harus memenuhi asumsi distribusi normal, tetapi pada kenyataannya data yang ada tidak selalu berdistribusi normal. Metode ini juga tidak dapat digunakan untuk data terbatas, karena matriks varian-kovarian yang terbentuk pada metode ini merupakan matriks singular. Pada skripsi ini akan dibahas pembentukan bobot portofolio optimal dengan metode Median Variance untuk data terbatas, yang merupakan perluasan dari metode Mean Variance yang diperkenalkan oleh Markowitz. Metode Median Variance ini tidak memerlukan asumsi normalitas data return serta dapat digunakan untuk data terbatas. Studi kasus dalam skripsi ini menggunakan data harga saham harian selama 1 bulan. Akan dibentuk portofolio dari  20 saham yang termasuk dalam sektor bahan tambang yang diharapkan akan memberikan keuntungan yang optimal dari metode Median Variance untuk data terbatas.","Forming a portfolio consisting of several stocks or diversification is one way to minimize risk. Portfolio can be formed as much as possible with various combinations of shares involved, but if investors want an optimal profit investors should choose an optimal portfolio. Markowitz introduced how to obtain an optimal portfolio, known as the Mean Variance method. In this method the data used must require the assumption of normal distribution, but in fact the existing data isnâ€™t always normally distributed. This method also can not be used for limited data, because the varian-covarian matrix that formed on this method is a singular matrix. This paper discussed about the formation optimum portfolio weights using Median Variance method for limited data, which is an extension of the method of Mean Variance introduced by Markowitz. The Median Variance method does not require the assumption of return normality and can be used for limited data. The case study in this paper uses daily stock price data for 1 month. 20 stocks of the mining sector will be formed a portfolio that is expected to give optimum profits by using this method.","Kata Kunci : Portofolio, Median, Median Variance"
http://etd.repository.ugm.ac.id/home/detail_pencarian/159444,GRAFIK PENGENDALI ROBUST BERDASARKAN MODIFIKASI PEMANGKASAN PADA STANDAR DEVIASI SEBAGAI ALTERNATIF UNTUK SHEWHART DAN MEDIAN ABSOLUTE DEVIATION (MAD),"LUH PUTU ARLIN SUMARTINI, Prof. Dr.rer.nat. Dedi Rosadi, S.Si, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Dalam proses pengendali kualitas statistik, grafik pengendali adalah alat yang paling baik digunakan untuk melihat penyimpangan proses dalam industri. Grafik pengendali yang paling sering digunakan adalah grafik pengendali Shewhart S dan xbar. Dimana grafik pengendali Shewhart S ini digunakan untuk  memantau variabilitas proses sedangkan grafik xbar digunakan untuk memantau atau mengontrol rata-rata proses berdasarkan asumsi dasar yang mendasari yaitu normalitas. Grafik robust S_trim dan xbar_trim juga dianggap kuat untuk mengendalikan variabilitas dan mengontrol rata-rata proses pada proses normal maupun non-normal. Grafik robust S_trim dan xbar_trim ini dilakukan dengan cara memotong data tertinggi dan terendah tergantung pada proses pemangkasan. Simulasi dilakukan dengan membangkitkan data random berasal dari normal standar untuk mengetahui pemangkasan minimal dan maksimal yang dapat dilakukan. Contoh numerik juga dilakukan untuk menggambarkan tampilan grafik robust S_trim dan xbar_trim dan membandingkannya dengan grafik Shewhart dan MAD. Dari hasil simulasi, grafik robust S_trim dan xbar_trim memiliki batas pengendali yang lebih kecil daripada grafik Shewhart dan Robust MAD yang menandakan tampilan dari grafik robust S_trim dan xbar_trim lebih baik. Selain itu dilakukan perhitungan nilai ARL untuk mengetahui performa grafik robust modiffied trimmed standar deviation yang dinamakan S_trim dan xbar_trim lebih baik daripada MAD dan Shewhart.","In the process of statistical quality control, the control chart is the best tool to view process aberrations in the industry. The most commonly used control charts are Shewhart S and xbar control charts. Shewhart S control chart is used to monitor process variability whereas xbar graph is used to monitor or control the process average based on the underlying basic assumption of normality. The robust S_trim and xbar_trim graphics are also considered powerful for controlling variability and controlling the average process in normal and non-normal processes. Robust graphics S_trim and xbar_trim is done by cutting the highest and lowest data depending on the trimming process. Simulations are performed by generating random data derived from normal standards to determine the minimal and maximum cuts that can be performed. Numerical examples are also performed to illustrate the robust view of S_trim and xbar_trim graphs and compare them with the Shewhart and MAD charts. From the simulation results, the robust graphics of S_trim and xbar_trim have smaller control limits than the Shewhart and MAD charts that indicate the appearance of the robust graphics S_trim and xbar_trim are better. In addition, an ARL value calculation was performed to determine performance of robust control chart based on modified trimmed standar deviation called S_trim and xbar_trim better than Shewhart and MAD.","Kata Kunci : Grafik Pengendali Robust, Pemangkasan, Trimmed Mean, Pergiliran Rata-Rata"
http://etd.repository.ugm.ac.id/home/detail_pencarian/162775,Distribusi Poisson-Lindley Untuk Menangani Overdispersi Pada Data Cacah,"ROSANITA, Drs.Zulaela, Dipl.Med.Stats.,M.Si",2018 | Skripsi | S1 STATISTIKA,"Distribusi Poisson merupakan salah satu distribusi yang sering digunakan untuk mengolah data cacah. Salah satu asumsi yang harus terpenuhi adalah ekuidispersi, yaitu nilai mean dan variansi memiliki nilai yang sama. Namun, dalam prakteknya sering didapatkan nilai variansi yang lebih besar dari nilai mean atau disebut dengan kondisi overdispersi. Keadaan overdispersi membuat penggunaan distribusi Poisson menjadi pilihan yang kurang tepat, karena akan menghasilkan estimator yang bias. Untuk mengatasi overdispersi, dibentuk distribusi alternatif baru dari distribusi Poisson, yaitu distribusi Poisson-Lindley. Distribusi Poisson-Lindley merupakan distribusi campuran baru hasil penggabungan distribusi Poisson dengan distribusi Lindley. Pada skripsi ini akan dibahas lebih lanjut tentang pembentukan distribusi Poisson-Lindley serta penanganan data cacah overdispersi dengan menggunakan distribusi ini. Estimasi parameter yang digunakan adalah metode Maksimum Likelihood. Distribusi Poisson-Lindley diaplikasikan pada data lahir mati Kabupaten Madiun Provinsi Jawa Timur pada tahun 2012-2014 dan 2016 serta dibandingkan dengan distribusi Poisson dan distribusi Negative-Binomial. Hasil analisis dengan menggunakan chi-square Goodness of Fit serta Akaike Information Criterion (AIC) dan Schwart Bayesian Criterion (SBC) terkecil menunjukkan model distribusi Poisson-Lindley lebih layak digunakan untuk menangani overdispersi pada data cacah.","Poisson distribution is one of the most commonly used distributions to process count data. One of the assumptions that must be met is the equisdispersion, ie have the same value for mean and variance. However, in practice it is often obtained the value of variance is greater than the mean value or called overdispersion conditions. The state of overdispersion makes the use of the Poisson distribution to be a less precise choice, since it will result in a biased estimator. To overcome the overdispersion, a new alternative distribution of the Poisson distribution, Poisson-Lindley distribution, is formed. The Poisson-Lindley distribution is a new mixed distribution of combinations of Poisson distributions with Lindley distribution. In this thesis will be discussed more about the formation of Poisson-Lindley distribution as well as the handling of overdispersion count data by using this distribution. Parameters estimation method that used are Maximum Likelihood Estimation. Poisson-Lindley distribution was applied to Madiun District of East Java's fetal dead data in 2012-2014 and 2016 and compared with Poisson distribution and Negative-Binomial distribution. The results of analysis by using chi-square goodness of fit and Akaike Information Criterion (AIC) and Schwart Bayesian Criterion (SBC) showed the smallest values, so model of Poisson-Lindley distribution has a better fit to be used to handle overdispersion on the count data.","Kata Kunci : Data Cacah, Distribusi Campuran, Distribusi Poisson, Distribusi Lindley, Distribusi Poisson-Lindley, Overdispersi./ Count Data, Compund Distribution, Poisson Distribution, Lindley Distribution, Poisson-Lindley Distribution, Overdispersion"
http://etd.repository.ugm.ac.id/home/detail_pencarian/167642,GRAFIK PENGENDALI ROBUST HOLT-WINTERS UNTUK MENGATASI DATA RUNTUN WAKTU YANG MENGANDUNG EFEK MUSIMAN,"ANTONIUS DICKY KURNIAWAN SUHARTO , Dr. Gunardi, M.Si. ; Vemmie Nastiti Lestari, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Grafik pengendali digunakan untuk mendeteksi perilaku menyimpang dalam suatu proses. Tujuan dari penggunaan grafik pengendali antara lain untuk memantau dan mengawasi suatu proses, mengurangi variabilitas proses, dan menaksir parameter proses. Menentukan batas pengendali adalah salah satu putusan penting yang harus dibuat dalam merancang grafik pengendali. Grafik pengendali dapat diterapkan dalam berbagai kasus data, salah satunya pada data runtun waktu. Secara umum, terdapat empat pola data runtun waktu yang lazim ditemui, yaitu pola horisontal, musiman, siklis, dan trend. Pola efek musiman merupakan pola yang sering dijumpai. Pola efek musiman terjadi bila nilai data dipengaruhi oleh faktor musiman. Salah satu metode forecasting yang dapat digunakan untuk data yang mengalami efek musiman adalah metode Holt-Winters. Metode Holt-Winters sering digunakan untuk forecasting data yang mengalami efek musiman karena memiliki sistematika forecasting yang sederhana. Metode Holt-Winters dapat digunakan untuk forecasting data yang mengalami efek musiman, namun metode ini masih kurang mampu mengatasi data yang mengandung outlier. Untuk mengatasi data yang mengandung efek musiman dan outlier, dapat digunakan metode Robust Holt-Winters. Metode Robust Holt-Winters mampu menganalisis penyimpangan observasi, atau outlier, dalam data runtun waktu, sehingga hasil forecasting yang diperoleh lebih akurat dan layak untuk digunakan sebagai nilai prediksi di masa yang akan datang. Kemudian, grafik pengendali digunakan sebagai koreksi dari hasil forecasting. Grafik pengendali perlu robust agar kehadiran outlier tidak mengganggu kinerja forecasting.","Control charts are used to detect deviant behavior in a process. The purpose of using control charts includes monitoring and supervising a process, reducing process variability, and estimating process parameters. Determining the control limit is one of the important decisions that must be made in designing control charts. Control charts can be applied in various cases of data, one of which is in time series data. In general, there are four patterns of time series data commonly encountered, namely horizontal, seasonal, cyclical, and trend patterns. The pattern of seasonal effects is a pattern that is often encountered. The pattern of seasonal effects occurs when data values are influenced by seasonal factors. One of the forecasting methods that can be used for data that experiences seasonal effects is the Holt-Winters method. The Holt-Winters method is often used for data forecasting that experiences seasonal effects because it has a simple forecasting system. Holt-Winters method can be used for data forecasting that experiences seasonal effects, but this method is still less able to handle data containing outliers. To overcome data that contains seasonal effects and outliers, Robust Holt-Winters method can be used. Robust Holt-Winters method is able to analyze observation deviations, or outliers, in time series data, so that the forecasting results obtained are more accurate and feasible to be used as predictive values in the future. Then, the control chart is used as a correction from forecasting results. Control charts need to be robust so that the presence of outliers does not interfere with forecasting performance.","Kata Kunci : grafik pengendali, Holt-Winters, robust, efek musiman, outlier / control chart, Holt-Winters, robust, seasonal effect, outlier"
http://etd.repository.ugm.ac.id/home/detail_pencarian/155101,ESTIMASI TIPE-M ROBUST UNTUK REGRESI SPLINE TERPENALTI DENGAN FUNGSI PEMBOBOT TUKEY DAN HUBER,"MUHAMMAD FUAD HASYIM, Yunita Wulan Sari, S.Si.,M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Regresi spline terpenalti adalah salah satu metode yang saat ini sering digunakan untuk smoothing noisy data. Model regresi spline terpenalti adalah alat statistik yang popular untuk masalah fitting kurva karena fleksibilitas dan efisiensi dalam kumputasinya. Pada regresi spline, estimasi kurva regresi dapat diselesaikan dengan Penalized Least Square. Namun metode estimasi ini rentan terhadap kehadiran pencilan, sehingga diperkenalkan metode estimasi-M robust terpenalti.  Oleh karena itu, metode estimasi kuadrat terkecil untuk regresi spline terpenalti diganti dengan metode estimasi-M robust yang mampu menangani kehadiran pencilan dalam data. Dengan tetap menjaga pembentukan model spline dan menjaga bentuk penalti, meskipun menggunakan estimator-M daripada estimator kuadrat terkecil, didapatkan metode estimasi yang robust dan cukup fleksibel untuk menangkap trend non-linear dalam data.  Dalam skripsi ini dibahas jugamengenai fungsi pembobotTukey dan Huber yang digunakan untuk menduga bentuk regresi nonparametrik, serta mempelajari juga bagaimana memilih secara robust parameter penalti ketika kemungkinan terdapat outlier pada data. Diberikan kriteria pemilihan parameter penalti robust berdasarkan generalized cross-validation. Data riil tentang lalu lintas internet digunakan untuk menggambarkan efektivitas prosedur.","Penalized regression splines are one of the currently most used methods for smoothing noisy data. Penalized spline regression models are a popular statistical tool for curve fitting problems due to their flexibility and computational efficiency. In spline regression, regression curve estimation can be solved by Penalized Least Square. However, this estimation method vulnerable to the presence of outliers. Therefore we proposed robust Penalized-M estimation method. Therefore, we replace the least squares estimation method for penalized regression splines by M-estimation method, that capable of addressing the presence of outlier in the data. By keeping the modeling of splines and by keeping the penalty term, though using M-estimators instead of least squares estimators, we arrive at an estimation method that is both robust and flexible enough to capture non-linear trends in the data. In this thesis, also discussed about the weighting functions of Tukey and Huber used to predict the form of nonparametric regression, and also learn how to choose robust penalty parameters when there may be outliers in the data. We propose a robust penalty parameter selection criteria based on generalized cross-validation. Real data on internet traffic is used to describe the effectiveness of procedures.","Kata Kunci : regresi nonparametrik, estimasi-M robust, regresi spline terpenalti, estimasi-M terpenalti,parameter pemulus, GCV, fungsi Tukey, fungsi Huber"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132584,GRAFIK PENGENDALI MIXED EXPONENTIALLY WEIGHTED MOVING AVERAGE-CUMULATIVE SUM (MEC) DALAM ANALISIS PENGAWASAN PROSES PRODUKSI,"MAULINDA, Dr. Abdurakhman, S.Si., M.Si.",2018 | Skripsi | S1 STATISTIKA,"Dalam proses produksi diperlukan adanya teknik dan manajemen untuk memantau hasil produksi. Dengan adanya proses pemantauan, proses produksi akan menghasilkan keluaran yang sesuai dengan target standar sehingga target kepuasan konsumen terjaga. Untuk itu perlu adanya aplikasi ilmu statistika yang berguna untuk menganalisis permasalahan kualitas produksi. Pengendalian kualitas statistik dapat dengan cepat menyidik terjadinya sebab-sebab terduga atau pergeseran proses. Analisa pergeseran proses tersebut akan menjadi fokus tindakan pembetulan dalam pengawasan produksi. Metode grafik pengendali yang akan dibahas kali ini adalah metode Cumulative-Sum (CUSUM), Exponentially Weighted Moving Average (EWMA), dan Mixed EWMA-CUSUM (MEC). Metode Mixed EWMA-CUSUM (MEC) akan menjadi fokus pembahasan. Mixed EWMA-CUSUM (MEC) adalah perpaduan antara metode Exponentially Weighted Moving Average dan Cumulative Sum. Perpaduan EWMA dan CUSUM bertujuan menambah kesensitifitasan grafik pengendali dalam mendeteksi kejadian out of control. Untuk membandingkan tingkat kesensitifitasan metode CUSUM, EWMA, dan MEC digunakan Average Run Length (ARL). Dari perbandingan nilai ARL diperoleh grafik pengendali MEC sebagai grafik pengendali dengan tingkat kesensitifitasan tertinggi dibandingkan dengan metode CUSUM dan EWMA. Hasil perbandingan tersebut didukung dengan studi kasus yang memberikan hasil analisis grafik Mixed EWMA-CUSUM lebih cepat mendeteksi adanya kejadian out of control dibandingkan dengan metode CUSUM dan EWMA.","In the process of production, technical and management are needed to monitor output is production. With the monitoring process, the production process will produce output in accordance with standard targets, and the satisfactions customer is maintained. So, we need application of statistics that is useful to analyze the production quality problem. Statistic quality control can quickly investigate the expected causes or shifting process. The analysis of process will be a focus corrective in production monitoring. Control charts methods that will be discussed is Cumulative Sum (CUSUM) method, Exponentially Weighted Moving Average (EWMA) method, and Mixed EWMA-CUSUM (MEC) method. The Mixed EWMA-CUSUM (MEC) method will be the focus of discussion. Mixed EWMA-CUSUM (MEC) is a combination of methods Exponentially Weighted Moving Average (EWMA) and Cumulative Sum (CUSUM). The combination of CUSUM and EWMA have purpose to increase the sensitivity of control chart in detecting out of control event. To compare the level of sensitifity from CUSUM, EWMA, and MEC methods is used Average Run Length (ARL). From the process comparison the value of ARL, we get that control chart with MEC method as graphic controller with highest level of sensitivity compared with CUSUM and EWMA methods. The result of comparison are supported by case studies that provide if the control chart with Mixed EWMA-CUSUM (MEC) method more quickly detect the out of control event instead of the CUSUM and EWMA methods.","Kata Kunci : grafik pengendali, cumulative sum, exponentially weighted moving average, mixed EWMA-CUSUM, CUSUM, EWMA, MEC, average run length, ARL."
http://etd.repository.ugm.ac.id/home/detail_pencarian/163309,Proyeksi Cadangan Klaim dengan Metode Munich Chain Ladder,"ISNAINI ANNISA WIDYATAMA, Dr. Adhitya Ronnie E., M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Cadangan klaim biasanya dihitung berdasarkan segitiga run-off. Metode yang sering digunakan dalam proyeksi cadangan klaim adalah Metode Mack Chain Ladder. Aktuaris menerapkan metode Mack Chain Ladder secara independen terhadap segitiga klaim yang dibayarkan dan klaim yang terjadi. Masalah yang sering muncul adalah proyeksi cadangan klaim memiliki gap atau selisih yang besar antara kerugian yang dibayarkan dengan kerugian yang terjadi. Lebih parahnya, proyeksi kerugian yang dibayarkan lebih besar dari kerugian yang terjadi. Metode Munich Chain Ladder memberikan solusi dengan memperhatikan korelasi antara kerugian yang dibayarkan dan kerugian yang terjadi dengan menggunakan rasio paid-incurred. Pada skripsi ini membahas mengenai proyeksi cadangan klaim dengan Metode Munich Chain Ladder dan membandingkan hasil proyeksi cadangan klaim antara Metode Munich Chain Ladder dengan Metode Mack Chain Ladder. Dengan Metode Munich Chain Ladder diperoleh proyeksi cadangan klaim dengan gap yang kecil antara kerugian yang terjadi dengan kerugian yang dibayarkan dan menghasilkan proyeksi cadangan klaim yang lebih baik dari pada Metode Mack Chain Ladder.","Claim reserving is usually calculated based on run-off triangle. The most used methods for the projection of claims reserve is the Mack Chain Ladder method. The actuaries apply the Mack Chain Ladder method independently to the paid claims and to the incurred claims triangles. The most common problem arises that the projection of claims reserve that have large gaps or differences between paid losses and incurred losses. Even worse, the projection based on paid losses are greater than the incurred losses. The Munich Chain Ladder method provides a solution regarding correlation between paid losses and incurred losses, using the paid-incurred ratios. This thesis discusses projection of claims reserve with the Munich Chain Ladder Method and compares the projected results from the Munich Chain Ladder Method with the Mack Chain Ladder Method. The Munich Chain Ladder Method derives projected reserve claims with a small gap between paid losses and incurred losses and produce a better projected reserve claim than the Mack Chain Ladder Method.","Kata Kunci : cadangan klaim, Munich Chain Ladder, segitiga run-off, Mack Chain Ladder, klaim yang dibayarkan, klaim yang terjadi, rasio P/I"
http://etd.repository.ugm.ac.id/home/detail_pencarian/166381,METODE K-MEDOIDS DENGAN ALGORITME CLARANS  PADA DATASET BESAR DENGAN PENCILAN,"ANASTASYA VIVIANA GUNAWAN, Prof. Dr. Sri Haryatmi, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"K-medoids merupakan metode analisis klaster yang digunakan untuk mengelompokkan objek-objek ke dalam beberapa kelompok berdasarkan kesamaan karakteristik dengan medoid sebagai pusat klasternya, di mana medoid merupakan objek yang memiliki jarak paling minimum, sehingga tangguh terhadap pencilan. Algoritme k-medoids yang digunakan dalam penelitian ini adalah Clustering Large Applications based on RANdomized Search (CLARANS), di mana CLARANS merupakan perbaikan dari algoritme Partitioning Around Medoid (PAM) dalam menangani dataset besar dan perbaikan dari algoritme Clustering Large Applications (CLARA) dalam meningkatkan kualitas klastering. Metode CLARANS menggunakan dua batasan yaitu numlocal, untuk membatasi iterasi dan maxneighbor, untuk membatasi neighbor pada suatu node. Pengelompokkan didasarkan pada ukuran jarak Euclidean dan Manhattan. Selanjutnya untuk mengetahui tingkat validasi digunakan silhouette width. Metode analisis klaster terbaik untuk mengelompokkan nilai ujian nasional SMA di Provinsi Jawa Barat, Jawa Tengah dan Jawa Timur pada tahun 2017 adalah metode CLARANS dengan jarak Manhattan, k=3, numlocal = 2 dan maxneighbor = 44. Pada studi kasus, dapat diketahui dengan nilai overall average silhouette width bahwa metode CLARANS lebih baik dari metode PAM dan CLARA.","K-medoids is a clustering method which used to cluster objects into several groups based on their characteristics similarity with the medoid as its center cluster, where medoid is an object that has the smallest minimum distance, which is robust to outliers. The k-medoids algorithm that used in this study is Clustering Large Applications based on RANdomized Search (CLARANS), where CLARANS is an improvement of the Partitioning Around Medoid (PAM) algorithm in handling large datasets and improvement of the Clustering Large Applications (CLARA) algorithm in improving quality of clusters. CLARANS method uses two parameters, named as numlocal, to limit the iteration and maxneighbor, to limit neighbors to a node. Clustering is based on Euclidean distance and Manhattan distance. Then, to determine the validation level used silhouette width as evaluation method. The best clustering method for classifying high school national exam scores in West Java, Central Java and East Java in 2017 is CLARANS method with Manhattan distance, k=3, numlocal = 2 and maxneighbor = 44. In the case study, can be known with the overall average silhouette width value that CLARANS method is better than PAM and CLARA methods.","Kata Kunci : k-medoids, Clustering Large Applications based on RANdomized Search, pencilan, dataset besar, silhouette width, Partitioning Around Medoid, Clustering Large Applications"
http://etd.repository.ugm.ac.id/home/detail_pencarian/163313,Perbandingan Antara Metode Kurva Bezier Dan Metode Loess Pada Penghalusan Estimator Hazard Kumulatif Untuk Data Tersensor Kanan,"ROBERTUS INDRAKURNIA, Dr. Adhitya Ronnie Effendie, M.Sc",2018 | Skripsi | S1 STATISTIKA,"Data tersensor kanan merupakan salah satu ketidaklengkapan data yang paling sering ditemukan dalam analisis data survival. Data tersensor kanan ini dapat terjadi ketika dalam suatu data survival terdapat observasi yang tidak mengalami event sampai waktu pengamatan selesai. 	 Metode Nelson-Aalen merupakan salah satu metode non parametrik untuk mengestimasi fungsi hazard kumulatif paling terkenal khususnya pada data tersensor kanan. Akan tetapi, fungsi hazard kumulatif yang dihasilkan merupakan fungsi diskrit, sehingga hanya dapat diketahui nilai estimasi hazard kumulatif pada waktu tertentu saja. Untuk mengatasi masalah tersebut, digunakan penghalusan untuk mengubah estimasi fungsi hazard kumulatif menjadi fungsi kontinu,sehingga dapat diketahui nilai estimasi pada setiap waktu. Pada skripsi ini, akan dibahas mengenai perbandingan dua metode penghalusan, yaitu metode kurva bezier dan metode regresi lokal (LOESS). Untuk memilih metode paling baik dalam melakukan penghalusan, digunakan indikator Mean Integrated Square Error (MISE). Metode yang menghasilkan nilai MISE paling kecil merupakan metode paling baik. Studi simulasi dalam skripsi ini menunjukkan bahwa kedua metode menghasilkan nilai MISE yang lebih kecil dibandingkan estimasi hazard kumulatif dengan metode Nelson-Aalen tanpa penghalusan pada data tersensor kanan.","Right censored data is one of data incompleteness most often found in survival data analysis. This right censored data can occur when in survival data there is observation that are not make event until the time of observation is complete. Nelson-Aalen method is one of most famous non parametric method to estimate cumulative hazard function especially for right censored data. But, the resulting cumulative hazard function is discrete function, so the value of cumulative hazard only be known at any given time. To resolve the issue, smoothing is used to transform the cumulative hazard function estimate become continue function so it can be known the value of estimation at all time. In this undergraduate thesis, will be discussed about comparison of two smoothing methods, that is bezier curve method and local regression method (LOESS). To choose the best method of smoothing, the Mean Integrated Square Error (MISE) is used. Method that results in the smallest MISE value is the best method. Simulation study in this undergraduate thesis shows both method result a smaller MISE value compared with cumulative hazard function by Nelson-Aalen method without smoothing for right censored data.","Kata Kunci : hazard kumulatif, nelson aalen, metode penghalusan, kurva bezier, regresi lokal, data tersensor kanan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132339,META ANALISIS MENGGUNAKAN RELATIVE RISK EFFECT SIZE UNTUK RANDOM-EFFECTS MODEL PADA DATA HUBUNGAN DIABETES MELITUS DAN KARDIOVASKULAR DI BEBERAPA NEGARA ASEAN,"M IFDHAL ZAKY ELYASA, Dr. Abdurakhman, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Meta analisis merupakan suatu metode yang menggabungkan beberapa penelitian dengan kriteria tertentu untuk mendapatkan satu kesimpulan dari berbagai kesimpulan dari penelitian-penelitian tersebut. Masalah yang seringkali dibahas dalam dunia kesehatan adalah penyakit kardiovaskular (stroke, serangan jantung, dan hipertensi) dengan salah satu penyebab utamanya ialah diabetes melitus. Akan dibahas tingkat risiko penyakit kardiovaskular jika terkena penyakit diabetes melitus di beberapa negara anggota Association of South East Asia Nations (ASEAN).	 Beberapa penelitian dikumpulkan dari portal jurnal EBSCO, Science Direct, dan ProQuest dengan dibatasi untuk tahun publikasi 2014-2017 dan dilakukan di negara anggota ASEAN. Penelitian tersebut dianalisis dengan relative risk (RR) effect size pada random-effects model, setelahnya dianalisis RR gabungan dengan menggunakan metode inverse variance. Pada akhir analisis, dilakukan analisis sub grup untuk mengetahui adanya faktor yang diantisipasi oleh peneliti yang menyebabkan perbedaan dalam pengambilan kesimpulan pada meta analisis. Pada penelitian yang dilakukan, didapatkan hasil bahwa seseorang mengidap penyakit diabetes melitus maka ia mempunyai risiko sekitar 1,6 kali lebih besar terkena penyakit kardiovaskular dibandingkan dengan seseorang yang tidak mengidap penyakit diabetes melitus. Analisis sub grup berdasarkan usia pasien diperoleh hasil bahwa tidak adanya perbedaan pengambilan kesimpulan antara sub grup pertama (usia sama dengan atau di bawah 64,6 tahun) dan sub grup kedua (usia di atas 64,6 tahun).","Meta-analysis is a method that combines several studies with certain criteria to get a conclusion from the conclusions of the studies. Problems often discussed in the world of health are cardiovascular disease (stroke, heart attack, and hypertension) with one of the main causes is diabetes mellitus. This thesis will discuss the risk of cardiovascular disease if exposed to diabetes mellitus in some member countries of the Association of South East Asia Nations (ASEAN). Several studies have been collected from the journal portal EBSCO, Science Direct and ProQuest with limited to the publication year 2014-2017 and conducted in ASEAN member countries. The research was analyzed by relative risk (RR) effect size in random-effects model, then combined RR was analyzed by using inverse variance method. At the end of the analysis, a sub-group analysis was conducted to determine the factors anticipated by the researcher that led to differences in the conclusions in the meta-analysis, so it can be seen whether there are differences in conclusions caused by these factors.  In research conducted, the results obtained that a person suffering from diabetes mellitus then he has a risk of about 1.6 times more affected by cardiovascular disease compared with someone who does not have diabetes mellitus. The sub-group analysis based on the age of the patients obtained the result that there was no difference in the conclusions between the first sub-group (age equal to or below 64.6 years) and the second subgroup (age above 64.6 years).","Kata Kunci : Meta analisis, relative risk effect size, metode inverse variance, kardiovaskular, diabetes melitus"
http://etd.repository.ugm.ac.id/home/detail_pencarian/156148,JACKKNIFED LIU ESTIMATOR UNTUK MODEL LINEAR DENGAN HETEROSKEDASTISITAS PADA ERROR,"INDAH RINI SETYOWATI, Dr. Herni Utami, M.Si.",2018 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis statistik yang digunakan untuk mengetahui hubungan antara variabel dependen (respon) dengan independen (prediktor), serta untuk memprediksi nilai variabel dependen melalui variabel independennya. Metode yang sering digunakan adalah Metode Ordinary Least Square, dengan beberapa asumsi klasik yang harus terpenuhi diantaranya tidak adanya multikolinearitas dan eror yang bersifat homoskedastisitas. Ketidakvalidan Metode Ordinary Least Square tersebut disebabkan oleh asumsi klasik yang tidak terpenuhi, yaitu adanya multikolinearitas dan eror bersifat heteroskedastisitas.  Pada skripsi ini akan dibahas mengenai metode estimasi parameter pada regresi linear menggunakan Metode Jackknifed Liu Estimator. Metode ini merupakan pengembangan dari Liu Estimator yang diperkenalkan oleh Liu Kejian (1993) dengan adanya heteroskedastisitas pada error. Untuk kasus heteroskedastisitas pada error dan multikolinearitas, estimator yang diperoleh dengan Metode Jackknifed Liu Estimator lebih baik daripada metode-metode sebelumnya, yaitu OLS, GLS, dan Liu Estimator. Pada skripsi ini, Metode  Jackknifed Liu Estimator diaplikasikan pada data uang beredar di Indonesia dan faktor-faktor yang mempengaruhinya dari Januari 2004 hingga September 2017.","A regression analysis is one of statistics analysis used to know the relationship between a dependent variable called response and  an independent variable called predictor, and to predict the dependent variable's value by its independent variable. A method which is frequently used is Ordinary Least Square Method, within some classic assumptions that have to be fulfilled such as no multicollinearity and homoscedastic error. The invalidity of Ordinary Least Square Method is caused by classic assumptions which are not fulfilled such as multicollinearity and heteroscedastic error. An estimation method in linear regression using Jackknifed Liu Estimator Method will be discussed in this thesis. The method is a development of Liu Estimator Method that is introduced by Liu Kejian (1993) with a heteroscedastic error. To solve the heteroscedastic error and the multicollinearity cases, the estimator obtained by Jackknifed Liu Estimator Method is better than previous methods, they are OLS, GLS, and Liu Estimator. In this thesis, Jackknifed Liu Estimator Method is applied on money circulating in Indonesia and the factors that influence it. The data used in this thesis is from January 2004 to September 2017.","Kata Kunci : multikolinearitas, heteroskedastisitas, ordinary least square, generalized least square, liu estimator, metode jackknife, jackknifed liu estimator, mean square error"
http://etd.repository.ugm.ac.id/home/detail_pencarian/165111,Penentuan Harga Opsi Beli Tipe Eropa Menggunakan Model Hull-White,"MARDIANA NUR WAHIDAH, Dr. Herni Utami, M.Si",2018 | Skripsi | S1 STATISTIKA,"Model Black-Scholes merupakan model penentuan harga opsi yang banyak digunakan. Model ini memiliki asumsi bahwa return saham berdistribusi normal dan volatilitas konstan. Namun, asumsi tersebut mendapat bantahan karena tidak sesuai dengan kenyataan yang ada di pasar, dimana volatilitas tidak konstan yaitu volatilitas memiliki kecenderungan turun dan pada suatu saat akan naik lagi. Maka dikembangkanlah model volatilitas stokastik untuk memprediksi perilaku dari volatilitas.  Model volatilitas stokastik mengganti volatilitas yang konstan dengan volatilitas yang mengikuti proses stokastik. Model volatilitas stokastik ini adalah model Hull-White untuk menentukan harga opsi beli tipe Eropa. Selain meggunakan asumsi volatilitas yang mengikuti proses stokastik, model Hull-White juga memiliki asumsi bahwa variansi bergantung pada waktu yang deterministik, jadi dengan mengganti variansi pada rumus Black-Scholes dengan rata-rata variansi selama usia opsi. Selanjutmya, dilakukan perbandingan antara harga opsi yang diperoleh dan model Hull-White dan model Black-Scholes dengan harga opsi di pasar. Dengan menggunakan Squared Relative Price Error (SRPE) sebagai kriteria penentuan harga opsi, didapatkan hasil yang menunjukkan bahwa model Hull-White lebih baik dibandingkan dengan model Black-Scholes.","Black-Scholes model is a model which widely used to determine the price of options. The assumption of this model is stock return are normally distributed and constant volatility. But, in fact this assumption is not confirmed with the market data, where the volatility is not constant that has a tendency to go down and will go up again at some point.  So, stochastic volatility model was developed to predict the behavior of the volatility. Stochastic volatility model replace the constant volatility by volatility to be a stochastic process. This volatility model is Hull-White model to price European call options. Beside has volatility to be a stochastic process assumption, Hull-White has assumption that variance depends on time in a deterministic way then one has only to replace the variance in the Black-Scholes formula with the average over the optionÃƒÂ¢Ã¯Â¿Â½Ã¯Â¿Â½s life. Furthermore, we compare the option price obtained by Hull-White model and Black-Scholes model with option market price. Using Squared Relative Pricing Error (SRPE) as the criterion of option pricing, the result demonstrates that Hull-White model performs better than Black-Scholes model.","Kata Kunci : harga opsi, Black-Scholes, Hull-White, volatilitas stokastik / option pricing, Black-Scholes, Hull-White, stochastic volatility"
http://etd.repository.ugm.ac.id/home/detail_pencarian/167672,ANALISIS KLASTER MENGGUNAKAN ALGORITMA CURE (CLUSTERING USING REPRESENTATIVES) UNTUK DATASET BESAR DENGAN PENCILAN,"PRIMA LESTARI HANIF, Dr. Danardono, M.P.H., Ph.D.",2018 | Skripsi | S1 STATISTIKA,"Metode clustering hierarki agglomerative merupakan metode analisis klaster yang pada mulanya menganggap setiap titik objek merupakan satu klaster sendiri, kemudian dikelompokkan berdasarkan kemiripan atau kesamaan karakteristik yang diukur menggunakan ukuran ketidaksamaan. Metode clustering hierarki agglomerative terbagi menjadi beberapa algoritma, diantaranya Single Linkage, Complete Linkage, Average Linkage, Wards Method, dan Centroid Method. Namun, metode-metode ini sangat sensitif terhadap pencilan sehingga tidak dapat bekerja dengan baik dalam menangani klaster yang berbentuk non spherical. Oleh karena itu, dalam penelitian ini akan dibahas mengenai algoritma Clustering Using Representatives (CURE) yang mengkombinasikan pendekatan metode berbasis centroid dengan metode berbasis semua titik yang representatif.  Dalam prosesnya, CURE tidak menggunakan semua titik yang ada pada data melainkan hanya beberapa titik perwakilan yang cukup mewakili bentuk klaster dan ketersebaran data, titik ini dinamakan titik representatif. Sehingga, CURE efektif digunakan untuk dataset berukuran besar. Selain itu, CURE juga menyusutkan titik-titik data yang tersebar menuju pusat klaster menggunakan sebuah faktor penyusutan untuk mengurangi efek buruk karena pencilan. Untuk mengetahui kualitas dan validitas dari klaster, digunakan validasi internal silhouette width.","Agglomerative hierarchical clustering method is a cluster analysis method that initially considers each point of the object is a cluster, then grouped based on the similarity of characteristics that measured using dissimilarity measures. Some popular methods of agglomerative hierarchical clustering are Single Linkage, Complete Linkage, Average Linkage, Wards Method, and Centroid Method. However, these methods are very sensitive to outliers, so they cannot work properly in non-spherical clusters. Therefore, in this work, we will discuss the Clustering Using Representatives (CURE) algorithm which combines the centroid-based method approach with all representative point-based methods. 	In the process, CURE does not use all the points of data but only a few points that can represent the shape of clusters and spread of data, this point is called a representative point. So, CURE is effectively used for large datasets. In addition, CURE also shrinks the scattered data points towards the center of the cluster using a shrinking factor to reduce the effect of outliers. To determine the quality and validity of the cluster, internal validation silhouette width was used.","Kata Kunci : analisis klaster, Clustering Using Representatives, faktor penyusutan, pencilan, dataset besar, silhouette width"
http://etd.repository.ugm.ac.id/home/detail_pencarian/132347,METODE CONVOLUTIONAL NEURAL NETWORK UNTUK PENGENALAN CITRA WAJAH,"NUR FADHILLAH RAMADHANI NST, Prof. Dr. Sri Haryatmi Kartiko, M.Sc.",2018 | Skripsi | S1 STATISTIKA,"Dunia tengah berada pada era digital, kecanggihan teknologi yang diciptakan oleh manusia untuk memudahkan berbagai tugas manusia sehari-hari terus mengalami perkembangan. Salah satu kecanggihan teknologi di era digital ini adalah pengenalan citra oleh mesin (komputer). Banyak peneliti dan perusahaan teknologi  yang mengembangkan teknik-teknik statistika untuk data citra. Deep learning yang merupakan salah satu metode pembelajaran statistika, mengenalkan metode Convolutional Neural Network (CNN) yang memiliki kinerja sangat baik dalam melakukan pengenalan citra. CNN dirancang menyerupai fungsi otak manusia dan memiliki kedalaman jaringan yang tinggi. Para peneliti telah membuktikan kinerja CNN dalam menafsirkan dunia visual dan mengenali berbagai jenis objek pada citra. Oleh karena itu, skripsi ini melakukan penelitian pengenalan citra terhadap 1000 citra wajah dari 10 orang dengan menggunakan tiga metode, yaitu CNN dengan menerapkan metode regularisasi dropout, CNN, dan Multilayer perceptron (MLP). Pengenalan citra wajah ini bertujuan untuk mengklasifikasikan data citra berdasarkan identitas, jenis kelamin, dan pekerjaan. Hasil penelitian menunjukkan bahwa metode CNN yang menerapkan dropout ke dalam jaringannya memiliki kinerja model yang paling baik jika dibandingkan dengan metode CNN dan MLP, karena memiliki akurasi pengenalan citra wajah paling tinggi. Namun, metode CNN tanpa dropout pun memiliki kinerja model yang baik untuk pengenalan citra wajah karena akurasinya juga cukup tinggi. Jadi, metode CNN sangat efektif untuk pengenalan citra. Selain itu, menerapkan dropout ke dalam CNN dapat meningkatkan kinerja model dalam melakukan tugas pengenalan citra.","The world has now in the digitalized era with many advanced technologies created by human in purpose to facilitate their works in daily basis. Image recognition by computers is one of the examples. Many researchers and technology companies developed statistical techniques for image data. Deep learning which is one of statistical learning methods, introducing Convolutional Neural Network (CNN) method which has very good performance in doing image recognition. CNN is designed to have the function similar to human brain and has a high network depth. The researchers have proven the performance of CNN in interpreting the visual world and recognizing many different objects in the image. Therefore, this undergraduate thesis is focusing on the research on image recognition for 1000 face images from 10 people by using three methods, i.e. CNN by implementing dropout regularization method, CNN, and Multilayer perceptron (MLP). The purpose of this face image recognition is to classify the image data based on the identity, gender, and occupation. The results show that CNN method which implemented dropout into its network has the best model performance compared to CNN and MLP methods because it has the highest accuracy of face image recognition. However, CNN method without dropout also has a good model performance for face image recognition as it has a quite high accuracy. Thus, CNN method is proved as very effective for image recognition, and implementing dropout into CNN could increase the model performance in doing image recognition task.","Kata Kunci : Convolutional Neural Network, citra wajah, dropout, klasifikasi, pengenalan citra"
http://etd.repository.ugm.ac.id/home/detail_pencarian/165117,PEMODELAN TOPIK DENGAN MENGGUNAKAN STRUCTURAL TOPIC MODEL PADA DATA REVIEW MASKAPAI GARUDA INDONESIA,"CRISTINA TIKA NATALIA, Dr. Herni Utami, S.Si., M.si.",2018 | Skripsi | S1 STATISTIKA,"Pada kumpulan data tekstual terdapat berbagai informasi seperti tanggal, nama penulis, gender dan lainnya, yang disebut sebagai metadata. Sering kali metadata tersebut ingin diikutsertakan sebagai kovariat dalam analisis text mining salah satunya yaitu pemodelan topik. Namun, analisis pemodelan topik pada umumnya tidak mampu mengakomodasi metadata tersebut. Oleh karena itu, dikembangkan suatu pemodelan topik yang disebut Structural Topic Model (STM) yang mampu mengatasi masalah tersebut. STM merupakan suatu pemodelan topik yang dikembangkan dari model Latent Dirichlet Allocation (LDA) dan Correlated Topic Model (CTM) yang telah dibahas pada skripsi sebelumnya. Analog dengan kedua model tersebut, estimasi parameter pada model ini menggunakan metode Bayesian dimana nilai estimasi diberikan melalui distribusi posterior dan menggunakan Variational Expectation Maximization (VEM) dalam menghitung estimasi distribusi posterior tersebut. Ada 2 komponen dalam model STM yaitu topical prevalence yang berfungsi untuk mengetahui seberapa sering topik dalam suatu dokumen didiskusikan dan topical content yang mengatur frekuensi suatu kosakata dalam masing-masing topik. Dalam skripsi ini, model STM akan diterapkan untuk data tekstual berupa kumpulan review yang diberikan oleh penumpang Garuda Indonesia pada www.airlinequality.com dimana jenis layanan yang disediakan oleh maskapai tersebut menjadi kovariatnya. Kemudian dilakukan perbandingan terhadap ketiga jenis layanan tersebut. Sehingga, selain topik dominan, hasil yang diperoleh dari pemodelan topik tersebut adalah penilaian yang diberikan penumpang Garuda Indonesia berdasarkan jenis layanan yang mereka gunakan relatif sama.","In the textual dataset, there is various information about that data such as date, author's name, gender, and many more. That kind of information is called metadata. Sometimes the metadata wish to be included as a covariate in the text mining analysis, one of that is topic modeling. However, the prior topic modeling analysis is not able to include that metadata. Therefore, a topic modeling called Structural Topic Model (STM) was developed to overcome this problem. STM is a topic modeling developed from the Latent Dirichlet Allocation and the Correlated Topic Model, which has discussed in the previous thesis. Analogous to those models, estimation of the parameter in this model using the Bayesian method where the estimated value is given by the posterior distribution and Variational Expectation Maximization (VEM) is used to calculate the estimated posterior distribution. Moreover, there is two component in this model, which are topical prevalence, to discover how often the topics are discussed in a document, and topical content, which controls the frequency of the terms in each topic. In this thesis, the STM will be applied to textual data which is a dataset of reviews given by Garuda Indonesia's passengers at www.airlinequality.com where the type of service provided by this airline becomes its covariate. Afterward, the result of each type of service will be compared. So, besides the dominant topic, the result obtained from this topic modeling is the reviews given by Garuda Indonesia's passengers based on the types of service they used are relatively similar.","Kata Kunci : pemodelan topik, Structural Topic Model, review, Variational Expectation Maximization, text mining"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114178,PENYELESAIAN MODEL POHON REGRESI EFEK CAMPURAN UNTUK MEMODELKAN DATA BERKLUSTER,"LARAS SEPTIARI, Prof. Dr. Sri Haryatmi K., M.Sc. ; Dr. Adhitya Ronnie E., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Mixed Effects Regression Tree (MERT) merupakan perpanjangan pohon regresi standar untuk data berkluster yang dapat menangani efek random. Metode yang diusulkan dapat menangani kluster tidak seimbang, memungkinkan observasi dalam kluster untuk dilakukan pemilahan dan dapat menggabungkan efek random dan kovariat pada level observasi. MERT menggunakan algoritma pohon regresi standar pada algoritma EM. Estimasi linier dari komponen efek tetap pada model linier efek campuran diganti dengan algoritma pohon regresi standar. Hasil simulasi menunjukkan bahwa performa MERT lebik baik daripada pohon regresi standar ketika menggunakan data dengan efek random. Dalam kasus data kemiskinan, model Mixed Effects Regression Tree (khususnya Random Intercept Tree Model) menghasilkan PMSE lebih kecil daripada pohon standar. Variabel yang berpengaruh pada model pohon standar adalah belanja daerah (X5). Sedangkan variabel pada model Random Intercept adalah PDRB (X1) dan belanja daerah (X5).","Mixed Effects Regression Trees (MERT) is an extension of standard regression trees for clustered data that can deal with random effects. The proposed method can handle unbalanced clusters, allows observations within clusters to be splitted and can incorporate random effects and observation level covariates. MERT using a standard tree algorithm on EM algorithm. Linear estimation of the fixed component in the linear mixed effects model is replaced by standard regression tree algorithm. The simulation results show that performance of MERT better than standard tree algorithm when using data with random effects. In case of poverty data, MERT (particularly Random Intercept Tree model) results PMSE smaller than standard tree. The influential variable in standard tree model is government expenditure (X5). Whereas variables in Random Intercept Tree model are PDRB (X1) and government expenditure (X5).","Kata Kunci : Data berkluster, Pohon Regresi, Efek Campuran, Algoritma EM"
http://etd.repository.ugm.ac.id/home/detail_pencarian/116995,Regresi Kuantil dengan Quasi Likelihood pada Data Longitudinal,"HANIFA IZZATI, Drs. Danardono, M.P.H., Ph.D. ; Rianti Siswi Utami, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Regresi kuantil adalah metode statistik yang dapat mengatasi keterbatasan regresi linear dalam menganalisis sejumlah data yang berbentuk lonceng tidak simetris dan regresi kuantil sangat berguna jika distribusi data tidak homogen. Regresi kuantil kemudian mulai diterapkan pada jenis data longitudinal, mengingat perannya yang mudah ditemukan pada berbagai bidang. Pada data longitudinal, terdapat beberapa pengamatan di setiap individu sehingga terdapat pola korelasi yang perlu diketahui.        Pada skripsi ini akan dilakukan analisis regresi kuantil dengan metode quasi likelihood yang diterapkan pada data longitudinal. Pada data longitudinal, penerapan regresi kuantil dilakukan dengan metode quasi likelihood sehingga dapat diketahui struktur korelasi antarpengamatan. Struktur korelasi yang dipilih adalah Autoregressive (AR) dengan alasan irit parameter dan lebih realistis dalam hal nilai korelasinya yang semakin meluruh seiring jarak antarpengamatan yang semakin jauh. Untuk memudahkan proses komputasi, metode penghalusan induced diterapkan guna mengatasi keterbatasan fungsi yang tidak diferensiabel sehingga dapat diimplementasikan dengan algoritma Newton Raphson. 	Studi kasus dalam skripsi ini membahas faktor apa saja yang mempengaruhi volume sel darah merah. Hasil estimasi regresi kuantil dengan struktur korelasi AR kemudian akan dibandingkan dengan regresi kuantil yang mengasumsikan independensi antarpengamatan. Selanjutnya diperoleh kesimpulan bahwa estimasi regresi kuantil dengan struktur korelasi AR lebih baik dari estimasi regresi kuantil yang mengasumsikan independensi antarpengamatan.","Quantile regression is a statistical methodology that can be used to overcome the  limitations of linear regression in analyzing data that is not symmetric and useful if the distribution of data is not  homogeneous. Recently , in term of usage, quantile regression has been extended to the data such as longitudinal data, noting that it can be utilized in vary areas of study. In longitudinal data, there are particular observations in each subject, whose correlations between measures are incorporated. 	In this thesis the researcher will analyze quantile regression analysis with quasi likelihood method applied in longitudinal data. Quantile regression in longitudinal data is applied with quasi likelihood, thus the correlation structure between measures can be specified. The correlation structure used in this thesis is Autoregressive (AR), due to its characters of simplicity and realistic in which the correlation decreases exponentially in a simultanious way with the lags of the time points. In order to overcome the computational problem, induced smoothing method is applied in which can be implemented using Newton Raphson algorithm. 	The case study discusses the factors that affect the volume of red blood cell. The estimated result of quantile regression with AR correlation will be compared to quantile regression with independence assumption. Hence, it is found that quantile regression with AR correlation is better than quantile regression with independence assumption.","Kata Kunci : Regresi kuantil, Data longitudinal, Quasi likelihood, Korelasi / Quantile regression, Longitudinal data, Quasi likelihood, Correlation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/113927,Model Autoregresif Spasial dengan Dua Efek Dependensi Spasial,"RATNA RIZKIANA, Yunita Wulan Sari, S.Si., M.Sc. ; Widya Irmaningtyas, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Data spasial menyajikan observasi yang dihubungkan dengan titik region atau letak geografis, umumnya merupakan lokasi atau wilayah. Analisis regresi spasial merupakan analisis regresi untuk data yang di dalamnya mengandung unsur efek spasial. Spatial Autoregressive Model with Spatial Autoregressive Disturbances (SARAR) adalah salah satu spesifikasi model regresi spasial yang mengandung dua efek dependensi spasial yaitu spatial lag (respon) dan spatial error (disturbances). Efek dependensi ini dapat disebabkan oleh kedekatan satu lokasi dengan lokasi yang bertetanggaan. Sehingga  dapat dibuat matriks pembobot spasial berupa matriks ketetanggaan nxn berdasarkan metode queen contiguity yang dinormalisasi. Pemodelan SARAR didasarkan pada efek dependensi spasial, sehingga sebelum dilakukan pemodelan perlu dilakukan uji efek dependensi spasial menggunakan Uji Moransâ€™I dan Uji Lagrange Multiplier. Estimasi parameter untuk SARAR menggunakan Maximum Likelihood Estimation (MLE) yang memaksimumkan fungsi log-likelihood dari residual yang berdistribusi normal sehingga dihasilkan estimator yang konsisten bahkan dalam jumlah observasi yang besar. Berdasarkan hasil estimasi parameter SARAR menggunakan MLE, diperoleh faktor-faktor yang mempengaruhi tingkat kemiskinan tiap Kabupaten/Kota di Jawa Timur adalah rata-rata lama sekolah dan angka buta huruf di Kabupaten/Kota tersebut serta tingkat kemiskinan dan nilai error di Kabupaten/Kota tetangga.","Spatial data presents observation that are related to the point of region or geographical location, commonly known as a location or region. Spatial regression analysis is a regression analysis for data which contains spatial effect. Spatial Autoregressive Model with Spatial Autoregressive Disturbances (SARAR) is one of the spatial regression model specifications containing two spatial dependence effects namely spatial lag (response) and spatial error (disturbances). This dependence effect can be caused by the nearness of one location to near location. So we can make spatial weighted matrix into nxn neighborhood matrix based on normalized queen contiguity. SARAR modeling is based on the effect of spatial dependence, so before modeling, it is necessary to test the effect of spatial dependence using Morans'I Test and Lagrange Multiplier Test. The parameter estimate for SARAR uses the Maximum Likelihood Estimation (MLE) by maximizing the log-likelihood function of residuals that has normally distributed. MLE will resulting a consistent estimator even in large amounts of observation. Based on the estimation of SARAR parameter using MLE, the factors that influence the poverty rate of each regency in East Java are the mean of school duration and illiteracy rate from each regency and also the rate of poverty and error value from their neighbor.","Kata Kunci : Spatial, Moran's I, Lagrange Multiplier, SARAR, MLE."
http://etd.repository.ugm.ac.id/home/detail_pencarian/112136,PENERAPAN RESTRICTED LIU ESTIMATOR  DALAM MENANGANI MASALAH MULTIKOLINEARITAS PADA MODEL REGRESI LOGISTIK,"AJENG WIDI PANGESTI, Prof. Dr. Sri Haryatmi, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Regresi logistik merupakan salah satu model statistika yang digunakan untuk menganalisis hubungan antara variabel independen dengan variabel dependen, dimana variabel dependen bertipe kategorik atau kualitatif. Dalam regresi logistik, metode maksimum likelihood digunakan untuk mengestimasi parameternya. Namun, apabila terdapat informasi tambahan mengenai parameter ÃŽÂ² yang memenuhi pembatasan linear RÃŽÂ²=r maka metode estimasi yang seharusnya digunakan adalah metode estimasi terbatas. Pemodelan regresi logistik dengan pembatas linear tersebut menggunakan metode maksimum likelihood terbatas. Dalam regresi logistik,  asumsi yang harus dipenuhi adalah no multikolinearitas dan jika tidak terpenuhi maka estimasi parameternya akan jauh dari nilai seharusnya. Salah satu metode yang dapat menangani multikolinearitas yaitu estimator liu. Dalam pemodelan regresi dengan pembatasan linear, metode yang digunakan untuk menangani multikolinearitas adalah restricted liu estimator yang merupakan modifikasi estimator liu dengan mengganti estimator maksimum likelihood dengan estimator maksimum likelihood terbatas. Metode ini memberikan estimasi parameter dan mean square error yang lebih baik dari pada metode maksimum likelihood terbatas. Dalam skripsi ini, metode restricted liu estimator diaplikasikan untuk memodelkan faktor-faktor yang mempengaruhi risiko kredit macet di BPRS Khasanah Ummat Purwokerto.","Logistic regression is a statistical models were used to analyze the relationship between the dependent and independent variables, where the dependent variable type is categorical or qualitative. In logistic regression, maximum likelihood method used to estimate parameters. However, if there any additional information about ÃŽÂ² which satisfy the linear restriction RÃŽÂ²=r then the estimator should be used instead is a restricted estimator. This regression using restricted maximum likelihoos estimator. No multicollinearity is a assumption required in logistic regression and if there exists multicollinearity, parameter estimation would be badly apart from the actual coefficient. One of methods to overcome multicollinearity is liu estimator. In regression which satisfy the linear restriction, the methods used to handle multicollinearity is restricted liu estimator by modifying the restricted maximum likelihood estimator in liu estimator. This method give a better parameter estimation and mean square error than restricted maximum likelihood. In this thesis, restricted liu estimator was applied to analyze the factors that affect the credit risk in BPRS Khasanah Ummat Purwokerto.","Kata Kunci : Regresi Logistik, Pembatasan Linear, Multikolinearitas, Maksimum Likelihood Terbatas, Restricted Liu Estimator, Mean Square Error"
http://etd.repository.ugm.ac.id/home/detail_pencarian/108303,METODE KLASIFIKASI RANDOM K NEAREST NEIGHBOR (RKNN),"SINUNG JIWANGGA A, Yunita Wulan Sari, S.Si., M.Sc",2017 | Skripsi | S1 STATISTIKA,"INTISARI  METODE KLASIFIKASI RANDOM K NEAREST NEIGHBOR (RKNN)  Sinung Jiwangga Adi 09/283159/PA/12463   Data dimensi tinggi banyak tersedia di bioinformatika, kemometrika, perbankan dan aplikasi lainnya, sehingga kesuksesan analisis dan pemodelan data ini sangat menantang. Random K Nearest Neighbor (RKNN) terdiri dari serangkaian model berbasis k nearest neighbor, yang masing-masing diambil subset acaknya dari variabel input. Sebuah analisis teoritis dan empiris dilakukan terhadap kinerja RKNN. Berdasarkan RKNN yang diusulkan, dibangun sebuah metode seleksi fitur. Dalam meranking fitur yang penting dibuat kriteria yang dinamakan support, yang didefinisikan dan dihitung dari kerangka RKNN. Metode seleksi model dua tahap backward dikembangkan menggunakan support. Pendekatan RKNN dapat diaplikasikan untuk data respons kualitatif maupun kuantitatif, misalnya masalah klasifikasi dan regresi, serta aplikasinya dalam statistika, perbankan, machine learning, pengenalan pola, bioinformatika, dan lain-lain.  Kata kunci: klasifikasi, seleksi fitur, RKNN.","CLASSIFICATION USING RANDOM K NEAREST NEIGHBOR (RKNN)  Sinung Jiwangga Adi 09/283159/PA/12463  High dimensional data is widely available in bioinformatics, chemometrics, banking and other applications, so that the succesful analysis and modeling of these data is highly challenging. Random K Nearest Neighbor (RKNN) consist an ensemble of base k nearest-neighbor models, each taking a random subset of the input variables. A theoretical and empirical analysis of the performance of the RKNN is performed. Based on the proposed RKNN, a new feature selection method is devised. To rank the importance of the variables, a criterion, named support, is defined and computed on the RKNN framework. A two-stage backward model selection method is developed using supports. The RKNN approach can be applied to both qualitative and quantitative responses, i.e., classification and regression problems, and has applications in statistics, banking,  machine learning, pattern recognition and bioinformatics, etc.  Keywords: classification, feature selection, RKNN.","Kata Kunci : klasifikasi, seleksi fitur, RKNN"
http://etd.repository.ugm.ac.id/home/detail_pencarian/131856,PENGELOMPOKAN DATA OUTLIER MENGGUNAKAN METODE CENTROID LINKAGE,"AMIN SEPTIANINGSIH, Prof. Subanar, Ph.D",2017 | Skripsi | S1 STATISTIKA,"Analisi klaster merupakan pengelompokan sejumlah data atau objek yang memiliki kesamaan karakteristik dalam suatu klaster atau kelompok. Terdapat dua jenis metode dalam analisis klaster yang sering digunakan dalam pengelompokan data, yaitu hierarchical clustering dan partitioning clustering (non hierarchical clustering).   Pada penelitian studi kasus menggunakan analisis klastering sering ditemukan data yang tergolong outlier sehingga perlu kehatian mengambil keputusan perlakuan terhadap data outlier tersebut.  Data outlier pada suatu kondisi dan metode klastering tertentu akan mampu memberikan pengaruh kepada hasil klastering. Pada skripsi ini akan dibahas salah satu contoh metode hierarchical clustering yaitu centroid linkage yang memiliki karakteristik robust terhadap data outlier, berdasarkan perbandingan hasil metode ukuran jarak euclidean dan manhattan. Dalam analisis metode clustering diperlukan metode validasi yang digunakan untuk mengukur tingkat kevalidan hasil pengelompokan klaster. Dalam penulisan skripsi ini akan digunakan metode validasi coeffisient silhouette width untuk menentukan tingkat kevalidan klaster dan klaster terbaik  yang telah terbentuk. 	Sektor pariwisata merupakan sektor yang memiliki peran cukup potensial dalam pengaruh kontribusi peningkatan kondisi perekonomian di suatu Negara. Pada penulisan skripsi ini dilakukan analisis pengelompokan provinsi-provinsi di Indonesia berdasarkan kemajuan dan kesuksesan daya tarik sector pariwisatanya pada tahun 2014 untuk mengetahui pemetaan kondisi dan daya tarik dari sector pariwisata setiap provinsi. Kata kunci	: centroid linkage, jarak  euclidean, jarak  manhattan,, silhouette coefficient width","Cluster analysis is a multivariate statistical methods to classify objects that have similar characteristics into a cluster. The technique includes two methods, hierarchical clustering consists of divisive and agglomerative hierarchical clustering and partitioning clustering (non-hierarchical clustering). Furthermore, bottom-up approach is applied in agglomerative hierarchical method, starting in its own cluster and then pairing two similar clusters to reduce the number of clusters. This research was conducted using centroid linkage as a part of agglomerative hierarchical clustering. Centroid linkage which had robust characteristics to outlier data. To get the best clustering analysis result, a comparative analysis was done using Euclidean distance and Manhattan distance. Centroid linkage is a clustering method that determines cluster central point based on the mean of data in the same group and calculates each group centroid, before merges the groups within the nearest centroid distance. As in clustering method analysis, validity measurement of the clustering result is needed. In order to measure it, this research was using silhouette coefficient validity. Tourism holds significant importance in the economic development of oneÃƒÂ¢Ã¯Â¿Â½Ã¯Â¿Â½s country. This thesis was conducted by grouping provinces of Indonesia using tourism data in order to acknowledge the condition mapping and tourist attractions.  Keyword: centroid linkage, euclidean distance, manhattan distance, silhouette coefficient.","Kata Kunci : centroid linkage, jarak  euclidean, jarak  manhattan,, silhouette coefficient width"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129044,ALGORITMA  PARTICLE SWARM OPTIMIZATION (PSO) UNTUK ESTIMASI PARAMETER KURVA YIELD MENGGUNAKAN   NELSON SIEGEL SVENSSON,"ALYA PUSPITASARI, Dr. Gunardi, M.Si",2017 | Skripsi | S1 STATISTIKA,"Obligasi adalah salah satu instrumen investasi berpendapatan tetap. Keuntungan merupakan imbal hasil yang diterima investor sampai jatuh tempo yang disebut dengan yield to maturity. Analisis yang menjelaskan tentang hubungan yield to maturity dengan waktu jatuh tempo disebut dengan analisis struktur jangka waktu tingkat bunga atau term structure of interest rate. Struktur jangka waktu tersebut dapat digambarkan dengan grafik yang disebut dengan yield curve. Kurva yield ini memuat yield sebagai koordinat y dan waktu jatuh tempo sebagai koordinat x. Pada skripsi ini akan dibahas mengenai pembentukan atau pemodelan kurva yield dengan menggunakan model Nelson Siegel Svensson, yang dalam pembentukannya akan diestimasi menggunakan beberapa metode dan dipilih metode yang memiliki tingkat kesalahan prediksi dengan nilai yang paling kecil. Metode optimalisasi yang digunakan untuk mengestimasi parameter pada Nelson Siegel Svensson adalah algoritma Particle Swarm Optimization. Studi kasus pada skripsi ini menggunakan data obligasi pemerintah Indonesia seri FR dan Gabungan yang diperdagangkan pada tanggal 17- 21 Juli 2017. Hasil pemodelan diperoleh kesimpulan bahwa algoritma PSO dapat meminimumkan nilai RMSYE dan MAYE pada model Nelson Siegel Svensson.","Bonds are one of the fixed income investment instruments. Profit is the yield received by the investor to maturity called yield to maturity. The analysis that explains the relationship of yield to maturity with maturity is called the  term structure of interest rate. The time frame structure can be illustrated by a graph called a yield curve. This yield curve contains the yield as the y coordinates and the maturity time as x coordinates. In this paper we will discuss the formation or modeling of yield curves using Nelson Siegel Svensson model, which in its formation will be estimated using several methods and selected the method that has the prediction error rate with the smallest value. The optimization method used to estimate the parameters in the Nelson Siegel Svensson model is the Particle Swarm Optimization algorithm. The case study of this research uses Indonesian government bond FR data and bonds traded on July 17, 2017 until July 21, 2017. The modeling results obtained conclusions that the PSO algorithm can minimize RMSYE and MAYE values on the Nelson Siegel Svensson model.","Kata Kunci : Kurva Yield, Nelson Siegel Svensson, Particle Swarm Optimization"
http://etd.repository.ugm.ac.id/home/detail_pencarian/110614,Estimasi Value at Risk (VaR) Portofolio Multivariat Menggunakan Metode GARCH Student t - EVT - Vine Copula,"NADYA NIRVANDA KICHEN, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Value at Risk (VaR) adalah salah satu ukuran risiko finansial yang standar dalam manajemen risiko. Banyak metode estimasi VaR yang berkembang dalam dunia finansial, salah satunya adalah metode Variansi-Kovariansi. Namun, kebanyakan metode tersebut mengasumsikan return berdistribusi normal dan mengukur kebergantungan diantara saham dalam portofolio secara linear. Kenyataannya, data return umumnya bersifat fat tail dan leptokurtic. Kebergantungan antar saham yang non-linear juga tidak sesuai jika diukur dengan korelasi linear. Dengan demikian, estimasi VaR dengan metode konvensional ini tidak lagi akurat. Vine Copula adalah fungsi distribusi multivariat yang menggabungkan distribusi marginal return univariat dalam portofolio, sekaligus dapat menggambarkan struktur kebergantungan non-linear-nya. Adanya kebergantungan antar saham disebabkan oleh pergerakan ekstrim dari satu saham yang mempengaruhi harga saham lain. Kejadian-kejadian ekstrim ini perlu dimodelkan dengan pendekatan Generalized Pareto Distribution (GPD) untuk meminimalkan underestimate terhadap risiko. Karena GPD memerlukan asumsi data i.i.d., maka data return yang umumnya bersifat heteroskedastik terlebih dahulu dimodelkan dengan GARCH (1,1) dengan inovasi distribusi Student-t. Studi kasus skripsi ini menggunakan portofolio dari indeks saham JKSE, HSI, dan N225, yang dimodelkan dengan CD-Vine Copula dari kelas Eliptik dan Archimedean. Estimasi parameter copula menggunakan metode IFM berbasis maksimum likelihood. Copula terbaik untuk memodelkan data dalam studi kasus ini adalah copula Gumbel D-Vine. Hasil backtesting menunjukkan bahwa performa metode GARCH Student T - EVT - Vine Copula lebih baik dibandingkan metode Variansi-Kovariansi dalam mengestimasi VaR.","Value at Risk (VaR) is a standards measurement of financial risk in risk management. Many VaR estimation methods are developed in financial field, one of which is the variance-covariance method. However, most of these methods assuming normally distributed returns and measure the dependence between stocks in the portfolio using linear correlation. In fact, returns are generally fat tail and leptokurtic. Non-linear dependence between stocks also not appropriate when measured by linear correlation. Thus, the estimated VaR of this conventional method is no longer accurate. Vine Copula is a multivariate distribution function that combines distribution of univariate marginal returns in portfolio, as well as to describe the structure of non-linear dependence. The existence of dependence between stocks caused by extreme movement of a stock that affects another stock prices. These extreme events need to be modeled by Generalized Pareto Distribution (GPD) approach to minimize risk underestimation. Because of GPD requires assumptions i.i.d., so the returns that are generally heteroskedastic have to modeled by GARCH (1,1) with Student-t distribution innovation. The case study in this undergraduate thesis is using a portfolio of stock indexes of JKSE, HSI, and N225, that are modeled with CD-Vine Copula of Elliptic and Archimedean classes. Copula parameters are estimated using IFM that based on maximum likelihood estimator method. The best Copula that fit the data in this case study is Gumbel D-Vine Copula. Backtesting result shows that the performance of GARCH Student T-EVT-Copula Vine method is better than the variance-covariance method in estimating VaR.","Kata Kunci : Value at Risk, Vine Copula, Generalized Pareto Distribution, GARCH, Variance-Covariance."
http://etd.repository.ugm.ac.id/home/detail_pencarian/114203,Robust Jackknife Ridge Regression dengan Estimator MM untuk Mengatasi Multikolinearitas dan High Leverage Point,"AZIZ ARDIANSYAH, Prof. Dr. Sri Haryatmi, M.Sc.; Rika Fitriani, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis dalam statistika untuk menganalisa dan memodelkan hubungan antara variabel dependen (Y) dengan variabel independen (X). Secara umum, metode Ordinary Least Square atau metode kuadrat terkecil digunakan untuk mendapatkan estimasi koefisien regresi. Analisis dengan metode ini harus didasari terpenuhinya asumsi regresi klasik, salah satunya adalah tidak ada multikolinearitas. Jika terdapat multikolinearitas, estimasi parameter metode kuadrat terkecil menjadi kurang baik.  	Metode regresi ridge seringkali digunakan untuk menyelesaikan masalah multikolinearitas. Konsep dari regresi ridge ini adalah menambah tetapan k ke dalam matriks korelasi Z'Z. Namun regresi ridge memiliki nilai bias meskipun dapat menghilangkan multikolinearitas pada regresi linear ganda. Metode regresi jackknifed ridge merupakan metode yang dapat menghilangkan multikolinearitas serta menurunkan nilai bias akan tetapi tidak dapat digunakan untuk data yang mengandung high leverage point. Metode yang dapat digunakan untuk mengatasi high leverage point adalah regresi robust dengan estimator MM. Sehingga untuk mengatasi multikolinearitas dan high leverage point secara bersamaan dapat digunakan metode Robust Ridge Regression dan Robust Jackknife Ridge Regression berdasarkan estimator MM. Melalui nilai Mean Square Error, Akaike Information Criterion dan Bayesian Information Criterion diperoleh bahwa metode Robust Jackknife Ridge Regression dengan estimator MM lebih baik dibandingan dengan metode Robust Ridge Regression dengan estimator MM.","Regression analysis is an analysis for modelling a relation between dependent variable (Y) and independent variable (X). In general, ordinary least square method is used to obtain the estimation of regression coefficients. This method should be based on the fulfillment of classical regression assumptions, which one of them is no multicollinearity. If there is multicollinearity, the parameter estimation of ordinary least square method becomes deficient.  Ridge regression method often used to solve multicollinearity problem. The concept of ridge regression is adding a constant k to correlation matrix Zâ€™Z. But the ridge regression has a bias although it can eliminate multicollinearity in multiple linear regression. Jackknifed Ridge Regression method is a method that can handle multicollinearity and reduce bias but can not be used for data containing high leverage point. The method can be used to handle high leverage point is a robust regression based on MM estimator. So, to overcome multicollinearity and high leverage point simultaneously can use Robust Ridge Regression and Robust Jackknife Ridge Regression with MM estimator. Based on the value of Mean Square Error, Akaike Information Criterion and Bayesian Information Criterion, Robust Jackknife Ridge Regression is better than Robust Ridge Regression with MM estimator.","Kata Kunci : Multikolinearitas, high leverage point, regresi ridge, regresi jackknife ridge, robust, estimator MM"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114206,Model Kenaikan Linear dalam Mengatasi Data Hilang pada Analisis Data Longitudinal,"IMRON AMIRULLOH, Drs. Danardono, M.P.H, Ph.D. ; Rianti Siswi Utami, S.Si., M.Sc.,",2017 | Skripsi | S1 STATISTIKA,"Data merupakan sesuatu yang penting karena dari data dapat diambil sebuah informasi yang nantinya dapat berguna. Data dapat diperoleh dari sebuah penelitian atau dari sebuah survei. Salah satu masalah yang berkaitan dengan data adalah data hilang. Data hilang sering terjadi pada penelitian yang berbasis klinis atau dalam hal kesehatan. Salah satu penelitian dalam hal kesehatan yaitu penelitian longitudinal. Penelitian longitudinal merupakan penelitian yang pengamatannya dilakukan beberapa kali pada individu yang sama, sehingga membutuhkan waktu yang cukup lama sehingga data hilang dalam penelitian ini rentan terjadi. Ilmu statistika menawarkan solusi dalam penanganan data hilang, yaitu dengan imputasi atau mengisi data hilang dengan sebuah estimasi. Kenaikan linear merupakan salah satu metode yang digunakan untuk menangani data hilang pada data longitudinal. Kenaikan linear mempunyai prosedur yang sederhana namun cukup baik dalam mengimputasi. Metode ini baik digunakan untuk mengimputasi data variabel respon kontinu dengan interval waktu pengamatan yang sama. Ide dasar dari metode ini adalah kenaikan yang merepresentasikan perubahan nilai pengamatan tiap waktu atau proses perkembangan variabel respon yang terobservasi tiap waktunya. Kenaikan ini kemudian dimodelkan secara linear, sehingga metode ini disebut Model Kenaikan Linear. Mengestimasi nilai data hilang pada data longitudinal dengan model kenaikan linear dengan cara mengimputasi pada data yang hilang, yang dalam skripsi ini disebut imputasi Linear Increments â€“ Least Square (LI-LS). Model kenaikan linear memberikan hasil imputasi yang sangat baik dalam mengestimasi data hilang penelitian berat badan tikus galur wistar.","Data is very important because from data can be obtained information that useful. Data can be obtained from research or from survey. One of the problem that related to data is missing data. Missing data often happen in clinical research or in health sector. One of research in health sector is longitudinal research. Longitudinal research is research that observation taken in several times on the same individual, so missing data in this research often happen. 															Statistics solution for missing data can used imputation or fill the missing data with estimation. Linear Increments is method that can be used to handle missing data in longitudinal research. Linear Increments have a very simple procedure, however very good to imputate. This method good to used to imputate continous response variabel with interval of observation time is same. Basical idea from this method is increments, that represent the change of observation value from response variabel. This Increments than modeled as linear model, so its called Linear Increments Model. Estimate missing longitudinal data using Linear Increments can be used imputation approach, on this minithesis called Linear Increments-Least Square (LI-LS). Linear Increments Model give a good result of imputation from research data of mouse with wistar strain weight.","Kata Kunci : Data hilang, Data longitudinal, Kenaikan linear, Metode kuadrat terkecil, imputasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/131368,PEMODELAN GARCH-GPD DENGAN METODE ESTIMASI PARAMETER PROBABILITY WEIGHTED MOMENTS UNTUK MENGESTIMASI VALUE AT RISK (VaR),"REGINA HUSNUN NAZILA, Prof. Dr. Sri Haryatmi, M.Sc",2017 | Skripsi | S1 STATISTIKA,"INTISARI  Pemodelan Garch-Gpd dengan Metode Estimasi Parameter Probability Weighted Mmoments untuk Mengestimasi Value at Risk (VaR)  Oleh Regina Husnun Nazila 13/349496/PA/15510  	Value at Risk (VaR) adalah salah satu ukuran risiko finansial yang standar dalam menajemen risiko. Banyak metode estimasi VaR yang mengasumsikan return berdistribusi normal, kenyataannya, data return umumnya bersifat fat tail dan leptokurtic.  Pergerakan saham juga sangat fluktuatif sehingga dapat menghasilkan nilai-nilai ekstrim. Kejadian-kejadian ekstrim ini perlu dimodelkan dengan pendekatan Generelized Pareto Distribution (GPD) untuk meminimalkan underestimate terhadap resiko. Karena GPD memerlukan asumsi data i.i.d., maka data return yang umumnya bersifat heteroskedastik terlebih dahulu dimodelkan dengan GARCH (1,1) dengan inovasi distribusi Student-t. Selanjutnya tentu perlu dilakukan Fitting Distribution atau melakukan estimasi parameter distribusi, metode estimasi parameter yang populer adalah Maximum Likelihood, namun karna tidak terbentuk closed form, maka dibahas metode lain yakni metode Probability Weighted Moments (PWM).","ABSTRACT  Garch-Gpd Modelling Using Estimation Parameter Method Of Probability Weighted Moments to Estimate Value at Risk (VaR) by Regina Husnun Nazila 13/349496/PA/15510  	Value at Risk (VaR) is one of the standard financial risk measures in risk management. Many VaR estimation methods assume normal distributed returns, in fact, data returns are generally fatty and leptokurtic. The movement of stock is also very volatile so it can produce extreme values. These extreme events need to be modeled with the Generelized Pareto Distribution (GPD) approach to minimize the underestimate of risk. Since GPD requires assumption of i.i.d. data, then the heteroskedastic return data is first modeled with GARCH (1,1) with Student-t distribution innovation. Furthermore, Fitting Distribution is required or to estimate the distribution parameters, the popular parameter estimation method is Maximum Likelihood, but because it is not formed closed form, then discussed another method of Probability Weighted Moments (PWM) method.","Kata Kunci : Probability Weighted Moments, Generelized Pareto Distribution, Value at Risk, Fitting Distribution, GARCH."
http://etd.repository.ugm.ac.id/home/detail_pencarian/117291,Perbandingan Distribusi Stable dengan Model GARCH-GPD untuk Mengestimasi Value at Risk,"DANANG AKBAR RIANO, Dr.Gunardi, M.Si.",2017 | Skripsi | S1 STATISTIKA,"Sifat leptokurtik yang dimiliki data return saham menghadirkan tantangan tersendiri untuk para investor untuk mengelola modal investasinya. Dalam skripsi ini akan dibahas dua jenis pemodelan statistika yang mampu memodelkan data return saham dengan volatilitas dan sifat leptokurtik. Pertama adalah distribusi stable, distribusi ini memiliki kemampuan untuk memodelkan data dengan variansi tak hingga dan bersifat leptokurtik. Distribusi stable memiliki empat jenis parameter yaitu alfa (stability index), beta (skewness parameter), sigma (scale parameter), dan ÃŽÂ¼miu(location parameter). Densitas distribusi stable tidak terdefinisi secara umum sehingga dibutuhkan metode numerik untuk mengestimasi parameternya. Metode kuantil yang dikembangkan McCulloch merupakan salah satu pendekatan numerik yang dapat digunakan untuk parameterisasi distribusi stable. Kedua adalah model GARCH-GPD, model tersebut dikonstruksikan dengan model Generalized Autiregressive Conditional Heteroskedasticity (GARCH), dan Generalized Pareto Distribustion (GPD). GARCH adalah model runtun waktu yang digunakan untuk mengatasi heteroskedastisitas sebuah data, sementara GPD dimanfaatkan dalam memodelkan nilai ekstrim pada data.  Pada skripsi ini seluruh parameter distribusi model GARCH-GPD akan diestimasi menggunakan Maximum Likelihood Estimation (MLE). Pengukuran risiko sebuah aset Kedua model tersebut berikutnya dibandingkan performanya dalam mengestimasi risiko aset saham yang dinyatakan dalam ukuran Value at Risk (VaR).","The nature of leptokurtik owned stock return data presents a challenge for investors to manage their investment capital. In this thesis will discuss two types of statistical modeling that is able to model stock return data with volatility and leptokurtik properties. First is the stable distribution, this distribution has the ability to model data with unlimited variance and is leptokurtik. The stable distribution has four parameter types alfa (stability index), beta(skewness parameter), sigma (scale parameter), and miu (location parameter). The density of the stable distribution is not generally defined so that numerical methods are required to estimate the parameters. The quantitative method developed by McCulloch is one of the numerical approaches that can be used for stabilizing the distribution distribution. Second is the GARCH-GPD model, the model is constructed with Generalized Autiregressive Conditional Heteroskedasticity (GARCH) model, and Generalized Pareto Distribustion (GPD). GARCH is a time series model used to overcome the heteroskedasticity of a data, while GPD is utilized in modeling extreme values in the data. In this thesis all parameters of GARCH-GPD model distribution will be estimated using Maximum Likelihood Estimation (MLE). Measurement of an asset's risk The two models are subsequent to their performance in estimating the risk of stock assets expressed in the Value at Risk (VaR) measure.","Kata Kunci : Stable Distribution, GARCH-GPD, Quantile Methode, Maximum Likelihood Estimation, Value at Risk"
http://etd.repository.ugm.ac.id/home/detail_pencarian/131897,KOMPOSISI BOBOT PORTOFOLIO BERDASARKAN MEAN-VARIANCE-SKEWNESS-KURTOSIS,"ELISABETH KRISSINTIA, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Investasi adalah komitmen atas sejumlah dana atau sumber daya lainnya yang dilakukan saat ini, dengan tujuan memperoleh keuntungan di masa datang (Tandelilin, 2001). Poin penting dalam ilmu investasi adalah bagaimana seorang investor dapat meminimalisir risiko kerugian. Ada satu cara dalam investasi untuk meminimalisir kerugian investasi yaitu diversifikasi. Diversifikasi adalah usaha penganekaragaman yang dilakukan untuk memaksimalkan keuntungan sehingga arus kas dapat lebih stabil. Salah satu bentuk dari diversifikasi adalah optimasi portofolio. Portofolio adalah istilah keuangan yang merupakan kombinasi dari sekumpulan aset yang dimiliki oleh investor. Dalam pembentukan portofolio,  investor berusaha untuk memaksimalkan return dengan tingkat risiko tertentu. Dalam skripsi ini akan dibahas mengenai pembentukan bobot portofolio menggunakan optimasi momen yang lebih tinggi yakni mean-variance-skewness-kurtosis melalui model polynomial goal programming. Metode ini merupakan perluasan dari metode mean-variance yang dipelopori oleh Markowitz (1952).  Studi kasus penelitian menggunakan data saham harian pada periode 22 Juni 2017 hingga 22 September 2017 dari  saham-saham anggota Indeks LQ45.  Dari ke-45 saham akan dibentuk suatu portofolio yang diharapkan dapat memberikan keuntungan yang optimal dengan menggunakan metode ini.","Investment is a commitment to a number of funds or other resources currently undertaken, with the aim of gaining future benefits (Tandelilin, 2001). An important point in investment science is how an investor can minimize the risk of loss. There is one way in investment to minimize investment losses that is diversification. Diversification is an effort undertaken to maximize profits so that cash flow can be more stable. One form of diversification is portfolio optimization. A portfolio is a financial term that is a combination of a set of assets owned by an investor. In the formation of a portfolio, investors seek to maximize returns with a certain degree of risk. In this paper will be discussed about the formation of portfolio weight using higher moment optimization that is mean-variance-skewness-kurtosis through polynomial goal programming model. This method is an extension of the mean-variance method pioneered by Markowitz (1952). The case study used daily stock price data in the period from 22 June 2017 to 22 September 2017 from stock members of the LQ45 Index. Of the 45 stocks will be formed a portfolio that is expected to provide optimal benefits by using this method.","Kata Kunci : Portofolio, Mean-Variance-Skewness-Kurtosis, Polynomial Goal Programming"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114233,REGRESI COX DENGAN PENDEKATAN ESTIMASI PENALIZED MAXIMUM LIKELIHOOD,"AFAF HAIFA, Prof. Drs. Subanar, Ph.D.",2017 | Skripsi | S1 STATISTIKA,"Regresi Cox adalah salah satu model regresi yang sering digunakan untuk memodelkan suatu data survival dengan sensor. Bagaimanapun juga, pada prakteknya ada beberapa hal yang menyebabkan estimasi parameter untuk variabel Ãƒï¿½Ã‚Â¢ÃƒÂ¯Ã‚Â¿Ã‚Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½ variabel penjelas tidak bisa didapatkan menggunakan metode regresi Cox biasa. Salah satu kendala tersebut adalah adanya kejadian monotone likelihood yaitu keadaan dimana minimal ada 1 parameter yang nilainya divergen. Hal ini dapat diatasi dengan pendekatan metode estimasi penalized maximum likelihood yaitu dengan menghilangkan bias pertama orde pertama pada maximum likelihood estimation. Pada skripsi ini, menjelaskan prosedur estimasi parameter dari variabel Ãƒï¿½Ã‚Â¢ÃƒÂ¯Ã‚Â¿Ã‚Â½ÃƒÂ¯Ã‚Â¿Ã‚Â½ variabel penjelas dengan menggunakan pendekatan metode estimasi penalized maximum likelihood. Dengan memanfaatkan algoritma Newton-Raphson sehingga prosedur estimasi menjadi lebih sederhana.","Cox regression is well-known approach for modeling survival data with censored. However, in practice there are some things that cause the parameter estimates for the explanatory variables can not be obtained using standart Cox regression. One of obstacle is the phenomenon of monotone likelihood where there are at least one parameter estimate diverges. This can be overcome by an approach penalized maximum likelihood estimation method is to removes the bias first order on maximum likelihood estimation. This thesis, describes parameter estimation procedure of the Cox regression using the approach penalized maximum likelihood estimation method. By utilizing Newton-Raphson Algorithm so that the estimation procedure becomes simpler.","Kata Kunci : Analisis survival, regresi Cox, penalized maximum likelihood estimation, reduksi bias, modifikasi fungsi skor"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129350,Penentuan Harga Beli Opsi Asia Menggunakan Metode Ekspansi Deret Edgeworth dan Pendekatan Metode Black-Scholes,"TRIA PRATIWI, Prof. Dr. Dedi Rosadi, M.Sc ;Yunita Wulan Sari, S.Si., M.Sc",2017 | Skripsi | S1 STATISTIKA,"Opsi Asia merupakan sebuah opsi yang memiliki fungsi keuntungan yang bergantung pada rata-rata harga aset selama periode tertentu atau selama masa hidup opsi tersebut. Oleh karena itu harga opsi beli Asia lebih murah dibandingkan dengan harga opsi beli Eropa, sehingga memberikan keuntungan yang lebih besar kepada investor dan juga melindungi investor dari kecurangan yang mungkin terjadi diakhir masa berlakunya opsi. Penentuan harga opsi Asia dapat dilakukan melalui rata-rata geometrik dan rata-rata aritmatik. Pada rata-rata geometrik dapat dilakukan dengan pendekatan terhadap model Black-Scholes  karena telah memenuhi asumsi dari model tersebut. Sedangkan pada rata-rata aritmatik tidak dapat dilakukan secara analitik dengan pendekatan Black-Scholes karena distribusi dari fungsi distribusinya tidak diketahui. Untuk mengatasi permasahalan tersebut maka perlu dilakukan aproksimasi terhadap fungsi densitas yang tidak diketahui dengan menggunakan distribusi lognormal. Aproksimasi tersebut dilakukan dengan menyamakan dua momen pertama dan menggunakan ekspansi deret Edgeworth yang menyertakan perhitungan selisih skewness dan kurtosis.","The Asian option is an option that has a profit function which depends on the average asset price over a certain period or during the lifetime of the option. Therefore, the purchase price of Asian options is cheaper than the purchase price of European options, thus providing greater profit to investors and also protecting investors from possible frauds at the end of the option period. The pricing of Asian options can be made through geometric and arithmetic average. On geometric averages can be done with an approach to the Black-Scholes model because it has met the assumptions of the model. Meanwhile, the arithmetic average can not be done analytically with the Black-Scholes approach because the distribution of the distribution function is unknown. To overcome the problem, it is necessary to approximate the unknown density function by using the lognormal distribution. The approximation is done by equalizing the first two moments and using an Edgeworth series expansion that includes the calculation of skewness and kurtosis differences.","Kata Kunci : Opsi Asia, Ekspansi Deret Edgeworth, Black-Scholes/Asian Options, Edgeworth series expantion, Black-Scholes"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129353,MODEL PORTOFOLIO LOWER PARTIAL MOMENT DERAJAT 2 MENGGUNAKAN METODE QUADRATIC PROGRAMMING,"AGUNG SURYO BUDI P, Prof. Subanar, Ph.D.;Yunita Wulansari, S.Si, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Skripsi ini membahas mengenai optimisasi portofolio saham menggunakan metode mean-LPM derajat 2 dengan quadratic programming dan mean-semivariance dengan pendekatan heuristik. Kedua metode tersebut merupakan alat pengukuran downside risk, yaitu risiko terjadinya return di bawah target/rata-rata return. Karena bagi investor, risiko yang sebenarnya adalah risiko dimana return yang diperoleh di bawah target return. Kedua metode tersebut diaplikasikan pada data harga saham yang terdaftar dalam indeks LQ-45 selama tahun 2016. Hasil yang diperoleh menunjukkan  bahwa portofolio saham menggunakan metode mean-LPM derajat 2 dengan quadratic programming lebih baik dari portofolio mean-semivariance dengan pendekatan heuristik. Dalam hal ini, diperoleh model mean-LPM derajat 2 menghasilkan nilai risk adjusted return yang lebih besar dari mean semivariance.","This minithesis discusses about the optimization of stock portfolio using mean-LPM method of degree 2 with quadratic programming and mean-semivariance with heuristic approach. Both methods are a downside risk measurement tool, that is the risk of returns under the target / average return. Because for investors, the real risk is the risk that the return earned is below the target return. Both methods are applied to the stock price data listed in the LQ-45 index during 2016. The results show that the stock portfolio uses the mean-LPM method of degree 2 with quadratic programming better than the mean-semivariance portfolio with the heuristic approach. In this case, obtained the mean-LPM model of degree 2 resulted in a risk adjusted return value greater than the mean-semivariance.","Kata Kunci : Kata kunci : portofolio saham, downside risk, mean-LPM derajat 2, mean-semivariance, quadratic programming."
http://etd.repository.ugm.ac.id/home/detail_pencarian/108362,ESTIMASI PARAMETER REGRESI LOGISTIK BINER MENGGUNAKAN METODE PENALIZED MAXIMUM LIKELIHOOD ESTIMATION,"ANGGA NURADIKA, Dr. Abdurakhman, M.Si",2017 | Skripsi | S1 STATISTIKA,"Analisis regresi logistik digunakan untuk mengetahui pengaruh suatu peubah prediktor terhadap peubah respon bersifat kategorik. Parameter model regresi logistik diduga dengan metode Maximum Likelihood Estimation (MLE) dan untuk mempermudah perhitungan digunakan metode iterasi Newton Raphson. Namun metode Maximum Likelihood Estimation (MLE) tidak dapat digunakan jika terdapat kasus pemisahan. Terdapatnya kasus pemisahan disebabkan data yang mempunyai sampel kecil sehingga mengakibatkan bias pada estimasi parameter metode Maximum Likelihood Estimation (MLE). Kasus seperti ini dalam bidang statistika disebut dengan kasus pemisahan (Albert dan Anderson,1984). Masalah pemisahan perlu diselesaikan karena mengakibatkan nilai estimasi parameter pada regresi logistik tidak mendekati nilai estimasi parameter yang sebenernya. Kata kunci : regresi logistik, maximum likelihood, iterasi Newton Raphson,  pemisahan, penalized maximum likelihood estimation.","Logistic regression analysis was used to determine the effect of a predictor variable on the response variable is categorical. Parameter logistic regression models estimated by Maximum Likelihood Estimation (MLE) and to simplify the calculation used Newton Raphson iteration method. However Maximum Likelihood Estimation (MLE) can not be used if there are cases of separation. The presence of cases of separation due to data that have a small sample resulting bias in the estimation of parameters Maximum Likelihood Estimation (MLE). Cases like this in the field of statistics referred to the case of separation (Albert and Anderson, 1984). Separation problem needs to be solved, because it caused the value of the parameter estimation logistic regression did not approach the estimated value of the parameter actually.","Kata Kunci : ogistic regression, maximum likelihood, Newton Raphson iteration, separation, penalized maximum likelihood estimation."
http://etd.repository.ugm.ac.id/home/detail_pencarian/114250,REGRESI ROBUST DENGAN ESTIMASI TAU,"ARUM INDRAWINALOKA, Prof.Dr.Sri Haryatmi Kartiko, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Analisis  regresi  merupakan  salah  satu  cabang  ilmu  statistika   yang bertujuan  mengetahui  hubungan  antara  dua  variabel,   yaitu  variable  dependen  (respon)  dan  variable  independen   (prediktor).  Pada  umumnya  regresi menggunakan  metode  OLS  (  Ordinary  Least  Square  )  atau  estimasi  kuadrat  terkecil  untuk  mengestimasi  koefisien  regresi.  Namun  metode  ini  tidak  peka terhadap  keberadaan  data pencilan  pada data. Oleh karena itu  digunakan metode  estimasi  regresi  robust  yang  mampu  menangani  keberadaan  pencilan  pada  data. Dan  salah  satu  metodenya  adalah  estimasi-Tau .  Estimasi-Tau merupakan  metode  estimasi robust yang memiliki nilai breakdown tinggi dan menghasilkan estimator  dengan  nilai  efisiensi  tinggi.  Pada  studi  kasus  diberikan  ilustrasi  pendeteksian  pencilan  dan  perbandingan  antara  nilai  standard  error  model  regresi menggunakan  metode  OLS  dengan  metode  robust  estimasi-tau .  Data  yang  digunakan  adalah  data  presentase  penduduk  miskin  sebagai  variable  dependen serta data angka melek huruf,  data rata-rata lama sekolah, data garis kemiskinan  dan data pengeluaran per kapita sebagai  variable  independen per kabupaten / kota  di Pulau Jawa pada tahun 2010.","Regression  analysis  is  a  branch  of  statistical  science  that  aims  to determine  the  relationship  between  two  variables,  the  dependent  variable  (response)  and  independent  variables  (predictors).  Generally,  regression  uses OLS  (Ordinary  Least  Square)  or  least  squares  estimation  to  estimate  the  regression  coefficients.  But  this  method  is  not  sensitive  to  the  presence  of  data  outliers  in the data. Therefore,  robust regression estimation method  is used,  that  is able to handle the presence of outliers in the data. And one  of the method is Tau-estimation.Tau -Estimation  is  robust estimation method that has a high breakdown value  and  generate  value  estimator  with  high  efficiency.  Case  study  is  given  to  illustrate  outlier  detection  and  find  the  comparison  between  the  value  of  the standard error of regression model using OLS and Tau-estimation.  The data used is  the  percentage  of  poor  population  as  the  dependent  variable  and  the  data  on literacy  rates,  average  length  of  the  school,  the  poverty  line  and  per  capita expenditure data as independent variables per districts / cities in Java in 2010.","Kata Kunci : Pencilan, Regresi  Robust, Estimasi-tau ,  Breakdown Point, Standard  Error"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114000,SISTEM BONUS-MALUS BINOMIAL NEGATIF - WEIBULL PADA ASURANSI KENDARAAN BERMOTOR,"NOVITASARI LINDA CHRISTANTI, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2017 | Skripsi | S1 STATISTIKA,"Sistem Bonus-Malus adalah sistem asuransi yang besar preminya dipengaruhi oleh frekuensi klaim dan besar klaim yang diajukan oleh pemegang polis pada tahun sebelumnya. Pengurangan jumlah premi yang harus dibayar (bonus) diberikan kepada pemegang polis apabila tidak ada klaim yang diajukan. Sedangkan malus adalah kenaikan jumlah premi yang harus dibayar oleh pemegang polis apabila terdapat klaim yang diajukan. Besar kenaikan premi tergantung riwayat klaim yang diajukan oleh pemegang polis. Pada skripsi ini akan dibahas mengenai distribusi binomial negatif sebagai distribusi frekuensi klaim, dan distribusi Weibull sebagai distribusi besar klaim. Estimasi parameter untuk distribusi binomial negatif dan distribusi Weibull dicari menggunakan metode maximum likelihood. Dalam pembentukan tabel premi, digunakan metode Bayesian. Pada studi kasus, dibandingkan model premi dengan besar klaim yang dimodelkan dengan distribusi Weibull dan Pareto. Didapatkan hasil bahwa dengan besar klaim berdistribusi Weibull menghasilkan karakteristik tabel premi lebih baik. Selain itu, besar klaim yang dimodelkan dengan distribusi Weibull dapat mengatasi masalah Bonus-Hunger, yaitu keadaan dimana pemegang polis tidak melaporkan klaim yang terjadi dan menanggung biaya kerusakan sendiri untuk menghidari kenaikan premi.","Bonus Malus system is an insurance system under which a premium is set by taking into account the frequency and severity of the claims of each policyholder from the previous year. Reduction in the amount of the required premium (Bonus) is offered to the policyholder if there is no claim being proposed. In the other hand, Malus is an increase in the amount of premium that must be paid by the policyholder if there is claim being proposed. The premium depends on the history of the policyholders proposed claim frequency and severity. This undergraduate thesis aims to discuss the application of Negative Binominal distribution to model the claim frequency data and Weibull distribution that models claim severity. The estimated parameter for the Negative Binominal distribution and Weibull distribution uses Maximum Likelihood Estimation. The Bayesian model is used in the making of premium table. In the case study, there is a comparison between the premium model of the claims that are modeled by the Weibull distribution and Pareto. This study demonstrates that the Weibull distribution produces better premium table. In addition, claim severity that is modeled by Weibull can prevent the issue of Bonus-Hunger. Bonus Hunger is condition that policyholders is not reporting claim incurred and pay the cost themselves to avoid the rise premium.","Kata Kunci : premi, sistem Bonus Malus, distribusi binomial negatif, distribusi Weibull, Bonus Hunger / Premium, Bonus Malus system, negative binomial distribution, Weibull distribution, Bonus Hunger"
http://etd.repository.ugm.ac.id/home/detail_pencarian/117072,ESTIMASI VALUE AT RISK MENGGUNAKAN MODEL GARCH ASIMETRIS STUDENT T,"ISNA SHOFIA M, Dr. Herni Utami, S.Si., M.Si.",2017 | Skripsi | S1 STATISTIKA,"Model ARCH dan GARCH banyak digunakan pada data finansial untuk mendeskripsikan bentuk volatilitasnya. Model ARCH dan GARCH mengasumsikan bahwa residual return positif dan residual return negatif akan memberikan pengaruh yang sama atau simetris terhadap volatilitasnya. Namun pada kenyataannya asumsi ini seringkali tidak terpenuhi. Sehingga Untuk mengatasi data yang bersifat heteroskedastis dan mengandung keasimtetrikan digunakan model GARCH asimetris diantaranya adalah EGARCH dan TGARCH. Penelitian ini bertujuan untuk mengestimasi nilai Value at Risk (VaR) dengan pemodelan volatilitas menggunakan model EGARCH dan TGARCH dengan asumsi data berdistribusi student t. data yang digunakan adalah data saham Perusahaan Gas Negara Tbk (PGAS) dengan periode 1 Juli 2013 sampai 28 April 2017 yang berisi sebanyak 932 data harga penutupan (closing price). Penelitian ini diawali dengan memilih model mean terbaik untuk data return. Berdasarkan model mean terbaik yang telah diperoleh dibentuk model volatilitas EGARCH(1,1) dan TGARCH(1,1). Kemudian dilakukan perbandingan terhadap kedua model ini untuk mengetahui model volatilitas mana yang lebih baik.pemilihan model terbaik dipilih berdasarkan nilai log likelihood yang maksimum dengan nilai BIC dan AIC yang kecil. Dari ketiga kriteria tersebut dapat disimpulkan bahwa model EGARCH(1,1) merupakan model volatilitas terbaik untuk return PGAS. Kemudian dilakukan estimasi perhitungan nilai VaR berdasarkan model volatilitas yang diperoleh. Kemudian dilakukan backtesting untuk melihat apakah VaR yang dihasilkan valid atau tidak.","ARCH and GARCH model are widely used in financial data to describe its volatility pattern. ARCH and GARCH model assume that the positive and negative return residual will give same or symmetric influence on its volatility. However in reality this assumption is frequently violated. Therefore in order to deal with heteroscedasticity and asymmetric data, the Asymmetric GARCH models, which is EGARCH and TGARCH model are used. This research aims to estimate the Value at Risk (VaR) with volatility modelling using EGARCH and TGARCH models which assume that data has a student t distribution. Data which used in this research is the stock closing price data of Perusahaan Gas Negara Tbk (PGAS) with period from 1 July 2013 until 28 April 2017 that contain 932 data. 	This research is begin with choosing the best mean model for return. Based on the best mean model that have been obtained, volatility models EGARCH(1,1) and TGARCH(1,1) are formed. Then the two models are compared to find out which volatility model is better. Model selection is based on maximum value of log-likelihood and minimum value of AIC and BIC. From that three model criteria, it can be cocluded that EGARCH(1,1) model is the best volatility model for return data of PGAS. After that, the VaR estimation is computed based on the best volatility model which has been obtained. At last, the backtesting is performed to check the validity of estimated VaR.","Kata Kunci : GARCH, GARCH asimetris, Value at Risk (VaR), backtesting."
http://etd.repository.ugm.ac.id/home/detail_pencarian/128592,ESTIMASI MODEL SEEMINGLY UNRELATED REGRESSION DENGAN ERROR BERPOLA AUTOREGRESSIVE ORDE SATU,"MIFTAHUL JANNAH, Dr. Herni Utami, S.Si., M.Si.",2017 | Skripsi | S1 STATISTIKA,"Model regresi yang terdiri dari beberapa persamaan regresi yang saling berhubungan dikenal dengan model Seemingly Unrelated Regression (SUR). Model ini mengasumsikan adanya korelasi error pada sistem persamaan. Error antar persamaan berkorelasi secara contemporaneous. Dalam kondisi tersebut, model SUR dengan metode GLS akan  lebih efisien dibandingkan metode OLS karena metode OLS membuang kemungkinan adanya hubungan antar persamaan. Salah satu asumsi dalam model SUR adalah tidak terdapat autokorelasi pada tiap persamaan. Namun pada data runtun waktu dimana pengamatan variabel pada setiap persamaan dilakukan secara frekuentif dari waktu ke waktu, error dari persamaan cenderung menunjukkan pola korelasi serial. Dalam tugas akhir ini, dilakukan estimasi model SUR dimana error pada masing-masing persamaan penyusunnya memiliki pola autoregressive orde satu (AR(1)). Langkah awal yang dilakukan adalah melakukan transformasi data pada masing-masing persamaan tunggal. Kemudian metode Feasible Generalized Least Square diterapkan pada model SUR dengan data hasil transformasi guna memperoleh estimator yang efisien. Kesimpulan yang diperoleh yaitu akan lebih tepat jika dilakukan transformasi pada model SUR dimana error pada tiap persamaan tunggalnya berkorelasi. Estimasi model SUR pada data transformasi lebih efisien dibandingkan estimasi model SUR tanpa transformasi.","Models that consists of several correlated regression equations are known as Seemingly Unrelated Regression (SUR) model. This  model assumes an error correlation in the system. Errors are contemporaneously correlated between equations. In this condition, SUR model with the GLS method will be efficient compared with the OLS method. It is because OLS method discards the possibility of relationship between equations. One of the assumptions on the SUR model is that there is no correlated error in each equation. However, in time series data where the observation of the variables in each equation was done frequently over time, the errors of the equation are likely to exhibit serial correlation. In this final task, the SUR model is estimated where each equation exhibit first-order autoregressive (AR(1)) disturbances. First, each equation is transformed based on their estimated correlation coefficient and the covariance of residuals matrix. Then, Feasible Generalized Least Square (FGLS) method is applied to SUR model with the transformed data to obtain an efficient estimator. The Conclusion from this study suggest that it would be more appropriate to transform the SUR model with autocorrelated errors. SUR model estimation on the transformed data is efficient compared with SUR model estimation on the non-transformed data.","Kata Kunci : Seemingly Unrelated Regression, First-Order Autoregressive"
http://etd.repository.ugm.ac.id/home/detail_pencarian/110681,ANALISA REGRESI FUNGSIONAL (PREDIKSI TOTAL CURAH HUJAN TAHUNAN PADA BEBERAPA STASIUN PENGAMATAN CUACA DI PULAU JAWA),"DINY AMALYA , Dr. Abdurakhman, S.Si., M.Si. ; Yunita Wulan Sari, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Regresi merupakan salah satu teknik analisis statistika yang digunakan untuk menduga variabel dependen berdasarkan variabel independen. Salah satu masalah yang dihadapi pada saat melakukan analisis regresi adalah menentukan estimasi persamaan atau model jika jumlah observasi jauh lebih kecil daripada jumlah variabel independen.  Terdapat tak hingga kemungkinan model regresi sehingga nilai parameter dalam model regresi tidak dapat diestimasi dengan baik. Seiring dengan berkembangnya zaman, ditemukan metode untuk mengatasi masalah tersebut yaitu Analisa Regresi Fungsional. Konsep dari Regresi Fungsional adalah dengan mengestimasi parameter beta menggunakan eskpansi linier basis fungsi. Variabel independen yang memiliki dimensi yang tinggi akan diekspansi menggunakan basis fungsi sehingga memiliki dimensi yang lebih kecil namun tidak menghilangkan informasi yang ada dalam data. Untuk data yang bersifat periodik digunakan Fourier Basis System sebagai basis system dalam ekpansi linier. Setelah dilakukan ekspansi linier pada variabel independen dan terbentuk model Regresi Fungsional, didapatkan hasil prediksi yang akurat dengan nilai MSE (Mean Square Error) yang kecil.","Regression is a statistical analysis that used to predict the dependent variable based on independent variables. The problem when a number of observation smaller than the number of independent variables is the parameters in regression model can not be estimated properly thus cause the prediction will not be accurate. There is another method called Functional Reggression to solve that problems. The concept of Functional Regression is by estimated parameters beta using basis expansion. Independent variables with high dimensional will be expanded using a basis functions so that it has smaller dimensions but does not eliminate the information from the data. For periodically data, Fourier Basis System used to linear expansion. After linear expansion on independent variables and formed Functional Regression model, obtain accurate prediction by small value of Mean Square Error (MSE).","Kata Kunci : Regresi, Regresi Fungsional, Basis Fungsi,  Fourier Basis System, Dimensi Tinggi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114016,METODE K-MEDOIDS PADA DATA DENGAN PENCILAN,"ANNISA LARASATI, Dr. Abdurakhman, M.Si.",2017 | Skripsi | S1 STATISTIKA,"Analisis klaster adalah metode statistika multivariat yang bertujuan untuk mengelompokkan objek-objek yang memiliki kemiripan karakteristik ke dalam suatu klaster. K-means merupakan metode analisis klaster dengan menggunakan mean sebagai pusat klasternya. Namun, mean tidak robust terhadap adanya pencilan, sehingga algoritma k-means sangat sensitif terhadap data yang mengandung pencilan. Untuk mengatasi hal tersebut, dapat digunakan metode k-medoids untuk mengelompokkan data yang mengandung pencilan. Medoid merupakan objek yang letaknya terpusat di dalam suatu klaster, sehingga robust terhadap adanya pencilan. Metode k-medoids yang popular digunakan adalah Partitioning Around Medoids (PAM). Pada analisis klaster, objek-objek dikelompokkan berdasarkan kemiripannya. Untuk mengukur tingkat kemiripan tersebut digunakan ukuran jarak, yaitu jarak Euclidean dan jarak Manhattan. Selanjutnya, untuk mengetahui kualitas hasil analisis klaster dilakukan uji validasi dengan silhouette width. 	Metode analisis klaster terbaik untuk mengelompokkan provinsi-provinsi di Indonesia berdasarkan produksi tanaman pangan pokok tahun 2015 adalah metode k-medoids dengan jarak Euclidean. Dapat diketahui pula bahwa metode k-medoids dengan jarak Euclidean lebih robust dibandingkan metode k-means dengan jarak Euclidean untuk mengelompokkan data dengan pencilan.","Cluster analysis is a multivariate statistical methods to classify objects that have similar characteristics into a cluster. K-means is a clustering method using mean as its cluster center. However, mean is not robust to the presence of outliers, so k-means algorithm is sensitive for data with outliers. To overcome this problem, k-medoids methods can be used to classify data with outliers. Medoid is the most centrally located object in a cluster, so itÃ¢ï¿½ï¿½s robust to outliers. One of the popular methods for k-medoids is Partitioning Around Medoids (PAM). In cluster analysis, the objects are grouped by the similarity. To measure the similarity, it can be used distance measures, Euclidean distance and Manhattan distance. Then, to determine the quality of the clustering results can be used validity index with silhouette width.  	The best clustering method to classify the provinces in Indonesia based on staple food production in 2015 is k-medoids with Euclidean distance. Furthermore, it can be concluded that k-medoids with Euclidean distance is more robust than k-means with Euclidean distance to classify data with outliers.","Kata Kunci : k-medoids/k-medoids, Partitioning Around Medoids/Partitioning Around Medoids, pencilan/outlier, silhouette width/silhouette width"
http://etd.repository.ugm.ac.id/home/detail_pencarian/117091,META ANALISIS DENGAN EFFECT SIZE RISK RATIO,"RISKA PUJI ASTUTI, Dr. Abdurakhman, S.Si., M.Si.",2017 | Skripsi | S1 STATISTIKA,"Meta analisis merupakan teknik statistika untuk mengkombinasikan hasil sejumlah penelitian yang telah ada secara sistematis dan kuantitatif dengan mencari nilai effect size untuk memperoleh kesimpulan secara menyeluruh. Effect size dalam meta analisis bergantung pada jenis data yang digunakan dalam penelitian. Penulis akan membahas prosedur meta analisis dengan effect size risk ratio. Effect size risk ratio digunakan untuk data dikotomi pada uji klinis. Langkah dalam meta analisis dengan random effects model dan effect size risk ratio adalah sebagai berikut : menentukan topik, mengumpulkan dan menyeleksi penelitian yang berkaitan dengan topik, input data yang akan dianalisis, mencari risk ratio dan variansi masing-masing penelitian dengan Fixed-Effects Models, mencari faktor pembobotan dengan Random-Effects Model, identifikasi heterogenitas, analisis Variabel Moderator, mencari Risk ratio gabungan. Untuk studi kasus, meta analisis effect size risk ratio diaplikasikan pada data penelitian jenis obat parasetamol dibandingkan dengan terapi biasa pada penderita sakit kepala akut dengan intensitas sering.","Meta-analysis is a statistical technique to combine the results of a number of existing studies in a systematic and quantitative way by looking for an effect size to get a conclusion thoroughly. The effect size in the meta-analysis depends on the type of data used in the study. The author will discuss the meta-analysis procedure with effect size risk ratio. Effect size risk ratio is used for data dichotomy in clinical trials. 	The steps in the meta-analysis with random effects model and effect size risk ratio are as follows: determine topics, collect and select research related to the topic, input data to be analyzed, look for risk ratio and variance of each research with Fixed-Effects Models, looking for weighting factors with Random-Effects Model, heterogeneity identification, Variable Moderator analysis, finding Risk ratio combined. 	For the case study, the meta-analysis of effect size risk ratios was applied to the research data of the paracetamol drug type compared with the usual therapy in acute headache patients with frequent intensity.","Kata Kunci : meta analisis, effect size, risk ratio/meta-analysis, effect size, risk ratio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129124,ALGORITMA K-MEANS CLUSTERING PADA SEGMENTASI CITRA,"JAZI MUNJAZI, Dr. Danang Teguh Qoyyimi, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Analisis klaster merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Pada era teknologi informasi, analisis klaster dapat juga digunakan untuk segmentasi citra. Segmentasi citra dengan pendekatan analisis klaster adalah mengelompokkan data piksel multidimensi citra ke dalam beberapa klaster berdasarkan kedekatan jarak antar piksel. Segmentasi citra sangat diperlukan dalam usaha memahami ciri citra secara lengkap dan merupakan langkah awal dalam analisis citra. Algoritma k-means clustering salah satu teknik analisis klaster yang sering digunakan dalam segmentasi citra karena kemudahan dan kemampuannya dalam mengelompokkan data besar dengan sangat cepat.  Berdasarkan observasi pada citra daerah UGM dan sekitarnya tahun 2007 dan 2016 menggunakan algoritma k-means clustering, menunjukan adanya peningkatan luas ruang terbuka hijau sebesar 111,4601 ha. Hasil segmentasi memiliki nilai MSE berturut-turut untuk data 2007 dan data 2016 yaitu 0,009790288 dan 0,011215424. Dan nilai PSNR berturut-turut sebesar 46,26364 dan 44,90465 untuk data 2007 dan data 2016. Segmentasi citra dikatakan baik jika nilai MSE mendekati nilai nol dan nilai PSNR diatas 30 desibel (dB).","Clustering analysis is one of methods to classify data which have similarity characteristics. In the era of information technology, clustering analysis also can be used for image segmentation. Image segmentation with clustering analysis approach is to classify multidimensional pixel data into some clusters based on proximity of distance among pixels. Image segmentation is needed for understanding the feature of detail images and it is the first step of image analysis. The k-means clustering algorithm method is one of clustering analysis techniques which is most used for image segmentation because its easiness and skill for classifying big data in a while. In the satellite image, the area border is presented really well. Based on the observation of area around UGM and its surrounds in 2007 and 2016 using k-means clustering algorithm, there is an increasing of green openspaces about 111,4601 hectares. The results of MSE value are 0,009790288 in image 2007 and 0,011215424 in image 2016 and the PNSR value are 46,26364 in image 2007 and 44,90465 in image 2016. The image segmentation can be classified as good if the MSE value is nearly zero and the PSNR value is above 30 decibel (dB).","Kata Kunci : citra, segmentasi citra, algoritma k-means-clustering"
http://etd.repository.ugm.ac.id/home/detail_pencarian/128871,BAYESIAN MODEL AVERAGING PADA REGRESI LOGISTIK,"MUTHMAINAH, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2017 | Skripsi | S1 STATISTIKA,"Salah satu metode statistika yang sering digunakan dalam menganalisis hubungan antara variabel independen dengan variabel respon biner adalah regresi logistik. Model regresi logistik terbaik diperoleh berdasarkan uji signifikansi variabel serta kriteria pemilihan model tertentu. Inferensi terkait prediktor hanya terpusat pada model tunggal yang terpilih untuk prediksi. Hal ini tentu mengabaikan ketidakpastian terkait variabel yang tidak masuk ke dalam model maupun kemungkinan model lain yang dirasa layak untuk dipertimbangkan. 	Bayesian Model Averaging memberikan solusi untuk mengatasi ketidakpastian terkait pemilihan model serta variabel, dengan merata-ratakan distribusi posterior dari sekelompok model, yang diboboti oleh probabilitas posterior modelnya. Metode ini melibatkan seluruh model yang mungkin terbentuk dari kombinasi faktor-faktornya. Untuk mempermudah komputasi, model yang dimasukkan ke dalam analisis dipilih berdasarkan algoritma Occams Window. Kontribusi setiap faktor dalam model yang terpilih dapat dilihat melalui probabilitas posterior dari koefisien variabel, sehingga diperoleh interpretasi yang lebih jelas terkait pengaruh faktor terhadap variabel respon yang diteliti. 	Studi kasus yang digunakan dalam skripsi ini bertujuan untuk mengetahui faktor yang mempengaruhi warna air sungai di Kabupaten Bantul. Model terbaik yang diperoleh dari regresi logistik akan dibandingkan dengan model hasil analisis dari metode Bayesian Model Averaging. Berdasarkan kemampuan prediksi dan persentase akurasi, diperoleh bahwa metode Bayesian Model Averaging memberikan model prediksi yang lebih baik daripada regresi logistik.","One of the most commonly used statistical methods in analyzing the relationship between independent variables and binary response variable is logistic regression. The best logistic regression model is obtained based on the significance variable test and certain model selection criteria.Inference about the predictors is then made from a single model chosen for prediction. This subsequently ignores uncertainty associated with both variables not included into the model and possibility of other worthy considered models.     	Bayesian Model Averaging provide solution to overcome uncertainty regarding model and variable selection, by averaging the posterior distribution of a set of models, weighted by their posterior model probabilities. This method involve all possible models which formed from combination of factors. In order to facilitate computation,the models included into the analysis are selected using Occams Window algorithm. Contribution of each factor in the selected models can be seen through the posterior probability of coefficient of variable, which has a clearer interpretation about factors influencing the studied response variable. 	The case study used in this thesis aims to determine factors that affect the color of river water in Bantul regency. The best model obtained from standard logistic regression will be compared with model from Bayesian Model Averaging method. Based on predictive performance and accuracy percentage, it was found that Bayesian Model Averaging method gives better prediction model than standard logistic regression.","Kata Kunci : Bayesian Model Averaging, Regresi logistik, Occams window, Logistic regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/108146,GRAFIK PENGENDALI  HOTELLING T^2 MENGGUNAKAN ESTIMATOR JAMES-STEIN,"RENI AGUSTINI, Drs. Zulaela, Dipl. Med. Stats, M.Si; Widya Irmaningtyas, S.Si., M.Sc",2017 | Skripsi | S1 STATISTIKA,"Pengendalian kualitas statistik multivariat merupakan suatu proses produksi yang dapat mengendalikan suatu proses yang memiliki lebih dari dua karakteristik kualitas. Salah satu grafik pengendali multivariat yang umum digunakan yaitu grafik pengendali Hotelling T2. Asumsi yang diperlukan dalam analisis pengendalian kualitas statistik yaitu asumsi distribusi normal.  Untuk distribusi normal multivariat dengan vektor mean yang tidak diketahui, estimator mean pada umumnya diketahui inadmissible dalam fungsi kerugian kudrat error ketika dimensi variabel lebih dari dua. Oleh karena itu, digunakan estimator James-Stein yang dapat mengestimasi mean dalam fungsi kerugian kuadrat error yang memiliki dimensi variabel lebih dari tiga.  Kemudian, akan digunakan algoritma ARL (Average Run Length) untuk mengetahui batas pengendali dari Hotelling T2 menggunakan estimator James-Stein dimana grafik pengendali Hotelling T2 menggunakan estimator James-Stein memiliki nilai ARL yg lebih kecil daripada grafik pengendali Hotelling T2 konvensional. Semakin kecil nilai ARL maka akan menghasilkan grafik pengendali yang lebih baik dan optimal.","Multivariate statistical quality control is a production process for monitoring the process which consist more than two quality characteristics. One of the most commonly use from multivariate control charts  is Hotelling T2 control chart. The assumption for this case is multivariate normal distribution.  For a multivariate normal distribution with unknown mean vector, the usual estimator  is known to be inadmissible under the squared error loss function when the dimension of the variables is greater then two. So, we utilize the James-Stein estimator which estimated mean under the squared error when the dimension of the variable is greater then three.  Then, we use ARL (Average Run Length) algorithm to identify control limit from Hotelling T2 control chart using James-Stein estimator which have a small ARL than the conventional Hotelling T2 control chart. The smaller ARL value have a better and optimal control chart.","Kata Kunci : Grafik pengendali multivariat, Hotelling T2, estimator James-Stein/Multivariate control chart, Hotelling T2, James-Stein estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114290,ANALISIS DISKRIMINAN LINEAR ROBUST MENGGUNAKAN ESTIMATOR MINIMUM COVARIANCE DETERMINANT,"WIWIN AMALIA RITONGA, Yunita Wulan Sari, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Analisis diskriminan adalah salah satu teknik data multivariat yang bertujuan untuk mengklasifikasikan suatu objek ke dalam suatu kelompok yang sudah ada sebelumnya. Analisis diskriminan linear mempunyai asumsi seperti variabel independen berdistribusi normal multivariat dan kesamaan matriks varians-kovarians (homokedatisitas). Apabila terdapat data outlier, maka analisis diskriminan klasik tidak dapat bekerja dengan baik karena akan mempengaruhi matriks varians-kovarians. Oleh sebab itu, diperlukan suatu estimator yang robust agar analisis diskriminan tetap optimal dalam hasil pengklasifikasiannya. Dalam skripsi ini, digunakan metode analisis diskriminan dengan menggunakan penaksir Minimum Covariance Determinant (MCD). Penaksir MCD merupakan kovariansi dari sebagian pengamatan yang meminimumkan determinan matriks kovariansi. Selain itu, dalam perhitungan estimator MCD digunakan algoritma  fast-MCD agar analisis dapat dilakukan dalam data berdimensi besar. Analisis diskriminan linear robust dengan menggunakan algoritma fast-MCD pada skripsi ini diaplikasikan untuk menentukan status seseorang terhadap ada atau tidaknya virus dengue dalam darah. Data diperoleh dari Rumah Sakir dr. Asmir di Salatiga pada bulan Desember 2014 s.d Januari 2015. Untuk mengukur tingkat kesalahan klasifikasi, digunakan metode Apparent Error Rate (APER). Hasil analisis menunjukkan bahwa klasifikasi dengan menggunakan analisis diskriminan linear robust dengan menggunakan MCD memberikan hasil yang lebih baik dari pada analisis diskriminan linear klasik yang ditunjukkan oleh nilai APER yang kecil.","Discriminant analysis is one of mutivariate data techinique to classify  new observations into one of the known groups. Linear discriminant analysis has  assumptions such as independent variables are multivariate distribution normal and equal variance-covariance matrices (homoscedasticity). If there are data outliers, then the classical discriminant analysis do not work well because it will affect variance-covariance matrices. Therefore required a robust estimator in order to keep optimum discriminant analysis result. In this essay is used discriminant analysis robust using minimum covariance determinant estimator. MCD estimator is covariance from some observations that minimize the determinant of variance-covariance matrices. The algorithm to calculate MCD estimators is fast-MCD because it can be use in large dimensions. This essay applies robust linear discriminnat analysis using algorithm fast-MCD to classify new observations to  there is a dengue virus in their blood or not. Data obtained from dr. Asmir Hospital in Salatiga in December 2014 until January 2015. Apparent Error Rate (APER) used to measure the rate of misclassification. The analysis result show that classification using robust linear discriminant analysis is better than classic linear discriminant analysis as shown by the smallest APER.","Kata Kunci : Analisis Diskriminan Linear, Analisis Diskriminan Linear Robust, oulier, Minimum Covariance Determinant, fast-MCD, Apparent Error Rate (APER)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/108147,ANALISIS KURVA SURVIVAL PADA TITIK WAKTU TERTENTU DENGAN MENGGUNAKAN KINERJA DARI TES NAIF DAN BEBERAPA TES ALTERNATIFNYA,"ASTRI MARDHIKA PUTRI S., Dr. Danardono, MPH, Ph.D ; Widya Irmaningtyas, S.Si., M.Sc",2017 | Skripsi | S1 STATISTIKA,"Masalah umum yang sering dihadapi pada praktik medis adalah perbandingan dari kurva survival. Perbandingan tersebut seringkali dilakukan tidak di keseluruhan kurva, melainkan hanya difokuskan pada perbandingan di titik waktu tertentu saja. Pada kebanyakan kasus, perbandingan ini menggunakan tes naif yang berdasarkan pada perbedaan estimasi fungsi survival. Dalam penulisan ini, akan diuji kinerja dari tes alternatif terhadap tes naif. Termasuk diantaranya yaitu beberapa tes yang berdasarkan transformasi fungsi survival dan tes yang berdasarkan Generalized Linear Model (GLM) untuk pengamatan semu. Pendekatan nilai-semu juga dapat diterapkan pada analisis regresi survival pada titik waktu tertentu yang lebih mendetail. Metode-metode dalam penulisan ini diilustrasikan melalui penelitian yang membandingkan kelangsungan hidup antara tipe pengemudi yang kebiasaan mengonsumsi alkohol dengan yang tidak mengonsumsi alkohol dari suatu kecelakaan lalu lintas di Amerika Serikat.","A common problem encountered in many medical applications is the comparison of survival curves. Often, rather than comparison of the entire survival curves, interest is focused on the comparison at a fixed point in time. In most cases, the naive test based on a difference in the estimates of survival is used for this comparison. In this note, we examine the performance of alternatives to the naive test. These include tests based on a number of transformations of the survival function and a test based on a generalized linear model for pseudo-observations. The pseudo-value approach is also applicable in more detailed regression analysis of the survival probability at a fixed point in time. The methods in this essay are illustrated on a the study comparing survival probability between alcoholics and non-alcoholics drivers of a traffic accident in United State.","Kata Kunci : transformasi menstabilkan variansi, estimator Kaplanâ€“Meier, data tersensor, Generalized Linear Models (GLM), pendekatan nilai-semu / variance stabilizing transformation, Kaplanâ€“Meier estimators, censored data, Generalized Linear Models (GL"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129396,PENYUSUTAN KOEFISIEN DENGAN METODE ADAPTIVE LASSO,"FAERITA AGNINA F, Prof. Drs. Subanar, Ph.D.",2017 | Skripsi | S1 STATISTIKA,"Seleksi variabel merupakan salah satu analisis yang populer digunakan di bidang statistik, dan skripsi ini memfokuskan bahasan pada teknik penalisasi atau penyusutan koefisien. Adaptive Lasso yang dikenalkan oleh Zhou adalah metode yang manfaat dari penggunaannya sudah tidak diragukan lagi, dengan kemampuannya menyusutkan beberapa koefisien mendekati nol hingga tepat nol, sehingga terjadi penyeleksian variabel secara langsung. Dengan menggunakan bobot adaptive, Adaptive Lasso menjadi metode yang merupakan perkembangan dari Lasso. Bobot adaptive ini memberikan nilai penalty yang berbeda untuk setiap koefisien, yang menyebabkan keakuratan prediksi untuk estimasi koefisien. Hal ini disebabkan karena nilai penalty yang besar digunakan untuk variabel yang tidak signifikan dan nilai penalty yang kecil digunakan untuk variabel yang signifikan, menghasilkan model akhir yang mudah untuk diinterpretasi. Di akhir skripsi ini, kita membandingkan akurasi prediksi untuk estimasi koefisien dan seleksi variabel dari Adaptive Lasso dengan OLS dan Lasso dengan menggunakan Mean Square Error (MSE).","Variable selection has received a lot of attention in the statistics literature in the recent years, and the focus has been on penalized or shrinkage type of estimators. Adaptive Lasso by Zhou is a well-established method, with its capabilities to shrink some estimated coefficients, close to zero or exactly zero, and to serve as a variable selection simultaneously. Using data-adaptive weights, the Adaptive Lasso method is a modified Lasso method. The weights allow the Adaptive Lasso to apply different amounts of shrinkage to different coefficient and consequently, accomplish an accurate estimation of regression coefficients by large amount of shrinkage used for the insignificant variables and small amount of shrinkage of significant variables, resulting a representative and interpretable final model. In the end of this thesis, we compared the accuracy of prediction of Adaptive Lasso method with the estimation of OLS and Lasso, in term of variable selection and accuracy in predicting estimated coefficients using Mean Square Error (MSE).","Kata Kunci : Ordinary Least Square, Lasso, Adaptive Lasso, Teknik Penalisasi, Penalized Regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/108155,SUPPORT VECTOR REGRESSION YANG DIOPTIMASI MENGGUNAKAN ALGORITMA GENETIKA UNTUK MEMPREDIKSI PERTUMBUHAN GROSS DOMESTIC PRODUCT INDONESIA,"BENI SETIAJI, Dr. Gunardi, M.Si ; Vemmie Nastiti Lestari, S.Si., M.Sc",2017 | Skripsi | S1 STATISTIKA,"Situasi dan pertumbuhan perekonomian suatu negara dapat dilihat melalui salah satu konsep yaitu Gross Domestic Product (GDP). Umumnya GDP memiliki pola yang tidak linear. Salah satu metode regresi yang cocok untuk mengatasi kasus ini adalah metode Support Vector Regression (SVR). Jenis SVR yang akan digunakan adalah nu-SVR (v-SVR) dengan bantuan Kernel tunggal. Metode tersebut mampu memprediksi GDP tanpa harus memenuhi asumsi klasik stasioneritas dan asumsi linearitas. Pada metode v-SVR terdapat beberapa parameter yang mempengaruhi akurasi prediksi dan generalisasi model, diantaranya yaitu, v, C, dan sigma. Sangat penting untuk menentukan nilai dari ketiga parameter tersebut agar dihasilkan model v-SVR yang reliabel. Parameter C mengontrol keseimbangan antara maksimasi margin dan minimasi eror. Semakin besar C akan mengurangi training error, tetapi beresiko kehilangan sifat generalisasinya dan hal ini biasa disebut dengan overfitting. Parameter v menentukan batas atas pada fraksi margin error sekaligus menjadi batas bawah fraksi jumlah support vector. Parameter sigma adalah lebar kernel, semakin kecil nilainya, model lebih dekat pada overfitting. Pada penelitian ini, dilakukan proses training dengan lima proporsi data training berbeda, yaitu 50%, 60%, 70%, 80%, dan 90%. Pada kasus ini, proporsi data training yang memberikan hasil prediksi pertumbuhan GDP Indonesia terbaik adalah proporsi data training sebesar 70% .","The economic situation and the economic growth of a country can be found from a concept such as Gross Domestic Product (GDP). Commonly GDP has non-linear pattern. One of regression method that can handle those pattern is Support Vector Regression (SVR) method. The kind of SVR that will be used here is nu-SVR (v-SVR) with single Kernel. The method able to predict GDP without having classic assumption such as stationarity assumption and linearity assumption. There are some parameters on v-SVR method that influence the prediction accuracy and generalization ability of the model, those are v,C, and sigma. It is very important to determine the value of those parameters in order to gain the reliabel v-SVR model. The parameter C control the tradeoff between margin maximization and error minimization. The bigger value of C will reduce the training error, but give the risk of  loosing the generalization characteristic which often called as overfitting. The parameter v determine the upper bound of margin error fraction and also determine the lower bound of fraction of total support vectors. The parameter sigma is the kernel width, if the value of sigma is low, the model will close to overfitting. This research run the training process in five different proportions of training data, the proportions are 50%, 60%, 70%, 80%, and 90%. In this case, training data proportion that give the best prediction on Indonesiaâ€™s GDP growth is 70%  proportion of training data.","Kata Kunci : Algoritma Genetika, Support Vector Regression, Pemilihan Lag"
http://etd.repository.ugm.ac.id/home/detail_pencarian/107654,METODE BLACK-SCHOLES DAN TRUNCATED BLACK-SCHOLES DALAM MENENTUKAN HARGA OPSI EROPA,"RISA CRISTIARI, Dr. Herni Utami, M.Si",2017 | Skripsi | S1 STATISTIKA,"Black dan Scholes (1973) mengembangkan suatu metode penentuan harga opsi yang telah banyak diterapkan baik dalam konteks akademik maupun praktis.  Asumsi praktis dalam metode Black-Scholes adalah return saham mengikuti distribusi normal dengan volatilitas konstan. Pada kenyataannya, bursa seringkali melakukan pembatasan terhadap pergerakan perubahan harga harian suatu aset. Sebagai akibat dari adanya pembatasan ini, range return (dalam bentuk logaritma) dari aset-aset yang diperdagangkan tersebut tidak lagi berada dalam interval (-âˆž,âˆž), akan tetapi terpotong di atas dan di bawah.  Adanya pembatasan pada perubahan harga harian aset tersebut akan berakibat pada kurang tepatnya metode Black-Scholes, sehingga dikembangkan suatu metode baru yaitu Truncated Black-Scholes. Selanjutnya, dilakukan perbandingan antara harga opsi yang diperoleh dengan metode Truncated Black-Scholes dan metode Black-Scholes terhadap harga opsi di pasar. Dengan menggunakan SRPE (Squared Relative Price Error) sebagai kriteria penentuan harga opsi, hasil menunjukkan bahwa metode Truncated Black-Scholes lebih baik dibandingkan metode Black-Scholes.","Black and Scholes (1973) developed an option pricing method which has been widely applied in both academic and practical contexts. Impractical assumptions made by the Black-Scholes method including constant volatility of stock return and normal distribution of return. In fact, daily price limits are implemented in many stock exchanges. As a result of the limitation, the range returns (in the form of logarithms) of the assets traded are no longer be in the interval (-âˆž,âˆž), but truncated up and low.  With the limitation on daily price changes of these assets, Black-Scholes method is considered less precise. Therefore, a new method called Truncated Black-Scholes is developed. Furthermore, we compare the option price obtained by Truncated Black-Scholes method  and the Black-Scholes method with option market price. Using SRPE (Squared Relative Pricing Error) as the criterion of option pricing, the result demonstrates that Truncated Black-Scholes method  performs better than Black- Scholes method.","Kata Kunci : harga opsi, Black-Scholes, Truncated Black-Scholes, Truncated Distribution"
http://etd.repository.ugm.ac.id/home/detail_pencarian/108429,DISTRIBUSI LINDLEY-EKSPONENSIAL  UNTUK MEMODELKAN DATA ANTAR KEJADIAN,"RINDA DESANTY V, Dr. Herni Utami, M.Si.;Dr. Danang Teguh Qoyyimi, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Fungsi hazard merupakan fungsi yang menunjukkan tingkat terjadinya suatu peristiwa. Fungsi ini cukup penting dalam analisis data antar kejadian. Data waktu menunggu merupakan salah satu contoh data antar kejadian yang memiliki banyak aplikasi. Distribusi yang sering digunakan untuk memodelkan data waktu menunggu adalah distribusi eksponensial. Selain itu, distribusi Lindley juga merupakan distribusi yang populer untuk memodelkan data waktu menunggu. Distribusi eksponensial memiliki bentuk fungsi hazard konstan, sedangkan distribusi Lindley memiliki bentuk fungsi hazard yang bergerak naik. Pada praktiknya, tingkat terjadinya peristiwa tidak selalu konstan ataupun naik, khususnya dalam kasus data waktu menunggu. Hal ini mengakibatkan distribusi eksponensial dan Lindley terkadang dianggap kurang mampu merepresentasikan data. Oleh karena itu, dibutuhkan cara untuk menangani bentuk fungsi hazard yang berbeda-beda. Pembentukan distribusi baru merupakan salah satu cara untuk menangani keanekaragaman bentuk fungsi hazard dari data. Distribusi baru ini dibentuk berdasarkan dua distribusi yang telah ada sebelumnya. Menggabungkan distribusi eksponensial dengan distribusi Lindley memungkinkan terbentuknya distribusi baru yang dapat menangani kenaikan, penurunan serta bentuk bak mandi terbalik dari fungsi hazard. Konsep pembentukannya adalah dengan transformasi integral serta transformasi invers melalui sebuah fungsi distribusi kumulatif. Metode ini dapat menghasilkan keluarga distribusi baru yang dapat digunakan untuk menyatukan dua distribusi. Uji kecocokan data waktu menunggu menunjukkan bahwa distribusi Lindley-eksponensial lebih sesuai dengan data dibandingkan distribusi eksponensial, distribusi Lindley dan power Lindley.","Hazard function is a function that indicates rate of the event occurrence. This function is adequately important in time to event data analysis. The waiting time data is an example of time to event data. Distribution that is often used to model the waiting time data is exponential distribution. Moreover, Lindley distribution is a popular distribution that can also be used. Exponential distribution has a constan hazard function shape, while Lindley distribution has an increasing hazard function shape. However, Exponential distribution has a constant hazard shape and Lindley distribution has an increasing hazard shape. In some cases, this condition is considered  as not able to represent the dataset. Practically, the rate of the event occurrence is not always constant and increasing, especially in the waiting time data case. Therefore, a particular method is needed to handle the difference of hazard function shape.   The establishment of new distribution is one of  the methods to handle the difference of hazard function shape. This new distribution is formed by two distributions that are previously known. Compounding the Exponential and Lindley distribution enable to generate a new distribution that can handle the increasing, decreasing and also the upside down bathtub shape of a hazard function. The new distribution generating concept uses the integral transform and inverse transform through a cumulative distribution function. This method can generate a new family distribution that can be used to compound those two distributions. The distribution fitting test for the waiting time data showed that Lindley-Exponential distribution fits the data better than exponential distribution, Lindley distribution or power Lindley distribution.","Kata Kunci : fungsi hazard, distribusi Lindley-eksponensial, transformasi integral, data waktu menunggu / hazard function, Lindley-exponential distribution, integral transform, waiting time data"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114319,UKURAN RISIKO GLUE-VALUE-AT-RISK PADA DISTRIBUSI LOG-ELLIPTICAL,"FARHAN TRUNNA MAHADIKA, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2017 | Skripsi | S1 STATISTIKA,"Pengukuran risiko digunakan untuk mengetahui besar atau kecilnya risiko yang akan terjadi. Untuk mengukur besarnya risiko pada tingkat signifikansi tertentu dapat digunakan suatu metode statistik, yaitu value-at-risk (VaR) dan tail-value-at-risk (TVaR). Ukuran risiko VaR tidak tepat dalam mengukur risiko, karena terlalu rendah dalam mengukur risiko pada data yang mempunyai probabilitas di ekor yang besar. TVaR yang dapat mengatasi permasalahan VaR, akan tetapi dianggap terlalu besar dalam mengukur risiko. Oleh karena itu, akan dibahas ukuran risiko yang nilainya berada diantara VaR dan TVaR yaitu GlueVaR. Ukuran risiko GlueVaR termasuk ukuran risiko terdistorsi, sehingga GlueVaR memenuhi tiga sifat, yaitu translational invariance, positive homogeneity, dan monotonicity. Oleh karena itu, GlueVaR merupakan ukuran risiko yang baik. Pada skripsi ini akan dibahas GlueVaR dan aplikasinya pada distribusi log-elliptical, yaitu distribusi log-normal dan log-Laplace. Pada studi kasus, dibandingkan ukuran risiko VaR, TVaR, dan GlueVaR. Didapatkan bahwa GlueVaR memiliki nilai diantara VaR dan TVaR.","Measuring risk is used to know how big risk will happen. Measuring risk at any significance level can be used statistics methods, such as value-at-risk (VaR) and tail value-at-risk (TVaR). Measuring risk of data which has big probability in the tail using VaR is not accurate, because can be underestimated. TVaR can solve this VaR's problem, but it is too high in measuring risk. So, it will be discussed risk measure which has a value between VaR and TVaR, that is GlueVaR. GlueVaR is concluded risk distorted measure. So, GlueVaR meets three characters, among them translational invariance, positive homogeneity, and monotonicity. Because of that GlueVaR is a good risk measure. This undergraduate thesis aims to discuss GlueVaR and its application in log-elliptical distribution, among them log-norm distribution and log-Laplace distribution. This study is compared risk measure VaR, TVaR, and GlueVaR. In addition, GlueVaR has value between VaR and TVaR.","Kata Kunci : ukuran risiko, value at risk, VaR, tail-value-at-risk, TVaR, GlueVaR, distribusi log elliptical, distribusi log normal, distribusi log Laplace"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129170,Multivariate Adaptive Regression Splines,"BASKAMI MELIALA, Yunita Wulan Sari, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Metode Recursive Partitioning Regression (RPR) yang biasa digunakan untuk menangani kasus data nonparametrik dimana menghasilkan model yang tidak kontinu pada knots dan apabila diaplikasikan kepada data berdimensi tinggi akan menghasilkan model yang sangat sulit untuk diinterpretasikan.Maka untuk itu analisis Regresi Spline Multivariabel Adiptif digunakan untuk mengatasi kelemahan-kelemahan dari metode RPR. Friedman (1991) menunjukkan bahwa pemilihan model terbaik dengan metode Regresi Spline Multivariabel Adiptif  dilakukan dengan membandingkan semua nilai Generalized Cross Validation (GCV) yang dihasilkan tiap model kombinasi dari knots (MO), basis fungsi (BF), dan Interaksi (MI). Menggunakan data Indeks Harga Saham Gabungan (IHSG) akan dijelaskan variabel-variabel yang mempengaruhi naik dan turunnya nilai IHSG dan menginterpretasikannya.","Recursive Partitioning Regression (RPR) method commonly used to handle nonparametric data cases which produce non-continuous models on knots and when applied to high-dimensional data will produce models that are very difficult to interpret. So, for that Multivariate Adaptive Regression Splines (MARS) is used to overcome the weaknesses of the RPR method. Friedman (1991) showed that the best model selection by the Multivariate Adaptive Regression Splines method was done by comparing all Generalized Cross Validation (GCV) values generated by each combination model of knots (MO), basis function (BF), and Interaction (MI). Using the Composite Stock Price Index (IHSG) will explain the variables that affect the rise and fall of the IHSG and interpret the value.","Kata Kunci : regresi nonparametrik, spline, regresi partisi rekursif"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114068,ANALISIS KAPABILITAS PROSES PADA DATA TIDAK NORMAL DENGAN MENGGUNAKAN DISTRIBUSI BURR TIPE XII,"DESY DIAH PRAPTIWI, Dr. Danang Teguh Q., M.Sc.; Rianti Siswi Utami, S.Si, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Analisis kapabilitas proses merupakan suatu studi untuk mengukur kemampuan suatu proses dalam memenuhi spesifikasi yang ditentukan dengan asumsi proses dalam kondisi normal dan terkendali. Analisis ini dapat dilakukan dengan menghitung indeks kapabilitas proses. Untuk menghitung indeks kapabilitas proses, sebagian besar industri biasanya menganggap bahwa distribusi proses adalah normal. Namun dalam praktiknya, sebagian besar proses pengendalian kualitas tidak memenuhi asumsi normalitas dan dengan demikian akurasi indeks kapabilitas proses berdasarkan normalitas menjadi diragukan karena tidak benar-benar mencerminkan kinerja dari proses. Analisis kapabilitas proses untuk data tidak normal dapat dilakukan dengan menghitung indeks kapabilitas proses berdasarkan metode Clements. Metode tersebut menerapkan metode kuantil data tidak normal untuk menghitung indeks kapabilitas proses pada berbagai bentuk distribusi. Pada skripsi ini, indeks kapabilitas proses dengan metode tersebut akan diterapkan dengan distribusi Burr XII karena distribusi ini sangat fleksibel dan mencakup jangkauan yang luas pada berbagai bentuk distribusi. Analisis kapabilitas proses dengan menggunakan distribusi Burr tipe XII diterapkan pada data kelenturan benang jenis 30 TR 1004 Cone. Nilai indeks kapabilitas proses akan diperoleh dengan mengetahui terlebih dahulu batas spesifikasi bawah, batas spesifikasi atas dan nilai target proses produksi benang tersebut. Sebagai perbandingan, dilakukan perhitungan indeks kapabilitas proses dengan asumsi normal. Dihasilkan kesimpulan bahwa indeks kapabilitas proses dengan distribusi Burr XII memiliki nilai yang lebih rendah, sehingga dapat dikatakan indeks kapabilitas proses dengan distribusi Burr XII lebih sensitif dalam menganalisa kapabilitas proses.","Process capability analysis is a study to measure the capability of a process to meet the set of specifications, based on the assumption that the process is in normal condition and under control. This analysis can be done by calculating the process capability indices. To get the calculation, most industries generally assumed that the process has a normal distribution. However, in practice, most of the quality control processes did not meet the normality assumption, thus the accuracy of the process capability indices based on the normality assumption is becoming questionable as it does not truly reflect the performance of the process. Process capability analysis for a non-normal data can be obtained by calculating the process capability indices based on ClementsÃƒÂ¢Ã¯Â¿Â½Ã¯Â¿Â½ method. This method uses the quantile of the non-normal data to calculate the process capability indices on various distributions. This thesis applies the process capability indices with Burr XII distribution as it has a very flexible distribution that can express a wide range of distribution shapes. Process capability analysis with Burr type XII distribution is used on the elongation data from yarn 30 TR 1004 Cone. The values from the process capability indices are calculated by knowing in advance about the lower specification limit, upper specification limit, and the target value of the yarn production process. This thesis also calculates the process capability indices based on the normality assumption as a comparison. The result gives a conclusion that the process capability indices with Burr XII distribution has a lower value. Therefore, it can be said that the process capability indices with Burr XII distribution is more sensitive to analyze the process capability.","Kata Kunci : indeks kapabilitas proses, metode Clements, distribusi Burr tipe XII / process capability index, Clements Method, Burr XII distribution"
http://etd.repository.ugm.ac.id/home/detail_pencarian/112028,Optimisasi Portofolio dengan Metode Mean Variance Skewness,"ADELIA OLDIENA, Dr. Abdurakhman, M. Si.",2017 | Skripsi | S1 STATISTIKA,"Investasi merupakan kegiatan menempatkan sejumlah dana yang dimiliki pada masa kini dengan harapan bisa memperoleh suatu keuntungan di masa mendatang. Dalam melakukan investasi di pasar modal khususnya saham, investor harus melakukan manajemen agar tujuan investasi yang diinginkannya dapat tercapai dengan cara melakukan optimisasi portofolio. Portofolio adalah gabungan atau kombinasi dari berbagai instrumen atau aset investasi yang disusun untuk mencapai tujuan investasi investor. Di dalam pembentukannya, tentu setiap investor berusaha untuk memaksimalkan expected return dari investasi dengan tingkat risiko tertentu. Dengan kata lain, portofolio yang dibentuk dapat memberikan tingkat risiko terendah dengan expected return tertentu, atau dapat memberikan expected return tertinggi dengan tingkat risiko tertentu. Portofolio yang dapat mencapai tujuan di atas disebut dengan portofolio yang efisien. Pada skripsi ini akan dibahas mengenai pembentukan bobot portofolio menggunakan metode Mean Variance Skewness. Metode ini merupakan perluasan dari metode Mean Variance yang dipelopori oleh Markowitz (1952). Metode Mean Variance Skewness tidak memerlukan asumsi normalitas return seperti yang harus dipenuhi pada metode Mean Variance. Studi kasus penelitian ini menggunakan data saham harian periode 2 Maret 2015 hingga 2 Maret 2017 dari 5 saham. Sebagai simulasi nilai return yang diamati dari 5 saham yaitu Google, Yahoo, Ebay, Amazone, dan Priceline akan dibentuk suatu portofolio yang diharapkan dapat memberikan keuntungan yang optimal dengan menggunakan metode Mean Variance Skewness.","Investment is an activity by putting some funds that held today hoping to gain an advantage in the future. Doing investment in the capital market especially in stocks, investors has to do a management in order that the investments goal can be accomplished. Portfolio is combinations of various instruments or investment asset which is structured to achieve the investments goal. In the formation, of course every investor seeks to maximize the expected return of an investment with a certain level of risk. In other words, the portfolio was formed to provide the lowest risk level with a certain expected return, or can deliver the highest expected return at a certain risk level. Portfolio that can achieve the goal is called efficient portfolio.  This research will discuss the formation of a portfolio weights using the Mean Variance Skewness method. This method is an extension of Mean Variance method developed by Markowitz (1952). Mean Variance Skewness method does not require the assumption of return normality as must be met in the Mean Variance method. This research case study is using daily stocks data period of March 2, 2015 until March 2, 2017 from 5 stocks. As the simulation, return value observed from 5 stocks namely Google, Yahoo, Ebay, Amazone, and Priceline will be formed a portfolio which is expected to give optimum advantage by using Mean Variance Skewness method.","Kata Kunci : portofolio, Mean Variance Skewness"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129439,PENERAPAN METODE ADJUSTED RIDGE REGRESSION UNTUK MENGATASI MASALAH MULTIKOLINIERITAS,"CHOIRUN NISA, Drs. Zulaela, Dipl.Med.Stats., M.Si",2017 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan analisis statistika yang digunakan untuk memodelkan hubungan antara variabel dependen (Y) dan variabel independen (X). Secara umum, metode Ordinary Least Square (OLS) digunakan untuk mengestimasi koefisien regresi. Dalam metode tersebut ada asumsi klasik harus dipenuhi, diantaranya adalah tidak adanya multikolinieritas. Jika dalam model regresi terdapat multikolinieritas, menyebabkan hasil estimasi menjadi tidak stabil. Metode yang sering digunakan untuk menyelesaikan masalah multikolinieritas adalah regresi ridge. Konsep dari regresi ridge ini adalah menambah tetapan bias k dalam matriks korelasi X^T X. Namun dalam pemilihan tetapan bias k pada regresi ridge, banyak peneliti yang memberikan penentuan nilai k yang berbeda. Hal ini menjadikan tidak ada formula tunggal dari metode regresi ridge. Pada skripsi ini akan dibahas mengenai adjusted ridge regression yang dikembangkan oleh Dorugade (2016). Metode ini merupakan modifikasi dari metode regresi ridge yang dikenalkan oleh Hoerl dan Kennard (1970). Konsep regresi ridge ini yaitu mengganti tetapan bias k dengan memanfaatkan nilai dari vektor Z^T Y. Studi kasus ini menggunakan data produksi minyak kayu putih di Gunung Kidul dan faktor yang mempengaruhi pada September 2012 sampai Desember 2015. Diperoleh kesimpulan bahwa metode adjusted ridge regression memberikan nilai Mean Square Eror (MSE) yang lebih kecil daripada regresi ridge .","Regression is a statistical analysis used to model a relationship between dependent variables (Y) and independent variables (X). Ordinary least square method is used generally to obtain the estimation of regression coefficients. This method is based on the fulfillment of classical regression assumptions, which is including there is no multicollinierity between the independent variables. The existance of multicollinearity will cause the parameter estimation of ordinary least square method become unstable. 	One of the popular method to solve multicollinearity problem is the ridge regression. The concept of the ridge regression is adding a constant k to correlation matrix X^T X. But on selecting the constant k for ridge regression, many researcher proposed different approximations for it. There is no explicit formula for ridge regression method. This paper will discuss about adjusted ridge regression developed by Dorugade (2016). This method is a modification method of ridge regression estimator proposed by Hoerl and Kennard (1970). The concept of the adjusted ridge  regression is replacing the constant k with the value of vector Z^T Y. This paper case study is using eucalyptus oil  in Gunung Kidul and the factors that affecting it from September 2012 till Desember 2015. The conclusion is the adjusted ridge regression estimator gives a smaller MSE than the  ridge  regression estimator.","Kata Kunci : multicollinearity, ordinary least square, ridge  regression, adjusted ridge  regression, MSE."
http://etd.repository.ugm.ac.id/home/detail_pencarian/114336,PERSAMAAN ESTIMASI TERGENERALISASI PADA DATA LONGITUDINAL TERIMPUTASI GANDA,"FARIZ BUDI ARAFAT, Dr. Danardono, MPH",2017 | Skripsi | S1 STATISTIKA,"Statistika adalah ilmu yang mempelajari tentang data dan salah satu tujuannya adalah untuk mengetahui hubungan suatu variabel respon dengan variabel penjelas. Ilmu statistika yang dapat menerangkan hal tersebut salah satunya adalah analisis regresi. Pada data longitudinal, analisis regresi biasa tidak layak digunakan sehingga digunakan persamaan estimasi tergeneralisasi untuk mengatasi regresi pada data longitudinal. Pada data longitudinal, terjadinya kasus data hilang sering terjadi, sehingga data yang hilang tersebut perlu diestimasi agar penelitian yang telah dilakukan tidak sia-sia. Estimasi yang dilakukan adalah dengan imputasi ganda untuk mengatasi kesalahan yang mungkin terjadi pada imputasi tunggal. Imputasi ganda dengan persamaan estimasi tergeneralisasi yang digunakan diadaptasi dari prosedur yang telah ada. Pada perangkat lunak R telah tersedia paket mice untuk melakukan imputasi ganda dan paket gee untuk melakukan persamaan estimasi tergeneralisasi. Performa dari metode ini akan ditunjukkan melalui simulasi dan analisis menggunakan data nyata yaitu uji toksisitas ekstrak etanonik herba pacing.","Statistics is a study about data and one of its purpose is to know the relationship between responds and predictors. Statistical analysis which shows the relationship is regression. On longitudinal data, regression is not correct to be used so generalized estimating equations is used to estimate regression parameter on longitudinal data. On longitudinal data, case of missing value is often happen, so the missing value need to be estimated in order to make the research is not wasteful. Estimates that is used is multiple imputation to maintain mistakes that may happen in single imputation. Multiple imputation generalized estimating equations is used by adapting currently available procedures. R software provides a mice package which is used to do multiple imputation and gee package which is used to do generalized estimating equations. The performance of this method will be demonstrated through simulation and analysis of real data on experiment the toxicity of etanolic extract of Coctus speciosus.","Kata Kunci : persamaan estimasi tergeneralisasi, generalized estimating equations, imputasi ganda, multiple imputation, mice, data longitudinal"
http://etd.repository.ugm.ac.id/home/detail_pencarian/110755,MODEL REGRESI HURDLE BINOMIAL NEGATIF (Studi Kasus: Pemodelan Frekuensi Klaim Kendaraan Bermotor),"RATNA MUTIA KHARISMANINGRUM, Prof. Dr. Sri Haryatmi, M.Sc.",2017 | Skripsi | S1 STATISTIKA,Model regresi Poisson umum digunakan untuk memodelkan data yang memuat variabel dependen berupa data cacah. Asumsi dari model ini adalah equidispersi dimana nilai mean dan variansi pada variabel dependen harus bernilai sama. Akan tetapi data yang sesungguhnya lebih sering memiliki mean dan variansi yang berbeda. Overdispersi adalah keadaan dimana mean data lebih kecil daripada variansinya. Sedangkan underdispersi adalah keadaan dimana mean data lebih besar dari variansinya. Selain itu analisis regresi Poisson juga tidak mampu mengatasi kelebihan nilai nol pada variabel dependen. Salah satu alternatif untuk mengatasi masalah kelebihan nol dan pelanggaran asumsi equidispersi adalah medel regresi Hurdle Binomial Negatif.  Model ini merupakan gabungan dari model regresi binomial negatif dengan regresi logistik. Variabel respon dimodelkan kedalam 2 buah model yaitu model zero dan model count. Metode Maximum Likelihood Estimator dengan algoritma BFGS digunakan untuk mengestimasi parameter model ini. Kriteria pemilihan model menggunakan nilai AIC dan loglikelihood.     Dalam skripsi ini model Hurdle Binomial Negatif diaplikasikan pada data frekuensi klaim kendaraan bermotor dan diperoleh niali AIC dari model Hurdle Binomial Negatif lebih kecil dibandingkan dengan menggunakan model regresi Poisson dan regresi Binomial Negatif.,"Poisson Regression is commonly used to model a discrete response variable with independent variables. One of the most important assumptions in this model is equidispersion in which the response variable has an equal value of variance and expected value (mean). A real data usually has different mean and variance. Overdispersion occurs when the value of the mean lower than the variance whereas when the variance lower than the mean can be called as an underdispersion. Besides, Poisson regression cannot overcome response variable with excess zero. An alternative solution to overcome excess zero and overdispersion or underdispersion is Hurdle Negative Binomial regression model.  Hurdle Negative Binomial regression model consists of logistic regression and negative binomial regression. The response variable is modeled in two parts, zero part and count part. Maximum Likelihood Estimator with BFGS algorithm is used to estimate parameter in this model. While AIC and log-likelihood value are used to select the best model.  In this paper, Hurdle Negative Binomial is applied to modeling frequency of claims in vehicle insurance. The result is Hurdle Negative Binomial has a smaller value of AIC and largest value of log-likelihood. The study case reveals an information that Hurdle Negative Binomial performs best to overcome excess zero and underdispersion or overdispersion than Poisson regression and negative binomial regression.","Kata Kunci : overdispersi, underdispersi, kelebihan nol, hurdle binomial negatif, BFGS"
http://etd.repository.ugm.ac.id/home/detail_pencarian/108198,ALGORITMA FAST GREEDY UNTUK MENDETEKSI OUTLIER,"SEFTIAN HARI MURTI, Yunita Wulan Sari, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Data mining adalah ekstrasi atau penggalian data yang diinginkan dari data jumlah yang besar. Sedangkan outlier adalah sebuah titik data pada suatu dataset dimana sangat berbeda dibandingkan dengan titik data pada dataset pada umumnya. Outlier tidak bisa langsung disimpulkan sebagai sebuah keuntungan atau sebagai masalah namun harus dipandang dalam konteks analisi dan harus dievaluasi mengenai kemungkina informasi yang bisa didapatkan. Fungsi deteksi outlier adalah suatu teknik untuk mencari obyek dimana obyek tersebut mempunyai perilaku yang berbeda dari obyek-obyek yang lainya. Penerapan deteksi outlier dapat ditemukan pada sejumlah bidang kegunaan, seperti deteksi kecurangan (fraud detection), deteksi penyusupan (intrusion detection), gangguan ekosistem, noise pada citra, kesehatan masyarakat dan kedokteran. Kebanyakan metode yang ada didesain untuk data numerik. Algoritma fast greedy akan mengatasi masalah yang ada pada aplikasi di dunia nyata yang mengandung data katagorikal. Algortima fast greedy dapat mendeteksi outlier dengan menggunakan rumus fungsi entropy untuk perhitungannya.","Data minig is the extraction the desired data from large amounts of data. ehile outlier defined as a data point a dataset where us different than point from common dataset. Outliers can not be directly inferred as an advantage or as a problem, but shouldbe viewed in the context og the analysis and should be evaluated for possibility thar information could be obtained. Outlier detection funsction is a technique to lookfor objects to which these objects have different behaviors from other objects. Application of outlier detection can be found in some areas of usability, such as the fraud detection, intrusion detection, disruption of ecosystems, noise in the imgages, public health and medicine. Most exiting methods are designed for numeric data. Fast greedy algorithm wlii solve the existing problems in real world applications containing categorical data. Fast greedy algorithm can detect outlier using entropy function formula for calculation.","Kata Kunci : data mining, outlier, outlier detection, fast greedy, entropy"
http://etd.repository.ugm.ac.id/home/detail_pencarian/111018,Model Berbasis Klaster Jaringan Sosial dengan Metode Markov Chain Monte Carlo pada Data Teks Pembukaan Undang-Undang Dasar 1945,"AYYOUB MAULANA H, Dr. Danardono, MPH; Yunita Wulan Sari, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Jaringan sosial merupakan sebuah kumpulan simpul-simpul yang memiliki relasi satu dengan yang lainnya. Seiring bertambah pesatnya perkembangan teknologi, maka data jaringan sosial bertambah banyak dan tentunya semakin heterogen. Untuk dapat memperoleh suatu hal yang informatif dari data tersebut, maka digunakan metode klastering untuk mengetahui subjek-subjek yang memiliki kedekatan/kesamaan. Kekurangan dari klastering untuk data jaringan sosial adalah perlu adanya nilai-nilai laten yang harus digunakan, maka dengan menggunakan model berbasis klaster jaringan sosial dengan metode Markov Chain Monte Carlo, klastering data jaringan sosial dapat ditangani. Studi kasus pada penelitian ini menggunakan data teks Pembukaan UndangUndang Dasar 1945, dimana teks tersebut akan dipecah menjadi kumpulan kata, dan setiap kata akan diklasterkan. Kumpulan kata yang terdapat disekitar rata-rata klaster atau yang memiliki rata-rata dari prediksi probabilitas relasi yang besar dalam suatu klaster, dianggap menjadi sebuah topik dominan. Maka, dengan metode ini, topik-topik yang dominan dalam teks dapat diperoleh.","Social networks is a collection of nodes which has a relation each other. With an increasing of technology development, then there are big amount of social networks data and more heterogen. To obtain informative information from this data, clustering method is used to find out subjects which has closeness/simmilarity. The Disadvantage of clustering for social networks data is need to existing latent values to be used, then using model based clustering social networks with Markov Chain Monte Carlo method, clustering for social networks data is can be handled. Case study in this research is using Preamble text data of Undang-Undang Dasar 1945, where this text will be broken into a group of words, and every word will be clustered. A group of words which near to cluster mean or has a big mean of relation probability prediction in a certain cluster, can be set to dominant topic. So, with this method, topics in the text can be obtain.","Kata Kunci : Text Clustering, Markov Chain Monte Carlo, Preamble of UndangUndang Dasar 1945"
http://etd.repository.ugm.ac.id/home/detail_pencarian/116650,UKURAN JARAK BARU (NEW DISSIMILARITY) DALAM ALGORITMA CLUSTERING K-MODES,"HAQIQI HARDANDY, Yunita Wulan Sari, S.Si., M.Sc.;Vemmie Nastiti Lestari, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"K-modes merupakan salah satu teknik dalam analisis klaster yang merupakan perluasan dari algoritma k-means untuk mengelompokkan data kategorik. Analisis algoritma k-modes memodifikasi pengukuran jarak k-means standar menggunakan ukuran jarak kecocokan sederhana (simple matching dissimilarity), namun pengukuran jarak tersebut menghasilkan kesamaan dalam klaster yang lemah karena tidak mempertimbangkan frekuensi relatif dari nilai atribut masing-masing klaster. Penelitian ini membahas pengukuran jarak sederhana dengan pengukuran jarak yang telah diperbaharui. Pengukuran jarak baru memperhitungkan frekuensi relatif dari nilai atribut masing-masing klaster. Selain untuk memperbaharui formula menghitung jarak dalam k-modes, pengukuran jarak baru bertujuan membuktikan objek masuk dalam keanggotaan klaster memiliki kehomogenan yang kuat. Hasil klaster yang terbentuk lebih merepresentasikan jarak sebenarnya dan menghasilkan kesamaan di dalam klaster yang lebih kuat sehingga hasil klaster lebih akurat.","K-modes algorithm is one of cluster analysis as an extension of k-means algorithm to classify the categorical objects. The k-modes algorithm modifies the measure of standard k-means distance using simple matching dissimilarity, but it yields such weak similarity in cluster because it does not consider the relative frequency of the attribute value in each cluster. This research aims to describe the simple matching dissimilarity measure and the new dissimilarity measure. The new dissimilarity measure considers calculating on the relative frequency of the attribute value in each cluster. It is not only to update the formula of calculating the distance in k-modes, but also to prove that the objects entered as the member of each cluster have the strong homogeneous. This methode results in a better clustering accurary because the result represents the distance of objects to central cluster for more real and has stronger similarity in cluster.","Kata Kunci : algoritma k-modes, analisis klaster, data kategorik"
http://etd.repository.ugm.ac.id/home/detail_pencarian/110766,DISTRIBUSI NEGATIF BINOMIAL-BETA EKSPONENSIAL PADA KASUS OVERDISPERSI,"KARTIKA ELFRIDA BR SINGARIMBUN , Prof.Drs. Subanar, Ph.D (Pembimbing Utama); Rianti Siswi Utami, S.Si.,M.Sc. (Pembimbing 2)",2017 | Skripsi | S1 STATISTIKA,"Distribusi Poisson merupakan model standar untuk menganalisis data cacah dengan asumsi ekuidispersi, yaitu kondisi dimana nilai rataan dan variansi data bernilai sama. Tetapi pada kenyataannya, data cacah seringkali mengalami overdispersi dimana variansi data lebih besar dari rata-ratanya. Adanya overdispersi dalam data menyebabkan nilai prediksi menjadi tidak tepat sehingga distribusi Poisson tidak layak digunakan. Salah satu alternatif yang dapat digunakan untuk mengatasi masalah overdispersi adalah dengan menggunakan distribusi negatif binomial beta eksponensial yang merupakan distribusi campuran baru dengan mencampurkan distribusi negatif binomial dengan beta eksponensial.  Parameter dari distribusi negatif binomial-beta eksponensial akan diestimasi menggunakan metode Maksimum Likelihood dan dengan bantuan algoritma Newton-Raphson. Distribusi negatif binomial-beta eksponensial diaplikasikan pada data kecelakaan di daerah Provinsi Sulawesi Tenggara dari Januari 2012 sampai dengan Desember 2015 serta dibandingkan dengan distribusi Poisson dan negatif binomial. Hasil analisis dengan menggunakan chi-square goodness of fit serta nilai Akaike Information Criterion (AIC) dan Bayesian Information Criterion (BIC) terkecil menunjukkan model distribusi negatif binomial-beta eksponensial lebih layak digunakan pada data cacah yang mengandung overdispersi.","Poisson distribution is a standard model to analyze count data, by assuming equidipersion, i.e. it has same mean and variance value. But in fact, count data often has variance value greater than mean value, this condition is called by overdispersion. The existence of overdispersion problem in the data can cause the value of the prediction is not valid so that Poisson distribution can not be applied. One of alternative solution to overcome this problem is by using a negative binomial-beta exponential. Negative binomial-beta exponential distribution is a new mixed negative binomial distribution obtain by mixing the negative binomial distribution with a beta exponential distribution. Parameter of negative binomial-beta exponential distribution will be estimated using maximum likelihood method with the help of Newton Raphson algorithm. Negative binomial beta exponential distribution was applied to the accident data in the area of South-east Sulawesi from January 2012 until December 2015 and compared with Poisson and negative binomial distribution. Based on chi-square goodness of fit and the smaller value of Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) it is clear that negative binomial beta exponential provides better fit for count data with overdispersion.","Kata Kunci : Kata kunci : data cacah, distribusi campuran negatif binomial, overdispersi./Keywords : count data, mixed negative binomial distribution, overdispersion."
http://etd.repository.ugm.ac.id/home/detail_pencarian/117166,PENENTUAN HARGA OPSI BARRIER DENGAN MODEL TRINOMIAL INTERPOLASI DAN TRINOMIAL RITCHKEN,"ANANG MAULANA, Prof. Dr.rer.nat. Dedi Rosadi, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Metode tentang penentuan harga opsi terus mengalami perkembangan dari waktu ke waktu,tidak terkecuali untuk opsi barrier. Opsi barrier merupakan salah satu jenis opsi path-dependent, yaitu opsi yang nilainya bergantung dari perjalanan harga saham selama masa hidup opsi. Penentuan harga opsi barrier dapat dilakukan dengan solusi analitik maupun numerik. Formula Merton merupakan penentuan harga opsi barrier secara analitik. Sedangkan secara numerik, dapat digunakan metode lattice yang salah satunya adalah model trinomial, metode ini digunakan karena lebih fleksibel dan lebih sederhana. Namun, model trinomial standar memberikan hasil numerik harga opsi barrier yang tidak akurat dan cenderung berfluktiatif, hal ini disebabkan karena nilai barrier yang digunakan dalam pohon trinomial bukanah nilai barrier yang sesungguhnya. Maka dari itu dibentuklah modifikasi dari model trinomial standar ini, yaitu model trinomial interpolasi dan model trinomial Ritchken. Model trinomial interpolasi memanfaatkan node diantara nilai barrier yang kemudian dilakukan teknik interpolasi. Sedangkan model trinomial Ritchken memodifikasi parameter lambda sehingga nilai barrier tepat pada suatu node pada pohon trinomial.","The method of option pricing keeps progressing over time, including barrier option. Barrier option is one type of path-dependent options, that is an option whose value depends on the path of stock price during the period of the option. Barrier option pricing can be done with numerical or analytical solution. The Merton formula is pricing barrier option analytically. While numerically, it can use the lattice methods which one of them is trinomial model. This method is used because it is more flexible and simpler. However, the standard trinomial model gives innacurate and fluctuative price of the barrier option. It is because the barrier value that used in trinomial tree is not the true barrier value. Therefore, it is needed to use the modification of the standard trinomial model that is the interpolation trinomial and Ritchken trinomial model. The interpolation trinomial model uses the node between the barrier value and then performing interpolation technique. While the Ritchken trinomial model modifies the lambda parameter so the barrier value is exactly on a node of the trinomial tree.","Kata Kunci : opsi barrier, pohon trinomial, trinomial interpolasi, trinomial Ritchken"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129459,REDUKSI DIMENSI MENGGUNAKAN REGRESI SLICED INVERSE,"NANDA KURNIAWAN, Drs. Zulaela, Dipl. Med.Stats., M.Si.",2017 | Skripsi | S1 STATISTIKA,"Pada masa ini, data hasil pengamatan seringkali mempunyai jumlah fitur yang sangat banyak, yang dikenal dengan data berdimensi tinggi. Proses analisis data ini akan memerlukan waktu komputasi yang lama, sehingga diperlukan reduksi dimensi untuk memperkecil dimensi data tanpa kehilangan informasi. Regresi Sliced Inverse (Sliced Inverse Regression/SIR) merupakan metode yang dapat diterapkan dalam pereduksian dimensi pada data berdimensi tinggi. Metode SIR melakukan pereduksian dimensi variabel independen tanpa melalui proses model-fitting, baik proses parametrik maupun nonparametrik.","In this age, observation data often have many fitures, commonly known as high-dimensional data. This data analysis process certainly takes a long computational time.  The process of this data analysis will need a long computational time, thus dimension reduction is needed to reduce data dimension without loss of information. Sliced Inverse Regression is a method that can be applied in dimension reduction on high-dimensional data. SIR conduct independent variable dimension reduction without model-fitting process, either parametric or nonparametric.","Kata Kunci : Analisis Regresi, Reduksi Dimensi, Regresi Sliced Inverse, Regression Analysis, Dimension Reduction, Sliced Inverse Regression."
http://etd.repository.ugm.ac.id/home/detail_pencarian/131512,PENANGANAN KETIDAKSEIMBANGAN KELAS MENGGUNAKAN ADAPTIVE SYNTHETIC SAMPLING APPROACH (ADASYN) UNTUK KLASIFIKASI MENGGUNAKAN METODE RANDOM FOREST,"IDFI TANIYA KUSUMA, Prof. Dr.rer.nat Dedi Rosadi, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Masalah ketidakseimbangan terjadi apabila terdapat kelas yang sangat kurang terwakili dibandingkan kelas lainnya dalam suatu dataset. Ketidakseimbangan kelas akan menjadi masalah pada kasus klasifikasi, sebab algoritma klasifikasi yang bekerja pada data yang tidak seimbang akan menghasilkan prediksi yang menyesatkan dan tingkat akurasi yang buruk. Oleh karena itu muncul metode ADASYN sebagai salah satu cara dalam menyeimbangkan kelas dengan cara menghasilkan data sintetis pada kelas minoritas agar algoritma klasifikasi dapat bekerja lebih baik. ADASYN  efektif  bekerja pada variabel prediksi berjumlah 2 kelas (binary class) dengan tipe data variabel respon numerik. Keefektifan ADASYN akan ditunjukkan melalui implementasinya pada dataset pasien wanita diabetes. Metode ini akan dikombinasikan dengan algoritma klasifikasi Random Forest untuk mendiagnosis penyakit diabetes pasien. Hasilnya terbukti bahwa dengan menyeimbangkan kelas algoritma klasifikasi Random Forest memiliki performa yang lebih baik. Hal ini ditunjukkan dengan nilai Recall, Precision dan F-measure yang lebih tinggi daripada algoritma Random Forest yang bekerja pada data yang tidak seimbang.","The problem of imbalance occurs when there is a class that is poorly represented than the other classes in a dataset. The class imbalance will be a problem in the classification case, because classification algorithms working on unbalanced data will produce misleading predictions as well as poor accuracy. Therefore, ADASYN method appears as one way to balance the class by generating synthetic data in minority class so that the classification algorithm can work better. ADASYN effectively works on predictive variables of 2 classes (binary class) with numerical response data type variables. The effectiveness of this ADASYN method will be demonstrated through its implementation on the dataset of diabetic female patients. This method will be combined with the Random Forest classification algorithm to diagnose diabetic patients. The result proved that by balancing the class, Random Forest classification algorithm has better performance. This is indicated by the value of Recall, Precision and F-measure that higher than Random Forest algorithms working on imbalanced data.","Kata Kunci : ketidakseimbangan kelas, klasifikasi, ADASYN, binary class, Random Forest"
http://etd.repository.ugm.ac.id/home/detail_pencarian/131261,ANALISIS REGRESI RIDGE TERGENERALISIR DUA TAHAP UNTUK MASALAH MULTIKOLINIERITAS DAN AUTOKORELASI PADA ERRORS  (GENERALIZED TWO STAGES RIDGE REGRESSION ANALYSIS FOR MULTICOLLINEARITY AND AUTOCORRELATED ERRORS ),"FATHIA NUR RAHMANI, Yunita Wulan Sari.,M.Sc",2017 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan salah satu analisis di dalam statistika yang digunakan untuk memodelkan suatu hubungan antar variabel independen dan variabel dependen. Namun, di dalam permasalahan ekonomi hubungan variabel ekonomi tidak hanya bersifat satu arah tetapi bersifat saling mempengaruhi yang disebut sebagai model persamaan simultan. Metode yang digunakan adalah two stages teast square. Dalam analisis regresi terdapat beberapa asumsi klasik yang harus terpenuhi, diantaranya adalah tidak ada multikolinieritas dan tidak ada autokorelasi pada error. Jika kedua asumsi tersebut tidak terpenuhi, maka dapat menyebabkan hasil estimasi yang tidak stabil dan tidak efektif. Salah satu metode yang digunakan untuk menangani kedua masalah tersebut adalah metode generalized ridge regression. Pada skripsi ini akan dibahas mengenai analisis regresi pada suatu model persamaan simultan yang mengandung multikolinieritas dan autokorelasi pada error yaitu generalized two stage ridge regression yang dikembangkan oleh Eledum, Hussein & Abdalla Ahmed Alkhaifa (2012) yang nantinya akan dibandingkan dengan two stage ridge regression dalam skripsi Astrini, E. W (2013). Studi kasus ini membahas mengenai faktor-faktor yang mempengaruhi jumlah uang beredar pada Januari 2012 sampai Juni 2017. Diperoleh kesimpulan bahwa generalized two stage ridge regression memberikan nilai MSE yang lebih kecil dibandingkan two stage ridge regression.","Regression analysis is one of statistics analysis used to model a relationship between independent and dependent variable. In the economics problem, relationship between economics variable is not singular equation and we can call it simultaneous equation model. The method that we use is two stages teast square. In regression analysis there are classical regression assumptions that must be fulfilled, which are no multicollinearity and no autocorrelated errors. If both of them do not fulfill, it makes the result of estimation is unstable and uneffective. One of the method to solve that problems is generalized ridge regression. This paper will discuss about regression analysis in simultaneous equation model with multicollinearity and autocorrelated errors, that is generalized two stage ridge regression developed by Eledum, Hussein & Abdalla Ahmed Alkhaifa (2012) and then we want to compare with two stage ridge regression in paper by Astrini, E. W (2013). Case study in this paper is using factors that influence with money supply variable in January 2012 until June 2017. The conclusion is generalized two stage ridge regression estimator gives smaller MSE than two stage ridge regression estimator.","Kata Kunci : Persamaan Simultan, Two Stage Least Square, Multikolinieritas, Autokorelasi Errors, Generalized Ridge Regression, MSE/Simultaneous Equation, Two Stage Least Square, Multicollinearity, Autocorrelated Errors, Generalized Ridge Regression, MSE."
http://etd.repository.ugm.ac.id/home/detail_pencarian/107971,METODE HYBRID KOMBINASI DARI MODIFIED K-PROTOTYPES DAN C5.0 UNTUK CREDIT SCORING,"GILANG RAMADHAN, Dr. Gunardi, M.Si ; Widya Irmaningtyas, M.Sc",2017 | Skripsi | S1 STATISTIKA,"Credit scoring merupakan salah satu teknik manajemen risiko untuk meminimalisasi terjadinya gagal bayar. Dengan menggunakan pemodelan statistika, credit scoring dapat dikerjakan menggunakan dua metode, yaitu unsupervised learning dan supervised learning. Perbedaan antara kedua metode tersebut terletak pada pembentukan model dan pengelompokkan objek. Pada supervised learning terdapat proses pelatihan data dalam membentuk modelnya dan data akan diklasifikasi terhadap kelas yang telah ditentukan sebelumnya, sedangkan pada unsupervised learning tidak. Pada tugas akhir ini, akan digunakan kombinasi antara kedua metode tersebut yang selanjutnya dikenal dengan metode hybrid. Pada tahap pertama, debitur akan dikelompokkan menurut kesamaan karakteristik kedalam beberapa klaster dengan menggunakan analisis klaster modified k-prototypes. Metode tersebut merupakan salah satu metode unsupervised learning. Jumlah klaster optimal akan ditentukan menggunakan silhouette width dengan membandingkan beberapa jumlah klaster. Pada tahap kedua, pohon keputusan akan dibangun untuk masing-masing klaster dengan menggunakan algoritma C5.0. Algoritma tersebut merupakan salah satu metode supervised learning. Pada akhirnya, akan terbentuk aturan untuk masing-masing klaster dalam mengklasifikasikan debitur baik dan debitur buruk.","Credit scoring is one of risk management analysis method for minimizing default. Using statistical modeling, credit scoring can be used in two ways: unsupervised learning and supervised learning. The difference between these two methods are in forming the model and grouping the object. In supervised learning  there is learning process in forming models and objects will be classified into pre-classified class, while in unspervised learning does not need that all. In this research, will use a combination of both method, called hybrid method. In the first step, by using modified k-prototypes which is one of unsupervised  learning method, debtors are segmented into clusters with similar features. The optimal number cluster is defined using silhouette width, by comparing some cluster number. In the second step, by using C5.0 algorithm which is one of supervised learning method, decision trees are built for each cluster to define classification rules for good debtor and bad debtor.","Kata Kunci : Credit scoring, analisis klaster, modified k-prototypes, pohon keputusan, C5.0, silhouette width"
http://etd.repository.ugm.ac.id/home/detail_pencarian/110531,PERHITUNGAN PREMI TOTAL MENGGUNAKAN DISTRIBUSI NEGATIVE BINOMIAL - GENERALIZED EXPONENTIAL (NB-GE) PADA PEMODELAN FREKUENSI KLAIM ASURANSI,"IKA PURNAMASARI, Dr. Herni Utami, M.Si. ; Dr. Adhitya Ronnie Effendie, S.Si., M.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Perhitungan premi yang efektif perlu dilakukan perusahaan asuransi untuk dapat mencapai keseimbangan finansial perusahaan, selain agar tidak terlalu membebani pemegang polis. Prinsip perhitungan premi sederhana yang dapat dilakukan perusahaan adalah menghitung nilai ekspektasi resiko asuransi, yang kemudian ini disebut premi murni. Resiko yang dimaksud berupa klaim yang diajukan pemegang polis, meliputi frekuensi klaim dan besar klaim (severity). Oleh karena itu, perlu diketahui distribusi yang paling cocok menggambarkan penyebaran data frekuensi klaim dan juga data besar klaim (severity). Frekuensi klaim dimodelkan dengan distribusi Negative Binomial - Generalized Exponential karena datanya yang mengandung zero excess dan heavy tail, sehingga kurang cocok dimodelkan menggunakan distribusi Poisson maupun distribusi Negative Binomial. Hal ini karena data mengalami kasus overdispersi. Sementara itu, data besar klaim (severity) dimodelkan dengan distribusi Lognormal, ini yang paling cocok dibandingkan distribusi Weibull dan distribusi Eksponensial. Setelah itu, dihitung nilai ekspektasi dari frekuensi klaim dan besar klaim (severity). Nilai ini digunakan untuk menghitung premi murni.","The effective insurance premium calculation needs to do by the insurance company to achieve financial balanced of the company, other than that so as not to overburden the policy holder. The simplest premium principle calculation do by insurance company is the expected value of insurance risk, which then is called net premium. The risks referred to in the form of policy holder claims filed, including the frequency of claims and amount of claims (severity). Therefore, important to know the most suitable distribution which can describe the distribution of claims frequency and the severity. The frequency of claims is modeled by Negative Binomial distribution - Generalized Exponential because the data that contains zero excess and heavy tail, making it less suitable modeled using a Poisson distribution and Negative Binomial distribution. This is because the data is overdispersed. Meanwhile, the amount the claims (severity) modeled by the Lognormal distribution, which is the most suitable than modeled by Exponential distribution and Weibull distribution. After that, calculated the expected value of the frequency of claims and the severity. This value is used to calculate net premiums.","Kata Kunci : frekuensi klaim, zero excess, heavy tail, overdispersi, besar klaim (severity), premi murni (net premium)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/131272,Segmentasi Pasar dengan Metode Ensembel QROCK pada Data Campuran Kategorik dan Numerik,"MIYA SALSABILA, Dr. Gunardi, M.Si.",2017 | Skripsi | S1 STATISTIKA,"Dalam mengoptimalkan pelayanan terhadap konsumen, salah satu hal yang dapat diterapkan perusahaan adalah segmentasi pasar. Di mana segmen-segmen yang nantinya dihasilkan, berasal dari data konsumen itu sendiri. Tidak jarang dengan adanya data konsumen yang sangat banyak, diperlukan suatu metode untuk terlebih dahulu mengelompokkan data-data tersebut agar dapat dianalisis lebih lanjut dalam mendapatkan informasi yang berguna. Namun kerap kali data terdiri dari data kategorik dan numerik. Pengelompokkan untuk data tersebut kurang akurat jika dilakukan masing-masing untuk data kategorik dan numerik. Maka diperlukan adanya metode untuk pengelompokkan data campuran kategorik dan numerik. Salah satu metode pengelompokkan untuk data campuran adalah metode ensembel dengan algCEBMDC.  	Tahap yang dilakukan dalam metode ensembel dengan algCEBMDC` adalah mengelompokkan dahulu masing-masing data kategorik dan numerik. Untuk data kategorik menggunakan algoritma QROCK dan untuk data numerik menggunakan algoritma agglomerative. Kemudian mengelompokkan data campuran hasil pengelompokkan pada tahap sebelumnya dengan algoritma QROCK. Pada setiap tahap dilakukan proses evaluasi. Evaluasi yang dipilih menggunakan perbandingan nilai rasio SW dan SB. Di mana kinerja suatu metode pengelompokan semakin baik jika semakin kecil rasio antara SW dan SB. Artinya bahwa terdapat homogenitas maksimum dalam kelompok dan heterogenitas maksimum antar kelompok.","In order to optimizing service for consumer, market segmentation can be used by company. Segments which are obtained by market segmentation, are taken from consumer data. Oftentimes, existence of many of data consumer needs a method for clustering data so that it can be analyzed in obtaining a useful information. But often data is composed by categorical and numeric dataset. Clustering for that dataset is less accurate  if it is analyzed respectively categorical and numeric dataset. Therefore, it needs a method for clustering mixed categorical and numeric dataset. One of that methods is ensemble method using algCEBMDC. 	First of ensemble method using algCEBMDC step is clustering each categorical and numeric dataset. Categorical data uses QROCK algorithm and numeric data uses agglomerative algorithm. Then clustering mixed data that obtained from previous step using QROCK algorithm. In each step, there is evaluation process. Evaluation that used in this method is comparison value of ratio SW and SB. Performance of clustering method is better when ratio SW and SB is smaller than the others. That means, there is maximum homogeneity within cluster and maximum heterogeneity between clusters.","Kata Kunci : market segmentation, clustering, ensemble, QROCK, agglomerative, segmentasi pasar, pengelompokan, ensembel"
http://etd.repository.ugm.ac.id/home/detail_pencarian/113613,Analisis Regresi Deming,"ANGELA MERICI YOSHINTA R, Dr. Herni Utami, M.Si",2017 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis yang digunakan untuk mengetahui hubungan antara satu atau lebih variabel dependen dengan variabel independen. Regresi klasik mengasumsikan kesalahan dalam variabel dependen saja sedangkan variabel independen merupakan variabel tetap. Jika variabel independen mengandung kesalahan pengukuran maka apabila menggunakan regresi linier klasik akan menyebabkan bias. Regresi deming digunakan untuk mengatasi masalah ini.  Regresi deming merupakan metode statistik penting yang harus digunakan ketika membandingkan dua pengukuran yang keduanya memiliki kesalahan pengukuran. Pada model regresi deming, variabel dependen dan variabel independen merupakan variabel acak dimana errornya berdistribusi normal . Sebagai rasio presisi dari dua metode pengukuran, Î» merupakan parameter yang sangat penting dalam regresi deming. Î» dihitung menggunakan beberapa pengukuran berulang dari setiap subyek. Kesalahan standar dari koefisien regresi dan nilai prediksi dihitung menggunakan metode jacknife terhapus-1.","Regression analysis is an analysis used to determine the relationship between one or more independent variables with the dependent variable. Classical regression assumes errors only in the dependent variable and the independent variable is a fixed variable. If the independent variable contain measurement error then using classical linear regression will be biased. Deming regression is used to solve this problem.  Deming regression is an important statistical method that should be used when comparing two measurements that both have measurement error. The independent variables and the dependent variable is a random variable which the errors is normally distributed . As the precision ratio of two measurement methods, Î» is a very important parameter in deming regression. Î» is calculated using multiple measurements from each subject. The standard errors of the regression coefficients and predicted values are calculated using the jackknife leave-one-out method.","Kata Kunci : regresi  deming, kesalahan  pengukuran, random  X, estimasi  maksimum likelihood,metode jacknife"
http://etd.repository.ugm.ac.id/home/detail_pencarian/108239,Model Durbin Spasial,"IRMA RACHMAWATI, Drs. Zulaela, Dipl. Med. Stats., M.Si.",2017 | Skripsi | S1 STATISTIKA,"Model regresi spasial adalah model yang terbentuk dari regresi umum yang terdapat pengaruh spasial (lokasi). Anselin (1988) mengenalkan satu Model Autoregresi Spasial (SAR) yang terdapat efek lag spasial pada variabel dependen. Kasus khusus dari metode SAR adalah penambahan efek lag pada variabel independen. Model ini disebut Model Durbin Spasial (SDM) yang berkembang karena dependensi spasial tidak hanya terdapat pada variabel dependen tetapi terdapat pula pada variabel independen. Pengujian Spatial Dependence dilakukan untuk melihat data setiap variabel memiliki pengaruh spasial pada lokasi. Pemodelan didasarkan pada pengaruh dependensi spasial, sehingga sebelum dilakukan pemodelan perlu dilakukan pengujian pengaruh spasial yang terkandung dalam data menggunakan statistik uji Lagrange Multiplier (LM). Pada tugas akhir ini, berdasarkan hasil pemodelan SDM diketahui bahwa faktor-faktor yang berpengaruh terhadap Angka Putus Sekolah usia SMP di Jawa Tengah adalah rata-rata anggota rumah tangga dan persentase desa terpencil di tiap kabupaten/kota.","Spatial regression model is a model that formed from the general regression which be found the effect of spatial (location). Anselin (1988) has shown spatial autoregressive model (SAR) is spatial lag effect on the dependent variable.  Special cases of SAR mode is add lag effect of the independent variable. This model is called Spatial Durbin Model (SDM) which developed because the dependencies in the spatial relationships not only occur in the dependent variable, but also on the independent variables. Spatial Dependence test performed to see data for each variable has an effect on the spatial location. The modeling is based on the effect of spatial dependence, so that prior to modeling necessary to test the spatial effect of data using statistical Lagrange Multiplier (LM) test. In this thesis, based on the results of modeling SDM note that the factors affecting of junior high school dropout rates in Central Java is the average of household members and percentage of remote villages in each district/city.","Kata Kunci : Spasial, SAR ,Morans' I,  Lagrange Multiplier, dan  SDM."
http://etd.repository.ugm.ac.id/home/detail_pencarian/114403,METODE ESTIMASI-MM ROBUST PADA ANALISIS REGRESI LINEAR,"ANGGRAYNY PUTRY M, Danang Teguh Qoyyimi, M.Sc., Ph.D.",2017 | Skripsi | S1 STATISTIKA,"Analisis regresi linear merupakan analisis yang bertujuan untuk mengetahui hubungan antar variabel. Salah satu metode yang sering digunakan untuk mengestimasi parameter dalam analisis regresi linear ialah metode kuadrat terkecil. Meskipun metode estimasi kuadrat terkecil ini relatif mudah, metode ini memiliki asumsi klasik yang harus terpenuhi, salah satunya yaitu residual berdistribusi normal. Pada kenyataannya, masih terdapat data yang tidak memenuhi asumsi klasik tersebut, sehingga metode estimasi ini tidak dapat dilakukan. Skripsi ini akan membahas tentang metode estimasi robust pada analisis regresi linear yang merupakan metode alternatif dari metode kuadrat terkecil saat residual tidak berdistribusi normal atau saat model dipengaruhi oleh keberadaan outlier. Metode estimasi robust dapat mengatasi outlier dengan mencocokkan model regresi dengan sebagian besar data, sehingga analisis regresi linear dengan metode estimasi robust dapat menghasilkan estimasi yang adaptif terhadap outlier. Metode estimasi robust yang diteliti dalam skripsi ini ialah estimasi MM. Metode estimasi MM merupakan perkembangan dari estimasi M dengan nilai breakdown point yang tinggi. Data yang digunakan untuk menunjukkan hasil menggunakan estimasi MM pada skripsi ini ialah data tentang Profil Kesehatan Indonesia 2015, khususnya tentang kasus kematian neonatal yang terjadi di Indonesia berdasarkan hubungan yang terbentuk antara banyaknya kematian neonatal dengan beberapa variabel independen-nya.","Linear regression analysis is an analysis that aims to determine the relationship between variables. The most often used method to estimate the parameters in a linear regression is the least square method. Although the method of least square estimation is relatively simple, this method has classic assumptions that must be fulfilled, One of them is normally distributed residual. In fact, there are data that does not fulfill the classis assumptions, so this estimation method can not be used. This paper discusses the robust regression analysis which is an alternative method of linear regression analysis when the residuals are not normally distributed or when the model is influenced by the presence of outliers. Robust Regression analysis can overcome the outlier by comparing the regression model with most of the data, so linear regression analysis with robust regression can produce estimation analysis regression that are resistant against outliers. The estimation methods of robust regression discussed in this paper is MM estimation. MM estimation is a development of M estimation with a higher point of breakdown value. The data that is used to indicate the result by using MM estimation at this paper is data about Indonesias Health Profile 2015, in particular on neonatal death cases occurring in Indonesia based on the relationship that exists between neonatal death with some of its independent variables.","Kata Kunci : Outlier, Robust, Tukeys bisqsuare function, MM-Estimation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/110829,Estimasi Fungsi Survival untuk Data Tersensor Interval dengan Metode Kurva Bezier,"SILVIA BUDHI PRAMAESTHI , Dr. Danardono, MPH; Rianti Siswi Utami, S.Si., M.Sc",2017 | Skripsi | S1 STATISTIKA,"Dalam studi medis sering ditemukan data survival tidak lengkap seperti data tersensor interval. Tidak seperti data tersensor kanan atau kiri, data tersensor interval mempunyai struktur yang khusus dan lebih rumit sehingga dalam melakukan inferensi statistik diperlukan metode yang khusus pula untuk menanganinya.           Metode kurva Bezier adalah salah satu metode yang digunakan untuk mengestimasi fungsi survival pada data tersensor interval dengan pendekatan grafik kontinu. Dengan menggunakan pendekatan grafik tentunya hasil estimasi fungsi survival lebih mudah diterima dan dipahami oleh peneliti kesehatan. Secara esensi keuntungan metode ini adalah tanpa menggunakan asumsi distribusi dan dapat digunakan untuk mengestimasi nilai survival di semua titik waktu pengamatan karena bersifat kontinu. Menggunakan metode imputasi titik tengah dapat dicari fungsi survival Kaplan-Meier yang akan dijadikan titik-titik awal pembangun kurva Bezier. Adapun ukuran kesalahan yang digunakan untuk mendapatkan nilai eror dari hasil estimasi fungsi survival yaitu MISE (Mean Integrated Square Error).","Incomplete survival data such as interval-censored is often found in medical studies. Interval-censored data has a special structure which makes it more complicated than right and left censored data. Thus, interval-censored data requires a special method in conducting statistical inference. Bezier curve method is one of the methods that used to estimate the survival function in interval-censored data with continuous graph approach. Hence, the estimation result of survival function is more acceptable and understandable by health researcher. The advantage to this method is it can estimate survival function at all time points of observation without using distribution assumption because of its continuity. The Kaplan-Meier survival function is used as starting points to build Bezier curve which can be obtained by using the midpoint imputation method. The error measurement that used in this survival function estimation is MISE (Integrated Mean Square Error).","Kata Kunci : interval-censored data, survival function, midpoint imputation method, Bezier curve method,data tersensor interval, fungsi survival, metode imputasi titik tengah, metode kurva Bezier"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114157,Estimator Liu Pada Regresi Poisson,"OKY SURYANA, Prof.Dr. Sri Haryatmi, M.Sc ; Dr. Danang Teguh Q, M.Sc",2017 | Skripsi | S1 STATISTIKA,"Regresi Poisson merupakan salah satu model statistika yang digunakan untuk menganalisis hubungan antara variabel respon Y dengan variabel prediktor X, dimana variabel Y berupa data diskrit dan variabel X berupa data diskrit, kontinu, kategorik atau campuran. Dalam regresi Poisson, metode Maximum Likelihood (ML)digunakan untuk mengestimasi parameternya. Akan tetapi, estimasi menggunakan metode MLmenjadi tidak valid jika variabel independennya saling berkolerasi atau dikenal sebagai kondisi multikolinearitas. Oleh karena itu, estimator Liu pada model regresi Poisson digunakan untuk menangani masalah multikolinearitas.  Dalam skripsi ini, estimator Liu pada analisis regresi Poisson diaplikasikan untuk memodelkan faktor-faktor yang mempengaruhi banyaknya penderita kanker kulit non-melanoma pada wanita Tahun 1970 di Minnesotta dan Texas. Maka diperoleh nilai Root Mean Square Error (RMSE) dari estimator Liu dan estimator ML. Kemudian dibandingkan dan diperoleh hasil menunjukkan estimator Liu memberikan nilai RMSEyang lebih kecil dibandingkan estimator ML.","Poisson regression is a statistical models were used to analyze the relationship between the response variable Y with the predictor variables X, whereY in the form of discrete data and variable X is discrete data, continuous, categorical or mixed. In Poisson regression, Maximum Likelihood (ML) is used to estimate the parameters. However, estimation using the ML method becomes invalid if the independent variable correlated with each other, otherwise known as multicollinearity condition. Therefore, Liu estimator on the Poisson regression model is used to deal with the problem of multicollinearity.  In this thesis, Liu's estimator on Poisson regression analysis was applied to model factors affecting the large number of non-melanoma skin cancer patients in women in 1970 in Minnesotta and Texas. It is obtained the Root Mean Square Error (RMSE) value from Liu estimator and ML estimator. Then compared and the results obtained show the Liu estimator gives a smaller MSE value than the ML estimator.","Kata Kunci : Regresi Poisson, Maximum Likelihood, Estimator Liu, Estimator Maximum Likelihood, Root Mean Square Error, Multikolinearitas."
http://etd.repository.ugm.ac.id/home/detail_pencarian/110830,Optimisasi Portofolio dengan Algoritma Genetika dalam Manajemen Dana Indeks,"NAFTALIA NUR INDAH RACHMAWATI, Prof. Dr. rer. nat. Dedi Rosadi, M.Sc., Rianti Siswi Utami, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Dana indeks disusun dari jumlah yang relatif kecil dari saham. Jika ingin menyiapkan dana indeks sempurna, perlu menempatkan setiap perusahaan yang termasuk dalam indeks ke portofolio dana indeks. Penempatan setiap emiten ke dalam portofolio dana indeks tersebut membutuhkan biaya yang mahal dan tidak praktis. Oleh karena itu, dana indeks mencoba untuk meniru pergerakan indeks dengan jumlah yang relatif kecil dari saham. Untuk itu, diusulkan skema portofolio Genetic Algorithm (GA) dalam pengelolaan dana indeks. Skema tersebut mengeksploitasi GA dan memberikan pilihan yang optimal. Pemilihan emiten dalam portofolio dilakukan dengan cara menentukan sektor industri yang memiliki pasar modal yang terbesar, kemudian dipilih emiten yang memiliki nilai prior yang terbesar sampai terbentuk portofolio dengan jumlah emiten yang diinginkan. Setelah itu, dilakukan optimasi portofolio menggunakan Algoritma Genetika. Portofolio yang dibentuk dengan Algoritma Genetika lebih mendekati indeks patokan yang ditiru. Hal tersebut dapat dilihat melalui bahwa nilai tracking error pada portofolio dengan metode Algoritma Genetika secara keseluruhan lebih kecil daripada portofolio metode klasik yaitu Mean-Variance.","Index fund composed of a relatively small number of stocks. If we want to prepare a perfect index funds, we need to put any company included in the index to the index fund portfolio. Placement of each issuer in the index fund portfolio is costly and impractical. Therefore, index funds attempt to mimic the movement of the index with a relatively small amount of stock. This article proposed scheme portfolio Genetic Algorithm (GA) for management of index funds. The scheme exploits GA and provide optimal choice. Selection of issuers in the portfolio is done by determining the industrial sector which has the largest capital market, then selected issuers that have the greatest value prior to form the portfolio with the desired number of issuers. After that, the portfolio optimization using Genetic Algorithms. Portfolio formed by the Genetic Algorithm closer to the benchmark index that imitated. This can be seen through that the value of tracking error in its portfolio with the overall genetic algorithm method is smaller than the portfolio of classic method which is the Mean-Variance.","Kata Kunci : Portfolio, Mean-Variance, Genetics Algorithm, Index fund, Portofolio, Mean-Variance, Algoritma genetika, Dana indeks"
http://etd.repository.ugm.ac.id/home/detail_pencarian/129518,GRAFIK PENGENDALI BERDASARKAN METODE TITIK-UBAH TUNGGAL UNTUK MENGAWASI PERGESERAN RATA-RATA,"USNA ANING YULIANTI, Herni Utami, S.Si., M.Si., Dr.; Vemmie Nastiti Lestari, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Statistical Process Control (SPC) merupakan sebuah alat dengan kapabilitas yang tinggi dalam menilai, mengevaluasi dan mengembangkan strategi bisnis untuk organisasi, perusahaan manufaktur, penyedia pelayanan kesehatan, dan badan-badan usaha milik pemerintah. Proses pengendalian kualitas membutuhkan metode statistika dalam mendeteksi perubahan pola data pada fase II. Metode statistika pada umumnya, seperti grafik pengendali Shewart, Cumulative Sum (CUSUM), dan Exponentially Weighted Moving Average (EWMA) membutuhkan nilai parameter yang telah terkendali yang dapat diperoleh melalui fase I yang selanjutnya dilakukan pengawasan proses untuk fase II dimana pada fase ini data dalam jumlah yang lebih besar dianalisa. Grafik pengendali univariat parametrik berdasarkan metode titik-ubah tunggal sangat efektif dalam mendeteksi adanya titik ubah karena memungkinkan peneliti untuk langsung melakukan analisa pada fase II. Ada tidaknya titik ubah diuji mengunakan Uji Generalized Likelihood Ratio. Grafik yang merupakan modifikasi dari grafik CUSUM dan grafik Self-Starting CUSUM ini sangat bergantung pada alarm rate dan Average Run Length (ARL). Berdasarkan studi kasus, grafik berdasarkan metode titik-ubah tunggal menghasilkan skema terbaik pada saat alarm rate bernilai 0,001 atau ARL terkendali sebesar 1000.","Statistical process control is a tool having a high capability to measure, evaluate and improve business strategy, manufacturers, helath care providers, and government agencies. Quality process control need statistical methodologies that detect changes in the pattern of data over time on phase II. The common methodologies, such as Shewart, Cumulative Sum (CUSUM), Exponentially Weighted Moving Average (EWMA) charting require the in-control values of the process parameter based on phase I and then the fase II with a streaming data to be monitored can be done. 	The univariat parametric chart based on single change point method is highly effective in allowing the user to progress seamlessly from thestart of phase I data gathering through phase II monitoring. The existence of change point is tested using the Generalized Likelihood Ratio Test. The control chart which is a modification of CUSUM and Self-Charting CUSUM chartings depends heavily on the alarm rate and the Average Run Length (ARL). Based on the cases study, this control chart provide a best scheme when the alarm rate is 0,001 or the in-control ARL is equal to 1000.","Kata Kunci : Kata kunci : grafik pengendali,fase I, fase II, titik ubah tunggal, statistical process control, Shewart, cumulative sum, self-starting cusum, CUSUM, generalized likelihood ratio, alarm rate, average run length, ARL."
http://etd.repository.ugm.ac.id/home/detail_pencarian/109298,GRAFIK PENGENDALI HYBRID EXPONENTIALLY WEIGHTED MOVING AVERAGE (HEWMA) : ANALISIS PERGESERAN MEAN PROSES,"ELVIRA RAHMAWATI M., Drs. Zulaela, Dipl.Med.Stats., M.Si.;Yunita Wulan Sari, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Pengendalian kualitas statistika berperan penting terhadap pemantauan proses produksi suatu perusahaan. Selama proses produksi, ketidaksesuaian barang, produksi di luar target perusahaan dapat terjadi karena beberapa faktor, diantaranya bahan baku, operator, mesin, lingkungan, ukuran, dan metode. Oleh karena itu diperlukan alat statistika untuk mendeteksi pergeseran atau sebab â€“ sebab terduga yang terjadi selama proses produksi. Semakin cepat sebab â€“ sebab terduga dideteksi maka tindakan perbaikan dapat segera dilakukan agar produksi dapat kembali stabil. Grafik pengendali Cummulative Sum (CUSUM) dan Exponentially Weighted Moving Average (EWMA) dikenal efektif dalam mendeteksi pergeseran mean proses yang kecil. Metode grafik pengendali yang akan dibahas kali ini yaitu grafik pengendali Cummulative Sum (CUSUM), Exponentially Weighted Moving Average (EWMA) dan Hybrid Exponentially Weighted Moving Average (HEWMA) dengan fokus pembahasan pada grafik pengendali Hybrid Exponentially Weighted Moving Average (HEWMA).  Grafik pengendali Hybrid Exponentially Weighted Moving Average (HEWMA) adalah penggabungan dua grafik pengendali Exponentially Weighted Moving Average (EWMA). Kelebihan dari grafik pengendali Hybrid Exponentially Weighted Moving Average (HEWMA) adalah mempunyai dua parameter lamda yaitu lamda1 dan lamda2 yang membuat grafik pengendali Hybrid Exponentially Weighted Moving Average (HEWMA)  lebih sensitif mendeteksi pergeseran mean proses yang kecil. Average Run Lengths (ARL) digunakan untuk mengevaluasi kesensitifitasan antara grafik pengendali CUSUM, EWMA dan HEWMA. Nilai ARL menunjukkan bahwa grafik pengendali HEWMA lebih sensitif daripada grafik pengendali CUSUM dan EWMA. Kesensitifitasan dari grafik pengendali Hybrid Exponentially Weighted Moving Average (HEWMA) didukung oleh performa grafik pengendali HEWMA yang lebih cepat mendeteksi kejadian di luar kendali (out of control).","Statistical  quality control have an important role to monitoring production process in company. During production process, the number of defectives and production out from the target may result because some factors include material, personnel, machine, enviroment, measurement, and method, so, statistical tools are needed to detect shifting or assured causes during process production. When assured causes are detected quickly then corrective action could be done  quickly, so production will stabilize. Cummulative Sum (CUSUM) control chart and Exponentially Weighted Moving Average (EWMA) control chart has known to detects small process mean shifts. Control charts that would discussed now are Cummulative Sum, Exponentially Weighted Moving Average, and Hybrid Exponentially Weighted Moving Average (HEWMA). The Hybrid Exponentially Weighted Moving Average (HEWMA) control chart will be the focus of discussion. Hybrid Exponentially Weighted Moving Average (HEWMA) control chart is proposed by mixing two Exponentially Weighted Moving Average (EWMA) control charts. The advantages of Hybrid Exponentially Weighted Moving Average (HEWMA) control chart is have two parameter lamda, those are lamda1 and lamda2, that makes Hybrid Exponentially Weighted Moving Average (HEWMA) control chart more sensitive. Average Run Lengths (ARL) is used to evaluate sesitivity between CUSUM control chart, EWMA control chart and HEWMA control chart. ARL value shown that HEWMA control chart is more sensitive than CUSUM control chart and EWMA control chart. Sensitivity of Hybrid Exponentially Weighted Moving Average (HEWMA) is supported by performance of HEWMA control chat that quickly to detect out of control events.","Kata Kunci : Grafik Pengendali, cummulative sum, exponentially weighted moving average, hybrid exponentially weighted moving average, average run lengths, parameter lamda, CUSUM, EWMA, HEWMA, ARL"
http://etd.repository.ugm.ac.id/home/detail_pencarian/111097,Estimasi Parameter Model Regresi Isotonik Bayesian dengan Polinomial Bernstein,"BAGUS SETYAWAN, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc. ; Widya Irmaningtyas, S.Si., M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Statistika sering digunakan untuk menentukan hubungan antara sebuah variabel dependen dengan sebuah variabel independen melalui suatu fungsi regresi. Regresi isotonik sebagai regresi nonparametrik dapat digunakan untuk menentukan fungsi yang tepat dari variabel dependen dan independen. Regresi isotonik digunakan jika hubungan antara variabel dependen dan independen adalah monoton naik. Salah satu model yang dapat digunakan dalam regresi isotonik adalah menggunakan polinomial Bernstein. Polinomial Bernstein digunakan untuk menghasilkan pembatasan bentuk pada fungsi regresi. Polinomial Bernstein digunakan karena mudah diimplementasikan, dapat didiferensialkan secara kontinu, dan mempunyai sifat-sifat teoritis baik lainnya.  Pendekatan Bayesian yaitu Gibbs sampling akan digunakan untuk menentukan parameter dalam fungsi regresi. Prosedur Bayesian yang digunakan diadaptasi dari prosedur yang telah ada. Pada perangkat lunak R tersedia paket bisoreg untuk analisis regresi isotonik Bayesian dengan polinomial Bernstein. Keefektifan dari metode ini akan ditunjukkan melalui simulasi dan analisis menggunakan data nyata yaitu Indeks Harga Konsumen (IHK) dan upah buruh industri di Indonesia.","Statistics is often used to determine the relationship between a dependent variable and a independent variable through a regression function. Isotonic regression as nonparametric regression can be used to determine the certain function of dependent and independent variable. Isotonic regression is used if the relationship between dependent and independent variable is monotonically increasing. One of the model that can be used in isotonic regression is using Bernstein polynomials. Bernstein polynomials have been used to impose certain shape restrictions on regression functions. Bernstein polynomials are used due to their ease of implementation, continuous differentiability, and theoritical properties.  Bayesian approach that is Gibbs sampling is used to determine the parameter in regression function. Bayesian procedure is used by adapting currently available procedures. R software provides a bisoreg package for doing Bayesian isotonic regression analysis using Bernstein polynomials. The effectiveness of this method will be demonstrated through simulations and analysis of real data that are Consumer Price Index (CPI) and wages of industrial labor in Indonesia.","Kata Kunci : Regresi isotonik, Regresi nonparametrik, Gibbs sampling, bisoreg, Polinomial Bernstein"
http://etd.repository.ugm.ac.id/home/detail_pencarian/113914,Grafik Pengendali Fuzzy Exponentially Weighted Moving Average (FEWMA),"SABRINA AGENG P, Dr. Herni Utami, M.Si.",2017 | Skripsi | S1 STATISTIKA,"Kualitas produk merupakan faktor yang berpengaruh terhadap kepuasan konsumen. Kualitas produk tersebut dikontrol dengan grafik pengendali. Salah satunya yaitu grafik pengendali Exponentially Weighted Moving Average (EWMA). Grafik pengendali EWMA adalah grafik pengendali yang digunakan untuk mengontrol proses dengan pergeseran kecil. Terkadang, terdapat ketidakpastian pada data apabila pada proses pengukuran ada campur tangan manusia. Data yang tidak pasti tersebut disebut data fuzzy. Apabila terdapat data fuzzy pada suatu proses pengendalian kualitas dan ingin dideteksi pergeseran kecil pada mean maka dibutuhkan grafik pengendali Fuzzy Exponentially Weighted Moving Average (FEWMA). Grafik pengendali FEWMA mengurangi kesalahan dalam pengambilan keputusan.  Dari hasil penelitian didapatkan bahwa grafik pengendali FEWMA lebih baik digunakan untuk data yang tidak pasti karena dapat mengurangi kesalahan pada pengambilan keputusan.","Product quality is one of many factor that affect to the consumer satisfaction. Control chart is widely use to control product quality. One of them is exponentially weighted moving average (EWMA) control chart. EWMA is a control chart that handling small shifted data. Sometimes, there is a data that classified as ""uncertain"" or ""vagueness"" due to human subjectivity at measurement process. That kind of data is called as fuzzy data. If there is a small shifted fuzzy data at quality control process then it needs a fuzzy EWMA control chart to handle such things.  The experiment shows that FEWMA control chart is more properly used for small shifted fuzzy data than EWMA control chart because it can reduce false alarm at decision making.","Kata Kunci : Grafik Pengendali/Control chart; Pengendalian Kualitas/Quality Control, EWMA, Fuzzy EWMA, Pergeseran Proses/Shifting processs"
http://etd.repository.ugm.ac.id/home/detail_pencarian/114175,UJI CHRISTOFFERSEN PADA NILAI RESIKO DENGAN PENDEKATAN TRANSFORMASI JOHNSON,"RIZKI NUR OKTA S, Prof. Dr. Sri Haryatmi, M.Sc.",2017 | Skripsi | S1 STATISTIKA,"Value at Risk (VaR) merupakan salah satu ukuran resiko yang dapat digunakan untuk mengetahui maksimal kerugian pada periode selanjutnya.   Ada beberapa metode dalam perhitungan VaR seperti metode parametrik dimana data harus berdistribusi tertentu, metode nonparametrik untuk data yang tidak memerlukan distribusi tertentu dan metode semiparametrik merupakan kombinasi antara metode parametrik dan nonparametrik. Sering dijumpai return dari sebuah aset saham bersifat leptokurtic (kurtosis > 3). Pendekatan transformasi Johnson SU ialah salah satu penyelesaian masalah analisis nilai resiko dalam kasus return bersifat leptokurtic. Akan tetapi metode tersebut hanya berguna jika dapat memprediksi resiko diperiode mendatang secara tepat. Oleh karena itu, metode tersebut perlu dievaluasi dengan backtesting.   Analisis nilai resiko menggunakan metode nonparametrik (simulasi historis) akan dibandingkan dengan metode pendekatan transformasi Johnson SU. Nilai rasio pelanggaran pada metode VaR yang memberikan nilai mendekati 1 serta memiliki VaR volatility kecil merupakan metode VaR yang lebih baik. Kemudian dilakukan backtesting menggunakan uji Christoffersen pada metode VaR terbaik untuk tingkat konfidensi 90%, 95%, dan 99%.","Value at Risk (VaR) is one of the measure of risk that can be used to determine the maximum loss for the next period.  There are several methods in the calculation of VaR as parametric method that certain data must be distributed, nonparamteric method for data that does not require specific distribution and semipramateric method which is combination of parametric and nonparametrik method. Often, some return on a asset is leptokurtic ( kurtosis > 3 ) johnsonâ€™SU transformation approach is one of problem solving risk value analysis on the return leptokurtic. The method is only useful if it can predict risk in the next period appropriately, so the method needs to be evaluated by backtesting.   Analysis of Value at Risk using nonparametric methods (historical simulations) will be compared with Johnson Suâ€™s transformation approach. The violation ratio value of the VaR method that gives a value close to 1 and has a small volatility of VaR is a better VaR method. Backtesting was performed using Christoffersen test on best VaR method for confidence level 90%, 95%, and 99%.","Kata Kunci : Value at Risk, leptokurtic, Johnson SU, VaR volatility, Backtesting, Uji Christoffersen."
http://etd.repository.ugm.ac.id/home/detail_pencarian/102402,GRAFIK PENGENDALI NP UNTUK MENGAMATI MEAN VARIABEL BERDASARKAN PEMERIKSAAN SIFAT,"ASEP ZULKA KHAERUL U, Dr. Danardono, MPH",2016 | Skripsi | S1 STATISTIKA,"Pada skripsi ini akan dibahas grafik pengendali np, yang disebut grafik pengendali  , yang memeriksa sifat suatu variabel (memeriksa apakah sebuah unit sesuai atau tidak sesuai) untuk mengamati nilai rata-rata dari variabel x. Perbedaan grafik pengendali   dengan grafik pengendali konvensional adalah grafik pengendali   menggunakan batas peringatan statistik untuk menggantikan batas spesifikasi. Dengan mengoptimalkan batas peringatan, grafik pengendali   mengungguli performa grafik pengendali  . Tidak seperti grafik pengendali np konvensional, grafik pengendali   tidak membutuhkan batas pengendali bawah. Grafik pengendali   biasanya menjadi indikator dari masalah yang paling utama dan memungkinkan operator untuk mengambil tindakan perbaikan sebelum dihasilkan suatu cacat. Pada skripsi ini, grafik pengendali   akan diaplikasikan pada data pengendalian kualitas karakteristik brix gula pada stasiun kristalisasi PT Madubaru Yogyakarta, Periode Juni 2013.","In this article an np control chart will be studied, called the   control chart, that inspects the attribute of a variable (inspecting whether a unit is conforming or nonconforming) to monitor the mean value of a variable x. The distinctive feature of the   chart is using the statistical warning limits to replace the specification limits. By optimizing the warning limits, the   chart usually outperforms the   chart. Unlike a conventional np chart, the   chart does not need a lower control limit. The   chart often works as a leading indicator of trouble and allows operators to take corrective action before any defective is actually produced. In this article, the   control chart will be applied to brix characteristic quality control data of sugar at crystallization station PT. Madubaru Yogyakarta, in June 2013.","Kata Kunci : Pengendalian kualitas, Pengendalian proses statistik, grafik pengendali variabel, grafik pengendali sifat"
http://etd.repository.ugm.ac.id/home/detail_pencarian/98566,SUPPORT VECTOR MACHINE DENGAN REDUKSI DIMENSI MENGGUNAKAN ANALISIS REGRESI LOGISTIK DAN ORTHOGONAL DIMENSION REDUCTION UNTUK CREDIT SCORING,"QONIT ARIFAH AZKA, Dr. Gunardi, M.Si. ; Vemmie Nastiti Lestari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Credit scoring merupakan salah satu teknik analisis manajemen risiko untuk meminimalisasi terjadinya debitur yang gagal bayar. Analisis ini dilakukan menggunakan Support Vector Machine (SVM) untuk mengklasifikasikan debitur ke dalam kategori layak atau tidak. Banyaknya variabel yang digunakan sebagai pertimbangan dalam analisis credit scoring, membuat input variabel pada SVM berubah menjadi data berdimensi tinggi. Hal tersebut menyebabkan terjadinya fenomena curse of dimensionality yang berdampak dengan munculnya korelasi antarvariabel dalam analisis SVM dan merenggangnya titik sampel, sehingga menyebabkan prediksi menjadi tidak akurat. Dengan demikian, untuk menghindari hal tersebut, maka akan diterapkan reduksi dimensi pada tahap pre-processing data dengan analisis regresi logistik untuk variabel kategorik dan Orthogonal Dimension Reduction (ODR) untuk variabel numerik. Setelah proses reduksi dimensi diterapkan, tingkat akurasi prediksi pada SVM menjadi lebih tinggi jika dibandingkan dengan SVM tanpa menggunakan reduksi dimensi pada tingkat proporsi tertentu.","Credit scoring is one of risk management analysis methods for minimizing default. This analysis is solved using Support Vector Machine (SVM) to classify debtors into two groups which are credithworthy or not. There are many variables used as a consideration of classifying debtors, so this case make the SVM input have a high dimensional data. They will be a reason of curse of dimensionality phenomenon that cause correllation among the variables come up and make sample set become sparse. Due to this phenomenon, the result of credit scoring cannot predict accurately. Therefore to avoid that phenomenon, so there will be implemented dimension reduction technique on pre-processing data using logistic regression analysis for categorical variables and Orthogonal Dimension Reduction (ODR) for numerical variables. After dimension reduction techniques are implemented on SVM, it actually can predict better than SVM without dimension reduction technique.","Kata Kunci : Credit scoring, Support Vector Machine (SVM), Reduksi Dimensi,  Analisis Regresi Logistik, Orthogonal Dimension Reduction (ODR)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96266,PEMODELAN EXCESS HAZARD PADA REGRESI RELATIVE SURVIVAL DENGAN ALGORITMA EM,"PERTIWI HANDAYANI W , Dr. Danardono, MPH.",2016 | Skripsi | S1 STATISTIKA,"Tujuan dari metode relative survival yaitu untuk membandingkan nilai survival dari suatu angkatan atau survival observasi dengan expected survival dalam suatu populasi. Fungsi relative survival digunakan ketika informasi penyebab kematian tidak akurat atau tidak diketahui. Metode relative survival dapat dinyatakan sebagai regresi relative survival atau relative survival aditif yang mengasumsikan bahwa hazard setiap individu adalah jumlahan dari dua komponen, yaitu hazard populasi yang diperoleh dari life table dan excess hazard yang disebabkan oleh kondisi tertentu atau penyakit yang diamati. 	Pada skripsi ini akan dibahas sebuah pendekatan untuk memodelkan excess hazard pada regresi relative survival secara lebih fleksibel. Pendekatan tersebut tidak memerlukan asumsi tentang baseline excess hazard. Prosedur yang digunakan yaitu menggunakan algoritma EM dengan menganggap penyebab kematian sebagai missing variable. Baseline excess hazard diestimasi secara bersama-sama dengan koefisien dari model. Model ini juga dianggap sebagai model Cox yang diperumum. Hasil dari perhitungan excess hazard dapat diinterpretasikan sebagai resiko berlebih berdasarkan kovariat yang mana resiko tersebut disebabkan oleh kondisi khusus misalnya kanker. Metode ini akan digunakan pada data survival tersensor kanan dari pasien Colon Carcinoma Cancer di Finlandia.","The goal of relative survival methodology is to compare the survival experience of a cohort or observed survival proportion with the expected survival in a population. Relative survival function is used when the cause of death information is not accurate or not available. Method for relative survival can be expressed as relative survival regression or additive relative survival which assumes that each personâ€™s hazard is a sum of two components, the population hazard obtained from life table and an excess hazard due to the specific condition or disease of interest. 	In this paper will explained an approach to modelling excess hazard in  relative survival regression more flexible which requires no assumptions about the baseline excess hazard. The procedure using the EM algorithm which treating the cause of death as missing variable. Baseline excess hazard is estimated simultaneously with the coefficients of the model. This is can be seen as a generalization of the Cox model. The outcome measure of excess hazard is interpreted as excess risk based on covariate that risk was caused by specific condition for example cancer. The method is applied to a right censored data set on survival Colon Carcinoma Cancer in Finland.","Kata Kunci : relative survival, additive model, EM algorithm"
http://etd.repository.ugm.ac.id/home/detail_pencarian/99339,Model Regresi Finite Mixture Binomial Negatif,"TAUFIQ DWI A, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2016 | Skripsi | S1 STATISTIKA,Model campuran merupakan model gabungan dari dua atau lebih distribusi. Distribusi yang digunakan dalam model campuran bisa distribusi yang sama dengan parameter berbeda maupun distribusi yang berbeda. Model ini mengelompokkan data di dalam suatu dataset menjadi kelompok kelompok data yang sebelumnya tidak terdefinisikan. Salah satu model campuran yang dikembangkan adalah model regresi finite mixture Binomial Negatif. Model ini dapat mengatasi adanya overdispersi pada regresi Poisson sekaligus mengatasi adanya keheterogenan pada data. Estimasi parameter model regresi finite mixture Binomial Negatif diestimasi dengan metode Maximum Likelihood Estimator (MLE) yang menggunakan algoritma Ekspektasi Maksimalisasi (EM) untuk mendapatkan parameter MLEnya. Pemilihan model terbaik dilakukan dengan cara membandingkan nilai BIC dan AIC dengan model regresi cacah lainnya. Model regresi finite mixture Binomial Negatif diaplikasikan pada data tentang junlah ketidakhadiran siswa sekolah menengah pada mata pelajaran Bahasa Portugis di Negara Portugal. Hasilnya menunjukkan model ini memiliki AIC dan BIC terkecil.,Mixture model is a combination model of two or more distributions. Distribution in the model mixture can consist of distribution identic with different parameter or non-identic. This model classify data within dataset into groups of data that previously were not defined. One of the models is finite mixture Negative Binomial regression model. This model can handle overdispersion on Poisson regression and heterogeneity in the data. Parameter estimation for finite mixture Negative Binomial regression model is estimated by Maximum Likelihood Estimator (MLE) which uses Expectation Maximization (EM) algorithm for getting the MLEÃ¢ï¿½ï¿½s parameter. The best model selection is evaluated by comparing the AIC and BIC score with other discrete regression models. Finite mixture Negative Binomial regression model is applied on number of absences from middle school students on the subjects of Portuguese language in the State Portugal. The result showed this model has the lowest AIC and BIC.,"Kata Kunci : overdispersi, heterogen, model campuran, finite mixture, binomial negatif"
http://etd.repository.ugm.ac.id/home/detail_pencarian/99596,Optimisasi Portofolio dengan Kataoka Safety-First (studi kasus saham Bursa Efek Indonesia),"VIDIA FEBIANY, Dr. Gunardi, M.Si.",2016 | Skripsi | S1 STATISTIKA,"Dalam melakukan investasi di pasar modal khususnya saham, investor harus melakukan manajemen agar tujuan investasi yang diinginkannya dapat tercapai dengan cara melakukan optimisasi portofolio. Salah satu metode optimisasi portofolio adalah metode optimisasi Mean-Variance yang dikembangkan oleh Markowitz (1952). Metode ini memiliki konsentrasi pada upside risk di mana pendekatan statistika standar deviasi digunakan sebagai ukuran risikonya. Padahal, pada kenyataannya investor masih menganggap bahwa risiko yang didapatkan dari sebuah investasi adalah downside risk yaitu risiko yang menyebabkan kerugian.   Metode optimisasi portofolio untuk jenis risiko downside risk dinamakan safety-first. Salah satu konsep safety-first yang dijadikan bahan utama dalam pembahasan skripsi ini adalah Kataoka Safety-First yang dikembangkan oleh Kataoka Shinji (1963). Selain memaksimalkan return portofolio, metode Kataoka Safety-First juga memberikan nilai benchmark return optimal sebagai nilai return portofolio terendah yang diharapkan investor pada suatu tingkat risiko.  Skripsi ini membahas pembobotan portofolio dengan Kataoka Safety-First pada tiga kelompok saham yaitu EXCL, ISAT, dan TLKM; INAF, BRPT, dan SRIL; dan ADHI, ASII, BMRI, MEDC, dan SMCB. Pembobotan portofolio dilakukan dengan tiga metode yaitu Mean-Variance, Kataoka Safety-First dengan asumsi data return berdistribusi eliptikal, dan Kataoka Safety-First dengan asumsi data return berdistribusi irregular. Kemudian kinerja dari ketiga metode tersebut dibandingkan berdasarkan return portofolio, Sharpe ratio, dan benchmark return. Hasilnya adalah metode optimisasi portofolio Kataoka Safety-First eliptikal menghasilkan keuntungan optimal dan state investasi yang aman ketika diaplikasikan pada data historis saham dengan kecenderungan tren naik serta fluktuatif. Untuk data historis dengan kecenderungan tren turun, metode Kataoka Safety-First asumsi data return berdistribusi eliptikal menghasilkan bobot short-selling pada tingkat risiko yang cukup besar. Pada skripsi ini akan dibahas pembatasan short-selling untuk mengamankan status investasi investor dari praktik short-selling. Oleh karena itu, Kataoka Safety-First dapat menjadi pilihan optimisasi portofolio alternatif bagi investor.","Doing capital market's investment especially in stocks, investor has to do a management in order that the investment's goal can be accomplished. Investment management in stocks can be done by portfolio optimization. One of portfolio optimization methods is Mean-Variance invented by Markowitz (1952). This method has an upside risk concentration using statistical approach such as standard deviation as its risk framework. In fact, a lot of investor still consider that a risk obtained from an investment is downside risk concentration that causes a loss.  Portfolio optimization method that supports downside risk concentration is portfolio optimization method with safety-first concept. One of safety-first concepts used as a main topic in this essay is Kataoka Safety-First by Kataoka Shinji (1963). In addition to maximizing the portfolio return, Kataoka Safety-First gives an optimal benchmark return as the lowest portfolio return that investor can expect with an exact level of risk.  This essay discusses portfolio weighting by Kataoka Safety-First into three group of stocks combination such as EXCL, ISAT, and TLKM; INAF, BRPT, and SRIL; and ADHI, ASII, BMRI, MEDC, and SMCB. Portfolio weighting is done by three methods, Mean-Variance, Kataoka Safety-First with elliptical distribution as return's assumption, and Kataoka Safety-First with irregular distribution as return's assumption. Then, the performance of those three methods are compared according to the value of portfolio return, Sharpe ratio, and optimal benchmark return. The result is portfolio optimization with Kataoka Safety-First with elliptical distribution as return's assumption obtains optimal return and safe investment to do when it was applied to stocks with uptrend tendencies and fluctuating tendencies in the historical data. Kataoka Safety-First Elliptical causes a short-selling weight with higher level of risk when it was applied to stocks with downtrend tendencies. But, in this essay, it will be solved by the limitation of short-selling to make safe investment for investor. Therefore, Kataoka Safety-First can be an alternative choice for portfolio optimization for investor.","Kata Kunci : Optimisasi portofolio, safety-first, Kataoka Safety-First, benchmark return optimal, distribusi eliptikal, distribusi irregular, downside risk"
http://etd.repository.ugm.ac.id/home/detail_pencarian/98064,"KOMBINASI METODE PARAMETRIK, SEMI-PARAMETRIK, DAN  NONPARAMETRIK  PADA REGRESI  FUNGSI SURVIVAL DENGAN METODE STACKED","ATINA HUSNAQILATI, Dr. Herni Utami, M.Si",2016 | Skripsi | S1 STATISTIKA,"Estimasi fungsi survival atau biasa disebut regresi fungsi survival merupakan estimasi yang digunakan dalam analisis  survival dimana kovariat merupakan parameter dari estimasi tersebut, sehingga estimasi tersebut tergantung pada hasil kovariatnya. Metode ÃƒÂ¢Ã¯Â¿Â½Ã¯Â¿Â½stackedÃƒÂ¢Ã¯Â¿Â½Ã¯Â¿Â½ merupakan metode untuk  menggabungkan beberapa model regresi fungsi survival yaitu model parametrik, nonparametrik, dan semi-parametrik  dimana setiap model memiliki bobot masing-masing. Pembentukan bobot untuk masing-masing model tersebut berdasarkan nilai minimum eror pada model gabungan tersebut. Fungsi kerugian yang digunakan untuk mendapatkan nilai eror tersebut adalah Brier score dimana digunakan untuk data tanpa sensor sedangkan untuk data survival dengan data tersensor menggunakan Inverse Probability of Censoring Weights (IPCW) Brier score sebagai pendekatannya untuk menentukan bobot masing-masing model. ISSE yang didapatkan pada model stacked untuk data pasien pada klinik Edward Health Center (EHC) adalah 0,069 dan dari data German Breast Cancer Study (GBCS) adalah 0,043.","Estimated survival function or survival function regression estimation is survival analysis where covariates are parameters of these estimates, so these estimates depends on the outcome of covariate. The approach to combine multiple survival models (parametric, semi-parametric, and nonparametrik) use ""stacked"" method where each model has a weight. The Weights are determined for each model are based on the minimum value of the combined errors in the models. The loss function that use to get the value of error is Brier score used data without censored while for censored survival use the Inverse Probability of Censoring Weights (IPCW) Brier score as his approach to determining Weights for each model. ISSE obtained on stacked models for clinic Edward Health Center (EHC) data is 0.069, and from German Breast Cancer Study (GBCS) data is 0.043.","Kata Kunci : regresi fungsi survival, metode stacked, Brier score, Inverse Probability of Censoring Weights (IPCW), integrated survival squared error"
http://etd.repository.ugm.ac.id/home/detail_pencarian/103698,"STRUCTURE OPTIMIZED GREY MODEL (2,1) SEBAGAI PERAMALAN JANGKA PENDEK UNTUK JUMLAH DATA KECIL","DHANY INDRA HERMAWAN, Dr. Gunardi, M.Si.;Yunita Wulan Sari, S.Si.,M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Metode peramalan merupakan suatu teknik untuk memprediksi atau memperkirakan suatu nilai pada masa yang akan datang dengan memperhatikan data atau informasi masa lalu maupun saat ini secara statistik. Peramalan kerap kali digunakan sebagai perencanaan dan operasi kontrol dalam berbagai bidang. Namun, tidak setiap data mudah didapatkan atau data yang diperoleh memiliki jumlah yang terbatas. Untuk mengatasi masalah jumlah data yang terbatas tersebut, digunakan teknik peramalan yang disebut Grey Forecasting Model. Pada skripsi ini, digunakan metode Structure Optimized Grey Model (2,1) atau SOGM (2,1) yang merupakan salah satu bentuk dari Grey Forecasting Model dengan menggunakan bentuk diferensial orde dua dan satu variabel. Structure Optimized Grey Model (2,1) atau SOGM (2,1) ini digunakan untuk peramalan jangka pendek pada data yang terbatas.","Forecasting method is a technique for predicting or forecasting a value in the future through noticing data or information in the past or present statistically. Forecasting is often used as the planning and control operation in various fields. In the contrary, not all data can be acquired or data which are acquired are limited. For overcoming such problem, the forecasting technique called Grey Forecasting Model is used. In this thesis, Structured Optimized Grey Model (2,1) or SOGM (2,1) is used and it is one of the form of Grey Forecasting Models which uses two orders and one variable differential form. Structure Optimized Grey Model (2,1) or SOGM is used for short term prediction in limited data.","Kata Kunci : Peramalan jangka pendek, data terbatas, Grey Forecasting Model, Structure Optimized Grey Model"
http://etd.repository.ugm.ac.id/home/detail_pencarian/103959,Modikasi Grafik Pengendali P Pada Proses Dengan Kualitas Tinggi Berdasarkan Ekspansi Cornish-Fisher,"ERLINFIA INDARWATI, Prof. Subanar, Ph. D",2016 | Skripsi | S1 STATISTIKA,"Peningkatan kualitas yang diwujudkan dalam bentuk perbaikan secara terus menerus pada proses produksi ditemukan pada beberapa tahun belakangan dalam bidang produksi dan pelayanan. Situasi tersebut terjadi lebih serius pada kasus grafik pengendali atribut, banyaknya peningkatan kualitas produksi memberikan dampak terjadinya proporsi bagian tidak sesuai yang sangat rendah. Proses tersebut dikenal dengan istilah ""high quality processes"". Biasanya, tingkat ketidaksesuaian produk  dipelajari dengan grafik pengendali konvensional shewart p 3-sigma yang didasarkan pada pendekatan distribusi normal. Grafik pengendali Shewart banyak digunakan dalam bidang industri dikarenakan kemudahan dalam penggunaannya dan kemudahan dalam menjelaskan kondisi suatu proses (terkendali atau tidak terkendali). Akan tetapi, grafik pengendali p tersebut ternyata sudah tidak sesuai lagi digunakan dalam hal  pemodelan proses dan spesifikasi batas-batas pengendalinya ketika tingkat proporsi bagian tidak sesuainya kecil. Hal tersebut dikarenakan meningkatnya nilai resiko tanda bahaya yang salah. Oleh karena itu, dilakukan modifikasi untuk memperoleh bentuk suatu grafik pengendali p baru yang lebih baik dibandingkan dengan grafik pengendali p  konvensional. Grafik pengendali p baru tersebut didasarkan pada ekspansi Cornish-Fisher.","An increasing emphasis of quality, framed in the implementation of a process of continous improvement has been observed in recent years in the field of production and services. This situation is more serious in the case of attributes charts, many processes has been improved giving a very low non-conforming fraction. These processes are known as "" high quality processes"". Traditionally, the study of the rate of nonconformities was carried out using the conventional 3-sigma p control chart (Shewart), constructed by the normal approximation. Shewart control charts has been widely used in manufacturing industry due to the simplicity of its implementation and the ease of interpretation of the process status (in control or out of control).  But this p chart suffers a serious inaccuracy in the modeling process and in control limits specification when the true rate of nonconforming items is small since they increase significantly the risk of false alarm. Due to that reason, an improved p chart which can provide a large  improvement over the usual p chart for attributes is presented. This new chart is based on the Cornish-Fisher expansion.","Kata Kunci : modification attribute control chart, very low non-conforming fraction, shewart p chart, modification p chart based cornish-fisher expansion, high quality processes, false alarm risk, cornish-fisher expansion, Cornish-Fisher attribute chart."
http://etd.repository.ugm.ac.id/home/detail_pencarian/107546,GRAFIK PENGENDALI MULTIVARIAT ROBUST DENGAN ESTIMATOR MINIMUM VOLUME ELLIPSOID,"RIFKI ANISA, Drs. Zulaela, Dipl. Med. Stats., M.Si.; Vemmie Nastiti Lestari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Grafik pengendali yang umum digunakan untuk produksi dengan dua atau lebih dari dua karakteristik yaitu grafik pengendali Hotelling's T kuadrat  . Grafik pengendali Hotelling's T kuadrat  menggunakan estimator klasik untuk mean dan standar deviasi dalam perhitungan batas pengendali. Mean dan standar deviasi dengan menggunakan estimator klasik mempunyai breakdown point nol yang berarti tidak sensitif dengan adanya pencilan, sehingga dapat dikatakan estimator tersebut tidak bersifat robust. Estimator Minimum Volume Ellipsoid merupakan salah satu estimator robust yang dapat digunakan untuk grafik pengendali Hotelling's T kuadrat   untuk menanggulangi apabila terdapat pencilan. Estimator Minimum Volume Ellipsoid mempunyai breakdown point optimal yaitu sebesar 0,5. Skripsi ini mengaplikasikan grafik pengendali multivariat robust untuk menganalisis kualitas proses produksi Solar 48 pada Kilang minyak Pusdiklat Migas Cepu, Jawa Tengah. Dengan membandingkan grafik pengendali multivariat standar dengan grafik multivariat robust diperoleh bahwa grafik pengendali robust menghasilkan grafik yang lebih baik apabila data mengandung pecilan.","Control chart which is commonly used for the production by two characteristics or more than two  production characteristics that is Hotelling's T quadrate  control chart. Hotelling's T quadrate    control chart uses classic estimator for the mean and standard deviation in controlling limit calculation. Mean and standard deviation by using classic method has zero breakdown point which means insensitive to the presence of outliers, so that the estimators are not robust. Minimum Volume Ellipsoid Estimator is one of robust estimators that can be used to Hotelling's T quadrate    control chart to overcome if there are outliers. Minimum Volume Ellipsoid Estimator has an optimal breakdown point which is equal to 0,5. This thesis applies robust multivariate control charts to analyze the quality of the diesel fuel 48 production processes at the Pusdiklat Migas Cepu Refinery, Central Java. The result of comparing the standard multivariate control chart to multivariate robust charts showed that robust control chart produces the better graphics, if the data contains outlier.","Kata Kunci : multivariat, pencilan, robust, minimum volume elipsoid, grafik pengendali multivariat"
http://etd.repository.ugm.ac.id/home/detail_pencarian/102428,Metode Grey Double Exponential Smoothing (GDES) untuk Meramalkan Data Time Series Berpola Tren,"LAILA RIZKI KARIMA, Dr. Abdurakhman, M.Si.; Dr. Danang Teguh Qoyyimi, M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Secara umum tujuan dari peramalan adalah untuk mendapatkan nilai ramalan yang mendekati nilai aktual. Metode Exponential Smoothing menggambarkan sebuah kelas dari metode-metode peramalan. Metode tersebut mempunyai sifat yang prediksinya adalah kombinasi terbobot pengamatan-pengamatan sebelumnya dengan pengamatan yang lebih baru diberikan bobot relatif lebih berat daripada pengamatan yang lebih lama. Double Exponential Smoothing merupakan perluasan dari exponential smoothing yang dirancang untuk time series yang mengandung tren linier. Pada praktiknya, terkadang data bersifat acak dan berfluktuasi sehingga diperlukan metode yang lebih fleksibel untuk menangani keacakan data. Untuk menyelesaikan permasalahan yang ada antara keinginan untuk mendapatkan efek pemulusan yang baik dan keinginan untuk memberikan bobot tambahan pada perubahan terbaru, grey accumulated generating operator yang dapat memuluskan gangguan acak data dimasukkan ke dalam metode Double Exponential Smoothing. Hasil dari simulasi dan analisis terhadap bea masuk dan produksi hasil kilang minyak diesel di Indonesia menunjukkan bahwa metode Grey Double Exponential Smoothing lebih baik daripada metode Double Exponential Smoothing dalam masalah peramalan.","A common aim of forecasting is obtaining predicted value that is close to the actual value. The Exponential Smoothing (ES) method describes a class of forecasting methods. This method has characteristic that the forecast is weighted combinations of past observations, where newer observations are weighted more than the older. The double exponential smoothing (DES) is an extension of ES is designed for linear trend time series. Practically, in many cases data is random and fluctuated so we need a more flexible method to handle this randomness. To resolve the problem between desire for a good smoothing effect and desire to give additional weight to the recent change, a grey accumulated generating operator that can smooth the random interference of data is introduced into the double exponential smoothing method. The result of simulation and of analysis of import duty and diesel oil refinery production in Indonesia showed that the grey double exponential smoothing method outperform the traditional double exponential smoothing method in forecasting problems.","Kata Kunci : peramalan, Grey Double Exponential Smoothing, Double Exponential Smoothing"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96286,estimasi robust pada model seemingly unrelated regression,"FELLA SHUFA NUR MASFUFA, Drs. Danardono, MPH, P.hD; Yunita Wulan Sari, S.Si, M.Sc",2016 | Skripsi | S1 STATISTIKA,"Model Seemingly Unrelated Regression (SUR) merupakan suatu sistem persamaan linear yang terdiri dari  persamaan yang mana error pada persamaan tersebut saling berkorelasi secara contemporaneously. Dalam hal ini, metode Kuadrat Terkecil (Ordinary Least Square, OLS) dapat digunakan untuk mengestimasi parameter dari masing-masing persamaan, namun kelemahan metode OLS ini adalah membuang informasi kemungkinan adanya hubungan pada sistem persamaan. 	Dalam tugas akhir ini, digunakan metode Generalized Least Square (GLS) untuk mengestimasi parameter model SUR. Namun GLS kurang mampu bertahan terhadap kehadiran outlier, maka digunakan estimasi robust S untuk mengestimasi parameter model SUR yang mengandung outlier. Estimator S adalah estimator equivariant dari regresi dan dapat mencapai breakdown point setinggi 50%, berarti dapat menangani hampir separuh dari observasi buruk dan memberikan pengaruh yang baik. Uji Lagrange Multiplier digunakan untuk menguji adanya korelasi contemporaneously pada error estimasi. Pembahasan akan diakhiri dengan studi kasus mengenai faktor-faktor yang mempengaruhi Penanaman Modal Asing (PMA) pada dua negara yaitu Indonesia dan Filipina. Pada studi kasus disimpulkan bahwa estimasi robust S lebih baik dari estimasi GLS.","Seemingly Unrelated Regression (SUR) model is a system of linear equations consisting  equation which errors in different equations are contemporaneously correlated. In this case, the Least Square method (Ordinary Least Squares, OLS) can be used to estimate the parameters of each equation, but the weaknesses of  OLS method is remove information on a possible correlation on system equations. This final task, the method of Generalized Least Square (GLS) used to estimate parameters SUR. However GLS less able to withstand the presence of outliers, then used a robust S to estimate model parameters SUR containing outliers. Estimator S is equivariant estimator of regression and the breakdown point can reach as high as 50%, meaning it can handle almost half of bad observation and provides good leverage. Lagrange Multiplier test and Likelihood Ratio test (Likelihood Ratio, LR) is used to test the correlation contemporaneously on the error. The discussion will conclude with a case study on the factors that affect the Foreign Direct Investment (FDI) in the two countries, Indonesia and the Philippines. In the case study concluded that the robust estimation S is better than the GLS estimates.","Kata Kunci : Seemingly Unrelated Regression, Generalized Least Square, Robust S, Lagrange Multiplier test"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96289,DISTRIBUSI LOG-LINDLEY UNTUK PEMODELAN BESAR KLAIM ASURANSI PADA PERHITUNGAN PREMI TOTAL,"ANISYA OKTAVIANA ANINDYATRI , Dr. Abdurakhman, M.Si.; Vemmie Nastiti Lestari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Perhitungan premi asuransi yang efektif merupakan salah satu hal penting yang dapat menjadikan pemegang polis dan perusahaan asuransi benar-benar dapat berada pada posisi ekonomi yang seimbang dengan adanya asuransi tersebut. Bagi perusahaan asuransi prinsip perhitungan premi yang paling sederhana adalah dengan menghitung nilai ekspektasi dari risiko asuransi. Premi ini disebut premi murni. Risiko ini dapat berupa klaim yang meliputi frekuensi klaim dan besar klaim. Oleh karena itu dibutuhkan persetujuan antara perusahaan asuransi dengan pemegang polis mengenai distribusi dari klaim tersebut. Namun, terkadang tidak ada kesepakatan mengenai distribusi tersebut. Sehingga perusahaan asuransi membuat keputusan untuk menambahkan loading pada risiko asuransi yang dijamin, yang merefleksikan adanya bahaya (berkaitan dengan risiko) yang mungkin terjadi. Salah satu cara yang dapat dilakukan adalah dengan melakukan transformasi terhadap CDF awal besar klaim menggunakan fungsi distorsi yang dapat menghasilkan CDF baru dari besar klaim. Distribusi Log-Lindley dapat digunakan sebagai fungsi distorsi terhadap CDF awal distribusi besar klaim tersebut karena sifatnya yang kontinu dan merupakan fungsi tidak turun. Premi hasil perhitungan ini akan dibandingkan dengan premi murni dan dual power premi, dan diperoleh hasil bahwa nilai premi ini lebih besar dari premi murni dan lebih kecil dari dual power premi.","The effective insurance premium calculation is one of important thing that make the policy holders and insurance company in the balanced economy position with the insurance. For insurance company, the simplest premium principle calculation is by calculating the expected value of insurance risk. This premium called net premium. Claim that consist of frequency and severity can be said as one of insurance risk. It needs agreement between the policy holders and insurance company about the risk distribution to calculate the expected value of insurance risk. But , sometimes, there is no consensus about the risk distribution. So, the insurance company makes a decision to add a loading to the risk that reflect the danger associated with the risk that can happen. One way to do this is to transform the initial CDF of claim severity with distortion function that can produce a new CDF of it. The Log-Lindley distribution can be used as distortion function because it is continous and non-decreasing function. Then, this premium will be compared with the net premium and the dual power premium, and the result show that this premium greater than the net premium and smaller than the dual power premium.","Kata Kunci : besar klaim, premi murni, fungsi distorsi , distribusi Log-Lindley, dual power premi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/97569,UJI NEW TWO-SAMPLE STUDENTIZED WILCOXON,"AYU WIDYAWATI, Dr. Herni Utami, M.Si",2016 | Skripsi | S1 STATISTIKA,"Uji sum rank Wilcoxon banyak diaplikasikan pada berbagai disiplin ilmu, seperti ilmu ekonomi, dan lain-lain. Uji ini sangat populer karena kesederhanaan dalam penggunaannya. Meskipun demikian, kenyataan yang terjadi pada hampir semua aplikasi uji sum rank Wilcoxon pada berbagai jurnal akademik seringkali tidak tepat. Secara umum uji ini terlalu sering menolak H_0 sehingga tidak dapat mengontrol probabilitas kesalahan tipe I meskipun  secara asimtotik. Selain itu, masalah yang melekat umum dari uji sum rank Wilcoxon, menolak H_0 bukan karena menerima H_1, tetapi karena asumsi pokok distribusi yang identik tidak dapat dipenuhi. Oleh karena itu, dilakukan modifikasi terhadap uji sum rank Wilcoxon yang disebut uji new two-sample studentized Wilcoxon, dengan mempertimbangkan distribusi dari populasi kedua sampel. Uji new two-sample studentized Wilcoxon tidak memerlukan asumsi distribusi yang identik untuk kedua sampel, selain itu juga dapat mengontrol probabilitas kesalahan tipe I secara asimtotik.","The Wilcoxon rank sum test has been widely used in a broad range of scientific research, including economics, etc. The test is very popular because the simplicity with which it can be performed. In spite of the fact that, most applications of the Wilcoxon rank sum test in accademic journals turn out to be inaccurate. Generally, the Wilcoxon rank sum test rejects the null hypothesis too often, therefore it fails to control the probability of the Type 1 error even asymptotically, thus often give inaccurate conclusion. In addition, common problem of the Wilcoxon rank sum test, rejecting the null hypothesis not because accept the alternative hypothesis, but because the fundamental assumption of the identical distributions fails to hold. Therefore, modified the Wilcoxon rank sum test which called new two-sample studentized Wilcoxon test, consider both of  the populations distribution of the sample. The new two-sample studentized Wilcoxon test doesnÃƒÂ¢Ã¯Â¿Â½Ã¯Â¿Â½t need identical distribution assumption of the two samples, in addition it can control the probability of the Type 1 error asymptotically.","Kata Kunci :  Kata kunci :	mean dua sampel, uji permutasi, distribusi permutasi, uji sum rank Wilcoxon."
http://etd.repository.ugm.ac.id/home/detail_pencarian/98081,GRAFIK PENGENDALI ATRIBUT BERBASIS DISTRIBUSI GEOMETRIK DENGAN ESTIMASI BATAS PENGENDALI,"DWI RULLY HARDINA, Prof. Dr. Sri Haryatmi, M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Industri penghasil barang dan jasa dihadapkan pada tantangan yang cukup berat yaitu kepuasan konsumen terhadap kualitas produksi. Untuk mengahasilkan produk berkualitas tinggi, dibutuhkan suatu proses berkualitas tinggi (high quality process). Proses berkualitas tinggi menginginkan proporsi bagian tidak sesuai atau p-nol sangat kecil. Pengendalian kualitas untuk proses berkualitas tinggi dapat dilakukan menggunakan grafik pengendali atribut/sifat untuk jumlah atau proporsi bagian tak sesuai. Grafik pengendali atribut yang umum dipakai adalah grafik pengendali berdasarkan konsep Shewhart atau k-sigma. Namun keterbatasan dari grafik pengendali k-sigma yaitu memiliki false alarm rate yang tinggi ketika proporsi bagian tak seuai sangat kecil tetapi ukuran sampel tidak cukup besar. Selain itu, batas pengendali bawah grafik pengendali k-sigma akan bernilai negatif sehingga membuat grafik tidak optimal. Sebagai alternatif lain, akan digunakan grafik pengendali geometrik dengan batas probabilitas untuk mengendalikan bagian tidak sesuai dengan menggunakan data bagian sesuai. Pada skripsi ini juga akan dibahas mengenai estimasi parameter dari grafik pengendali geometrik untuk mendapatkan batas pengendali yang sesuai. Selain itu, akan dibahas mengenai false alarm rate dan average run length dari grafik pengendali geometrik. Average run length digunakan untuk melihat performa dari grafik pengendali.","The industry faces a tough challenge of customer satisfaction on the quality of production. To produce a high quality product, we need a high quality process. High quality process wants probability of nonconforming items or p-nol very small. Quality control for high-quality process can be analyzed by attribute control chart for number or proportion of nonconforming items. The common used attribute control chart is based on Shewhart control chart or k-sigma. However, k-sigma control chart has high false alarm rate when value of p-nol is very small and sample size not large enough. And also, lower control limit of k-sigma control chart can be easily negative. As another alternative, will be used geometric control chart with limit probability for quality control of nonconforming item by using data  conforming item. This thesis will discuss about estimated parameter of geometric control chart to get the appropriate control limits. Furthermore, false alarm rate and average run length of the geometric control chart will be discussed. Average run length can be used to indentify the performance of geometric control chart.","Kata Kunci : grafik pengendali geometrik, false alarm rate, high quality process, average run length"
http://etd.repository.ugm.ac.id/home/detail_pencarian/104226,PENENTUAN HARGA OPSI BELI TIPE EROPA MENGGUNAKAN POHON MARKOV,"INDRIA DEWI, Dr. Abdurakhman, M.Si.; Rianti Siswi Utami, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Penelitian tentang penentuan harga opsi terus mengalami perkembangan dari masa ke masa. Salah satunya adalah metode diskrit dengan menggambarkan pergerakan harga saham mengikuti pohon binomial. Model ini mengasumsikan harga saham ke depan memiliki dua kemungkinan yaitu naik atau turun. Namun, model ini secara implisit juga mengasumsikan bahwa data log return harian dari saham merupakan IID. Namun cukup logis jika berpikir bahwa harga saham hari ini tidak akan independen melainkan masih mempertimbangkan harga pada periode sebelumnya. Maka dikembangkanlah model pohon Markov yang mampu mengakomodir saham dengan log return harian yang non IID, dengan memasukkan unsur rantai Markov orde 1 yaitu ketika data pada saat ini hanya dipengaruhi oleh data satu periode sebelumnya. Pada satu periode pertama, saham memiliki kemungkinan untuk naik atau turun. Pada periode kedua dan seterusnya, saham akan memiliki empat gambaran pergerakan dengan probabilitasnya masing-masing yakni kembali naik setelah naik, turun setelah naik, naik setelah turun dan kembali turun setelah turun. Selanjutnya, harga opsi yang diperoleh menggunakan model pohon Markov dibandingkan dengan model Black-Scholes. Berdasarkan nilai RMSE (Root Mean Square Error), harga opsi berdasarkan pohon Markov lebih mendekati harga opsi sebenarnya.","Research on option pricing has continued to experience over time. One of them is a discrete method that describe the stock price movements follow the binomial tree model. This model assume that stock price movement for one period ahead will follow two condition: either increase or decrease. But, this model also implicitly assume that the daily log returns for stock are IID. But, it is logic to think that stock prices wonÃ¢ï¿½ï¿½t move independently except still considering the price of past periods. So, Markov tree model was developed to accommodate the stock with non IID daily log returns, by inserting first order Markov behaviour that allows the jump of the tree to depend on one previous jump. At the first period, the stock price has the possibility to go upward or downward. Furthermore, the stock price has four possible movements with each possibility: go up after upward, go down after upward, go up after downward and go down after downward. Option price result by Markov tree method be tested against Black-Scholes model. Based on its RMSE (Root Mean Square Error), Markov TreeÃ¢ï¿½ï¿½s option prices are much closer to market prices.","Kata Kunci : harga opsi, pohon binomial, pohon Markov, non IID, orde Markov"
http://etd.repository.ugm.ac.id/home/detail_pencarian/101667,Pemodelan Regresi Weibull AFT (Accelerated Failure Time) dengan Nilai Parameter Scale dan Shape yang Tak Konstan,"REZA IMAM MALIK, Yunita Wulan Sari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi Weibull merupakan metode regresi yang seringkali digunakan untuk mengolah data tahan hidup (survival). Model Weibull AFT ini merupakan bagian dari regresi Weibull yang digunakan untuk mengestimasi nilai log dari waktu suatu atau seorang individu mengalami event. Pada umumnya, parameter shape pada metode regresi Weibull yang sering digunakan bernilai konstan. Dengan menggunakan metode Maximum Likelihood Estimator (MLE), iterasi Newton-Raphson dan transformasi koefisien regresi dengan parameter shape tak konstan, didapatkan dua model regresi Weibull dengan parameter shape konstan dan scale tak konstan. Kedua model dibandingkan dengan menggunakan nilai Mean Squared Error (MSE) dan Akaike Information Criterion (AIC). Studi kasus yang diolah menggunakan data acak hasil simulasi yang berdistribusi Weibull dengan menggunakan paket program pendukung olah data dalam penulisan skripsi ini.","Weibull regression analysis is a regression method which is usually used to analyze survival data. This AFT Weibull model is one of Weibull regression method which estimates log value of the sampleâ€™s failure time. 	In general, Weibull regression method uses a constant shape parameter. Using the Maximum Likelihood Estimator (MLE), Newton-Raphson iteration and transformed regression coefficient, we can get both Weibull regression with constant shape parameter and nonconstant shape parameter. The two models are compared then by the values of Mean Squared Error (MSE) and Akaike Information Criterion (AIC). For the case of study, we used the randomly Weibull-distributed simulation data using statistical softwares as main supports in this thesis.","Kata Kunci : regresi, Weibull, shape, tak konstan, data tahan hidup, survival, Maximum Likelihood Estimator, MLE, Newton-Raphson, Mean Squared Error, MSE, Akaike Information Criterion, AIC, regression, Weibull, shape, nonconstant, survival, Maximum Likelihood, MLE, Ne"
http://etd.repository.ugm.ac.id/home/detail_pencarian/101159,ESTIMASI MODEL REGRESI WEIBULL NONPROPORTIONAL HAZARD,"IKA NUR JANNAH F, Drs. Danardono, MPH., Ph.D.",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi parametrik Weibull pada umumnya adalah regresi parametrik Weibull hazard proporsional. Model regresi Weibull hazard proporsional mempunyai parameter scale fungsi dari kovariat dan parameter shape konstan. Namun pada kenyataanya tidak semua data yang berdistribusi Weibull dapat di estimasi menggunakan regresi Weibull hazard proporsional. Oleh karena itu, diperlukan penggeneralan model dengan membuat parameter shape fungsi kovariat. Model yang dihasilkan dari penggeneralan ini disebut model Weibull hazard nonproporsional. Metode estimasi yang digunakan untuk memperoleh estimasi parameter regresi adalah metode Maximum Likelihood Estimation (MLE) dan metode iterasi Newton Raphson dengan bantuan software R. Analisis dilanjutkan dengan menguji koefisien parameter pada model regresi yang dihasilkan.","Most of Weibull model on parametric regression analysis is proportional hazard Weibull. Proportional hazard Weibull has scale parameter a function of covariate and constat shape parameter. In fact, not all the data has Weibull distribution can be estimated with a proportional hazard Weibull. Therefore, necessary generalization models by making the shape parameter a function of covariate. The resulting model of this generalization is called nonproportional Weibull hazard models. The estimation method which is used is the Maximum Likelihood Estimation (MLE) and the iterative Newton Raphson method with R software. Analysis following the testing of the coefficient parameter of regression being produced.","Kata Kunci : Weibull, Nonproporsional, Hazard, Maximum Likelihood Estimation (MLE)/ Weibull, Nonproportional, Hazard, Maximum Likelihood Estimation (MLE)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/106535,Rancangan Modifikasi Grafik Pengendali Tukey,"TYAS PRIYANDANI, Prof. Dr. rer.nat. Dedi Rosadi, M.Sc",2016 | Skripsi | S1 STATISTIKA,"Pengendalian kualitas proses merupakan faktor penting untuk meminimalkan penyimpangan dalam proses industri. Terdapat banyak grafik pengendali atau diagram kontrol yang tersedia, tetapi sering kali terdapat asumsi pembuatan grafik pengendali yang tidak terpenuhi, misalnya normalitas. Salah satu grafik pengendali untuk mengatasi permasalahan tersebut adalah Tukey's Control Chart (TCC). Tetapi performa TCC kurang baik ketika distribusi data tidak simetris. Kemudian diperkenalkan Modified Tukey's Control Chart (MTCC) sebagai alternatif dari TCC. Dalam skripsi ini MTCC diaplikasikan pada data pengendalian kualitas produksi pertasol CB di Pusdiklat Migas Cepu untuk karakteristik Density 15oC dan Final Boiling Point (FBP). Selanjutnya performa dari MTCC akan dibandingkan dengan Tukey's Control Chart (TCC) berdasarkan perhitungan nilai Average Run Length (ARL). ARL merupakan jumlah rata-rata titik atau sampel yang harus digambarkan sebelum sebuah titik atau sampel menyatakan suatu keadaan tidak terkendali. Kemudian diperoleh hasil yang menunjukkan bahwa performa MTCC lebih baik daripada TCC.","Quality control process is an important factor to minimize deviation in the process industry. There are many graphics controllers or control charts available, but often there is an assumption charting controls are not met, for example normality. One of the control charts to resolve that problems is Tukey's Control Chart (TCC). But TCC peformance is not good when the data distribution is not symmetric. MTCC was then introduced as an alternative to TCC. In this study MTCC applied to the data of quality control production of pertasol CB in Pusdiklat Migas Cepu for characteristics Density 15oC and Final Boiling Point (FBP). Furthermore the performance of the MTCC will be compared with Tukey's Control Chart ( TCC ) based on the calculation of the value of Average Run Length (ARL). ARL is the average number of points or samples should be drawn before a sample shown an out of control condition. Then the obtained results show that the performance of MTCC is better than TCC.","Kata Kunci : grafik pengendali Tukey, modifikasi grafik pengendali Tukey, Average Run Length"
http://etd.repository.ugm.ac.id/home/detail_pencarian/95020,PERBANDINGAN ANTARA ESTIMATOR   HANSEN-HURWITZ DAN HORVITZ-THOMPSON PADA KLASTER DUA TAHAP  : (PPS) SAMPLING DENGAN PENGEMBALIAN,"SEPTIANA WIDI S, Dr. Danardono,MPH",2016 | Skripsi | S1 STATISTIKA,"Prosedur penarikan sampel dimana peluang terpilihnya suatu unit sampel sebanding dengan ukuran disebut sebagai sampling berpeluang sebanding dengan ukuran atau sampling with probability proportional to size(PPS). Pengambilan sampel klaster dua tahap adalah metode pengambilan sampel untuk mengatasi populasi yang besar dengan tingkat keheterogenan yang besar. Untuk memperoleh keheterogenan sampel klaster dua tahap diawali dengan memilih sampel klaster dari populasi kemudian memilih elemen sampel dari tiap sampel klaster yang terpilih. Dalam skripsi ini, estimator Hansen-Hurwitz dan estimator Horvitz-Thampson akan dibandingkan dan digunakan untuk mengestimasi populasi menggunakan metode pengambilan sampel klaster dua tahap dengan pengembalian. Dalam studi kasus dilakukan pengambilan sampel untuk mengestimasi rata-rata dan jumlah pengguna alat kontrasepsi peserta KB yang terdapat di Provinsi DIY 2014.","Sampling procedures where chance of election of unit comparable to size of the sample referred to as sampling with probability proportional to size(PPS). Two-Stage cluster sampling is a sampling methods to overcome a large population with a high degree of heterogeneity. Two stage cluster sampling is obtained by selecting sample clusters from the population and used as the primary unit, and then choose the sample elements of each sample clusters were selected. In This thesis, Hansen-Hurwitz and Horvitz-Thompson estimator will be compared and used to estimated the population using two-stage cluster sampling with replacement. In the case studies carried out sampling to estimate the mean number and total number of users contraceptives in the Province DIY.","Kata Kunci : PPS Sampling, Two-Stage Cluster Sampling, Horvitz-Thompson Estimator, Hansen-Hurwitz Estimator."
http://etd.repository.ugm.ac.id/home/detail_pencarian/100143,PEMODELAN STRUKTURAL RISIKO OPERASIONAL PERBANKAN MELALUI INFERENSI BAYESIAN,"MERDIANNAWATY, Dr. Abdurakhman, M.Si.;Yunita Wulan Sari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Risiko operasional merupakan salah satu risiko yang dihadapi bank dalam kegiatan usahanya. Pengukuran risiko operasional dapat dilakukan dengan metode Advanced Measurement Approach (AMA). Metode AMA memungkinkan bank untuk mengembangkan sendiri model empiris dalam mengkuantifikasikan potensi kerugian risiko operasional. Untuk memenuhi peraturan persyaratan Basel II untuk metode AMA, maka model empiris yang dikembangkan bank harus meliputi penggunaan data historis internal, data historis eksternal, dan analisis skenario. Kuantifikasi risiko operasional yang melibatkan data historis internal dan analisis skenario dapat membentuk model yang tidak hanya bersifat backward looking, tetapi juga forward looking. Metode inferensi Bayesian merupakan teknik statistika yang cocok untuk dapat menggabungkan beberapa sumber data yang berbeda dalam kerangka statistik yang konsisten. Sehingga penggunaan konsep inferensi Bayesian dapat memungkinkan pemodelan struktural risiko operasional dengan melibatkan antara data historis internal dan analisis skenario (maupun antara data historis internal dan data historis eksternal). Pengembangan model empiris melalui inferensi Bayesian ini dilakukan dengan menyertakan analisis skenario ataupun data historis eksternal dalam penentuan estimasi distribusi prior parameter model.","Operational risk is one of the risk faced by banks in their business activities. Operational risk measurement can be quantified by Advanced Measurement Approach (AMA) method. The AMA method allows the banks to develop an internal empirical model to quantify potential operational risk losses. To meet the Basel II requlatory requirements for the AMA method, then the bankâ€™s empirical model must include the use of internal historical data, external historical data, and scenario analysis. Quantification of operational risk including internal historical data and scenario analysis can establish the model not only backward looking, but also forward looking. Bayesian inference is a statistical technique well suited to be able to combine several different data sources in a consistent statistical framework. So the use of Bayesian inference concept allows the structural modelling of operational risk including internal historical data and scenario analysis (as well as internal historical data and external historical data). Development of empirical model via Bayesian inference is carried out by including scenario analysis or external historical data in determining estimation prior distribution of parameter model.","Kata Kunci : risiko operasional, Basel II, model distribusi kerugian, inferensi Bayesian, simulasi Monte Carlo, Value at Risk (VaR)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/104496,MODEL REGRESI MULTILEVEL ZERO-INFLATED GENERALIZED POISSON,"LINTANG IKA PERMATA, Dr. DANARDONO, MPH",2016 | Skripsi | S1 STATISTIKA,"Model regresi Multilevel Zero Inflated Generalized Poisson merupakan model regresi yang digunakan untuk memodelkan data pengamatan hierarkis atau bertingkat yang berkorelasi di mana datanya berupa data cacah yang mengalami kelebihan nol. Estimasi parameter model regresi Multilevel Zero Inflated Generalized Poisson diestimasi dengan metode Maximum Likelihood Estimator (MLE) yang menggunakan algoritma Ekspektasi Maksimalisasi (EM). Model regresi Multilevel Zero Inflated Generalized Poisson diaplikasikan pada data tentang jumlah kematian ibu pada ibu hamil di provinsi Nanggroe Aceh Darussalam, Sumatera Utara dan Jawa Tengah tahun 2014. Hasilnya menunjukkan bahwa model regresi Multilevel Zero Inflated Generalized Poisson lebih baik dalam menangani data bertingkat yang mengalami kelebihan nol.","Multilevel Zero Inflated Generalized Poisson regression model is a regression model which is used to model multilevel or hierarchical correlated observation data where the data is count data with excess zeros. Multilevel Zero Inflated Generalized Poisson regression model parameter estimation is estimated with MLE method using Expectation Maximization (EM) algorithm. Multilevel Zero Inflated Generalized Poisson regression model is applied to the number of pregnant mothers death data within the province of Nanggroe Aceh Darussalam, North Sumatera, and Central Java in 2014. The result shows that Multilevel Zero Inflated Generalized Poisson regression model is better at handling multilevel data with excess zeros.","Kata Kunci : count data, multilevel, Multilevel Zero Inflated Generalized Poisson"
http://etd.repository.ugm.ac.id/home/detail_pencarian/97845,REGRESI LOGISTIK KONDISIONAL UNTUK DESAIN STUDI CASE CONTROL DENGAN METODE MATCHING BEBERAPA KONTROL,"NUR M ARIF DARMA WJY, Drs. Zulaela, Dipl.Med.Stats, M.Si.",2016 | Skripsi | S1 STATISTIKA,"Tujuan penulisan ini mempelajari desain studi matched case control sebagai salah satu alternatif untuk menghilangkan pengaruh faktor pengganggu (confounding factor) yang dapat menyimpangkan hubungan sebenarnya antara penyakit (disease) dan paparan (exposure). Mempelajari penggunaan regresi logistik untuk desain studi yang dikondisionalkan, di mana pada penulisan ini yaitu di-matching-kan lebih dari 1 control (match case control 1:R) mampu mengaplikasikan regresi logistik kondisional pada data kesehatan di mana ingin diketahui faktor-faktor yang mempengaruhi risiko terkena kanker endometrium pada wanita dengan riwayat menggunakan atau memakai estrogen. Jumlah data sebanyak 63 kasus dan 252 kontrol. Metode analisis data menggunakan Frequencies, McNemar Test, Analisis Regresi Linier, dan Regresi Logistik Kondisional. Hasil analisis menunjukkan bahwa Matched Case Control dapat digunakan sebagai salah satu alternatif untuk menghilangkan pengaruh faktor pengganggu (confounding factor) yang dapat menyimpangkan hubungan sebenarnya antara penyakit (disease) dan paparan (exposure). Regresi logistik kondisional dapat diaplikasikan pada data kesehatan di mana ingin diketahui risiko terkena kanker endometrium pada wanita dengan riwayat menggunakan atau memakai estrogen.","The purpose of this paper is to study the matched case control study design as an alternative to eliminate the influence of confounding factors that may distorted the actual relationship between the disease and exposure. Studying the use of logistic regression to study condotional design studieswhere at this writing is matching case with more than 1 control (match case control 1: R), capable of applying logistic regression conditional on health data where we want to know the factors that affect the risk of endometrial cancer in women with a history of us estrogen.The amount of data as many as 63 cases and 252 controls. Data analysis methods using Conditional Logistic Regression. The analysis showed that the Matched Case Control can be used as an alternative to eliminate the influence of confounding factors that may distorted the actual relationship between the disease and exposure. Conditional logistic regression can be applied to the health data where we want to know the risk of endometrial cancer in women with a history of estrogen use or wear.","Kata Kunci : Conditional Logistic Regression, Matching, Matched Case Control, Endometrial Cancer"
http://etd.repository.ugm.ac.id/home/detail_pencarian/92985,REGRESI POLINOMIAL LOKAL DENGAN FUNGSI KERNEL GAUSSIAN,"AYU AULIA, Prof. Subanar, Ph.D.",2016 | Skripsi | S1 STATISTIKA,"Regresi polinomial lokal adalah salah satu estimator dari fungsi regresi kernel yang menggunakan bentuk polinomial dimana orde pada polinomial sesuai dengan yang diinginkan. Fungsi regresi kernel merupakan salah satu dari analisis regresi nonparametrik yang merupakan statistik alternatif saat analisis parametrik tidak dapat digunakan. Regresi polinomial lokal didasarkan pada prinsip memberi bobot pada setiap pengamatan dengan pembobot fungsi kernel, sedangkan ukuran bobot ditentukan oleh parameter bandwidth. Dalam skripsi ini akan dibahas cara mengestimasi Indeks Massa Tubuh (IMT) orang dewasa di Kecamatan Mlati, Kabupaten Sleman pada tahun 2015 menggunakan regresi Polinomial Lokal dengan fungsi kernel Gaussian serta metode pemilihan bandwidth adalah Bandwidth ""Rule of Thumb"", Modified Cross Validation, Biased Cross Validation dan Complete Cross Validation. Nilai MSE  dari setiap metode pemilihan bandwidth juga akan dilakukakan. Hasilnya menunjukan MSE yang paling kecil diperoleh menggunakan metode bandwidth ""Rule of Thumb"" dengan nilai sebesar 18.28869.","Local polynomial regression is one type of estimator of the kernel regression functions that use polynomial form in which the order of polynomial depend on reseacher. Kernel regression function is one of the nonparametric regression analysis which is an alternative to the current statistical parametric analysis can not be used. Local polynomial regression is based on the principle of giving weight to each observation with a weighting function of the kernel, while the size of the weights are determined by the parameters of bandwidth. In this thesis will be discussed about how to estimate the body mass index (BMI) of adult at Mlati, Sleman in 2015 using regression polynomial Local with Gaussian kernel and the method of selecting bandwidth is Bandwidth ""Rule of Thumb"", Modified Cross Validation, Biased Cross Validation and Complete Cross Validation. The value of MSE of each method for selecting bandwidth will be compared. The result shows that the smallest MSE obtained using methods bandwidth ""Rule of Thumb"" with a value of 18.28869.","Kata Kunci : Regresi Nonparametrik, Regresi Kernel, Fungsi Gaussian, Regresi Polinomial Lokal, Bandwidth"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96570,"UJI ONE STAGE , BROWN FORSYTHE DAN WELCH  SEBAGAI METODE ALTERNATIF UNTUK ANALISIS VARIANSI DENGAN PELANGGARAN ASUMSI KESAMAAN VARIANSI","RIZKI YUDHA PERWIRA, Dr. Gunardi, M.Si; Yunita Wulan Sari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Diketahui bahwa menggunakan Uji F klasik untuk menguji mean beberapa populasi membutuhkan asumsi data independen antar level faktor, kesamaan variansi antar level faktor dan normalitas data. Ketika asumsi-asumsi diatas khususnya untuk asumsi kesamaan variansi tidak terpenuhi, Uji F menjadi tidak robust dalam pengambilan keputusan. Hal ini menjadi masalah yang serius jika terus diterapkan, khususnya ketika digunakan untuk sampel kecil. Untuk mengatasi permasalahan ini digunakan dua metode uji alternatif untuk analisis variansi yaitu Uji One-stage, Uji Welch dan Uji Brown Forsythe. Selanjutnya ketiga metode alternatif ini dibandingkan kinerja dalam olah data riil dan studi simulasi perbandingan metode uji berdasarkan type I error rate. Hasil simulasi menunjukkan bahwa kekuatan Uji One-stage lebih baik daripada Uji Welch, Uji Brown Forsythe dan tes ini berkinerja baik dalam hal menolak rasio untuk sejumlah populasi dan ukuran sampel yang kecil.","It is known that using The classical F-test to compare several population means depend on the assumption of equality of variance of the population and the normality. When these assumption especiality the equality of variance is dropped, the classical F-test be not robust for decision making. This can be considered a serious problem in some applications, especially when the sample is not large. To deal with this problem, a number of tests are available in the literature. In this study One-stage, Welch and Brown Forsythe tests are alternative method of analysis variance that introduced and a simulation study is performed to compare these tests according to type I error rates. Simulation results indicate that the power of the singlestage test is better than the Brown Forsythe, Welch test and that this test performs well in terms of the reject ratios for small number of populations and sample sizes.","Kata Kunci : Analysis of variance, heterogeneous variance, One-stage, Welch, Brown Forsythe,Type I Error Rate"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96316,ESTIMASI MODEL CAMPURAN DUA DISTRIBUSI DENGAN ALGORITMA EKSPEKTASI MAKSIMISASI PADA DATA SURVIVAL,"DENY BUDI ASTUTI , Drs. Zulaela, Dipl.Med.Stats.",2016 | Skripsi | S1 STATISTIKA,"Model campuran merupakan model gabungan dari dua atau lebih distribusi. Distribusi yang digunakan dalam model campuran bisa distribusi yang sama dengan parameter berbeda maupun distribusi yang berbeda. Model ini mengelompokkan data di dalam suatu dataset menjadi kelompok-kelompok data yang sebelumnya tidak terdefinisikan. Dalam model campuran dua distribusi, distribusi eksponensial dan distribusi Weibull dapat membentuk tiga model campuran yang berbeda. Model yang terbentuk adalah model campuran eksponensial-eksponensial, model campuran eksponensial-weibull dan model campuran Weibull-Weibull. Dalam model campuran dua distribusi akan diberikan bobot atau berat pada dua distribusi yang digunakan. Jumlahan dari bobot harus sama dengan satu. Untuk mencari estimasi dari model campuran dua distribusi digunakan metode algoritma ekspektasi maksimisasi (algoritma EM) karena adanya data tidak lengkap (tersensor) dalam kasus data survival. Studi kasus menggunakan data pasien kanker paru-paru. Menggunakan pemrograman R, diperoleh kesimpulan bahwa tiga model campuran dua distribusi yang dibentuk sesuai digunakan untuk data pasien kanker paru-paru. Model campuran Weibull-Weibull merupakan model yang paling baik untuk data kanker paru-paru.","Mixture model is a combination model  of two or more distributions.  Distribution in the model mixture can consist of distribution identic with different parameter or non-identic. This model classify data within dataset into groups of data that previously were not defined. In the mixture two distribution models, exponential and Weibull distribution make three different models. This models are mixture models of exponential-exponential, exponential-Weibull, Weibull-Weibull. In the mixture two distribution models will be give weight of the two distributions. The sum of the weights should be one. To look the estimation of exponential Weibull mixture model used method algorithm EM because asumsy missing data. A case study on patient-lung-data by programming R, we concluded that the three models that form a mixture of two distributian coresponding to the dataset lung. Mixture model of Weibull-Weibull is the best model for dataset the lung.","Kata Kunci : analisis survival,model campuran dua distribusi,algoritma EM"
http://etd.repository.ugm.ac.id/home/detail_pencarian/97598,PEMILIHAN MODEL UNTUK REGRESI KUANTIL TOBIT DENGAN MENGGUNAKAN GIBBS SAMPLING,"FADHILAH FIQIH K A B, Prof. Sri Haryatmi, M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Regresi kuantil tobit digunakan untuk mengatasi keterbatasan regresi dalam menganalisis data tersensor yang bentuknya tidak simetris dan terdapat pencilan. Regresi kuantil tobit dapat diestimasi menggunakan metode bayesian yakni suatu metode analisis berdasarkan pada informasi yang berasal dari sampel dan informasi prior. Gabungan informasi sampel dan informasi prior ini dinamakan posterior. Dalam mencari distribusi posterior untuk parameter yang cukup banyak sering kali mengalami kesulitan. Teknik khusus yang dapat digunakan untuk mempermudah yaitu dengan menggunakan simulasi Gibbs sampling. Pada software R terdapat paket Brq untuk analisis regresi kuantil tobit dengan menggunakan Gibbs sampling secara lengkap. Studi kasus dalam skripsi ini membahas faktor apa saja yang mempengaruhi seseorang menjadi petani. Hasil estimasi regresi kuantil tobit akan dibandingkan dengan regresi logistik, dan dibandingkan dengan metode regresi tobit. Dengan menggunakan nilai MSE diperoleh kesimpulan bahwa regresi kuantil tobit menghasilkan estimasi yang lebih akurat dan presisi daripada estimasi dengan metode lainnya.","Tobit quantile regression can be used to overcome the limitation of regression to analyze censored data which not symmetric and outlier existed. Tobit quantile regression can be estimated by using bayesian method which based on the information derived from sample and prior information. The combination of sample and prior information is called by posterior. So difficult to find a posterior distribution wich include by many parameters. Therefore, there is a special techniques which will make easily, that can be uses by Gibbs Sampling. R software provides a Brq package for doing analysis in tobit quantile regression using Gibbs Sampling completely. The case study in this paper discussesthe factors that effect people become farmer. The estimation result of tobit quantile regression will be compared with logistic regression, and compared with tobit regression method. By using value of MSE provide a conclusion that estimation of tobit quantile regression more accurate and precision than other estimation.","Kata Kunci : Regresi Kuantil Tobit, Bayesian, Gibbs sampling, Asymmetric Laplace distribution"
http://etd.repository.ugm.ac.id/home/detail_pencarian/99393,"Analisis Block Kriging untuk Estimasi Endapan Nikel Laterit (Studi Kasus : Endapan Nikel Laterit di Bukit TLC4 Pomalaa Wilayah Penambangan PT. Aneka Tambang, Tbk)","ADITA MUBARIKA, Drs. Zulaela, Dipl. Med. Stats., M.Si.",2016 | Skripsi | S1 STATISTIKA,"Salah satu metode yang dikenal dalam ilmu geostatistika adalah metode kriging. Kriging merupakan metode yang memanfaatkan nilai pada lokasi tersampel untuk memprediksi nilai pada lokasi lain yang tidak tersampel. Ada beberapa jenis dalam metode kriging, diantaranya adalah block kriging. Metode ini mengestimasi nilai data di titik-titik dalam suatu blok berdasarkan set data di luar blok. Block kriging dapat digunakan untuk data dengan rata-rata tidak diketahui dan data yang tidak memiliki kecenderungan (trend) tertentu. Studi kasus yang digunakan adalah data kandungan nikel laterit di daerah Tambang Tengah Bukit TLC4 Pomalaa, wilayah konsesi penambangan PT Aneka Tambang, Tbk. Hasil estimasi 8820 titik yang tidak tersampel pada koordinat absis 345725 - 346025 dan ordinat 9530574 - 9530849 menunjukkan bahwa rata-rata estimasi kandungan nikel laterit adalah sebesar 1,4256 %.","One of the methods that popular in geostatistics study is a kriging method. Kriging is a method that use amount of values on sample area to estimate the value on unsampled area. There are many types of kriging methods, one of them is a block kriging. This methods estimate amount of values inside the block based on amount of values outside the block. Block kriging can be used on data which average is unknown and have no trend. 	Case studies been used are the content of lateritic nickel on TLC4 Pomalaa Middle Hill mine, the mining concession area of PT. Aneka Tambang, Tbk. The estimation results of 8820 points which are not sampled at coordinates abscissa 345725 - 346025 and ordinate 9530574 - 9530849 showed that the average content of lateritic nickel is estimated 1,4256 %.","Kata Kunci : geostatistika, data spasial, kriging, block kriging, semivariogram isotropi, nikel laterit, geostatistics, spatial data, kriging, block kriging, isotropy semivariogram, lateritic nickel"
http://etd.repository.ugm.ac.id/home/detail_pencarian/98114,ZERO ADJUSTED GAMMA UNTUK KELAYAKAN PINJAMAN JANGKA PANJANG,"CHANDRA AKASA, Prof.Dr.rer.nat Dedi Rosadi, M.Sc. ; Widya Irmaningtyas, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Penyedia jasa pinjaman kredit pada umumnya memiliki standar khusus dalam menentukan kebijakan pembiayaan kepada calon debitur sesuai dengan kemampuan, karakter dan jaminannya serta kesepakatannya. Pada beberapa kasus sering ditemukan adanya nilai nol dalam data pinjaman kredit. Nilai nol ini dapat diartikan sebagai tidak disetujuinya calon debitur dalam melaksanakan perjanjian kredit. Dalam skripsi ini akan dibahas Zero Adjusted Gamma yang dapat mengatasi adanya nilai nol dalam data pembiayaan ini. Model Zero Adjusted Gamma merupakan model campuran diskrit dan kontinu dengan model kontinyunya merupakan distrbusi gamma. Model ini sangat cocok untuk data yang mempunyai extreme right-skewness serta adanya nilai nol dalam data. Model Zero Adjusted Gamma di sini akan diestimasi menggunakan metode Maximum Likelihood dengan bantuan algoritma RS.","Credit loan services provider generally has a specific standard in determining funding policy to prospective borrowers according to their ability, character and collateral as well as agreement. In some cases often found their zero values in the data of credit loan. A value of zero can be interpreted as disapproval of prospective borrowers in executing loan agreements. In this paper will be discussed Zero Gamma Adjusted to overcome the zero value in the data of this financing .Zero Adjusted Gamma Model is a mixture of discrete and continuous models with Continues Model is gamma distribution . This model is perfect for data that have extreme right- skewness as well as their zero values in the data. Model Zero Adjusted Gamma here will be estimated using Maximum Likelihood method with the help of RS algorithm .","Kata Kunci : Zero Adjusted Gamma, Maximum Likelihood Estimation, Algoritma RS, Mixed Model,  Pinjaman Jangka Panjang"
http://etd.repository.ugm.ac.id/home/detail_pencarian/101194,ESTIMASI VALUE AT RISK (VAR) PORTOFOLIO BIVARIAT MENGGUNAKAN METODE COPULA-GARCH,"TRI MUTOHAROH , Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.;Yunita Wulan Sari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Value at Risk merupakan salah satu ukuran risiko yang popular dalam manajemen risiko. Banyak konsep teoritik dalam dunia finansial yang dikembangkan belakangan ini, termasuk didalamnya yaitu teori portofolio klasik dan pendekatan VaR dengan metode Variansi-Kovariansi. Kebanyakan metode yang dikembangkan mengasumsikan bahwa return dalam portofolio berdistribusi normal dan ukuran dependensi diantara saham-saham portofolio menggunakan korelasi linear. Tetapi kenyataannya, kebanyakan data return tidak mengikuti distribusi normal seperti yang diasumsikan. Selain itu, korelasi linear yang biasa digunakan untuk mengukur dependensi antar return dalam portofolio tidak lagi sesuai untuk mendeteksi hubungan dependensi yang non linear, dalam keadaan demikian estimasi VaR yang diperoleh menjadi kurang akurat. Copula merupakan suatu alternatif dalam pemodelan dependensi. Diberikan fungsi distribusi marginal return, maka dengan menggunakan fungsi copula kita dapat mengkontruksikan suatu fungsi distribusi bersama dari return tersebut dalam suatu portofolio. Mengingat bahwa pada umumnya return memiliki volatilitas tinggi dan variansi berbeda disetiap titik waktunya, maka masing-masing return dimodelkan terlebih dahulu dengan GARCH(1,1). Selanjutnya, dilakukan pemodelan copula dan estimasi VaR. Penulisan skripsi ini membahas tentang metode Copula-GARCH beserta aplikasinya dalam estimasi VaR portofolio yang terdiri dari dua indeks saham. Data yang digunakan adalah return indeks saham JKSE dan KLSE selama periode 12 agustus 2012 hingga 13 Mei 2016.","Value at Risk is one of the popular risk measure in risk management. Many of the theoretical concepts in finance developed over the past decades, including the classical portfolio theory and the variance-covariance method approach to VaR. Most of method developed assumes that individual return on the portfolio in normal distribution and size of the dependence among individual return in portfolio use linear correlation. But in reality, most of the individual returns are not normally distributed. In addition, the linear correlation as measure of dependence among individual return on the portfolio is no longer suitable for detecting non-linear dependence relationship, in this situation the VaR estimates obtained may be less accurate. Copula is an alternative in modeling dependence. Given marginal distribution functions return and Copula function, we can make a joint distribution function of the return in a portfolio. In generally return has high volatility and the variance is different every time point, then each return is modeled in advance by GARCH (1,1). Furthermore, we can make Copula models and estimate the VaR.  This paper discusses the Copula-GARCH method and its application in estimating VaR of portfolio composed by two indexes. In this research, we use historical price of stock index return JKSE and KLSE during the period August 12th , 2012 until May 13th , 2016.","Kata Kunci : GARCH, Copula, Value at Risk, Variansi-Kovariansi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/94289,ESTIMASI TOTAL LOSS MENGGUNAKAN REGRESI BERBASIS COPULA,"INDAH DWI PEBRIANTI, Dr. Adhitya Ronnie Effendie, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Penggunaan regresi berbasis copula dalam skripsi ini adalah untuk menghasilkan suatu estimasi dari total loss (kerugian) yang dialami suatu perusahaan asuransi yang berasal dari data klaim perusahaan tersebut. Estimasi total loss merupakan hal yang sangat penting dalam pekerjaan seorang aktuaris, karena hasil estimasi tersebut dapat menjadi acuan dalam menentukan harga kontrak asuransi, premi hingga manajemen resiko. Data klaim terdiri dari frekuensi (banyak klaim) dan severity (besar klaim) yang terjadi pada periode waktu tertentu. Dua model yang akan digabung untuk menjadi model distribusi bersama menggunakan copula yaitu model marginal frekuensi dan model marginal severity. Model marginal didapat dari analisis Generalized Linear Model (GLM), yaitu GLM ZTP untuk frekuensi dan GLM Gamma untuk severity. Dalam GLM, variabel dependen dipengaruhi oleh dua kovariat sebagai variabel independen. Jenis copula yang digunakan merupakan jenis copula Archimedean yaitu copula Clayton, Gumbel, dan Frank, serta jenis copula Elliptical yaitu copula Gaussian. Metode pemilihan model copula  yang terbaik untuk mendapatkan hasil prediksi yang sesuai menggunakan uji Aike Information Criteria (AIC). Hasil dalam studi kasus pada skripsi ini menunjukan bahwa model copula  clayton memiliki nilai AIC terendah sehingga terpilih menjadi model terbaik dengan hasil total loss sebesar 30,26 milyar rupiah.","The use of Copula-based regression in this thesis is to produce an estimate of the total loss (loss) experienced a taste of the insurance company that the company claims data. Claims data consist of frequency (number of claims) and severity (average the claims) that occurs in certain time period. Zero Truncated Poisson distribution frequency (ZTP) and severity Gamma distribution. Two models will be merged to become a joint distribution model using the model marginal Copula frequency and severity of marginal models. Marginal models obtained by analysis of Generalized Linear Model (GLM), which GLM  ZTP to the frequency and GLM Gamma for severity. In GLM, the dependent variable is affected by two covariates as independent variables. Copula type used is a type of Archimedean Copula that Clayton Copula, Gumbel, and Frank, as well as the type of Elliptical Copula namely Gaussian Copula. Copula model selection method is best to get the corresponding prediction using test Aike Information Criteria (AIC) . Results of the case studies in this paper show that the model Copula clayton has the lowest AIC value thus chosen to be the best model with the results of a total loss of 30.26 billion rupiah.","Kata Kunci : Total loss, Bivariat Copula, Generalized Linear Model, frekuensi, severity, AIC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/98641,PEMODELAN PREMI MURNI ASURANSI KENDARAAN BERMOTOR MENGGUNAKAN GENERALIZED LINEAR MODELS (GLM),"ZULFIANA NURUL L, Dr. Gunardi, M.Si",2016 | Skripsi | S1 STATISTIKA,"Perhitungan premi murni asuransi kendaraan bermotor biasanya didasarkan pada resiko yang dialami. Banyak metode yang digunakan untuk menghitung nilai premi murni. Salah satunya adalah Generalized Linear Models yang akan dibahas dalam skripsi ini. Penggunaan Generalized Linear Models dalam perhitungan premi murni asuransi kendaraan bermotor adalah dengan melihat resiko kendaraan melalui karakteristik kendaraan tersebut. Model ini mengkombinasikan nilai ekspektasi bersyarat dari frekuensi klaim dan nilai ekspektasi bersyarat biaya klaim. Pada skripsi ini menggunakan data polis asuransi kendaraan bermotor tahun 2014 dari salah satu perusahaan asuransi di Indonesia. Data tersebut juga dilengkapi karakteristik kendaraan yang diasuransikan. Estimasi parameter yang digunakan adalah metode maksimum likelihood. Syarat dari model ini adalah distribusi variabel respon merupakan anggota dari keluarga Eksponensial. Dari hasil uji kecocokan distribusi, frekuensi klaim mengikuti distribusi Poisson dan biaya klaim mengikuti distribusi Gamma. Fungsi link yang digunakan dalam model ini adalah log baik untuk memodelkan frekuensi klaim maupun untuk memodelkan biaya klaim. Berdasarkan analisis yang dilakukan, karakteristik kendaraan yang mempengaruhi premi murni adalah merk kendaraan, usia kendaraan, biaya pertanggungan, dan jenis asuransi.","Pricing pure premium for motor vehicle insurance usually based on risk of the motor vehicle. There are a lot of methods used to pricing auto insurance pure premium. One of them is Generalized Linear Models that will be discussed here. The used of Generalized Linear Models for pricing motor vehicle insurance pure premium considers the motor vehicle risk given the observable characteristics of the motor vehicle. This model combines the conditional expectation of the claim frequency with the expected claim amount. The case study here uses policy motor vehicle insurance data during the year 2014 from one of insurance company in Indonesia. The data also contain the characteristics of the motor vehicle. The parameter estimation method is used here is maximum likelihood estimation. The random variable distribution of variable Y is a particular family of distribution, namely the exponential family. From the result of checking distribution, the distribution of claim frequency is Poisson distribution and claim amount is Gamma distribution. Both claim frequency and claim amounts modelling use natural logarithm link function. In this case, the characteristics of motor vehicle that influence the pure premium are type of motor vehicle, age, guarantee fee, and the type of insurance.","Kata Kunci : pure premium, claim frequency, claim amounts, Poisson distribution, Gamma distribution, maximum likelihood estimation method, link function, Generalized Linear Models."
http://etd.repository.ugm.ac.id/home/detail_pencarian/98645,ESTIMASI DATA PROPORSI BERKORELASI SPASIAL MENGGUNAKAN MODEL BETA-BINOMIAL KRIGING,"ATIKA NARULITA SARI, Prof. Dr. rer. nat. Dedi Rosadi, M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Kriging adalah suatu metode geostatistika yang memanfaatkan nilai spasial pada lokasi tersampel dan semivariogram untuk memprediksi nilai pada lokasi lain yang belum tersampel di mana nilai prediksi tersebut bergantung pada kedekatannya terhadap lokasi tersampel. Berbagai model kriging telah diajukan untuk menganalisis data spasial yang tidak berdistribusi Normal, salah satunya yaitu beta-binomial kriging. Beta-binomial kriging hadir sebagai model alternatif yang menghasilkan estimasi yang lebih baik dengan penghitungan yang relatif lebih sederhana daripada analisis data spasial lain yang digunakan untuk mengestimasi data proporsi berkorelasi spasial. Data proporsi berkorelasi spasial seringkali muncul dalam berbagai disiplin ilmu. Pada skripsi ini, model beta-binomial kriging akan diaplikasikan pada data proporsi penduduk miskin per community di negara bagian California, Amerika Serikat. Dari 279 community yang ada di California, diambil sampel acak sebanyak 140 community untuk dibentuk model beta-binomial kriging, kemudian model tersebut digunakan untuk mengestimasi lokasi yang tidak tersampel. Dari hasil evaluasi model, didapat nilai evaluasi yang sangat kecil yaitu MSE sebesar 2.90259E-05 dan RMSE sebesar 0.005387569, sehingga dapat disimpulkan bahwa metode beta-binomial kriging menghasilkan estimasi yang baik untuk data tingkat kemiskinan di California. Dari hasil estimasi, dibuat peta tingkat kemiskinan di California yang menunjukkan bahwa tingkat kemiskinan di California memiliki korelasi spasial, di mana tingkat kemiskinan di daerah perbukitan lebih tinggi daripada tingkat kemiskinan di daerah pesisir.","Kriging is a geostatistical method that utilizes sample spatial location values and semivariogram to predict values at other locations that has not been sampled where the prediction values depend on proximity to the sample locations. Various kriging models have been proposed to analyze spatial data that are not normally distributed, one of which is beta-binomial kriging. Beta-binomial kriging present as an alternative model that produces a better estimate with relatively more simple calculation than other spatial data analyses that being used to estimate spatially correlated proportions. Spatially correlated proportions often appear in various disciplines. In this thesis, beta-binomial kriging model will be applied to the data of poverty rate per community in the state of California, United States. From the 279 existing community in California, 140 random samples were taken to set up a beta-binomial kriging model, then the model was used to estimate unsampled locations. From the modelÃ¢ï¿½ï¿½s evaluation result, MSE and RMSE values obtained were very small, that are 2.90259E-05 and 0.005387569 respectively, so it can be concluded that beta-binomial kriging method yields good estimates for poverty rates in California. From the estimation, a map of the poverty rates in California were made and it showed that the poverty rates in California are spatially correlated, where the poverty rates in hilly areas are higher than the poverty rates in coastal areas.","Kata Kunci : spatial data, geostatistics, kriging, beta-binomial kriging, semivariogram"
http://etd.repository.ugm.ac.id/home/detail_pencarian/93526,MODEL LINEAR EFEK CAMPURAN TERGENERALISASI DENGAN MENGGUNAKAN FUNGSI LINK LOGIT,"IMAS FAIZAL TAMAMI, Dr. Abdurakhman M.Si",2016 | Skripsi | S1 STATISTIKA,"Model Linear efek campuran tergeneralisasi, selanjutnya disebut dengan GLMM, merupakan salah satu jenis model statistik yang sangat diperhitungkan dengan menggabungkan karakter dari model linear tergeneralisasi (GLM) dan model campuran (LME). GLMM menangani secara luas dari berbagai distribusi respond dan skenario dengan observasi tersampel dalam grup. Salah satu tipe dari GLMM adalah dengan mengunakan fungsi link logit, yang tak lain adalah regresi logistik dengan komponen efek tetap dan efek random. Estimasi parameter dari model linear efek campuran tergeneralisasi dapat diperoleh menggunakan berbagai metode estimasi salah satunya maximum likelihood dengan pendekatan. Pemilihan model terbaik menggunakan uji rasio likelihood beserta Akaike Information Criterion (AIC) dan Bayesian Information Criterion (BIC).  Aplikasi data untuk model linear efek campuran tergeneralisasi yaitu data penelitian bidang psikologi mengenai emosional seseorang dalam bentuk verbal (verbal aggression) berdasarkan keadaan yang dialami.  Kata kunci: linear efek campuran, model linear tergeneralisasi, model linear efek campuran tergeneralisasi, regresi logistik, model marginal, model bersyarat,  maximum likelihood dengan pendekatan,  uji rasio likelihood, AIC dan BIC.","Generalized linear mixed models, or GLMMs, are a powerful class of statistic models that combine the characteristic of generalized linear models (GLMs) and mixed effect model (LME). They handle a wide range of response distributions, and a wide range of scenarios where observations have been sampled in some kind of groups. One of the type of GLMMs is using logit link function that is logistic model with fixed and random effects. Estimation parameter of GLMMs can be achieved using various method, one of them is maximum likelihood estimation with approximation. The best model selection using the likelihood ratio test alongside with Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). GLMMs is applied to real data, through psychology object about emotional people in verbal (verbal aggression) by situation that had been around.  Keywords : Linear mixed effects models, generalized linear models, generalized linear mixed models, logistic models, link function, marginal model, conditional models, maximum likelihood use approximation, likelihood ratio test, AIC and BIC","Kata Kunci : Keywords : Linear mixed effects models, generalized linear models, generalized linear mixed models, logistic models, link function, marginal model, conditional models, maximum likelihood use approximation, likelihood ratio test, AIC and BIC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96350,INISIALISASI PUSAT KLASTER (CENTROID) AWAL PADA K-MEANS DENGAN METODE CENTRONIT,"MARIA SILVIA, Dr. Abdurakhman, M.Si.; Yunita Wulan Sari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"K-Means merupakan salah satu algoritma analisis klaster yang sangat terkenal karena kemampuannya untuk mengelompokkan data besar dengan sangat cepat. Akan tetapi peforma K-Means sangat bergantung pada inisialisasi pusat klaster (centroid) awal. Biasanya inisialisasi pusat klaster awal pada K-Means ditentukan secara acak. Perbedaan pada pusat klaster awal akan memberikan perbedaan pula pada hasil klaster. Apabila pusat klaster awal yang diberikan adalah pusat klaster yang baik (pusat klaster yang dapat merepresentasikan anggota-anggota dalam suatu klaster), maka hasil pengklasteran dapat dipastikan juga baik. Namun hal tersebut tidak selalu terjadi, sehingga dapat dipastikan hasil klaster yang terbentuk berubah-ubah (tidak konsisten). Centronit merupakan pengembangan metode inisialisasi pusat klaster awal pada K-means. Algoritma pada metode ini berdasarkan pada nilai minimum rata-rata perhitungan jarak antar data. Metode ini juga robust terhadap data outlier. Dengan metode centronit, inisialisasi pusat klaster awal dapat ditentukan terlebih dahulu sehingga dapat dihasilkan hasil pembentukan klasternya yang lebih baik dan konsisten.","K-Means is one of very well known clustering algorithm because its ability to classify large data very quickly. However, clustering performance of the K-Means highly depends on the initial centroids. Usually initial centroids for the K-Means clustering are determined randomly. Difference in the initial centroid will make a difference in the results of clustering. If the initial given centroid is good (centroid  that can represent the members in a cluster), then the clustering results can be ensured also good. Yet, it does not always happen, so it can be ensured that its cluster formation changeable (inconsistent). Centronit is an initialization method development of the initial centroids in K-Means. The algorithm of this method is based on the calculation of the average distance of the nearest data use the minimum distance. This method also robust from outliers of data. By using centronit method, the initial centroids can be determined  in advance so it can produce the better and consistent cluster formation.","Kata Kunci : K-Means, Analisis klaster, Inisialisasi pusat klaster awal, Centronit"
http://etd.repository.ugm.ac.id/home/detail_pencarian/104033,Model Regresi Beta Binomial Untuk Kasus Overdispersi Dalam Regresi Logistik Biner,"RIZKA HIDAYAH, Prof.Dr.rer.nat Dedi Rosadi, S.Si., M.Sc.;Widya Irmaningtyas, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi logistik digunakan untuk menganalisis hubungan antara variabel independen dengan variabel respon yang bertipe kategorik. Dalam analisis logistik variabel respon mengikuti sebaran binomial dengan mengasumsikan variansi pengamatan sama dengan variansi yang diharapkan. Apabila asumsi tidak terpenuhi maka diindikasikan adanya masalah overdispersi. Adanya overdispersi pada data dapat menyebabkan nilai standar error yang underestimate, sehingga penarikan kesimpulan menjadi tidak tepat. Salah satu alternatif solusi untuk mengatasi masalah tersebut adalah dengan menggunakan model regresi beta-binomial. Estimasi parameter model regresi beta-binomial adalah dengan menggunakan metode Maximum Likelihood Estimation (MLE) yang menggunakan algoritma RS untuk mendapatkan parameter MLE-nya. Pada akhirnya, performa model regresi beta-binomial dibandingkan dengan model regresi logistik. Dengan melihat nilai Akaike Information Criteria (AIC) dan Bayesian Information Criteria (BIC). Model regresi beta-binomial diaplikasikan pada data tentang kasus campak di daerah Provinsi DIY dari Januari 2009 - Desember 2009. Data tersebut dianalisis dengan regresi logistic biner dan regresi beta-binomial. Hasilnya menunjukkan model regresi beta-binomial memiliki nilai AIC dan BIC yang lebih kecil, artinya model ini lebih baik dibandingkan model regresi logistik dalam menangani masalah overdispersi.","Logistic regression analysis was used to analyze the relationship between independent variables and the response variable of type kategorik. In logistic analysis, the response variable follows the binomial distribution with assuming the observation variance equal to the expected variance. If the assumptions are not fulfilled than indicated the existence of overdispersion problem. The existence of overdispersion problem in the data can cause the value of the standard error to underestimate, so the conclusion is not appropriate. One of alternative solution to overcome this problem is by using a beta-binomial regression model. Parameter estimation of beta-binomial regression model is using the Maximum Likelihood Estimation (MLE) method that using he RS algorithm to obtain the MLE parameters. Finally, the beta-binomial regression model performance is compared with the logistic regression model. By looking at value of Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC). Beta-binomial regression model was applied to the data on cases of measles in the area of Yogyakarta Province from January 2009 - December 2009. The data were analyzed with binary logistic regression and beta-binomial regression. The results show the beta-binomial regression model has a value of AIC and BIC smaller, it means this model is better than the logistic regression model in dealing with overdispersion.","Kata Kunci : regresi logistik, beta-binomial, overdispersi, logistic regression, beta-binomial, overdispersion."
http://etd.repository.ugm.ac.id/home/detail_pencarian/98146,TUKEY'S CONTROL CHART BERDASARKAN PERFORMA AVERAGE RUN LENGTH (ARL),"PUTRI IRMA YUNITA, Prof. Dr. rer.nat. Dedi Rosadi, M.Sc",2016 | Skripsi | S1 STATISTIKA,"Grafik pengendali kualitas adalah salah satu alat yang paling berpengaruh dan banyak digunakan untuk mendeteksi perilaku yang menyimpang dalam proses industri. Salah satu inovasi baru grafik pengendali yang efisien digunakan untuk mendeteksi pergeseran yang kecil dari data observasi tunggal adalah Tukey's Control Chart (TCC). Pengukuran performa dari TCC didapat melalui penentuan nilai Average Run Length (ARL). ARL merupakan rata-rata titik atau sampel yang harus digambarkan sebelum sebuah titik atau sampel menyatakan suatu keadaan tidak terkendali. Pada skripsi ini, TCC akan diaplikasikan pada pengendalian kualitas produksi Pertasol CB di Pusdiklat Migas Cepu.","Quality control chart is one of the tools of the most influential and widely used to detect the deviant behavior in industrial process. One of new inovation the control charts that efficiently used to detect small shift from individual observation is Tukey's Control Chart. The measurement of TCC is obtained through in determining the value of average run length (ARL). ARL is the average number of points or samples should be drawn before a sample shown an out of control condition. In this study, TCC will be applied in quality control production of Pertasol CB in Pusdiklat Migas Cepu.","Kata Kunci : Tukey's Control Chart, Average run length, grafik pengendali individu."
http://etd.repository.ugm.ac.id/home/detail_pencarian/102759,GRAFIK PENGENDALI MIXED CUMULATIVE SUM-EXPONENTIALLY WEIGHTED MOVING AVERAGE (MCE) : ANALISIS EFEKTIF DALAM PENGAWASAN PROSES PRODUKSI,"MAHARDIKA EKO WICAKSONO, Prof. Dr.rer.nat. Dedi Rosadi. M.Sc ; Yunita Wulan Sari, S.Si., M.Si",2016 | Skripsi | S1 STATISTIKA,"Dalam proses produksi diperlukan adanya teknik dan manajemen untuk memantau hasil produksi. Dengan adanya proses pemantauan, proses produksi akan menghasilkan keluaran yang sesuai dengan target standar sehingga target kepuasan konsumen terjaga. Untuk itu perlu adanya aplikasi ilmu statistika yang berguna untuk menganalisis permasalahan kualitas produksi. Pengendalian kualitas statistik dapat dengan cepat menyidik terjadinya sebab-sebab terduga atau pergeseran proses. Analisa pergeseran proses tersebut akan menjadi fokus tindakan pembetulan dalam pengawasan produksi. Metode grafik pengendali yang akan dibahas kali ini adalah metode Cumulative-Sum (CUSUM), Exponentially Weighted Moving Average (EWMA), dan Mixed CUSUM-EWMA (MCE). Metode Mixed CUSUM-EWMA (MCE) akan menjadi fokus pembahasan. Mixed CUSUM-EWMA (MCE) adalah perpaduan antara metode Cumulative-Sum dan Exponentially Weighted Moving Average. Perpaduan CUSUM dan EWMA bertujuan menambah kesensitifitasan grafik pengendali dalam mendeteksi kejadian out of control. Untuk membandingkan tingkat kesensitifitasan metode CUSUM, EWMA, dan MCE digunakan Average Run Length (ARL). Dari perbandingan nilai ARL diperoleh grafik pengendali MCE sebagai grafik pengendali dengan tingkat kesensitifitasan tertinggi dibandingkan dengan metode CUSUM dan EWMA. Hasil perbandingan tersebut didukung dengan studi kasus yang memberikan hasil analisis grafik Mixed CUSUM-EWMA lebih cepat mendeteksi adanya kejadian out of control dibandingkan dengan metode CUSUM dan EWMA.","In the process of production, technical and management are needed to monitor output's production. With the monitoring process, the production process will produce output in accordance with standard targets, and the satisfaction's customer is maintained. So, we need application of statistics that is useful to analyze the production quality problem. Statistic quality control can quickly investigate the expected causes or shifting process. The analysis of process will be a focus corrective in production monitoring. Control charts methods that will be discussed is Cumulative-Sum (CUSUM) method, Exponentially Weighted Moving Average (EWMA) method, and Mixed CUSUM-EWMA (MCE) method. The Mixed CUSUM-EWMA (MCE) method will be the focus of discussion. Mixed CUSUM-EWMA (MCE) is a combination of methods Cumulative-Sum (CUSUM) and Exponentially Weighted Moving Average (EWMA). The combination of CUSUM and EWMA have purpose to increase the sensitivity of control chart in detecting out of control event. To compare the level of sensitifity from CUSUM, EWMA, and MCE methods is used Average Run Length (ARL). From the process comparison the value of ARL, we get that control chart with MCE method as graphic controller with highest level of sensitivity compared with CUSUM and EWMA methods. The result of comparison are supported by case studies that provide if the control chart with Mixed CUSUM-EWMA (MCE) method more quickly detect the out of control event instead of the CUSUM and EWMA methods.","Kata Kunci : grafik pengendali, cumulative sum, exponentially weighted moving average, mixed CUSUM-EWMA, CUSUM, EWMA, MCE, average run length, ARL."
http://etd.repository.ugm.ac.id/home/detail_pencarian/100714,BETA PRODUCT CONFIDENCE PROCEDURE (BPCP) UNTUK PENENTUAN INTERVAL KONFIDENSI POINTWISE PADA DISTRIBUSI SURVIVAL,"NUR ADILLA NOVIANA, Drs. Danardono, MPH., Ph.D.; Rianti Siswi Utami, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Beta Product Confidence Interval (BPCP) merupakan prosedur perhitungan interval konfidensi non-parametrik untuk data tersensor kanan dengan asumsi sensor independen. Penentuan interval konfidensi dengan metode BPCP setara atau dapat membandingi penentuan interval konfidensi metode Kaplan-Meier. Dalam hal cakupan (coverage), metode BPCP lebih unggul dibandingkan metode Kaplan-Meier.  Pembuktian keunggulan metode BPCP dibandingkan metode Kaplan-Meier dalam hal cakupan (coverage) ditunjukkan dengan simulasi. Metode BPCP akan diaplikasikan pada tiga buah data yaitu waktu kematian wanita yang mengidap kanker payudara dari The Ohio State University Hospitals Cancer Registry, data pasien systemic sclerosis dari severe systemic sclerosis pilot trial, dan data death time of psychiatric patient dari University of Lowa Hospitals dan dibandingkan dengan metode Kaplan-Meier melalui plot. Interval konfidensi metode BPCP setara atau dapat membandingi interval konfidensi dengan metode Kaplan-Meier.","Beta Product Confidence Interval (BPCP) is a non-parametric confidence interval calculation procedure for right-cencored with data assuming independent censoring. The determination of confidence interval using BPCP method is equivalent or proportional to Kaplan-Meier method. The coverage of BPCP method is better than that of Kaplan-Meier method The simulation is to show how the coverage of BPCP method better than that of Kaplan-Meier method. The method of BPCP will be applied to three data. They are the data about death period of woman who are suffering from breast cancer from The Ohio State University Hospitals Cancer Registry, the data about systemic sclerosis patients from severe systemic sclerosis pilot trial, and the data about death period of psychiatric patient from University of Lowa Hospitals and will be compared with Kaplan-Meier method using a plot. Confidence interval of BPCP method equivalent with confidence interval of Kaplan-Meier method.","Kata Kunci : Beta Product Confidence Interval (BPCP) , Kaplan-Meier, interval konfidensi, cakupan (coverage)."
http://etd.repository.ugm.ac.id/home/detail_pencarian/93549,REGRESI LOGISTIK ROBUST DENGAN ESTIMATOR BIANCO-YOHAI,"AGA ASWANTA PUTRA, Prof. Drs. Subanar, Ph. D.",2016 | Skripsi | S1 STATISTIKA,"Kasus data yang memuat variabel respon kualitatif dengan dua kategorik sering ditemukan dalam penelitian, khususnya penelitian dalam bidang kesehatan. Regresi logistik biner biasanya digunakan untuk menganalisis kasus tersebut. Metode yang sering digunakan untuk menaksir parameter pada regresi logistik adalah Maximum Likelihood Estimator (MLE) dengan algoritma Newton Raphson atau bisa juga dengan algoritma Fisher Scoring. Permasalahan muncul bila data mengandung outlier, karena MLE kurang akurat dalam menaksir parameter dalam data tersebut. Oleh karena itu, untuk mengatasi permasalahan tersebut dapat digunakan estimasi Bianco-Yohai. Estimasi ini diaplikasikan pada data sekunder mengenai Cancer Prostate Study untuk melihat variabel apa saja yang mempengaruhi seseorang terkena kanker prostat. Hasil dari estimasi ini kemudian dibandingkan dengan hasil dari regresi logistik dan menghasilkan kesimpulan bahwa model estimasi Bianco-Yohai lebih baik dibanding model regresi logistik dilihat dari nilai MSE dan R-Square dari kedua model tersebut.","We often find data with qualitative response variable with binary categories in some researches especially health study. Binary logistic regression usually used to analyze that researches. A method to estimate parameters in this regression is Maximum Likelihood Estimation (MLE) with Newton Raphson or Fisher Scoring algorithm. The estimations from MLE is less accurate if there are some outliers in data sample. Therefore, we can use Bianco-Yohai estimator to solve this problem. We use this estimator to analyze the influences of parameters that can make someone affected by prostate cancer. Then, the result of this estimation will be compared with the result of binary logistic regression (MLE). From the MSE and R-Square value, we can conclude that the Bianco-Yohai estimator is more robust to outlier than binary logistic regression with MLE.","Kata Kunci : regresi logistik biner, maximum likelihood estimator, outlier, regresi robust, estimasi Bianco-Yohai, algoritma Fisher Scoring, algoritma Newton Raphson, Prostate Cancer Study"
http://etd.repository.ugm.ac.id/home/detail_pencarian/107118,PEMODELAN DATA  RESTRICTED MEAN SURVIVAL TIME BERDASARKAN OBSERVASI PSEUDO,"IKA YULIA TRININGTIYAS, Dr. Danardono, MPH., Ph.D.",2016 | Skripsi | S1 STATISTIKA,"Data survival dengan sebagian data tersensor dapat dimodelkan menggunakan regresi Cox Proportional Hazard. Namun, pemodelan tersebut membutuhkan asumsi proportional hazard. Untuk mengatasi permasalahan ini digunakan alternatif lain yaitu dengan memodifikasi variabel respon dengan membentuk restricted mean survival time berdasarkan observasi Pseudo. Observasi Pseudo yang terdiri dari estimator leave one out. Estimatornya dapat diperoleh dari restricted mean survival time dengan mengambil thau waktu yang telah ditentukan. Hasil dari observasi Pseudo akan dijadikan variabel hasil dalam generalized linear model. Nilai parameter diperoleh dari  estimating equation atau generalized estimating equations. Selanjutnya pada kasus data yang tidak berkorelasi dan tidak membutuhkan n perulangan, estimasi parameter dapat menggunakan estimating equation. Interpretasi dari eksponensial paramaternya merupakan ratio di antara dua thau restricted mean survival time untuk mengetahui resiko terjadinya event. Selanjutnya analisis ini akan diterapkan dalam data pasien pecandu heroin. Perhitungan dengan pendekatan observasi Pseudo menghasilkan terpenuhinya asumsi proportional hazard, yang nilai Pseudo mendekati mean.","Survival data with partial censored can be modeled using Cox Proportional Hazard Regression. However, the modeling requires proportional hazard assumption. To solve this problem is used other alternative that modification respon variabel to restricted mean survival time based on observation Pseudo. Pseudo observations consisting of estimator Jackknife one deleted. The estimator can be obtained from the restricted mean survival time by taking a specified time thau. Pseudo values will be outcome variabel from generalized linear model. Parameter regression can be obtained by estimating equation or generalized estimating equations. Furthermore, in uncorrelated data and not require n loop estimation parameter can be used estimating equation. The interpretation of exponential parameter is  ratio between the two thau restricted mean survival time to determine a risk occurrence event. Next, this analysis will be applied in the heroin addicted data. Pseudo observation produces  fulfilled proportional hazard assumption, the Pseudo value approaching mean.","Kata Kunci : Analisis Survival, Generalized Linear Model, Kaplan-Meier, Observasi Pseudo, Pemodelan Data Tersensor, Proportional Hazard, Regresi Cox, Restricted Mean Survival Time, AIC."
http://etd.repository.ugm.ac.id/home/detail_pencarian/106097,MODEL REGRESI POISSON HURDLE UNTUK MENGATASI OVERDISPERSI AKIBAT EXCESS ZEROS,"MUHAMMAD IQBAL JINNAN, Dr. Danardono, M.P.H.,Ph.D.;Widya Irmaningtyas, S.Si.,M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Regresi Poisson merupakan salah satu Generalized Linear Model (GLM) yang dapat memodelkan jenis data cacah. Dalam regresi Poisson mengasumsikan bahwa nilai mean dan variansi sama, biasa disebut equidispersi. Namun, dalam beberapa kasus, terdapat nilai nol yang terlalu banyak pada variabel respon. Hal tersebut menyebabkan nilai variansi tidak lagi sama dengan mean. Nilai variansi lebih besar daripada mean, biasa disebut overdispersi. Model regresi Poisson tidak lagi cocok untuk memodelkan jenis data seperti ini. Sehingga digunakan model regresi Poisson Hurdle. Terdapat dua bagian dalam model tersebut. Pertama, sebuah proses biner dimana terdapat dua pilihan, apakah proses membangkitkan nilai nol atau nilai positif. Bagian kedua, proses membangkitkan nilai positif saja. Kedua bagian, masing-masing akan diestimasi dengan menggunakan model biner dan zero-truncated count. Metode estimasi yang digunakan adalah metode maksimum likelihood.","Poisson regression is one of the Generalized Linear Model (GLM) that can model the kind of data count. Poisson regression assumes that the mean and variance are equal, or it is called equidispersion. However, in many cases, there are too many zero values on the response variable. This causes the value of variance is no longer equal to the mean. Values of variance greater than the mean, it is called overdispersion. Poisson regression model is not suitable anymore for this kind of data. Thus, it is suggested to use a Hurdle Poisson regression for handle overdispersion problem. There are two parts in the Hurdle Poisson regression model. First, a binary process where there are two options, whether the process generate a zero value or a positive value.The second part, the process generate positive value only. Two part of the model, each will be estimated by using binary model (first part) and zero-truncated count model (second part). The method of estimation use maximum likelihood method.","Kata Kunci : GLM, regresi Poisson, overdispersi, model Hurdle, model regresi Poisson Hurdle"
http://etd.repository.ugm.ac.id/home/detail_pencarian/107635,ESTIMASI PARAMETER REGRESI BINOMIAL NEGATIF MENGGUNAKAN ESTIMATOR LIU UNTUK KASUS MULTIKOLINEARITAS,"YULIANI NURMALASARI PUTRI, Prof.Dr. Sri Haryatmi, M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Regresi binomial negatif dapat digunakan untuk mengetahui hubungan antara variabel respon dan variabel penjelas, dengan variabel respon berdistribusi binomial negatif. Pada umumnya untuk mengestimasi parameter pada regresi binomial negatif digunakan maximum likelihood estimator. Namun, metode ini tidak stabil jika terdapat masalah multikolinearitas. Multikolinearitas adalah  adanya korelasi antara variabel penjelas. Oleh karena itu,estimator liu dapat digunakan untuk mengestimasi parameter regresi binomial negatif yang mengalami masalah multikolinearitas. Dalam skripsi ini akan digunakan estimator liu pada regresi binomial negatif yang mengalami permasalahan multikolinearitas, dan membandingkannya dengan metode maximum likelihood. Estimator yang menghasilkan nilai rata-rata kuadrat galat terkecil menunjukkan performa lebih baik dalam mengestimasi parameter regresi binomial negatif. Hasilnya menunjukkan bahwa estimator liu lebih baik dibandingkan dengan estimator maximum likelihood untuk mengestimasi parameter pada regresi binomial negatif yang mengalami multikolinearitas.","Negative binomial regression can be used in analyzing the relationship between response variable and explanatory variables, in which response variable has negative binomial distribution. Commonly, for estimating the parameter of negative binomial regression, the maximum likelihood estimator is used. But, that method became invalid if there is multicollinearity cases on this regression. Multicollinearity is correlation between the explanatory variables. Therefore, liu estimator can be used in estimating parameter of  negative binomial regression to deal multicollinearity problems. 	In this thesis, liu estimator will be used in negative binomial regression to deal with multicollinearty problem and compared it  with maximum likelihood estimator. The estimator that produced smallest value of mean square errors on negative binomial regression is the best estimator for estimating parameter. The obtained result showed that liu estimator is better than maximum likelihood estimator to estimate parameter for negative binomial regression for multicollinearity problems.","Kata Kunci : regresi binomial negatif, multikolinearitas,maximum likelihood, estimator liu, rata-rata kuadrat galat/negative binomial regression, multicollinearity, maksimum likelihood estimator, liu estimator, mean square errors."
http://etd.repository.ugm.ac.id/home/detail_pencarian/95606,Estimasi Parameter Model Regresi Logistik Menggunakan Metode Residual Bootstrap,"SYAHESTI NURUL F, Dr. Herni Utami, M.Si",2016 | Skripsi | S1 STATISTIKA,"Bootstrap merupakan salah satu metode estimasi inferensi statistika yang berbasis komputer. Prinsip kerjanya adalah menggunakan komputer dalam membangkitkan data dari sampel asli yang berukuran kecil untuk mendapatkan sampel tiruan. Sampel tiruan diperoleh dengan cara mengambil sampel random secara berulang kali dari sampel asli yang selanjutnya dapat digunakan untuk menghitung nilai estimator. Pengambilan sampel tiruan tersebut dilakukan dengan syarat pengembalian. Ada 3 metode Bootstrap untuk memperoleh sampel tiruannya, yaitu metode residual, metode korelasi, dan metode eksternal. Metode residual Bootstrap merupakan metode resampling yang baik karena akan menghasilkan nilai kesalahan baku yang kecil. Kelebihan dari metode Bootstrap diantaranya adalah mempunyai panjang interval konfidensi Bootstrap persentil yang lebih pendek daripada metode lainnya. Tujuan utama dari metode ini adalah untuk memperoleh estimasi yang sebaik-baiknya berdasarkan data yang minimal dengan bantuan komputer. Dalam skripsi ini, metode Bootstrap diterapkan untuk mengestimasi parameter model regresi logistik. Model regresi logistik merupakan salah satu bentuk analisis regresi untuk mengetahui suatu hubungan sebab akibat (kausalitas) apabila variabel respon Y hanya memiliki 2 kemungkinan nilai atau data bersifat dikotomus. Metode yang sering dipakai untuk menyelesaikan masalah regresi logistik adalah metode Maximum Likelihood Estimation (MLE) dimana proses penaksiran parameter didahului oleh pembentukan fungsi likelihood. Metode residual Bootstrap dalam estimasi parameter model regresi logistik tersebut diaplikasikan dalam penentuan seberapa besar pengaruh faktor-faktor hiperkolesterolemia pada pasien Balai Laboratorium Kesehatan Yogyakarta yang dipilih secara acak. Berdasarkan hasil analisis yang diperoleh, metode Bootstrap mampu memperkecil kesalahan baku sampai dengan Bootstrap dengan perulangan sebanyak 1000 kali.","Bootstrap is one of the estimation methods, computer-based statistical inference. Its working principle is using a computer in generating original data from a small sample to get a pseudo sample. Pseudo sample is obtained by taking samples random with repetition from the original sample can then be used to calculate the value of the estimator. This pseudo sample took with a replacement from the original sample. There are three Bootstrap procedures to obtain pseudo sample, namely the Bootstrap based on residuals, the paired Bootstrap, and the external Bootstrap. The Bootstrap based on residuals is better method because it will produce a small standar error values. The Bootstrap method's advantage is confidence interval percentiles Bootstrap length shorter than the others. The main purpose of this method is to obtain the best possible estimate based on minimal data with the help of computers. In this paper, a Bootstrap method is applied to estimate the parameters of a logistic regression model. Logistic regression model is a form of regression analysis to determine a causal relationship (causality) when the response variable Y has only two possible values or data are dichotomous. The method which is often used to solve the logistic regression problem is Maximum Likelihood Estimation (MLE) where the parameter estimation process is preceded by the formation of likelihood function. Bootstrap method in estimating parameters of the logistic regression model is applied in the determination of how much influence factors of hypercholesterolemia from sample random patients Yogyakarta Health Laboratory. Based on the results of the analysis, Bootstrap method is able to reduce the standar errors to Bootstrap with repetition as much as 1000 times.","Kata Kunci : Metode Bootstrap, Regresi Logistik, Maximum Likelihood Estimation (MLE)."
http://etd.repository.ugm.ac.id/home/detail_pencarian/93304,GRAFIK PENGENDALI ROBUST BIVARIAT DENGAN ESTIMATOR MEDMAD SEBAGAI ALTERNATIF GRAFIK PENGENDALI HOTELLING'S T KUADRAT,"SITA RAHMAHDEWI, Prof. Subanar, PhD",2016 | Skripsi | S1 STATISTIKA,"Grafik pengendali merupakan metode pengawasan suatu proses yang paling populer digunakan dalam bidang industri. Banyak proses industri yang melakukan pengawasan dengan memonitor beberapa karakteristik kualitas secara bersamaan karena terdapat variabel yang saling berkorelasi sehingga grafik pengendali Shewhart tidak bisa digunakan dan dapat menghasilkan informasi yang kurang tepat. Grafik pengendali Hotelling's T2 digunakan untuk memonitor data multivariat. Pada grafik ini digunakan estimator mean untuk parameter lokasi. Mean memiliki nilai breakdown point nol yang artinya tidak sensitif terhadap keberadaan outlier sehingga tidak bersifat robust. Akan diperkenalkan grafik pengendali robust bivariat dengan estimator MEDMAD. Estimator median dan median absolute deviation (MAD) memiliki nilai breakdown point maksimal yaitu 1/2 sehingga bersifat robust.  Untuk studi kasus, grafik pengendali diaplikasikan pada data kandungan silica modulus dan iron modulus pada proses produksi tepung baku di PT Holcim Indonesia kemudian dibandingkan dengan grafik pengendali Hotelling's T2 dan grafik pengendali robust bivariat dengan estimator MEDMAD.","Control charts are the most popular tools to monitor industrial process. Many industrial processes are monitored with several quality characteristics at the same time because there are correlated variables. However Shewhart control chart can't be used and can lead improper informations. Hotelling's T2 control chart is used to monitor multivariate process. It used mean as estimator for location parameter. Mean has null breakdown point which sensitive for detecting outlier and not robust. The proposed control chart use median and median absolute deviation (MAD) for location and scale parameters. Both of the estimators have breakdown points 1/2 and robust.  For study case, consider a production process of raw meal in PT Holcim Indonesia. The data consists of two quality characteristics which are silica modulus and iron modulus. Then the process are monitored using the proposed control chart and compared with Hotelling's T2 control chart and robust bivariate control chart using Minimum Covariance Determinant (MCD) estimator.","Kata Kunci : grafik pengendali bivariat, robust, MEDMAD, breakdown point"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96890,METODE ROBUST GANDA DALAM MENANGANI PERANCU OLEH KLASTER,"ISMI NUR IHSANTI , Drs. Zulaela, Dipl.Med.Stat., M.Si",2016 | Skripsi | S1 STATISTIKA,"Dalam desain berklaster, hubungan antara paparan dan hasil biasanya dirancukan oleh perancu klaster konstan dan perancu klaster bervariasi. Pengaruh perancu klaster konstan dapat dihilangkan dengan mempelajari hubungan antara paparan dan hasil dalam klaster, misalnya dengan analisis regresi. Tetapi pemodelan regresi biasanya digunakan untuk mengontrol perancu yang bervariasi serta teramati di dalam klaster. Masalahnya bahwa model regresi yang bekerja dapat menjadi tidak tetap, dalam hal ini perkiraan asosiasi dalam klaster mungkin bias.  Untuk mengurangi sensitivitas model, terdapat metode baru yaitu dengan menambah model standar untuk outcome (hasil) dan model tambahan untuk paparan. Metode yang digunakan adalah doubly robust conditional generalized estimating equation (DRCGEE). Metode DRCGEE menggabungkan dua model sedemikian rupa sehingga mereka akan konsisten jika salah satu modelnya benar, tidak harus keduanya. Dengan demikian, estimator DRCGEE memberikan peneliti dua peluang, bukan hanya satu untuk membuat kesimpulan yang valid pada asosiasi dalam-klaster.","In clustered designs, the exposure-outcome association is usually confounded by both cluster-constant and cluster-varying confounders. The influence of cluster-constant confounders can be eliminated by studying the exposure-outcome association within  clusters using a regression model, but additional regression modeling is usually required to control for observed cluster-varying confounders. A problem is that the working regression model may be misspecified, in which case the estimated within-cluster association may be biased. 	To reduce sensitivity model, there exist a new methods that augment the standard working model for the outcome with an auxiliary working model for the exposure. Then derive doubly robust conditional generalized estimating equation (DRCGEE) methods. This methods combines the two models in such a way that it is consistent if either model is correct, not necessarily both. Thus, the DRCGEE estimator gives the researcher two chances instead of only one to make valid inference on the within-cluster association.","Kata Kunci : desain klaster, perancu, metode Robust Ganda, clustered design, confounding, doubly robust methods"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96380,PENENTUAN HARGA OPSI BELI TIPE EROPA MENGGUNAKAN EKSPANSI GRAM-CHARLIER,"ZAKIATUL WILDANI, Drs. Zulaela, Dipl.Med.Stats, M.Si",2016 | Skripsi | S1 STATISTIKA,"Black dan Scholes (1973) mengembangkan suatu model penentuan harga opsi  yang telah banyak diterapkan baik dalam konteks akademik maupun praktis. Asumsi praktis dalam model Black-Scholes adalah return saham mengikuti distribusi normal dengan volatilitas konstan. Fakta bahwa terdapat return saham yang tidak berdistribusi normal mengimplikasikan bahwa model penentuan harga opsi yang akurat harus mempertimbangkan skewness dan kurtosis pada aset yang mendasari opsi. Model perkembangan ini mengadaptasi ekspansi Gram-Charlier untuk memberikan penyesuaian skewness dan kurtosis pada rumus Black-Scholes. Metode aproksimasi yang digunakan adalah pendekatan alternatif dengan polinomial Hermite.  Selanjutnya, dilakukan perbandingan antara harga opsi yang diperoleh dengan  ekspansi Gram-Charlier dan model Black-Scholes terhadap harga opsi di pasar. Dengan menggunakan SRPE (Squared Relative Price Error) sebagai kriteria penentuan harga opsi, hasil menunjukkan bahwa model ekspansi Gram-Charlier lebih baik dibandingkan model Black-Scholes.","Black and Scholes (1973) developed an option pricing model which has been  widely applied in both academic and practical contexts. Impractical assumptions made by the BS model including constant volatility of stock return and normal distribution of return. The fact that stock return are non-normally distributed implies that an accurate option pricing model should consider skewness and kurtosis in the underlying asset return. This development model adapts a Gram-Charlier expansion to provide skewness and kurtosis adjustment terms for Black-Scholes formula. Approximation method of Gram-Charlier expansion using alternative approach of hermite polynomial.  Furthermore, we compare the option price obtained by Gram-Charlier Expansion and the Black-Scholes model with option market price. Using SRPE  (Squared Relative Pricing Error) as the criterion of option pricing, the result  demonstrates that Gram-Charlier expansion model performs better than Black-Scholes model.","Kata Kunci : harga opsi, Black-Scholes, Ekspansi Gram-Charlier, Polinomial Hermite"
http://etd.repository.ugm.ac.id/home/detail_pencarian/97918,PENYELESAIAN REGRESI SEMIPARAMETRIK DENGAN MENGGUNAKAN REGRESI RANDOM FOREST,"ALFINA NUR FIRMANI, Dr. Gunardi, M.Si",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan analisis yang sering digunakan untuk mengetahui hubungan yang terjadi antara variabel dependen dan independennya. Namun metode ini masih cenderung terbatas pada asumsi dan keadaan data tertentu, terutama dalam penyelesaian regresi semiparametrik. Oleh karena itu muncul metode regresi yang sangat fleksibel dan mudah, yang merupakan pengembangan dari pohon keputusan. Metode ini dinamakan regresi Random Forest. Regresi Random Forest merupakan gabungan dari banyak CART yang ditumbuhkan sehingga akurasi yang dihasilkan akan lebih akurat dari pohon tunggal. Studi kasus yang digunakan pada penelitian adalah mengestimasi persentase kemiskinan berdasarkan faktor-faktor yang mempengaruhinya. Dari hasil regresi ini didapatkan hasil bahwa faktor angka melek huruf merupakan faktor utama penyebab kemiskinan. Dari hasil tersebut diusulkan saran penanggulangan kemiskinan dengan memfokuskan pada faktor utama, sehingga penekanan jumlah penduduk miskin menjadi lebih efisien.","Regression analysis is an analysis which often used to know the relation between dependent and independent variable. However this method is still limited on the assumption and condition of a certain data, especially in the semiparametric regression solving. Therefore a very simple and easy regression method was made, which appeared as the improvement of decision tree. This method named Random Forest regression. Random Forest regression is a combination from several CARTs which is built so that the accuracy obtained is more accurate than decision tree. The study case which is used in this experiment is predicting poverty percentage based on several factors that influence it. The result from this regression shows that lackness of reading skill is the major factor causing poverty. From this result, an advice suggested to decrease poverty is focusing at the major factor, so that decreasing the number of poor people become more efficient.","Kata Kunci : Semiparametric regression, Decision tree, CART, Random Forest, Variable Importance"
http://etd.repository.ugm.ac.id/home/detail_pencarian/103294,ALGORITMA NEUTROSOPHIC C-MEANS UNTUK CLUSTERING DATA (Studi Kasus : Segmentasi Karakteristik Kinerja Kantor Perwakilan Wilayah Perusahaan Asuransi Kendaraan Roda 4),"MOHCHAMMAD MAULANNA AFFANDI, Dr. Adhitya Ronnie Effendie, M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Analisis clustering sangat berguna dalam proses segmentasi atau pengelompokan data yang nantinya bisa digunakan acuan untuk pengambilan keputusan agar terciptanya kondisi yang diinginkan. Analisis clustering menitikberatkan jarak antara setiap data terhadap pusat data dari setiap cluster. Dalam prakteknya terkadang didapatkan keadaan uncertain data, dimana satu atau beberapa data berada di batas-batas wilayah cluster atau berupa data outlier bagi data-data lainnya. Hal ini bisa mengganggu nilai pusat yang akan dihasilkan. Hasilnya bisa tidak akurat dan tentunya akan menghasilkan keputusan yang kurang tepat pula. Dalam algoritma Neutrosophic C-Means Clustering diperkenalkan 2 jenis tipe penolakan baru terhadap layak tidaknya sebuah data masuk ke dalam sebuah cluster. Penolakan ambigu memperhatikan pola data yang dekat di daerah batas-batas cluster. Penolakan jarak memperhatikan data yang berada jauh dari semua cluster.","Clustering analysis is very useful in the process of segmentation of grouping of data that can later be used a reference for decision-making in order to create the desired conditions. Cluster analysis focuses in distance any data within the data center of each cluster. In practice sometimes uncertain state of the data obtained, where one or some of the data is in the boundaries of the cluster or in the form of a data outlier for other data. This could interfere with the central value that will be generated. The result can be inaccurate and will certainly result in a decision that is not quite right anyway. In algorithm Neutrosophic C-Means Clustering introduced two new types or rejection against the appropriateness of an in coming data into a cluster. Rejection ambiguity concerns the patterns lying near the cluster boundaries. Rejection distance to look at data that was far away from all the clusters.","Kata Kunci : segmentasi, Neutrosophic C-Means Cluster, Ambiguitas, Outlier, K-Means Cluster, Fuzzy C-Means Cluster"
http://etd.repository.ugm.ac.id/home/detail_pencarian/104576,Estimasi M Robust untuk Regresi Spline Terpinalti,"JUSRIATY M PURBA , Dr.Abdurakhman,S.Si.,M.Si ;Vemmie Nastiti L S.Si.,M.Sc",2016 | Skripsi | S1 STATISTIKA,"Regresi nonparametrik merupakan analisis regresi dengan pendugaan model dilakukan berdasarkan pendekatan yang tidak terikat asumsi bentuk kurva regresi tertentu, namun dibentuk sesuai dengan informasi yang ada dalam data. Salah satu jenis fungsi yang dapat digunakan untuk menduga bentuk regresi nonparametrik adalah fungsi Huber pada Regresi Spline Terpinalti. Regresi spline terpinalti ialah salah satu metode yang sering dipakai untuk smoothing noisy data.   Dalam analisis data, saat data mempunyai outlier dan outlier yang ada bukan merupakan suatu kesalahan taksiran yang diperoleh dengan metode Ordinary Least Square akan bias karena metode OLS tidak robust terhadap outlier. Pada regresi spline, estimasi kurva regresi dapat diselesaikan dengan kuadrat terkecil terpinalti atau Penalized Least Square. Namun metode estimasi ini rentan terhadap kehadiran pencilan. Sehingga diperkenalkan metode estimasi M robust terpinalti. 	 	Metode estimasi Penalized Least Square pada regresi spline terpinalti diganti dengan metode estimasi M robust yang mampu menangani kehadiran pencilan data. Pembobot dalam estimasi M bergantung pada residual dan koefisien, sehingga untuk menyelesaikan masalah tersebut perlu dilakukan prosedur iterasi yang disebut iteratively reweighted least squares (IRLS) yang dimana sering disebut dengan pseudo data. Pada skripsi ini juga mempelajari bagaimana memilih secara robust parameter penalti ketika kemungkinan terdapat outlier pada data. Diberikan kriteria pemilihan parameter penalti robust berdasarkan generalized cross validation. Contoh data ialah simulasi data dan data riil yang digunakan untuk menggambarkan efektivitas prosedur.","Nonparametric regression is a regression analysis to estimate the model is based on the approach that is not tied assuming the form of a specific regression curve, but formed in accordance with the information contained in the data. One type of function that can be used to predict the form of the nonparametric regression function is in Penalized Regression Spline with Huber. Penalized Regression Spline is one method that is often used for smoothing noisy data.  In the analysis of the data, when the data have outliers and outliers that there is not a fault estimate obtained by the method of Ordinary Least Square OLS will be biased because the method is not robust to outliers. In spline regression, regression curve estimation can be solved by the least squares penalized or Penalized Least Square. However, this estimation method is vulnerable to the presence of outliers. So the estimation method introduced - M robust penalized.  The estimation method Penalized Least Squares regression spline terpinalti replaced with the estimation method - M robust presence of outliers that can handle data. The weighting in the estimation - M depend on the residual and coefficients, so as to solve this problem is iterative procedure called iteratively reweighted least squares (IRLS) that which is often called the pseudo data. In this thesis also learn how to choose the robust parameter penalty when there might be an outlier in the data. Awarded a penalty parameter robust selection criteria based on generalized cross -validation. Sample data was simulated data and real data are used to illustrate the effectiveness of the procedure.","Kata Kunci : Estimasi  M robust, Regresi Spline Terpinalti, Estimasi M terpinalti, parameter pemulus, GCV, IRLS, pseudo data, regresi nonparametrik."
http://etd.repository.ugm.ac.id/home/detail_pencarian/106115,PENERAPAN METODE REGRESI RIDGE DUA PARAMETER UNTUK MENANGANI MASALAH MULTIKOLINEARITAS,"DEDE RUDIANTO, Drs. Zulaela, Dipl.Med.Stat., M.Si. ; Vemmie Nastiti Lestari, M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis dalam statistika untuk menganalisa dan memodelkan hubungan antara variabel dependen (Y) dengan variabel independen (X). Secara umum, metode Ordinary Least Square atau metode kuadrat terkecil digunakan untuk mendapatkan estimasi koefisien regresi. Analisis dengan metode ini harus didasari terpenuhinya asumsi regresi klasik, salah satunya adalah tidak ada multikolinearitas. Jika terdapat multikolinearitas, estimasi parameter metode kuadrat terkecil menjadi kurang baik. Selain itu, tanda dari koefisien regresi terkadang berubah dari negatif menjadi positif atau sebaliknya, yang dideteksi dari nilai net effects yang bernilai negatif. 	Metode regresi ridge seringkali digunakan untuk menyelesaikan masalah multikolinearitas. Konsep dari regresi ridge ini adalah menambah tetapan k ke dalam matriks korelasi X'X. Akan tetapi dalam penerapannya, ternyata analisis regresi ridge ini mempunyai kekurangan, yaitu nilai dari R^2 akan menurun jika nilai k terus dinaikkan. Dalam skripsi ini dibahas regresi ridge dua parameter, yaitu pengembangan dari regresi ridge dengan menambah satu tetapan q. Metode ini memiliki fungsi sama seperti regresi ridge tetapi lebih baik dalam nilai R^2, karena nilai R^2 tidak menurun secara cepat walaupun nilai k terus dinaikkan","Regression analysis is an analysis for modelling a relation between dependent variable (Y) and independent variable (X). In general, ordinary least square method is used to obtain the estimation  of regression coefficients. This method should be based on the fulfillment of classical regression asumptions,  which one of them is no multicollinearity. If there is multicollinearity, the parameter estimate of ordinary least square method becomes deficient. Furthermore, the sign of regression coefficients sometimes change from negative to positive or vice versa, that is detected by negative net effects. Ridge regression method often used to solve multicollinearity problem. The concept of ridge regression is adding a constant k to correlation matrix XÃ¢ï¿½ï¿½X. However in application, this ridge regression has a weakness, R2 value will decrease by the increase of k. This study will discuss about two parameter ridge regression, that is an improvement of ridge regression by adding a constant q. This method has the same function as ridge regression but better R^2 value, because the R^2 value is not decreasing even though the k value being increased.","Kata Kunci : regresi ridge, ridge regression, multikolinearitas, multicollinearity, net effects, R^2"
http://etd.repository.ugm.ac.id/home/detail_pencarian/94600,Estimasi Model Shared Frailty Gamma dengan Maximum Hierarchical Likelihood (MHL) dalam Regresi Cox,"IDOLA S S GULTOM, Dr. Danardono, MPH",2016 | Skripsi | S1 STATISTIKA,"Dalam analisis survival model frailty merupakan perluasan dari regresi Cox, dimana melalui model ini dapat diketahui unsur heterogenitas yang disebabkan oleh variabel-variabel penjelas yang tidak teramati. Gagasan munculnya model frailty (kelemahan) terjadi bahwa setiap individu pasti memiliki kelemahan yang berbeda-beda (heterogen) sehingga dapat dikatakan bahwa individu yang paling lemah (frail) akan mengalami kematian yang lebih cepat daripada individu lainnya. Regresi Cox atau sering dikenal dengan model hazard proporsional (proportional hazard model), dimana nilai hazard yang didapat dari fungsi hazard bernilai proporsional, artinya kovariat (variabel-variabel penjelas) sebanding dalam memberikan efek atau pengaruh terhadap suatu kejadian. Pada skripsi ini, menjelaskan prosedur estimasi parameter dari variabel-variabel penjelas dan frailty pada model shared gamma dengan menggunakan metode maximum hierarchical likelihood. Dengan memanfaatkan metode Breslow untuk mengestimasi parameter beta dan algoritma Newton Raphson, sehingga dapat menyederhanakan prosedur estimasi.","Frailty models in survival analysis is an extension of the Cox regression, where this model can be seen through the elements of heterogeneity caused by explanatory variables are unobserved. The idea of the appearance of the model frailty (weakness) occurs in which individual must have different weaknesses (heterogeneous) so that it can be said that the most vulnerable individuals (frail) will die faster than other individuals. Cox regression or commonly known as proportional hazard models (proportional hazard model), where the value obtained from the hazard valued proportional hazard function, meaning covariates (explanatory variables) are comparable in effect or influence of an event.  In this thesis, describes the parameter estimation procedure of the explanatory variables and the shared gamma frailty models using of maximum likelihood hierarchical method. By utilizing Breslow method to estimate the parameters beta and Newton Raphson algorithm, so as to simplify the estimation procedure.  Keywords	: Survival analysis, Cox Regression, Maximum Hierarchical Likelihood, Model shared gamma Frailty, Newton Raphson.","Kata Kunci :  Analisis survival, Regresi Cox, Maximum Hierarchical Likelihood, Model shared gamma frailty, Newton Raphson."
http://etd.repository.ugm.ac.id/home/detail_pencarian/97674,PENENTUAN RETURN EKSPEKTASI PADA PORTOFOLIO OBLIGASI MENGGUNAKAN LIQUIDITY-ADJUSTED CAPITAL ASSET PRICING MODEL (STUDI KASUS OBLIGASI PEMERINTAH INDONESIA),"APRILITA EVARISKA ANGGORO, Prof. Dr. rer. nat. Dedi Rosadi, S.Si., M.Sc. ; Vemmie Nastiti Lestari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Dalam pasar modal, terdapat berbagai aset yang diperdagangkan dan tidak semua aset dapat dijual dengan cepat atau likuid. Likuiditas menyatakan mudah atau tidaknya aset dijual di pasar modal dengan cepat pada harga yang wajar. Aset yang tidak likuid adalah aset yang tidak dapat dijual dengan cepat di pasar modal dikarenakan sedikitnya pembeli yang berminat pada aset tersebut. Aset tersebut mempunyai risiko yang dimiliki pemegang aset selama menahan aset sebelum akhirnya menjualnya di bawah harga pasar. Likuiditas aset dinyatakan dengan biaya likuiditas dan risiko likuiditas dimana kedua faktor mempengaruhi ekspektasi tingkat pengembalian dari aset. Aset yang digunakan adalah obligasi pemerintah.   Liquidity-Adjusted Capital Asset Pricing Model (LCAPM) merupakan suatu model yang mengikutsertakan likuiditas dalam menghitung return aset. Pada model LCAPM risiko likuiditas diwakili oleh tiga nilai kovariansi yaitu kovariansi antara likuiditas aset dengan likuiditas pasar, kovariansi antara return aset dengan likuiditas pasar dan kovariansi antara likuiditas aset dengan return pasar. Ukuran likuiditas yang digunakan adalah bid-ask spread. Dengan menggunakan nilai return ekspektasi model LCAPM akan dibentuk portofolio obligasi untuk berbagai masa jatuh tempo. Dari hasil perhitungan diperoleh bahwa portofolio LCAPM dengan obligasi masa jatuh tempo 6 tahun sebagai aset pembentuknya menghasilkan return ekspektasi yang cukup besar dengan risiko yang terkecil.","In the capital market, there are variety of assets that traded and not all of the assets can be sold quickly. Liquidity describe whether or not an asset is sold in the capital market quickly at reasonable price. Illiquid asset is an asset that can not be sold quickly in the capital market due to lack of interested buyer. The asset has a risk of the holder when holding the illiquid asset before selling it willingly below market price. Liquidity of an asset represented by liquidity cost and liquidity risk where both factors affect the expected return of an asset. Assets used are goverment bonds.   Liquidity-Adjusted Capital Asset Pricing Model (LCAPM) is a model which using liquidity risk to determine return of an asset. In LCAPM liquidity risk is represented by three covariance, covariance between assetÃ¢ï¿½ï¿½s liquidity and marketÃ¢ï¿½ï¿½s liquidity, covariance between assetÃ¢ï¿½ï¿½s return and marketÃ¢ï¿½ï¿½s liquidity and last covariance between assetÃ¢ï¿½ï¿½s liquidity and marketÃ¢ï¿½ï¿½s return. Measuring liquidity use bid-ask spread. By using the expected return LCAPM models will be formed bond portofolios for various maturities. This work provide LCAPMÃ¢ï¿½ï¿½s portfolio with bonds maturity period of 6 years as forming assets show higher expected return with smallest risk.","Kata Kunci : Bid-Ask Spread, Likuiditas, Obligasi, Portofolio, LCAPM"
http://etd.repository.ugm.ac.id/home/detail_pencarian/106643,Koefisien Korelasi Regresi untuk Model Regresi Poisson,"KRISTIANA YUNITANINGTYAS, Danang Teguh Q., M.Sc., Ph.D ; Yunita Wulan Sari, S.Si., M.Sc",2016 | Skripsi | S1 STATISTIKA,"Evaluasi dilakukan untuk memilih kandidat model terbaik diantara semua model statistik yang dibentuk dari data yang diberikan. Digunakan suatu kriteria tertentu untuk mengevaluasi model-model statistika, tak terkecuali Generalized Linear Model (GLM). Salah satu kriteria yang umum digunakan yaitu Akaike Information Criterion (AIC). Pada regresi Poisson dimana variabel responnya berbentuk diskrit, terdapat beberapa kekurangan pada AIC yaitu tidak dapat menentukan kesesuaian data dengan model statistik dan tidak mampu menentukan model terbaik dengan nilai yang terkecil. Untuk mengatasi permasalahan tersebut maka digunakan Regression Correlation Coefficient (RCC) sebagai metode alternatif dalam memprediksi kekuatan model. RCC merupakan nilai yang berasal dari populasi dan didefinisikan sebagai korelasi antara variabel respon dengan ekspektasi bersyarat variabel respon. Dalam kasus ini estimator dari RCC yaitu Rtopi dan Rtopi_cor. Diperoleh Rtopi sebagai estimator yang terbaik bagi RCC berdasarkan simulasi Monte Carlo. Rtopi merupakan pengukur kekuatan prediksi untuk memilih model terbaik sekaligus untuk mengukur ketepatan model regresi Poisson terhadap data.","Evaluation is usually conducted to find the best model among many possible statistical models created from a given data. For the evaluation of statistical models, such as the Generalized Linear Model (GLM), a certain criterion must be considered. One of the commonly used criteria is the Akaike Information Criterion (AIC). In Poisson regression, where count response variable used, various limitations are encountered by AIC such as it cannot judge whether the data fit the statistical model and it fails to find the best model through its lowest number. To deal with those limitations, Regression Correlation Coefficient (RCC) is performed as an alternative measure of predictive power. The RCC is a population value that is defined by the correlation between a response variable and the conditional expectation of the response variable. In this study, the sample values of RCC are  Rhat and Rhat_cor. Thus, Rhat is the best estimator for RCC according to Monte Carlo simulation. Rhat is a measure of predictive power to choose the best model and also to predict whether the Poisson regression model fits the data well.","Kata Kunci : Generalized Linear Model, pengukur kekuatan prediksi, koefisien korelasi regresi (RCC) , AIC, Goodness of fit/Generalized Linear Model, measure of predictive power, Regression Correlation Coefficient (RCC), AIC, Goodness of fit"
http://etd.repository.ugm.ac.id/home/detail_pencarian/107160,PENENTUAN HARGA OBLIGASI BENCANA ALAM PROPINSI DAERAH ISTIMEWA YOGYAKARTA DENGAN METODE PENDEKATAN DISTRIBUSI INVERSE GAUSSIAN UNTUK TOTAL LOSS,"BERNADETTA INFANTERI B., Dr. Gunardi, M.Si.",2016 | Skripsi | S1 STATISTIKA,"Propinsi Daerah Istimewa Yogyakarta merupakan daerah yang rawan bencana jika dilihat dari segi geofisik dan geologis karena dibatasi oleh jalur vulkanik Merapi yang aktif dan jalur subduksi lempeng Indo-Australia-Eurasia. Bencana alam dapat menimbulkan kerusakan dan kerugian yang sangat besar. Oleh karena itu, diperlukan suatu cara untuk memindahkan risiko bencana alam dari pemerintah sehingga dapat dikumpulkan dana yang cukup untuk mengatasi kerugian yang muncul tersebut, salah satunya dengan obligasi bencana alam. Suatu obligasi bencana alam diterbitkan oleh suatu special purpose vehicle (SPV). Dana yang terkumpul dari investor dan sponsor akan dikelola oleh SPV sedemikian rupa ke dalam investasi lain yang lebih stabil. Jika terjadi bencana alam, investor hanya mendapatkan sebagian aliran dana atau tidak sama sekali dari SPV sebab dana dialirkan kepada pihak sponsor untuk menutup kerugian akibat bencana alam tersebut. Diperlukan suatu metode yang tepat dan akurat dalam menentukan harga obligasi bencana alam tersebut agar tidak menimbulkan kerugian bagi pihak SPV. Studi kasus menunjukkan bahwa peluang suatu bencana alam menimbulkan nilai kerugian tertentu dapat dianalisis menggunakan pendekatan distribusi Inverse Gaussian untuk total kerugian. Penentuan harga obligasi bencana alam kali ini akan digunakan asumsi suku bunga model Cox-Ingersoll-Ross (CIR). Di bawah model ini, diperoleh harga dua macam obligasi bencana alam, yakni obligasi tanpa kupon dan obligasi dengan kupon beresiko. Selanjutnya, berdasarkan simulasi diperoleh hubungan antara harga jual obligasi dan parameter-parameter model suku bunga CIR, distribusi Inverse Gaussian, besar kupon, pemotongan aliran dana kepada investor, waktu jatuh tempo dan nilai threshold.","Daerah Istimewa Yogyakarta is a province with huge risk geophysically and geologically because it is bordered by the active Merapi volcanic paths and subduction path of Indo-Australian plate and Eurasia plate. Natural disasters can cause damage and huge losses. Therefore, we need a way to transfer the risk of natural disaster from the government so that it can be collected enough funds to cope with the resulting loss, e.g. catastrophe bonds. A catastrophe bonds issued by a special purpose vehicle (SPV). Funds raised from investors and sponsors will be managed by the SPV in such a way into other more stable investment. If a catastrophe is happened, investors only get partial cash flow or not at all from SPV because the cash flow is paid to the sponsors to cover losses due to natural disasters. We need a method that is precise and accurate in determining the price of catastrophe bonds in order not to cause any harm to the SPV. The case study shows that the chance of a natural calamity raises the value of certain losses can be analyzed using Inverse Gaussian distribution approximation method for total loss. This pricing of catastrophe bonds assumes the interest rate model of Cox-Ingersoll-Ross (CIR). We develop formulas for pricing two kinds of catastrophe bonds, i.e. zero coupon bonds and coupon at risk bonds. Furthermore, the relationship between price of catastrophe bonds and  CIRâ€™s model parameters, Inverse Gaussian distribution parameters, coupon rate, discounted cash flow rule, maturity date, and threshold value.","Kata Kunci : Obligasi bencana alam, model suku bunga Cox-Ingersoll-Ross, metode pendekatan, distribusi Inverse Gaussian, total kerugian, harga"
http://etd.repository.ugm.ac.id/home/detail_pencarian/99226,OPTIMALISASI PORTOFOLIO MENGGUNAKAN  ANALISIS KLASTER AVERAGE LINKAGE,"AWANI DWITA ROKHMAN, Dr. Abdurakhman, S.Si, M.Si",2016 | Skripsi | S1 STATISTIKA,"Saat ini, investasi portofolio merupakan salah satu usaha yang banyak digemari oleh para investor. Agar diperoleh hasil investasi yang memuaskan, maka diperlukan portofolio optimal, yakni portofolio dengan return maksimal dan risiko minimal. Salah satu usaha untuk memperoleh portofolio optimal yakni dengan metode mean variance. Akan tetapi penggunaan metode tersebut masih diragukan karena cenderung menekankan pada masalah estimasi kesalahan tanpa mempertimbangkan efek dari ekspektasi return. Untuk mengatasi masalah tersebut, telah dilakukan penelitian bahwa metode pengklasteran single linkage mampu mengatasinya akan tetapi hasil pembobotannya masih menghasilkan beberapa saham berbobot negatif (short selling). Untuk keadaan yang lebih realistis hasil pembobotan haruslah positif (tidak ada short selling), maka pada penelitian ini akan dilakukan studi kasus analisis portofolio mean variance dengan metode pengklasteran average linkage.  Data yang digunakan yakni data closing price saham bulanan dari 10 saham LQ-45. Dari data tersebut dihitung return saham bulanan dan dilanjutkan dengan uji normalitas. Setelah itu, dilakukan pembobotan portofolio mean variance menggunakan analisis klaster average linkage dan single linkage (sebagai pembanding). Untuk mengetahui keoptimalan hasil pembobotan, maka dilakukan perhitungan rasio sharpe. Hasil studi kasus menunjukkan bahwa pembobotan portofolio mean variance dengan analisis klaster average linkage menghasilkan nilai saham yang semuanya positif, sedangkan dengan analisis klaster single linkage beberapa saham bernilai negatif. Analisis klaster average linkage memiliki nilai rasio sharpe yang lebih tinggi (2,33), dibandingkan dengan analisis klaster single linkage (1,74). Sehingga dapat diperoleh informasi bahwa pembobotan portofolio mean variance menggunakan analisis klaster average linkage lebih optimal dan lebih stabil dari pada analisis klaster single linkage.","Nowadays, investment portfolio is one of the most favorite business by investors. In order to reach satisfied return on investment, so it needs to optimize the portfolio, which has maximum return and minimum risk. One of to get the optimum portfolio is the mean variance method. However, the using of this methods are still in doubt because it tends to emphasize the error estimation problem without considering the effect of the expected return. Solving the problem, there is research that single linkage clustering method is able to solve the problem but weighted result still obtain negative weight of some holding (short selling). For more real condition, weighted result should be positive (no short selling), this study will have a case studies about mean variance portfolio analysis with average linkage clustering method.  The using data is the holding closing price every month from 10 stock LQ-45. From these data will be calculated monthly stock returns and continued with normality test. Afterthat, the mean variance portfolio is weighted using average linkage cluster analysis and single linkage (as a comparison). To find out optimum weighting result, will calculated by sharpe ratio. Case study showed that mean variance portfolio weighting by average linkage clustering get all positive stock value, while the single linkage clustering get some negative stock. Average linkage cluster analysis has a value higher Sharpe ratio (2.33), compared with single linkage cluster analysis (1.74). From this study, we get informed that mean variance portfolio using average linkage clustering is more optimum and more stable than single linkage clustering.","Kata Kunci : Optimalisasi portofolio, short selling, analisis klaster, average linkage,  rasio sharpe"
http://etd.repository.ugm.ac.id/home/detail_pencarian/104347,Metode Jackknife untuk Mereduksi Eror pada Estimasi Regresi Eksponensial,"EMI HERNAWATI, Dr. Herni Utami, S.Si., M.Si.",2016 | Skripsi | S1 STATISTIKA,"Jackknife adalah salah satu metode estimasi koefisien regresi yang berbasis komputer. Cara kerja metode ini adalah membuat sampel tiruan dari membangkitkan data yang berukuran kecil. Sampel tiruan ini didapatkan dengan menghapus sampel asli satu per satu sehingga dapat dilakukan estimasi koefisien regresi. Metode ini lebih praktis karena tidak memerlukan asumsi.  	Dalam laporan skripsi ini, metode yang digunakan untuk mendapatkan estimasi parameter regresi eksponensial adalah Nonlinear Least Square kemudian dilanjutkan metode iterasi Gauss Newton dengan bantuan software R.","Jackknife is one of estimation methods to estimate coefficient regression which is computer-based. This method is generating a small data to make a pseudo sample. Pseudo sample is obtained by deleting sample one by one, in order to get regression estimate. This method is not used assumtion. 	In this paper, the methods which is used to estimate parameter of exponential regression are Nonlinear Least Square and then Gauss Newton iterative method with R software.","Kata Kunci : Jackknife Method, Exponential Regression, Nonlinear Least Square, Gauss Newton"
http://etd.repository.ugm.ac.id/home/detail_pencarian/98974,KLASTERING K-MODES DAN ATURAN ASOSIASI UNTUK MENGANALISIS DATA KECELAKAAN,"RIZKYANA FATIKHA, Zulaela, Drs., Dipl.Med.Stats., M.Si. ; Widya Irmaningtyas, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Aturan asosiasi adalah salah satu teknik data mining yang bertujuan untuk mengekstrak informasi dengan cara menganalisis adanya pola aturan yang terbentuk dari suatu data. Namun metode ini tidak bekerja secara maksimal pada data yang bersifat heterogen. Jika aturan asosiasi dipaksakan pada data yang heterogen, maka akan menghilangkan informasi yang terdapat pada data. Oleh karena itu untuk mengatasi permasalahan tersebut diperlukan klastering pada data terlebih dahulu sebelum mengekstrak informasi dengan menggunakan aturan asosiasi. Teknik klastering yang digunakan pada penelitian ini adalah dengan teknik klastering K-Modes. Studi kasus yang digunakan pada penelitian ini adalah data kecelakaan lalu lintas. Kecelakaan lalu lintas merupakan kejadian yang tidak menentu dan dapat terjadi sewaktu-waktu dengan berbagai macam kondisi yang berbeda-beda, sehingga data kecelakaan dapat dikatakan data yang bersifat heterogen. Dengan menggunakan kombinasi klastering K-Modes dan aturan asosiasi, dihasilkan informasi yang mungkin tersembunyi jika diaplikasikan pada data yang heterogen, salah satunya pada data kecelakaan.","The association rule is one of the data mining techniques which aims to extract information by analyzing the pattern of rules that are formed by the data. However, this method does not work optimally on heterogeneous data. If the association rule is to be imposed on heterogeneous data, it could eliminate the information contained within the data. Therefore, in order to solve these problems, a cluster analysis should be performed on the data prior to extracting information using the association rule. The clustering technique employed in this research is the K-Modes clustering technique. The case study used in this research is road accident data. Road accidents are inadvertent events which may occur at any time and in a variety of circumstances, so that the data is considered to be heterogeneous. The combination of K-Modes clustering and association rule techniques is used in order to generate information that might otherwise be hidden within the heterogeneous data, which includes the road accident data.","Kata Kunci : Kecelakaan Lalu Lintas, Klastering, Aturan Asosiasi/Road Accident, Clustering, Association Rule"
http://etd.repository.ugm.ac.id/home/detail_pencarian/104097,PENENTUAN PREMI AREA YIELD CROP INSURANCE MENGGUNAKAN MODEL ADITIF LINIER,"FERRI IS HANDOKO , Dr. Adhitya Ronnie E., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Menurut penelitian, secara relatif perubahan iklim ekstrim akan sering terjadi pada negara yang berada di daerah khatulistiwa, salah satunya adalah Indonesia. Tentunya perubahan iklim ini akan sangat terasa terhadap sektor perekonomian yang sangat bergantung terhadap cuaca yaitu sektor pertanian. Para petani harus mulai memikirkan berbagai resiko yang akan dihadapi sebelum masa panen. Hal ini mendorong pemerintah Indonesia untuk menyediakan jasa perlindungan hasil panen bagi para petani melalui asuransi lahan pertanian.  Salah satu jenis asuransi lahan pertanian yang dapat diterapkan di Indonesia adalah Area Yield Crop Insurance yang lebih akurat dan sesuai untuk pelaku usaha tani perseorangan karena dalam penentuan harga premi menggunakan data produktivitas hasil panen perseorangan atau kelompok tani tertentu. Pemodelan dalam asuransi lahan pertanian ini menggunakan model kombinasi linearitas dan pertambahan dari error terhadap area yield, yaitu Model Aditif Linier.","According to the study, relatively extreme climatic changes will often occur in countries that are on the equator, one of which is Indonesia. Certainly, climate change will be felt on the economic sector, agriculture, thatâ€™s very dependent on the weather. Farmers have to start thinking about the various risks to be faced before the harvest. This make the Indonesian government to provide protection to the farmers through crop insurance.  One type of insurance that can be applied to agricultural land in Indonesia is Area Yield Crop Insurance is more accurate and appropriate to individu because of the premium price determination using productivity data individual yield or groups of farmers yield. Modeling in crop insurance use model which is combination of linearity and additive error to the area yield, named Linear Additive Model.","Kata Kunci : asuransi lahan pertanian, Area Yield Crop Insurance, premi, hasil panen, Model Aditif Linier"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96165,PERBANDINGAN PERFORMANSI METODE  NAIVE BAYES CLASSIFIER DAN ALGORITMA J48 DALAM KLASIFIKASI DATA JAMUR,"CINNAMON, Dr. Herni Utami, M.Si.",2016 | Skripsi | S1 STATISTIKA,"Data mining merupakan sebuah alat yang sangat bermanfaat untuk memperoleh informasi dari data yang sangat besar. Salah satu bagian penting pada data mining adalah pengklasifikasian data. Klasifikasi digunakan untuk menggolongkan data berdasarkan sifat data yang sudah dikenali masing-masing kelasnya. Ada berbagai macam teknik yang digunakan untuk mengklasifikasikan data, seperti Naive Bayes Classifier dan Algoritma J48. 	Skripsi ini difokuskan pada metode klasifikasi Naive Bayes Classifier dan Algoritma J48 dan aplikasinya pada data jamur. Data pengklasifikasian jamur ini dibagi menjadi dua kelas, yaitu jamur yang dapat dimakan dan jamur yang beracun. Dengan menggunakan kedua metode tersebut, dapat diperoleh perbandingan akurasinya sehingga dapat disimpulkan metode yang paling cocok untuk mengklasifikasi data jamur serta dapat dilakukan prediksi berdasarkan variabel prediktornya.","Data mining is the useful tool for discovering knowledge from large data. One of the important thing is classification of the data. Classification is used to classify data based on their characteristics which is recognized each classes. There are some techniques to classify the data, Naive Bayes Classifier and J48 Algorithm. 	In this minithesis, concerned to classify mushroom dataset using Naive Bayes Classifier and J48 Algorithm. This mushroom divided into edible or poisonous mushroom. Using those methods, we can compare the accuracy so we can conclude the best method to classify the mushroom dataset and predict it consider to the predictor variable.","Kata Kunci : data mining, classification, Naive Bayes Classifier, J48 Algorithm, mushroom dataset."
http://etd.repository.ugm.ac.id/home/detail_pencarian/102567,SUBSET SELECTION PADA REGRESI LINEAR GANDA DENGAN METODE JACKKNIFED RIDGE M-ESTIMATOR,"SRI UTAMI , Drs.Danardono, MPH, Ph.D; Rianti Siswi Utami, S.Si, M.Sc",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Dalam asumsi yang terdapat pada analisis regresi klasik diantaranya adalah tidak adanya outlier pada variabel respon dan tidak adanya multikolinearitas. Jika dalam model regresi terdapat outlier dan multikolinearitas, hal itu dapat menyebabkan hasil estimasi dengan menggunakan metode kuadrat terkecil menjadi tidak valid. Pada skripsi ini akan dibahas mengenai penduga regresi linear ganda dengan menggunakan metode estimator jackknifed ridge M untuk subset selection dengan adanya multikolinearitas dan outlier pada variabel respon yang dikembangkan oleh Jadhav dan Kashid (2011). Metode ini merupakan pengembangan versi generalisasi dari statistik Sp yang dikemukakan oleh Kashid dan Kulkani (2002).  Studi kasus pada skripsi ini menggunakan data jumlah asap pada mobil dan variabel yang mempengaruhinya. Diperoleh kesimpulan bahwa metode jackknifed ridge M-estimator memberikan nilai statistik yang lebih mendekati jumlah parameter dalam model subset (p) daripada estimator M dan estimator ordinary ridge regression.","Regression analysis is a statistical analysis that is being used to estimate the relationship model between a dependent variable and independent variables. Two important assumption in classical regression analysis are no outlier in a dependent variable and no multicollinearity. If outlier and multicollinearity are present in a regression model, it can invalidate the estimated result predicted by the Least Squares Estimation method. This paper is aimed discussing the estimation of the multiple linear regression parameter using jackknife ridge M estimator for the subset selection in the linear model with multicollinearity and outlier in dependent variable that has been developed before by Jadhav and Kashid (2011). This method is expansion of the generalized version of statistic Sp, that was first published by  Kashid and Kulkani (2002). The data of car fumes quantity and their factors become the case study in this paper. The conclusion that can be obtained is that the Jackknifed Ridge M estimator gives closer statistical value of  the parameter numbers in a subset model (p) than the M estimator and ordinary ridge regression estimator.","Kata Kunci : multiple linear regression, subset selection, outlier, multicollinearity, jackknifed ridge M estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/102823,PENERAPAN METODE RESTRICTED RIDGE ESTIMATION DALAM MENANGANI MASALAH MULTIKOLINEARITAS PADA MODEL LINEAR TERBATAS,"SISKA SETYANINGSIH, Prof. Drs. Subanar, Ph.D.",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi bertujuan untuk memodelkan hubungan antara variabel dependen dan independen. Salah satu metode yang umum digunakan yaitu metode kuadrat terkecil. Namun, jika terdapat informasi tambahan mengenai parameter yang memenuhi pembatasan linear RÃŽÂ²=r maka model regresi menjadi terbatas. Pemodelan regresi dengan pembatasan linear tersebut menggunakan metode kuadrat terkecil terbatas. Dalam model regresi linear, no multikolinearitas adalah salah satu asumsi klasik yang harus dipenuhi dan jika tidak maka estimasi parameter akan jauh dari nilai yang seharusnya. 	Metode yang dapat menangani multikolinearitas yaitu regresi ridge dengan cara menambahkan matriks diagonal berisi tetapan biask ke matriks X'X. Dalam regresi liner terbatas, metode yang digunakan untuk mengangani multikolinearitas adalah restricted ridge estimationyang merupakan penerapan ridge dalam model linear terbatas. Metode ini memberikan estimasi parameter dan mean square error yang lebih baik dari pada metode kuadrat terkecil terbatas. Berdasarkan studi kasus, dapat disimpulkan bahwa metode ini dapat menganani multikolinearitas pada model regresi linear terbatas.","Regression analysis is used to perform relationship model among dependent and independent variable. One of the most popular method of this analysis is least square estimation. However, if there any additonal information about the unknown parameter which satisfy the linear restriction RÃŽÂ²=r then linear model being restricted.This regression using restricted least square estimation. No multicollinearity is a classic assumption required in linear model and if there exists multicollinearity, parameter estimation would be badly apart from the actual coefficient. 	To overcome multicollinearity, one of best methods is ridge regression with adding diagonal matrix of k to the correlation matrix X'X. In restricted linear regression, the methodsused to handle multicollinearity is restricted ridge estimation by applying ridge estimation to the restricted linear model. This method give a better parameter estimation and mean square error than restricted least square. From the case study, it can be conclude that this method can solve multicollinearity of the restricted linear model.","Kata Kunci : Pembatasan Linear, Kuadrat Terkecil Terbatas, Multikolinearitas, Regresi Ridge, Restricted Ridge Estimation, Mean Square Error"
http://etd.repository.ugm.ac.id/home/detail_pencarian/96424,"ANALISIS KLASIFIKASI TOPIK MENGGUNAKAN METODE NAIVE BAYES CLASSIFIER, NAIVE BAYES MULTINOMIAL CLASSIFIER, DAN MAXIMUM ENTROPY PADA ARTIKEL BERITA","NURUL MASITHOH, Dr. Herni Utami, S.Si., M.Si",2016 | Skripsi | S1 STATISTIKA,"Pada era pesatnya informasi yang berbasis website memberikan kemudahan dalam memperoleh informasi. Kebutuhan informasi yang cepat, akurat, dan berasal dari berbagai topik isu kehidupan telah menjadi gaya hidup. Informasi sebagian besar tersedia dalam bentuk data tekstual yang tidak memiliki pola terstruktur. Ilmu statistika semakin diaplikasikan pada berbagai bentuk data, salah satunya data tekstual. Text mining adalah analisis yang berperan dalam pengolahan data tekstual. Analisis text mining yang sering digunakan adalah klasifikasi teks atau klasifikasi topik. Dari hasil klasifikasi topik dapat disimpulkan jenis topik dari suatu dokumen yang tidak terstruktur dengan melihat karateristik dokumen. Metode yang akan dibahas kali ini adalah metode Naive Bayes Classifier, Naive Bayes Multinomial Classifier, dan Maximum Entropy untuk menentukan kelas topik suatu dokumen secara multikelas yaitu kelas topik bola, ekonomi, kesehatan, dan travel. Naive Bayes Classifier dan Naive Bayes Multinomial Classifier merupakan metode klasifikasi menggunakan aturan Bayesian memanfaatkan probabilitas prior dan probabilitas bersyarat dari kemunculan kata, untuk Naive Bayes Classifier dan frekuensi kemunculan kata untuk  Naive Bayes Multinomial Classifier pada masing-masing kelas dokumen training. Sedangkan metode Maximum Entropy menggunakan rata-rata informasi yang terkandung dalam dokumen yang dimaksimalkan sehingga semakin memberikan hasil klasifikasi yang akurat. Nilai tersebut akan digunakan untuk menentukan kelas topik suatu dokumen testing dengan melihat nilai maximum a posterior masing-masing kelas. Data yang telah diklasifikasikan kemudian dihitung tingkat akurasi kebenarannya. Dari perbandingan rata-rata nilai akurasi klasifikasi dengan ketiga metode di atas, diperoleh urutan yang paling tinggi menggunakan metode Maximum Entropy sebesar 99,31%, Naive Bayes Classifier sebesar 98,82%, dan yang paling rendah yaitu dengan metode Naive Bayes Multinomial Classifier sebesar 97,39%. Percobaan ini dilakukan pada 1440 artikel berita dimana setiap metodenya dilakukan 87 percobaan. Selain membandingkan ketiga metode, dilakukan analisis tentang aspek pengaruh penggunaan jumlah token, jumlah data, dan keseragaman jumlah data pada nilai akurasi. Diperoleh kesimpulan jika ketiga aspek kurang mempengaruhi nilai akurasi di setiap metode.","The rapid growth of website based communication nowdays gives easier access to obtain more information. The demand for better accuary and faster information is increasing now due to the modern lifestyle. Most of the data are provided in textual verse are not presented in any structured patterns. Statistics is playing a prominent role in this part, especially in completing textual data. Text mining is an analitical system that used in textual data. This method which are often applied in analytical process are text classification and topic classification. Based on result of topic classification, analyst can decide the topic type. The topic itself is concluded based on the document characteristic. This paper will discuss about Naive Bayes Classifier, Naive Bayes Multinomial Classifier, dan Maximum Entropy. Those methode will be applied to define the topic class based on multiclasses: ball topic class, health, economy, and tavel. Naive Bayes Classifier and Naive Byaes Multinomial Classifier are classifying methods that use Bayesian rules. The rules are including prior probability and conditional probability based on the appearance of words, for Naive Bayes Classifier and the frequency of words appearance for Naive Bayes Multinomial Classifier in each document training. While Maximum Entropy uses approximate information which is included in the document. This information is optimized so that the classification becomes more accurate. That value will be applied to decide the topic class, by looking at the maximum a posterior in each class. Classified data would be counted in order to find the accuracy level. Based on the comparision of approximate values from those methods, the leveling of accuracy. The highest level is Maximum Entropy (99,31%), then Naive Bayes Classifier (98,82%), and the lowest level is Naive Bayes Multinomial Classifier (97,39%). This research was applied on 1440 news articles. Each articles underwent 87 trials. Not only comparing those three methods, but also analyzing the spect of token data usage, number of data usage, and the similary number of data on value accuary. The conclusion is those three aspects do not have influence the data accuary in each method.","Kata Kunci : Klasifikasi topik,text mining,aturan Bayesian,Naive Bayes Classifier,Naive Bayes Multinomial Classifier,Maximum Entropy"
http://etd.repository.ugm.ac.id/home/detail_pencarian/103593,GRAFIK PENGENDALI S-BOOTSTRAP UNTUK MENGONTROL VARIABILITAS PROSES SEBAGAI ALTERNATIF GRAFIK PENGENDALI S-MAD,"MOHAMMAD BURHANUDDIN ZUHDY, Drs. Danardono, M.PH, P.hD ; Rianti Siswi Utami, S.Si., M.Sc",2016 | Skripsi | S1 STATISTIKA,"Dalam proses pengendali statistik, grafik pengendali adalah alat yang paling kuat untuk menilai perilaku suatu proses. Grafik S Shewhart adalah alat standar untuk mengendalikan variabilitas proses. Mirip dengan grafik S, grafik yang berdasarkan Median Absolute Deviation yang berasal dari sampel median atau yang disebut estimator MAD dan grafik S-Bootstrap juga dianggap kuat untuk mengendalikan variabilitas pada proses normal dan non-normal.  Statistik Sbar/C4  dan bn(MADbar) dianggap estimator tak bias dari sigma sehingga standar deviasi proses dapat diestimasi tetapi standar deviasi yang sebenarnya tidak dapat diestimasi. Berdasarkan sifat â€“ sifat baik dari metode bootstrap, telah diusulkan grafik S Bootstrap yang dapat mengestimasi standar deviasi proses  yang sebenarnya. Kinerja grafik S Bootstrap kemudian diukur kinerjanya dengan menggunakan indikator Average Run Lenght (ARL), Coverage Probability dan Interval Width. Simulasi penelitian berdasarkan Monte-Carlo kemuadian dijalankan dan hasilnya grafik S Bootstrap mempunyai performa lebih baik dari pada grafik S Shewhart dan grafik MAD di bawah asumsi normalitas. Lalu diaplikasikan pada rangkaian data praktis yang digunakan untuk membenarkan penemuan.","In statistical process control, the control charts are the most powerful tools for assessing the process ehaviour. The Shewhart S chart is a standard tool for determining process variability. Similar to S chart, the chart based on Median Absolute Deviation from the sample median namely MAD and Boostrap S chart estimator is alsoconsidered robust for both normal and non-normal processes.  As Sbar/C4  and bn(MADbar) are considered unbiased estimators of sigma so the process standard deviation can be estimated but the true standard deviation cannot be found because only one specific sample is considered. Under the remarkable properties of bootstrap methods, we have proposed bootstrap S chart through which the true process standard deviation can be estimated. The performance of proposed chart is estimated on the basis of in-control average run length, coverage  probability and interval width. As a result the proposed chart has performed better than the Shewhart  S and  MAD  charts  under  the  assumptioncof  normality. The simulation study based on monte-carlo runs is conducted for the purpose and the application on a practical data set is also discussed to justify the findings.","Kata Kunci : Average Run Length (ARL), bootstrap, coverage probability, interval width, Median Absolute Deviation about median (MAD)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/97709,ESTIMATOR LIU DALAM REGRESI LOGISTIK UNTUK MENGATASI MASALAH MULTIKOLINEARITAS,"RENY SULISTYANINGTYAS, Dr. Gunardi, M.Si ; Vemmie Nastiti Lestari,S.Si,M.Sc",2016 | Skripsi | S1 STATISTIKA,"Regresi logistik merupakan salah satu model statistika yang digunakan untuk menganalisis hubungan antara variabel independen dengan variabel dependen, dimana variabel dependen bertipe kategorik atau kualitatif. Dalam regresi logistik, metode maksimum likelihood digunakan untuk mengestimasi parameternya. Akan tetapi, estimasi menggunakan metode maksimum likelihood menjadi tidak valid jika variabel independennya saling berkorelasi atau dikenal sebagai kondisi multikolinearitas. Oleh karena itu, estimator liu pada model regresi logistik digunakan untuk menangani masalah multikolinearitas. Dalam skripsi ini, estimator liu pada analisis regresi logistik diaplikasikan untuk memodelkan faktor-faktor yang mempengaruhi risiko kredit macet di PD. BPR Bank Bantul dimana terdapat masalah multikolinearitas. Rata-rata kuadrat galat dari estimator ridge dan estimator liu juga akan dibandingkan. Kemudian diperoleh hasil yang menunjukkan bahwa estimator liu memberikan nilai rata-rata kuadrat galat yang lebih kecil dari estimator ridge.","Logistic regression is a statistical models were used to analyze the relationship between the dependent and independent variables, where the dependent variable type is categorical or qualitative. In logistic regression, maximum likelihood method used to estimate parameters. However, using the method of maximum likelihood estimation becomes invalid if the independent variables are correlated and called multicollinearity. Therefore, Liu estimator on logistic regression model was used to deal with problems multicollinearity. In this thesis, Liu estimator on logistic regression analysis was applied to analyze the factors that affect the credit risk in PD. BPR Bank Bantul where there are problems multicollinearity. The mean squared error of liu estimator and ridge estimator will also be compared. Then the obtained results show that the liu estimator gives the value of the mean squared error is smaller than the ridge estimator.","Kata Kunci : Kata kunci: regresi logistik, maksimum likelihood, multikolinearitas, estimator ridge, estimator liu, rata-rata kuadrat galat."
http://etd.repository.ugm.ac.id/home/detail_pencarian/93110,PRINCIPAL COMPONENT LOGISTIC REGRESSION (PCLR) UNTUK MULTIKOLINEARITAS DALAM REGRESI LOGISTIK BINER,"NAYLA AZMI AFIFI, Drs. Zulaela, Dipl.Med.Stats., M.Si.;Yunita Wulan Sari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Regresi logistik digunakan untuk memprediksi variabel respon yang biner dengan satu set variabel penjelas (prediktor). Estimasi parameter dapat menjadi tidak akurat, serta interpretasi odd ratio bisa salah jika terdapat multikolinearitas di antara prediktor. Untuk mengatasi masalah estimasi parameter dengan multikolinearitas, diperlukan pengurangan dimensi prediktor dengan menggunakan komponen utama optimal dari prediktor tersebut. Komponen utama ditentukan dengan menggunakan matriks kovariansi dari data yang telah dilakukan centering. Banyaknya komponen utama optimum dipilih berdasarkan beberapa kriteria yaitu Kaiser's Stopping Rule, Scree Test, dan Percent of Cumulative Variance. Pada akhirnya, performa model regresi logistik komponen utama dibandingkan dengan model regresi logistik yang variabel multikolinearnya telah dihapus. Dengan melihat Bayesian Information Criterion (BIC), model PCLR dianggap lebih baik daripada model yang variabel multikolinearnya telah dihapus karena memiliki nilai BIC yang lebih kecil. Berbeda dengan model yang variabel multikolinearnya telah dihapus, PCLR mampu mempertahankan prediktor sehingga tidak terjadi hilangnya informasi yang diperlukan.","Logistic regression is used to predict binary response variabel by a set of independent variables (predictors). The parameter estimation can be not too accurate, also the interpretation of odd ratio may be erroneous when there is multicollinearity among the predictors. In order to improve the estimation of the parameters under multicollinearity, we need to reduce the predictor dimension using optimum principal components of the covariates. Principal components are determined using covariance matrix of the centered data. The number of optimum principal components are selected based on several criterias, i.e.  Kaiser's Stopping Rule, Scree Test, and Percent of Cumulative Variance. Finally, the principal component logistic regression model performance is compared with the  logistic regression model with removed multicollinear predictors. By looking at the Bayesian Information Criterion (BIC), PCLR considered better than logistic regression model with removed multicollinear predictors because it has a smaller BIC. In contrast to logistic regression model with removed multicollinear predictors, PCLR can maintain the predictors so there is no loss of information needed.","Kata Kunci : regresi logistik, multikolinearitas, komponen utama"
http://etd.repository.ugm.ac.id/home/detail_pencarian/94139,PENENTUAN KELAYAKAN PERUSAHAAN PASANGAN USAHA PADA INVESTASI MODAL VENTURA MENGGUNAKAN ANALISIS OPSI RIIL,"NURUUL AINI UTAMI, Dr. Abdurakhman, M.Si",2016 | Skripsi | S1 STATISTIKA,Modal ventura merupakan suatu investasi dalam bentuk pembiayaan berupa penyertaan modal ke dalam suatu perusahaan swasta sebagai pasangan usaha (investee company) untuk jangka waktu tertentu. Kapitalis ventura (venture capitalist dapat disingkat VC) adalah seorang investor yang berinvestasi pada perusahaan modal ventura. Kapitalis ventura biasanya memiliki hak suara sebagai penentu arah kebijakan perusahaan sesuai dengan jumlah saham yang dimilikinya. Namun seringkali seorang kapitalis ventura mengalami kesulitan dalam membuat keputusan tentang pendanaan pada suatu perusahaan karena adanya ketidakpastian di masa depan tentang keberhasilan proyek perusahaan. Analisis opsi riil dapat mengatasi adanya ketidakpastian di masa depan yang dihadapi oleh kapitalis ventura dalam menentukan keputusan tentang peminjaman dana.  Analisis opsi riil merupakan penerapan teknik perhitungan harga opsi untuk keputusan penganggaran modal. Opsi ini akan digunakan VC untuk mempertimbangkan kelayakan suatu perusahaan memperoleh pinjaman dana. Opsi  yang digunakan adalah opsi majemuk dengan beberapa volatilitas. Untuk menghitung nilai klaim dari opsi tersebut dilakukan penurunan dua formula umum dari Black dan Scholes (1973) dan Geske (1979).,"Venture capital is an investment in the form of equity participation  into a private company as a bussiness partner for specified period. Venture capitalis have a high risk, however it also give a high returns. Venture capitalists is an investor who invest in the venture capital company. Generally, venture capitalist have a right as a determinant of company policies  based on the number of shares they own. However, a venture capitalist often find it difficult to make a decision about funding to a company because the uncertainty of the company projects will be success or not in the future. Real option analysis can get over  the uncertainty in the future that venture capitalists have faced to make a decision about loan.  Real option analysis is the application of option  price calculation technique for capital budgeting decisions. Venture Capital will use this option to considering the feasibility of a company to obtain a loan. Option are being used is compound option with some volatility. The claim value calculating from that option done by decreasing two general formula from Black and Scholes (1973) and Geske (1979).","Kata Kunci : analisis opsi riil, opsi majemuk dengan beberapa volatilitas, opsi Black Scholes"
http://etd.repository.ugm.ac.id/home/detail_pencarian/107196,GRAFIK PENGENDALI ROBUST BERDASARKAN INTERQUARTILE RANGE (IQR),"AMY FATMAYA HUTAMI, Prof. Dr. rer. nat. Dedi Rosadi, M.Sc.",2016 | Skripsi | S1 STATISTIKA,Pengendalian kualitas diperlukan guna menjaga kestabilan suatu proses produksi agar menghasilkan produk yang memenuhi standar yang telah ditetapkan dan memenuhi syarat agar dapat digunakan konsumen. Diagram kontrol (grafik pengendali) merupakan salah satu teknik pengendalian proses untuk mendeteksi perubahan karakteristik yang menyimpang dalam proses produksi. Grafik pengendali x-bar dan R adalah salah satu yang banyak digunakan sebagai teknik pengendalian proses statistik untuk mengontrol rata-rata dan variabilitas proses dengan asumsi dasar distribusi dari karakteristik kualitas adalah normal. Metode robust adalah salah satu metode statistik yang sering digunakan sebagai alternatif ketika asumsi normalitas yang mendasari tidak terpenuhi. Grafik pengendali robust berdasarkan interquartile range (X-bar_Q dan R_Q) merupakan salah satu alternatif pengganti grafik pengendali x-bar dan R ketika asumsi normalitas pada mean sampel yang mendasari tidak terpenuhi atau data sampel terkontaminasi adanya outlier.,Quality control are needed to ensure precess stability so that the products and services meet requirements . Control chart  is one of the tools of quality control used to detect the deviant behavior in industrial process. x-bar and R control chart is one of the most widely used as statististical quality control techniques developed to control the average and process variability based on the basic assumption that the underlying distribution is normal. A robust method is one of the statistical methods  most used as an option when the underlying assumptions of normality were not met. Robust control chart based on interquartile range (X-bar_Q and R_Q control chart) provides an alternative x-bar and R control chart when the underlying assumptions of normality were not met or outliers are present within subgroups.,"Kata Kunci : grafik pengendali, x-bar dan R, robust, interquartile range"
http://etd.repository.ugm.ac.id/home/detail_pencarian/93125,PENENTUAN HARGA OPSI ARITMATIK ASIA METODE TURNBULL DAN WAKEMAN,"PRASTYANI BETARI, Dr. Gunardi, M.Si.; Vemmie Nastiti Lestari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,Penentuan harga opsi Asia yang menggunakan rata-rata aritmatik sangat sulit dilakukan secara analitik karena distribusi dari fungsi distribusinya tidak diketahui. Namun berbagai penelitian telah dicoba untuk memecahkan permasalahan ini. Salah satunya adalah Turnbull dan Wakeman (1991) yang mengakproksimasi fungsi densitas yang tidak diketahui mengunakan distribusi lognormal dengan menyamakan dua momen pertama dan menggunakan ekspansi deret Edgeworth tergeneralisasi yang menyertakan perhitungan selisih skewness dan kurtosis. Pada skripsi ini akan memeriksa seberapa akurat pendekatan Turnbull dan Wakeman dengan membandingkan solusi pendekatan secara analitik Turnbull dan Wakeman dengan Monte Carlo anthitetic variates dalam penentuan harga opsi aritmatik Asia. Keuntungan dari Turnbull dan Wakeman ini adalah cepat dalam menentukan harga opsi aritmatik Asia.,"Pricing the arithmetic Asian options are difficult to price analytically because the distribution of the density function is unknown. However, various studies have attempted to solve this problem. Turnbull and Wakeman (1991) approximate the unknown density function using lognormal distribution by matching the first two moments and using a generelized Edgeworth series expansion which also takes differences in skewness and kurtosis into account. This final assignment investigates how accurate the Turnbull and Wakeman approach is by comparing values of arithmetic Asian options from Turnbull and Wakemanâ€™s analytic solution approach with Monte Carlo anthitetic variates. The advantage of this Turnbull and Wakeman method is the quick way to price arithmetic Asia options.","Kata Kunci : Opsi Asia, Aritmatik, Edgeworth, Monte Carlo anthitetic variates"
http://etd.repository.ugm.ac.id/home/detail_pencarian/99288,APLIKASI DISTRIBUSI GENERALIZED INVERSE GAUSSIAN (GIG) SEBAGAI PRIOR KONJUGAT PARETO UNTUK PERHITUNGAN PREMI REASURANSI,"TIYA OCTAVIANI, Dr. Gunardi, M.Si.",2016 | Skripsi | S1 STATISTIKA,"Asuransi di Indonesia saat ini semakin berkembang, begitu juga dengan asuransi kerugian. Semakin berkembangnya asuransi berpengaruh pada semakin meningkatnya jumlah polis sehingga klaim yang harus ditanggung juga mengalami peningkatan dari tahun ke tahun. Pada perusahaan asuransi, klaim dapat dipandang sebagai suatu kerugian yang dapat dimodelkan. Dengan permodelan klaim ini, maka dapat dilihat besarnya nilai resiko yang timbul pada masa mendatang. Salah satu cara untuk menangani risiko ini adalah dengan mentransfer risiko. Realisasi dari cara ini adalah dengan cara reasuransi, yaitu mengasuransikan klaim asuransinya ke suatu perusahaan reasuransi.  Distribusi kerugian pada data keuangan biasanya memiliki ekor kanan yang tebal (heavy-tailed). Salah satu distribusi yang tepat untuk memodelkan data tersebut adalah distribusi Invers Gaussian yang merupakan anggota dari keluarga Pareto. Perluasan dari distribusi Invers Gaussian adalah distribusi Generalized Inverse Gaussian (GIG). Distribusi ini merupakan distribusi dengan 3 (tiga) parameter dan mengandung fungsi Bessel pada fungsi probabilitasnya. Untuk mengestimasi parameter distribusi ini tidak bisa dilakukan secara mudah dengan estimasi likelihood atau metode momen. Oleh karena itu, estimasi Bayesian dilakukan untuk mengestimasi parameter pada distribusi ini dengan beberapa distribusi prior yang termasuk ke dalam keluarga Pareto. Selanjutnya, estimasi parameter ini akan digunakan dalam perhitungan premi reasuransi yang juga melibatkan nilai threshold (ambang batas optimum) serta probability tail dari data klaim.   Kata kunci : Distribusi Kerugian, Ekor Tebal, Reasuransi, Threshold, Probability Tail, Estimasi Bayesian, Generalized Inverse Gaussian, Premi Reasuransi.","Nowadays, the development of insurance in Indonesia grows bigger and bigger every day, including non-life insurance. If the insurance company grows bigger, the amount of insurance policy will grow more. So, the claim that should be handled by the company also increases from year to year. For an insurance company, the amount of claim that paid is one of loss distribution. Fitting loss distribution can be used to measure the risk that may appear in the future. One of the methods to manage the risk is transferring the risk to another company. Generally, it is called reinsurance method. Reinsurance is a way to manage the risk with insuring the claims to a reinsurance company.  Finance loss distribution usually has a heavy tail, so the distribution that proper to this distribution is heavy-tailed distribution, such as Inverse Gaussian Distribution. This distribution is a member of Pareto family. The Generalized Inverse Gaussian is a generalization of Inverse Gaussian Distributions that contains 3 (three) parameter and Bessel function. To estimate these parameters, it will be difficult using likelihood estimation or moment methods. To handle that matters, we can use Bayesian estimation to estimate these parameters with several prior distributions that come from Pareto family. Later, parameters from this estimation can be used to find reinsurance premium. This calculation involves threshold and probability tail.","Kata Kunci : Loss Distribution, Heavy-tailed, Reinsurance, Threshold, Probability Tail, Bayesian Estimation, Generalized Inverse Gaussian Distribution, Reinsurance Premium."
http://etd.repository.ugm.ac.id/home/detail_pencarian/96230,REGRESI RESPON ORDINAL,"ARKANDINI LEO H, Dr. Abdurakhman, M.Si.",2016 | Skripsi | S1 STATISTIKA,"Regresi respon ordinal dapat digunakan untuk melihat permodelan hubungan variabel dependen dan independen, dimana variabel dependennya mempunyai sifat kategorik bertingkat. Variabel respon ordinal yang diketahui dibentuk dari suatu variabel laten kontinu yang tidak diketahui nilainya. Nilai batas kategorik (threshold) pada variabel laten perlu diestimasi bersama-sama dengan koefisien regresi ordinal. Parameter regresi logistik ordinal didapat dengan menggunakan metode estimasi Maximum Likelihood kemudian dilanjutkan dengan menggunakan metode Newton Raphson. Pada studi kasus ini  menggunakan data penelitian tentang pengetahuan pencegahan efek buruk  kosumsi rokok. Dimana variabel dependennya adalah skor pengetahuan efek buruk rokok yang terkategori skor 0-7 sedangkan variabel independennya adalah pre test, metode CC, TV dan CCTV. Penelitian ini bertujuan memodelkan pengetahuan siswa dengan variabel-variabel independen tersebut.   regresi respon ordinal, Maximum Likelihood, Newton Raphson","Use of respon ordinal regression in this thesis is to estimate relationship between a  response variable and one or more explanatory variables which response variable is of ordinal scale. Categorical limit value (threshold) on the latent variables need to be estimated together with ordinal regression coefficients  by using  estimate the parameters. Parameter regression logistic ordinal is obtained by using Maximum Likelihood Estimation and then using Newton Raphson method. Research data in this case is about prevention of bad effect of cigarette. The dependent variable is score (0-7) about how far they know about the bad effect of cigarette, while the independent variables are pre test, CC, TV, and CCTV. This research used to categorize knowledge of bad effect of cigarette with those independent variables.","Kata Kunci : regresi respon ordinal, Maximum Likelihood, Newton Raphson"
http://etd.repository.ugm.ac.id/home/detail_pencarian/99052,ANALISIS NILAI MASA HIDUP PELANGGAN MENGGUNAKAN REGRESI KUANTIL BAYESIAN,"CORNELIA HEMA RETNANDA, Dr. Gunardi, M.Si. ; Yunita Wulan Sari, S.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Lingkungan bisnis yang kompleks menuntut perusahaan untuk dapat bersaing dengan para kompetitornya. Oleh karena itu, perlu adanya strategi perusahaan untuk mempertahankan hubungan dengan pelanggan yang menguntungkan dan membuat pelanggan yang kurang menguntungkan menjadi pelanggan yang menguntungkan. Salah satu metrik yang digunakan dalam mengelola hubungan dengan pelanggan adalah nilai masa hidup pelanggan. Nilai masa hidup pelanggan menggambarkan nilai sekarang dari aliran masa depan yang diharapkan sepanjang pembelian semasa hidup pelanggan. Dalam perhitungan nilai masa hidup pelanggan dibutuhkan kontribusi margin, tingkat diskonto dan tingkat retensi pelanggan.  	Dalam skripsi ini akan dilakukan estimasi nilai masa hidup pelanggan menggunakan regresi kuantil Bayesian. Regresi kuantil dapat digunakan untuk mengatasi keterbatasan regresi linear dalam menganalisis sejumlah data yang berbentuk lonceng tidak simetris dan regresi kuantil sangat berguna jika distribusi data tidak homogen. Regresi kuantil dapat diestimasi dengan menggunakan metode Bayesian. Metode Bayesian adalah metode analisis yang berdasarkan pada informasi dari sampel dan informasi prior. Gabungan informasi ini disebut distribusi posterior. Untuk mencari distribusi posterior seringkali menghasilkan perhitungan yang tidak dapat diselesaikan secara analitik sehingga digunakan pendekatan Gibbs sampling. Estimasi parameter dari model adalah mean dari distribusi posterior yang diperoleh dari proses Gibbs sampling tersebut. Dalam skripsi ini dibahas regresi kuantil menggunakan Asymmetric Laplace Distribution dari sudut pandang Bayesian.  	Studi kasus dalam skripsi ini membahas faktor apa saja yang mempengaruhi nilai masa hidup pelanggan. Hasil estimasi regresi kuantil dengan metode Bayesian akan dibandingkan dengan metode regresi kuantil. Selanjutnya diperoleh kesimpulan bahwa estimasi regresi kuantil Bayesian lebih baik dari estimasi regresi kuantil.","Complex business environment insists companies to be able to compete with its competitors. Therefore, the company needs strategies to maintain relationships with profitable customers and makes less profitable customers to be profitable customers. One of the metrics which is used to manage the relationship with the customer is customer lifetime value. Customer lifetime value describes the present value of the expected stream of future purchases throughout the lifetime of the customer. In calculating the customer lifetime value,it is required margin contribution, discount rate and customer retention rate. In this thesis the researcher will estimate the customer lifetime value using Bayesian quantile regression. Quantile regression can be used to overcome the limitations of linear regression in analyzing data that is not symmetric and quantile regression is useful if the distribution of data is not homogeneous. Quantile regression can be estimated using Bayesian methods. Bayesian method is a method of analysis which is based on information from the sample and prior information. This combined information is called the posterior distribution. To find the posterior distribution, it  often produces calculations that can not be solved analytically so that it is used Gibbs sampling approach. Estimation of the parameters of the model is the mean of the posterior distribution  that is obtained from that Gibbs sampling. In this thesis, it is discussed quantile regression using Asymmetric Laplace Distribution of Bayesianâ€™s viewpoint. The case study in this thesis discusses the factors that affect customer lifetime value. The estimation result of quantile regression using Bayesian method will be compared with quantile regression method. Furthermore, it is concluded that Bayesian quantile regression estimation is better than quantile regression estimation.","Kata Kunci : Nilai masa hidup pelanggan, Regresi Kuantil, Bayesian, Gibbs sampling, Asymmetric Laplace Distribution"
http://etd.repository.ugm.ac.id/home/detail_pencarian/103408,MODEL OPTIMASI PORTOFOLIO PROGRAM LINEAR STOKASTIK DUA TAHAP MEAN ABSOLUTE DEVIATION DENGAN BIAYA TRANSAKSI DAN TANPA SHORT SELLING,"RAHMAWATI S, Herni Utami, S.Si., M.Si.",2016 | Skripsi | S1 STATISTIKA,"Optimasi portofolio menjadi salah satu bidang penelitian yang penting dalam keuangan modern. Optimasi portofolio bertujuan memecah investasi ke dalam beberapa usaha dan mencari nilai optimal dari investasi tersebut. Karakter terpenting dalam masalah optimasi portofolio adalah ketidakpastian return di masa yang akan datang. Di bidang matematika, proses optimasi yang melibatkan ketidakpastian dapat diselesaikan dengan pemrograman stokastik. Sementara itu, dalam kasus optimasi untuk persamaan linear dapat diselesaikan dengan pemrograman linear stokastik yang digunakan sebagai dasar pemodelan pada skripsi ini. Representasi return  yang digunakan sebagai variable acak dalam masalah optimasi telah dimodelkan dalam sebuah masalah pemilihan portofolio dengan melibatkan biaya transaksi dan tanpa short selling sebagai masalah program linear stokastik dua tahap serta digunakan Mean Absolute Deviation sebagai ukuran risiko yang cocok dalam persamaan linear pada model ini. Model tersebut dibandingkan dengan beberapa model portofolio dengan ukuran risiko yang sama, diantaranya Mean Absolute Deviation Klasik dan Modified Mean Absolute Deviation menggunakan metode Index Sharpe dan beberapa grafik tingkat keuntungan. Dengan demikian, model ini menjadi salah satu penelitian yang signifikan dalam menentukan portofolio optimal.","Portfolio optimization has been one of important research fields in modern finance. Portfolio optimization aims to break down investment in some business and find the optimal value of the investment. The most important character within this optimization problem is uncertainty of future returns. In mathematics, the optimization process that involves uncertainties can be resolved with a stochastic programming. Meanwhile, in the case of optimization of linear equations can be solved with stochastic linear programming that is used as the basis for modeling in this thesis. The returnÃ¢ï¿½ï¿½s representation of random variable in optimization problems has been modeled a portfolio selection problem under transaction costs and without short selling involved as a matter of two-stage stochastic linear programming and Mean Absolute Deviation used as a measure of risk that fits in a linear equation in this model. The model is compared with some of the portfolio models with the same measure of risk, between Classical Mean Absolute Deviation Model and Modifed Mean Absolute Deviation Model using Index Sharpe method and Graph Profitability. Therefore, this model has been one of significant research fields in determining the optimal portfolio.","Kata Kunci : program linear stokastik, mean absolute deviation, optimasi portofolio, biaya transaksi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/107511,Estimasi Cadangan Klaim Individu di Asuransi Umum dengan Distribusi Skew-Normal Multivariat,"STELLA MARIA YOAN N, Dr. Adhitya Ronnie Effendie, M.Si., M.Sc.",2016 | Skripsi | S1 STATISTIKA,"Cadangan klaim yang akurat merupakan hal yang penting bagi perusahaan asuransi untuk dapat menyediakan pembayaran klaim di masa depan. Metode estimasi cadangan klaim tradisional biasanya didasarkan pada model tingkat makro dengan data agregat yang diringkas dalam segitiga run-off. Dalam beberapa tahun terakhir, beberapa literatur telah mengajukan model estimasi cadangan klaim berdasarkan data klaim individu dengan informasi terperinci yang disebut model tingkat mikro. Dalam skripsi ini, akan diajukan sebuah model estimasi cadangan klaim yang dirancang untuk klaim individu dalam waktu diskrit. Pendekatan parametrik digunakan untuk memodelkan kejadian klaim, serta waktu penundaan pelaporan, waktu hingga pembayaran pertama dan pembayaran-pembayaran yang dilakukan dalam perkembangan klaim. Model ini menggunakan faktor perkembangan seperti dalam metode chain-ladder. Distribusi skew-normal multivariat (MSN) digunakan untuk memodelkan faktor perkembangan. Performa dari model dievaluasi dengan membandingkan RMSEP cadangan klaimnya. Hasil yang didapatkan menunjukkan bahwa model estimasi cadangan klaim dengan distribusi MSN menghasilkan estimasi cadangan klaim yang lebih baik dibanding metode chain-ladder karena menghasilkan nilai error prediksi yang lebih kecil.","Accurate claims reserves are essential for an insurance company to be able to provide payments for future claim liabilities. Traditional claims reserving is usually based on macro-level models with aggregate data in a run-off triangle. In recent years, some literatures have proposed reserving models based on individual claims data with detailed information to estimate outstanding claim liabilites. These models are called micro-level models. This study proposes a claims reserving model designed for individual claims developing in discrete time. Parametric approach is used to model the occurence of claims, as well as their reporting delay, the time to the first payment, and the cash flows in the development process. This approach uses development factors similar to those of the chain-ladder method. Multivariate Skew Normal distribution is suggested as a multivariate distribution suitable for modeling these development factors. The performance of the models is evaluated by comparing the RMSEP of the reserve estimates. The results demonstrate that the skew-normal multivariate model outperforms the chain-ladder by generating reserve estimates with smaller prediction errors.","Kata Kunci : cadangan klaim, asuransi umum, distribusi skew-normal multivariat, chain-ladder, klaim individu"
http://etd.repository.ugm.ac.id/home/detail_pencarian/95480,UJI PENALIZED MAXIMAL F UNTUK MENDETEKSI PANJANG MUSIM KEMARAU DAN MUSIM PENGHUJAN,"PUTRI WIDYA DARU, Dr. Gunardi, M.Si",2016 | Skripsi | S1 STATISTIKA,Penalized Maximal F Test (PMFT) adalah uji yang digunakan untuk menentukan ada tidaknya pergeseran rata-rata pada waktu t=k dalam time series (X_t) dengan tren linear (Î²). Changepoint adalah suatu titik ubah yang menunjukkan suatu data dalam time series. Variabel Î¼_1 dan Î¼_2 merupakan besarnya perubahan pergeseran rata-rata sebelum dan sesudah terjadinya changepoint. PMFT bertujuan untuk mendeteksi ada tidaknya changepoint. Changepoint terjadi karena pergeseran rata-rata. Changepoint dapat terdeteksi karena adanya tren pada data.  Dalam skripsi ini akan dibahas cara mendeteksi panjang musim kemarau dan musim penghujan di Kabupaten Sleman dan Kulonprogo pada tahun 2012 sampai dengan 2014 menggunakan uji Penalized Maximal F. Hasil dari penelitian menunjukkan panjang musim kemarau dan hujan dari tahun 2012 hingga 2014 bahwa panjang musim hujan semakin pendek dan periode musim kemarau semakin panjang.,PMFT (Penalized Maximal F Test) is used to determine whether there is a mean shift at time t = k in the time series (X_t) using linear trend (Î²). Changepoint is a change point which show data of  a time series. Variable of Î¼_1 and Î¼_2 are the measurement of mean shift after and before changepoint. PMFT for detecting changepoint. Changepoint occurs because of mean shift. Changepoint can detected because of the trend of data.  In this final project will be discuss how to detect the length of dry season and rainy season in Sleman and Kulonprogo in 2012 through 2014 using Penalized Maximal F test. Results from the study indicated that dry season and rainy seasons from 2012 to 2014 the length of the rainy season is shorter than the length of dry season.,"Kata Kunci : shift, mean, changepoint, length, season"
http://etd.repository.ugm.ac.id/home/detail_pencarian/97785,CREDIT SCORING  MENGGUNAKAN REGRESI LOGISTIK LASSO,"AMALIA AFIFAH, Dr. Abdurakhman, M.Si ; Rianti Siswi U., S.Si., M.Sc",2016 | Skripsi | S1 STATISTIKA,"Credit scoring merupakan suatu metode berbasis analisis statistika yang digunakan untuk mengukur besaran resiko kredit. Metode klasifikasi yang paling populer digunakan untuk credit scoring adalah regresi logistik. Regresi logistik digunakan untuk memprediksi variabel respon yang biner dengan satu set variabel penjelas (prediktor). Regresi logistik mempunyai keterbatasan yaitu jika terdapat multikolinieritas (korelasi  yang  tinggi  antar  variabel  bebas) membuat model regresi yang didapat menjadi tidak lagi efisien karena nilai standar error koefisien regresi menjadi sangat besar (overestimate) atau dengan kata lain mengurangi akurasi dari estimasi. Oleh karena itu, diusulkan metode Least Absolute Shrinkage and Selection Operator  (LASSO) untuk mengatasi hal tersebut. LASSO akan menyusutkan koefisien (parameter Î²) yang berkorelasi, menjadi nol atau mendekati nol. Sehingga menghasilkan model akhir yang lebih representatif. Pada akhirnya, performa model regresi logistik LASSO dibandingkan dengan model regresi logistik. Dengan melihat nilai presentase ketepatan model, model regresi logistik LASSO dianggap lebih baik daripada model regresi logistik karena memiliki nilai presentase ketepatan model yang lebih besar.","Credit scoring is a method based on a statistical analysis is used to measure the amount of credit risk. The most popular method of classification used for credit scoring is logistic regression. Logistic regression was used to predict the response variable is a binary with a set of explanatory variables (predictors). Logistic regression have limitations, if there is multicollinearity (a high correlation between independent variables) obtained makes regression model becomes more not efficient because the value of the standard error of regression coefficient becomes very large (overestimate), or in other words reduce the accuracy of the estimate. Therefore, Least Absolute Shrinkage and Selection Operator (LASSO) method proposed to overcome it. LASSO would shrink coefficient (parameter Î²), which correlated, to zero or close to zero. Resulting in a final model more representative. In the end, the performance of the logistic regression model LASSO compared with logistic regression model. By looking at the value of the percentage of accuracy from the model, the logistic regression model LASSO considered better than the logistic regression model because it has larger percentage of of models accuracy.","Kata Kunci : credit scoring, multikolinearitas, regresi logistik, metode  LASSO"
http://etd.repository.ugm.ac.id/home/detail_pencarian/103418,EFISIENSI MODIFIED JACKKNIFE RIDGE REGRESSION ESTIMATOR UNTUK MENGATASI MASALAH MULTIKOLINEARITAS,"SUCI RAHMAWATI , Dr. Abdurakhman, S.Si., M.Si.",2016 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis statistika yang dilakukan untuk memodelkan hubungan antara variable dependen dan variable independen. Dalam asumsi yang terdapat pada analisis regresi klasik diantaranya adalah tidak adanya multikolinearitas., hal itu dapat menyebabkan hasil estimasi dengan menggunakan kuadrat terkecil menjadi tidak valid. Pada skripsi ini akan dibahas mengenai efisiensi dari metode Modified Jackknife Ridge Regression estimator untuk mengatasi masalah multikolinearitas yang dikembangkan oleh Feras Sh. M. Batah, Thekke V. Ramanathan and Sharad D. Gore (2008). Metode Modified Jackknife Ridge Regression ini merupakan pengembangan dari metode Jackknife Ridge Regression yang dikemukakan oleh Ozkale (2008), dan juga merupakan gabungan dari metode Generalized Ridge Regression dan Jackknife Ridge Regression. Studi kasus ini menggunakan data jumlah uang beredar di Indonesia dan faktor yang mempengaruhinya dari bulan Januari 2008 sampai Januari 2016. Diperoleh kesimpulan bahwa metode Modified Jackknife Ridge Regression lebih efisien digunakan untuk mengatasi masalah multikolinearitas dibandingkan dengan metode Generalized Ridge Regression dan Jackknife Ridge Regression dilihat berdasarkan kriteria MSEnya.","Regression analysis is a statistical analysis that used to perform model relationship between dependent variable and independen variables. One of the assumption in classical regression analysis is there is no multicollinearity. If there is multicollinearity in the regression model, it could cause the result of the model that using the method of Least Squares Estimator becomes invalid. This paper will discuss about the efficiency of Modified Jackknife Ridge Regression Method to resolve the problem of multicollinearity developed by Feras Sh. M. Batah, Thekke V. Ramanathan and Sharad D. Gore (2008). This method is extention of Jackknife Ridge Regression method pioneered by Ozkale (2008), and also a combination of Generalized Ridge Regression and Jackknife Ridge Regression method. This paper case study is using the amount of money circulating in Indonesia and the factors that affecting it from January 2008 until January 2016. The conclusion is That the Modified Jackknife Ridge Regression method more efficiently used to resolve the problem of multocollinearity than Generalized Ridge Regression and Jackknife Ridge Regression method, it seen by MSE criterion.","Kata Kunci : Multikolinearitas, Generalized Ridge Regression, Jackknife Ridge Regression, Modified Jackknife Ridge Regression, MSE."
http://etd.repository.ugm.ac.id/home/detail_pencarian/99838,PENENTUAN HARGA OPSI COMPLEX CHOOSER  DENGAN MODEL BLACK-SCHOLES,"NOVALIA P SINAGA , Prof.Dr.rer.nat Dedi Rosadi,. S.Si.M.sc",2016 | Skripsi | S1 STATISTIKA,"Opsi (option) adalah instrument finansial yang memberikan hak , bukan suatu kewajiban bagi pemegang kontrak (option holder) untuk membeli atau menjual aset tertentu kepada penjual kontrak (option writer) dalam jangka waktu yang telah ditentukan atau disepakati. Sebagai salah satu instrument turunan atau derivative di pasar modal, ada beberapa aset yang dapat melandasi opsi tersebut, yaitu saham, obligasi, mata uang, dan juga komoditi. Pihak-pihak yang terlibat dalam opsi adalah para investor dengan investor yang lainnya, dan tidak melibatkan perusahaan sekuritas yang dijadikan opsi. Opsi diterbitkan oleh investor untuk dijual kepada investor lainnya, sehingga perusahaan yang merupakan sekuritas dari saham yang dijadikan patokan tersebut, tidak mempunyai kepentingan dalam transaksi opsi tersebut. Chooser Option adalah jenis opsi yang mengijinkan pemegangnya (option holder) untuk memilih apakah menggunakan opsi beli (call option) atau opsi jual (put option) pada waktu yang disepakati. Opsi ini sering disebut as you like it options, dimana pemegang opsi dapat setelah beberapa waktu, memilih apakah opsi yang dipilih opsi beli atau opsi jual. Dasar (underlying) opsi pada chooser option biasanya European option dan mempunyai harga pelaksanaan (strike price) yang sama. Namun, berbeda halnya pada complex chooser option yang mempunyai harga pelaksanaan atau waktu jatuh tempo berbeda. Oleh karena itu, opsi ini akan lebih mahal daripada opsi yang individual misalnya hanya opsi beli atau opsi jual saja, karena pemegang opsi mempunyai hak untuk memilih.","Option is a financial instrument that gives the right but not an obligation for the holder of the contract ( option holder) to buy or sell a particular asset to the seller of the contract (option writer) within a predetermined time or agreed.One derivative instrument or derivative in the capital market , there are some assets that may underlie the option, ie stocks, bonds , currencies , and commodities . The parties involved in the options are the investors with other investors , and does not involve the company's securities are used as an option . Options issued by investors for sale to other investors, so companies which are securities of shares used as a benchmark does not have an interest in the option transaction . 	Chooser Option is an option that allows the holder ( option holder) to choose whether to use the call option )or a put option at the agreed time . This option is often called as you like it options , where the option holder can after some time , choose whether the selected option and purchase option or a put option . Basic (underlying ) option on chooser option typically European option and have an exercise price ( strike price ) the same . However , unlike the case in the complex which has a chooser option exercise price or different maturities . Therefore , this option would be more expensive than the options individually for example only the option of buying or selling an option, because the option holder has the right to choose .","Kata Kunci : Exotic option,complex chooser option, opsi Black Scholes"
http://etd.repository.ugm.ac.id/home/detail_pencarian/92673,PERMODELAN SISA USIA SEBAGAI VARIABEL RANDOM FUZZY,"TABAH WISNU VIAZTRI, Dr. Adhitya Ronnie E., M.Sc",2015 | Skripsi | S1 STATISTIKA,"Kunci utama dalam suatu perencanaan asuransi jiwa adalah variabel random dari sisa usia seseorang yang berusia x, yaitu T(x). Meskipun usia merupakan faktor penting dalam penentuan T(x), akan tetapi ada faktor lain yang juga relevan, seperti status kesehatan dan karakter hidup dari seseorang yang berusia x. Faktor-faktor lain ini seringkali dinyatakan dalam sistem fuzzy, seperti ""kurang dari rata-rata"". Dalam hal ini, T(x) mungkin lebih tepat apabila didefinisikan sebagai fungsi yang memberikan sebuah sub himpunan fuzzy untuk setiap kemungkinan hasil dari sebuah kejadian random, dan dinyatakan sebagai suatu variabel random fuzzy. Tujuan penelitian ini adalah untuk menyajikan beberapa pengamatan awal sehubungan dengan konseptualisasi sisa usia sebagai suatu variabel random fuzzy.","A key concept in life insurence planning is the random variable future lifetime of a life aged x, T(x). While age is an important factor in the determination of T(x), there are other relevant factor, like the state of health and the character of a life aged x. These latter factor often are encapsulated in a perceived fuzzy metric, like ""less than average future lifetime"". In these intances, T(x) might more appropriately be defined as a function that assigns a fuzzy subset to each possible outcome of a random event, and conceptualized as a fuzzy random variable. The purpose of this article is to presents some preliminary regarding this conceptualized of future lifetime as a fuzzy random variable.","Kata Kunci : sisa usia, variabel random fuzzy, fuzzy metric"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85251,GRAFIK PENGENDALI INDIVIDUAL NONPARAMETRIK DENGAN ESTIMATOR KERNEL UNTUK FUNGSI DISTRIBUSI KUMULATIF,"OVVIE JUNIA ASTARI, Dr. Danardono, MPH; Yunita Wulan Sari, S.Si, M.Sc",2015 | Skripsi | S1 STATISTIKA,"Salah satu alat yang digunakan untuk melakukan pengendalian kualitas statistik adalah grafik pengendali. Adakalanya data yang ditemukan tidak dapat dianalisis secara parametrik. Oleh karena itu, untuk mengatasi masalah tersebut digunakan analisis dengan grafik pengendali nonparametrik. Analisis nonparametrik dapat diestimasi dengan berbagai metode, salah satunya adalah estimasi kernel untuk fungsi distribusi kumulatifnya.  Dua hal yang sangat penting dari estimator kernel adalah pemilihan bandwidth dan fungsi kernel K. Dalam skripsi ini, metode pemilihan bandwidth yang digunakan adalah metode plug-in Altman and Leger dan fungsi kernel yang dipakai adalah epanechnikov.  	Skripsi ini mengaplikasikan grafik pengendali individual nonparametrik dengan estimator kernel untuk fungsi distribusi kumulatifnya untuk menganalisis kualitas produksi kekuatan benang jenis 30 RT Siro Kepyur Cop di PT PISMA PUTRA TEXTILE. Dengan membandingkan grafik pengendali nonparametrik ini dengan grafik pengendali individual Shewhart diperoleh bahwa grafik pengendali nonparametrik kernel menghasilkan analisis yang lebih baik.","One of the tools that used to perform statistical quality control is a control chart. Sometimes the data found can not be analyzed in parametric way. Therefore, to resolve the issue we use nonparametric control chart. Nonparametric analysis can be estimated by various methods, one of which is a kernel estimate for the cumulative distribution function. Two things that very important from the kernel estimator are the selection of the bandwidth and the kernel function K. In this paper, the method of selecting the bandwidth is the plug-in Altman and Leger, and the kernel function that used is epanechnikov. This thesis applies nonparametric individual control chart based on kernel estimator for the cumulative distribution function to analyze the quality of the yarn strength production type 30 RT Siro Kepyur Cop at PT PISMA PUTRA TEXTILE. By comparing this nonparametric control chart with Shewhart individual control chart, it shows that the nonparametric kernel control chart produce better analysis.","Kata Kunci : grafik pengendali individual, nonparametrik, estimator kernel untuk fungsi distribusi kumulatif, bandwidth, plug-in Altman and Leger, fungsi kernel epanechnikov."
http://etd.repository.ugm.ac.id/home/detail_pencarian/92420,PEMODELAN GENERALIZED PARETO UNTUK DATA KLAIM ASURANSI KENDARAAN BERMOTOR DENGAN METODE ESTIMASI TRIMMED-MOMENTS,"HIMATUL KHOIRIYAH, Dr. Gunardi M.Si; Vemmie Nastiti Lestari, S.Si, M.Sc",2015 | Skripsi | S1 STATISTIKA,"Asuransi di Indonesia semakin berkembang, termasuk juga asuransi kerugian. Dengan berkembangnya asuransi, jumlah nasabah pun semakin banyak sehingga klaim yang harus ditanggung perusahaan pun turut bergerak naik. Klaim yang ditanggung perusahaan dapat dipandang sebagai distribusi kerugian yang harus dimodelkan. Salah satu manfaat dari permodelannya adalah untuk mengukur resiko yang bisa timbul, yaitu dengan VaR. Loss Distribution pada data keuangan biasanya berekor gemuk sehingga salah satu model yang tepat adalah Generalized Pareto Distribution. Fitting Distribution dengan metode Makximum Likelihood tidak berbentuk closed form, sehingga dibahas Metode Trimmed-Moments","Insurance company in Indonesia grow bigger and bigger everyday, including non-life insurance.The amount of client increasing year by year, so the claim that should be handled by the company also increase. The amount of claim that paid by company is one of loss distribution. Fitting loss distribution can be used to measure the risk that may appear in the future. Value at Risk is one of the method to measure the risk from a lost function. Finance loss distribution usually have a heavy tail, so the distribution that proper to the condition is heavy-tailed distribution, such as Generalized Pareto Distribution (GPD). Fitting distribution using Maximum Likelihood give unclosed form result, so need a numeric solution. Beside that, there is a estimating method that can be used to fitting GPD. The method is Trimmed-Moments.","Kata Kunci : Trimmed-Moments, Generalized Pareto Distribution,Value at Risk, Fitting Distribution, Distribusi Kerugian,Ekor Gemu, Loss Distribution, Heeavy-tailed"
http://etd.repository.ugm.ac.id/home/detail_pencarian/92681,"PERAMALAN BANYAK KUNJUNGAN WISATAWAN KE PROVINSI NUSA TENGGARA BARAT (NTB) DENGAN MENGGUNAKAN IMPROVED GREY MODEL (1,1)","MUHAMMAD KHOIRUR R., Dr. Danardono, MPH.;Yunita Wulan Sari, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Proses peramalan adalah suatu  usaha untuk memperkirakan keadaaan di masa yang akan datang melalui kedaan di masa lalu. Peramalan dapat digunakan sebagai dasar dalam pengambilan berbagai keputusan. Selain itu, peramalan memberikan informasi agar masa depan dapat dikendalikan dan perubahan-perubahan dapat diperkirakan berdasarkan pola di masa lalu.  Namun sayangnya, seringkali data yang digunakan dalam proses peramalan sangat terbatas. Untuk mengatasi masalah tersebut, digunakan teknik peramalan yang disebut dengan Improved Grey Model (1,1) atau yang biasa disebut dengan IGM (1,1). Metode ini dapat digunakan untuk meramalkan data di periode yang akan datang untuk jumlah data yang tidak terlalu banyak. IGM (1,1) ini sendiri merupakan modifikasi dari Grey Model (GM) (1,1).","Forecasting is a process of predicting future conditions by using the data from the past. Forecasting can be used as a basis of decision making. Besides, it provides information that enables us to control future and to predict changes based on past patterns. Unfortunately, data used in forecasting process is often very limited. To overcome the problem, a forecasting technic named Improved Grey Model (1,1) - known as IGM (1,1) is used. This method can be applied to forecast data in the upcoming period, for small or medium amount of data. IGM (1,1) is a modification of Grey Model (GM) (1,1).","Kata Kunci : Peramalan, data terbatas, model peramalan Grey, nilai awal, nilai dasar, Improved Grey Model"
http://etd.repository.ugm.ac.id/home/detail_pencarian/92937,DISTRIBUSI DERET PANGKAT WEIBULL TERMODIFIKASI DAN TERGENERALISASI UNTUK PERMODELAN KETAHANAN HIDUP,"NANDA MEGA FELITA, Dr. Herni Utami, M.Si; Rianti Siswi Utami, S.Si., M.Sc",2015 | Skripsi | S1 STATISTIKA,"Permodelan ketahanan hidup adalah faktor penting dalam analisis survival. Salah satu distribusi yang sangat populer digunakan untuk permodelan ketahanan hidup secara parametrik adalah distribusi Weibull. Distribusi Weibull adalah distribusi yang sangat populer untuk permodelan data ketahanan hidup dengan fungsi hazard yang monoton. Meski demikian, distribusi Weibull bukan model parametrik yang layak untuk  memodelkan fenomena dengan fungsi hazard yang non-monoton seperti bentuk bathtub dan unimodal, yang mana biasanya terdapat dalam studi reliabilitas dan biologi. Sebuah distribusi baru dengan fungsi hazard menaik, menurun, berbentuk bathtub, dan berbentuk unimodal yang dinamai dengan distribusi deret pangkat Weibull termodifikasi dan tergeneralisasi (DPWMG) dibuat untuk memberikan solusi masalah tersebut. Distribusi tersebut dikonstruksikan sebagai gabungan dari distribusi Weibull termodifikasi dan tergeneralisasi (WMG) dan distribusi deret pangkat. Distribusi DPWMG memiliki sub model khusus diantaranya yaitu distribusi Poisson Weibull termodifikasi tergeneralisasi (PWMG), logaritma Weibull termodifikasi tergeneralisasi (LWMG), geometri Weibull termodifikasi tergeneralisasi (GWMG), dan binomial Weibull termodifikasi tergeneralisasi (BWMG). Kelebihan dari distribusi DPWMG terletak pada kemampuannya dalam memodelkan kegagalan yang monoton sebaik memodelkan kegagalan yang non monoton dalam reliabilitas.","Lifetime modelling is importance factor in survival analysis. One of many distributions that used for lifetime modelling with parametric fit is Weibull distribution. The weibull distribution is a very popular distribution for modelling lifetime data with monotone hazard rate. However, the Weibull distribution does not provide a reasonable parametric fit for modelling phenomenon with non-monotone hazard rate such as the bathtub-shape and unimodal which are common in reliability and biological studies.  A new distribution with increasing, decreasing, bathtub-shape and unimodal hazard rate forms called as the generalized modified Weibull power series (GMWPS) distribution is created to solve the problem. The new distribution is constructed by compounding general modified Weibull (GMW) and power series distributions. The new distribution contains, as special sub model such as generalized modified Weibull Poisson (GWMP), generalized modified Weibull logarithmic (GWML), generalized modified Weibull geometric (GMWG), and generalized modified Weibull binomial (GMWB). The beauty and importance of this distribution lies in its ability to model monotone as well as non-monotone failure rates in reliability.","Kata Kunci : distribusi Weibull, distribusi Weibull termodifikasi tergeneralisasi, distribusi deret pangkat, permodelan ketahanan hidup, uji goodness of fit, fungsi W Lambert, BFGS"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82699,Pendekatan Model Competing Risk dengan Hazard Cumulative Incidence Function (CIF),"NISAUL AUFA AF, Drs. Danardono, M.PH., Ph.D.",2015 | Skripsi | S1 STATISTIKA,"Data antar kejadian (data survival) merupakan data berupa lama waktu sampai suatu peristiwa (event) yang menjadi perhatian terjadi. Beberapa situasi adakalanya tidak sesuai dalam penerapan metode survival biasa. Salah satu situasi tersebut adalah ketika terjadi competing risk. Secara umum, situasi competing risk muncul ketika seorang individu dapat mengalami lebih dari satu jenis event, dan terjadinya event tersebut menghambat terjadinya jenis event yang lain. 	Seringkali, waktu terjadinya suatu event dalam competing risk dipengaruhi oleh satu atau beberapa variabel independen (kovariat). Analisis yang dapat digunakan yaitu analisis regresi hazard Cumulative Incidence Function (CIF). Pada model competing risk, koefisien regresi bersifat konstan dan nilainya tidak bergantung pada waktu (time). Metode yang digunakan untuk mengestimasi koefisien regresi pada model ini yaitu metode maximum partial likelihood seperti pada model regresi Cox. 	Dalam skripsi ini, analisis regresi hazard CIF dengan model competing risk digunakan untuk menganalisis variabel-variabel yang mempengaruhi lama waktu seorang pasien leukemia meninggal setelah melakukan transplantasi sumsum tulang belakang dikarenakan salah satu dari penyebab terjadinya seorang pasien tersebut meninggal, yang dirawat di European Society for Blood and Marrow Transplantation (EBMT). Diberikan grafik CIF yang digunakan untuk menginterpretasikan pengaruh dari masing-masing variabel independen serta untuk mengetahui resiko terbesar dari masing-masing penyebab terjadinya seorang pasien meninggal.","Time-to-event data is time length data until the event occurs. Some situations is not appropriate in the ordinary survival method application. One of the situations is the competing risk event. Generally, It occurs when an individual can face more than one type of events, and this event precludes the occurrence of another event. The times of event in the competing risk is effected by other independent variables (covariat). The analysis used is hazard regression analysis of Cumulative Incidence Function (CIF). In competing risk model, regression coefficients is fixed and the value does not depend on the time. The method used for estimate regression coefficients is maximum partial likelihood method similar with in Cox regression model. In this paper, CIF hazard analysis  regression with competing risk model is used to analyze some variables that affect time the leukemia patient death after bone marrow transplantation because one of causes type the patient death, that treated in European Society for Blood and Marrow Transplantation (EBMT). It is also presented that the CIF graphics used to interpret the affect from each independent variables and used to know the largest risk for every causes.","Kata Kunci : data antar kejadian, fungsi survival, fungsi hazard, competing risk, maximum partial likelihood, Cumulative Incidence Function (CIF), regresi Cox"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85517,VALUASI BIAYA UNTUK PILIHAN KONVERSI DARI POLIS ASURANSI JIWA BERJANGKA KE ASURANSI JIWA SEUMUR HIDUP,"ADE FEMIA PRISTYADI, Dr. Gunardi, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Salah satu keistimewaan pada asuransi jiwa berjangka adalah dapat diubah (convertible), pemegang polis dapat mengubah polis awal yaitu polis asuransi berjangka ke asuransi seumur hidup sebelum polis asuransi berjangka tersebut berakhir. Pemegang polis seringkali terdorong mengubah polis mereka rata-rata ketika kondisi kesehatan mereka menurun.  	Dalam skripsi ini akan dibahas valuasi biaya convertion option dengan mempertimbangkan perbaikan mortalitas yang digunakan sebagai dasar perhitungan. Untuk meramalkan tingkat mortalitas di masa depan, penulis menggunakan Lee Carter Model. Lee-Carter mengusulkan model untuk menggambarkan perubahan tingkat mortalitas sebagai fungsi dari indeks waktu. Untuk mengestimasi parameter dari Lee Carter Model digunakan metode Maximum Likelihood Estimation (MLE) dengan pendekatan distribusi poisson. Hasilnya biaya convertion option berdasarkan tabel mortalitas stochastic mempunyai nilai yang lebih rendah dibanding dengan biaya convertion option static.","One of the features in term life insurance can be changed (convertible), the policyholder may alter its initial policy to a whole life insurance policy before the term ends. Policyholders are often compelled to change their policy when their medical condition declined. 	In this thesis, we will discuss the valuation of the cost of conversion option which consider mortality improvement. Mortality improvement used as the basis for calculation. To predict future mortality rates, the authors use Lee Carter model. Lee-Carter proposed a model to describe the changes in the mortality rate as a function of time index. To estimate the parameters of Lee Carter model, it use Maximum Likelihood Estimation (MLE) with poisson distribution approach. The result of the convertion cost option based stochastic mortality table has a lower value than the static charge conversion option.","Kata Kunci : term life insurance discrete, conversion option, lee-charter models, maximum likelihood, newton unidimensional, whole life insurance discrete."
http://etd.repository.ugm.ac.id/home/detail_pencarian/92688,"OPTIMISASI DISCRETE GREY MODEL (1,1) SEBAGAI METODE PERAMALAN UNTUK UKURAN DATA KECIL","LAKSHMI NUR AZIZAH, Dr. Abdurakhman, M.Si",2015 | Skripsi | S1 STATISTIKA,"Peramalan merupakan dasar bagi perencanaan dan memiliki peranan  penting dalam proses perencanaan di waktu mendatang karena peramalan berguna untuk membantu para pengambil keputusan untuk melakukan antisipasi terhadap situasi yang akan terjadi di masa depan. Peramalan jangka pendek sangat diperlukan untuk menyelesaikan masalah tersebut. Namun, tak jarang data historis yang tersedia sulit diperoleh atau terbatas untuk melakukan suatu peramalan yang mengakibatkan hasil peramalan yang diperoleh tidak akurat. Grey Forecasting Model atau GM (1,1) merupakan model peramalan untuk data terbatas dengan menggunakan bentuk diferensial orde satu dan satu variabel. Pada skripsi ini, digunakan optimisasi Discrete Grey Forecasting Model atau DGM (1,1) yang merupakan salah satu bentuk dari Grey Forecasting Model. Optimisasi DGM (1,1) menggunakan bentuk diskrit baik dalam mengestimasi parameter maupun dalam mengestimasi nilai ramalan agar didapatkan hasil peramalan jangka pendek yang lebih akurat untuk jumlah data yang kecil atau terbatas.","Forecasting is the basis for planning and it plays an important role in the planning process for the future because forecasting can help decision makers to anticipate the situation that would occur in the future. Short term forecasting is very important to solves the problem. However, the historical data is hard to get and limited makes inaccurate forecast. Grey Forecasting Model or GM (1,1) is one of forecasting model with first order differential equation and one variable. In this thesis, used optimization of  Discrete Grey Forecasting Model or DGM (1,1) which is one form of Grey Forecasting Model. Optimization of DGM (1,1) using either discrete form in estimating the parameters as well as in estimating the value of forecasts in order to obtain short-term forecasting results more accurate for a small dataset.","Kata Kunci : small dataset, Grey Forecasting Model, Discrete Grey Model, optimisasi Discrete Grey Model, short-term forecasting."
http://etd.repository.ugm.ac.id/home/detail_pencarian/85779,IDENTIFIKASI PELANGGAN POTENSIAL PRODUK ASURANSI DENGAN TEKNIK KLASIFIKASI,"ADHITYA AKBAR, Drs. Zulaela, Dipl.Med.Stats., M.Si.",2015 | Skripsi | S1 STATISTIKA,"Pelanggan merupakan faktor keberhasilan yang utama dalam dunia bisnis. Pelanggan juga merupakan aset yang berharga yang harus dipertahankan dan ditingkatkan jumlahnya demi keuntungan yang maksimal. Untuk dapat meningkatkan jumlah pelanggan, sebuah perusahaan/badan usaha harus dapat mengidentifikasi dengan cermat orang-orang (pasar) yang berpotensi menjadi pelanggan barunya.  Identifikasi pelanggan potensial dimaksudkan agar kinerja perusahaan dapat efisien dalam menjaring calon pelanggan baru (akuisisi).   Klasifikasi merupakan salah satu teknik data mining yang dapat melakukan hal tersebut. Dua buah model klasifikasi menggunakan algoritma Decision Tree dan Naive Bayes yang dibandingkan tingkat akurasinya dalam memprediksi pelanggan potensial. Hasil penelitian menunjukkan Decision Tree sebagai model klasifikasi yang lebih baik dari Naive Bayes pada kasus nasabah bank yang berpotensi untuk membeli produk asuransi.","Customer is the main success factor in the business world. Customer also as the valuable asset which has to be retained and to be increased by it's number to get the maximum profit. To increase the number of it's customers, a company/enterprise has to identify the people (market) who have the potential to become their new customers. Identifying the potential customers has the function to get the candidates of the new customers efficiently (acquisition).  Classification is one of the data mining techniques which can do that thing. Two classification models using Decision Tree and Naive Bayes algorithm have been compared for it's accuracy rate to predict the potential customers. This research shows Decision Tree as the classification model that better than Naive Bayes in case of the bank's customers which have the potential to buy the insurance product.","Kata Kunci : Pelanggan Potensial, Data Mining, Klasifikasi, Decision Tree, Naive Bayes"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85268,REGRESI RELATIVE SURVIVAL DENGAN METODE GENERALIZED LINEAR MODEL POISSON (GLMP),"FATHONI FAWZI IDRIS, Dr. Abdurakhman, S.Si.,M.Si.",2015 | Skripsi | S1 STATISTIKA,"Secara teoritis, nilai relative survivaldidefinisikan sebagai rasio dari proporsi tahan hidup observasi (observed survival) dengan tahan hidup harapan (expected survival) dalam sebuah populasi. Fungsi relative survivaldigunakan ketika informasi penyebab kematian tidak akurat atau tidak ada. Untuk mengestimasi  relative survival  bisa menggunakan metode life table  yaitu metode Ederer I, Ederer II, dan Hakulinen, selain  itu  bisa menggunakan model regresi  relative survival, yaitu dengan metode  Generalized Linear Model poisson,  Esteve et al. full likelihood approach, dan Hakulinen-Tenkanen. Pada skripsi ini akan digunakan metode  Generalized Linear Model poisson untuk mengestimasi relative survivalpada data pasien Acute Myocardial Infarction (AMI) di Slovenia selama tahun 1982 sampai 1986.","Theoretically, the relative survival rate is defined as the ratio of the observed survival proportion with the expected survival in a population. Relative survival function is used when the cause of death information is inaccurate or not available. To estimate the relative survivalcould use the life table method is the method of Ederer I, Ederer II, and Hakulinen, besides it can use relative survival regression models, namely the Generalized Linear Model poisson, Esteve et al. full likelihood approach, dan Hakulinen-Tenkanen. In this paper will useGeneralized Linear Models poisson method to estimaterelative survival in patients data Acute Myocardial Infarction (AMI) in Slovenia during 1982 untill1986.","Kata Kunci : relative survival, generalized linear model poisson."
http://etd.repository.ugm.ac.id/home/detail_pencarian/88854,GRAFIK PENGENDALI MULTIVARIATE EXPONENTIALLY WEIGHTED MOVING AVERAGE (MEWMA) BERDASARKAN ARL DENGAN PENDEKATAN RANTAI MARKOV,"IGA AYU WIRATIH, Dr. Abdurakhman, S.Si., M.Si. ; Rianti Siswi Utami, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Grafik pengendali kualitas adalah salah satu alat yang paling berpengaruh dan banyak digunakan untuk mendeteksi perilaku yang menyimpang dalam proses industri. Salah satu grafik pengendali yang efisien digunakan untuk mendeteksi pergeseran yang kecil serta untuk mengendalikan proses dari dua atau lebih variabel yang berhubungan secara bersama-sama adalah grafik pengendali Multivariate Exponentially Weighted Moving Average (MEWMA). Pengukuran performa dari grafik MEWMA didapat melalui pendekatan rantai Markov dalam penentuan nilai average run length (ARL). ARL merupakan jumlah rata-rata titik atau sampel yang harus digambarkan sebelum sebuah titik atau sampel menyatakan suatu keadaan tidak terkendali. Pada skripsi ini, grafik pengendali MEWMA akan diaplikasikan pada pengendalian kualitas produksi air minum dalam kemasan di PT Tirtamas Lestari.","Quality control chart is one of the tools of the most influential and widely used to detect the deviant behavior in industrial process. One of the control charts that efficiently used to detect small shift and to control the process of two or more variables are Multivariate Exponentially Weighted Moving Average (MEWMA) control chart. The measurement of MEWMA control chart is obtained through Markov chain approach in determining the value of average run length (ARL). ARL is the average number of points or samples should be drawn before a sample shown an out of control condition. In this study, MEWMA control chart will be applied in quality control of water production in PT Tirtamas Lestari.","Kata Kunci : grafik Multivariate EWMA, Average run length, grafik pengendali, pendekatan rantai Markov"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82967,Analisis Bayesian untuk Regresi Binari Kuantil Terpenalti dengan Menggunakan Algoritma Gibbs Sampling,"DEVI NOVIYANTI RAHAYU, Dr. Gunardi, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Regresi  binari  kuantil  terpenalti  merupakan  perluasan  dari  regresi  kuantil  di  mana  variabel  dependennya  berskala  biner/dikotomus.  Regresi  ini  digunakan  untuk  menganalisis data yang mengandung pencilan dan heterokedastisitas.  Regresi binari kuantil  dengan  penalti  LASSO  (Least Absolute Shrinkage and  Selection Operator)  akan  menghasilkan estimasi parameter yang lebih tahan (robust)  terhadap  data  yang  mengandung  pencilan,  dapat  mengecilkan  galat,  serta  dapat  mengidentifikasikan variabel-variabel prediktor yang penting untuk variabel respon. Regresi  binari  kuantil  dengan  penalti  LASSO  dapat  diestimasi  dengan  menggunakan  metode  Bayesian.  Metode  Bayesian  adalah  metode  analisis  yang  berdasarkan  pada informasi yang berasal dari sampel dan informasi prior. Gabungan  informasi  ini  disebut  posterior.  Untuk  mencari  distribusi  posterior  seringkali  menghasilkan  perhitungan  yang  tidak  dapat  diselesaikan  secara  analitik  sehingga  digunakan pendekatan  Gibbs  sampling.  Estimasi  parameter  dari model adalah mean  dari distribusi posterior  yang  diperoleh dari proses  Gibbs sampling  tersebut. Dalam  skripsi ini dibahas regresi binari kuantil terpenalti menggunakan  Asymmetric Laplace Distribution dari sudut pandang Bayesian.  Dalam  skripsi  ini,  regresi  binari  kuantil  Bayesian  terpenalti  diaplikasikan  untuk  menganalisis  faktor-faktor  yang  mempengaruhi  kualitas  air  sungai  di  Kabupaten  Bantul.  Hasil  estimasi  regresi  binari  kuantil  Bayesian  terpenalti  akan  dibandingkan dengan regresi  logistik  dan regresi probit.  Dengan berdasarkan akurasi  model  dari  ketiga  metode  yang  digunakan,  diperoleh  kesimpulan  bahwa  estimasi  regresi binari  kuantil  Bayesian  terpenalti  lebih baik dari estimasi regresi  logistik  dan  regresi probit karena menghasilkan model yang lebih presisi.","Penalized  binary  quantile  regression  is  an  extension  of  quantile  regression  where the dependent variable scale is  binary/dichotomous. This regression is  used to  analyze the data containing outliers and heterocedastisity. Binary  quantile  regression  with  LASSO  (Least  Absolute  Shrinkage  and  Selection  Operator)  penalty  will  produce  estimation  of  parameters  that  more  resistant (robust) with data containing outliers, can shrink the  error, and can identify  which variables are important for the different quantile of the response variables. Binary  quantile  regression  with  LASSO  penalty  can  be  estimated  using  Bayesian  methods.  Bayesian  method  is  a  method  of  analysis  that  is  based  on  information  derived  from  the  sample  and  prior  information.  This  combined  information is called the posterior. To find the posterior distribution is often produce  calculations  that  can  not  be  solved  analytically  so  used  Gibbs  sampling  approach.  Estimation of the parameters of the model is the mean of the posterior distribution  obtained from the Gibbs sampling. In this  thesis  discussed  penalized  binary quantile  regression using Asymmetric Laplace Distribution of Bayesian viewpoint. The case study in this thesis,  Penalized  Bayesian binary  quantile  regression  was  applied  to  analyze  the  factors  that  affect  water  quality  in  Bantul.  Penalized  Bayesian  binary  quantile  regression  estimation  results  will  be  compared  with  the  logistic regression  and probit regression. With  a model based on the accuracy of  the  methods  are  used,  it  is  concluded  that  the  Penalized  Bayesian  binary  quantile  regression estimation better than logistic and probit regression estimation, because it  produces more precise models.","Kata Kunci : Regresi  Binari  Kuantil,  Penalti  Lasso,  Bayesian,  Gibbs  sampling,  Asymmetric Laplace Distribution"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85016,PENENTUAN HARGA PREMI ASURANSI LAHAN PERTANIAN BERBASIS INDEKS CURAH HUJAN MENGGUNAKAN PENDEKATAN OPSI,"URAY MARSYA AGLINA, Dr. Gunardi, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Sektor pertanian memiliki kontribusi yang cukup tinggi terhadap peningkatan Produk Domestik Bruto (PDB) sehingga dianggap cukup mampu untuk menjaga kestabilan dan ketahanan ekonomi suatu negara. Peningkatan dalam sektor pertanian akan diikuti dengan meningkatnya minat investor dalam melakukan investasi dalam bidang pertanian. Akan tetapi, investasi  dalam sektor pertanian memiliki risiko yang cukup tinggi dan tidak dapat diprediksi, terutama jika terjadi risiko alam seperti cuaca dan iklim. Hal ini yang akan mendorong para investor untuk mencari perlindungan dalam meminimalkan risko, salah satu cara yang digunakan adalah dengan membeli asuransi lahan pertanian untuk perlindungan terhadap lahan pertanian yang dimiliki serta pendapatan yang masuk dari hasil produksi jika terjadi kegagalan panen. 	Asuransi lahan pertanian berbasis curah hujan menjadi salah satu pilihan dalam meminimalkan risiko yang akan diterima jika terjadi bencana di luar dari prediksi para petani maupun investor petanian, terutama karena perubahan cuaca yang ekstrim. Penentuan harga premi dengan pendekatan opsi menjadi salah satu cara yang efektif untuk digunakan dalam pembentukan produk asuransi lahan pertanian dengan aset dasar berupa curah hujan. Asuransi lahan pertanian dengan indeks curah hujan memberikan konsep menarik tanpa memerlukan persyaratan yang rumit untuk mendapatkan produk asuransi sehingga memudahkan bagi semua kalangan dalam pembelian produk asuransi.","The agricultural sector has a relatively high contribution to the increasing in Gross Domestic Product (GDP) that is considered quite able to maintain the stability and resilience of countryÃ¢ï¿½ï¿½s economy. An increase in agricultural sector will be followed by the increasing interest of investors in investing in agriculture. However, investment in agriculture has a fairly high risk and unpredictable, especially in the event of natural risks such as weather and climate. This is will encourage the investors to seek refuge in minimizing risks, one way used is to buy crop-yield insurance for protection of their farms as well as revenue coming from production in case of crop failure. 	Crop insurance based on rainfall is one option to minimize the risks that would be acceptable in case of disaster prediction outside of farmers and investors in agricultural, mainly due to extreme changes in weather. Pricing the premium price using option approach to be one effective way to construct product of crop-yield insurance with rainfall as underlying asset. Crop-yield insurance based rainfall index provides an interesting concept without complexity requirements to obtain an insurance product, making it easier for all people to buy insurance products.","Kata Kunci : pertanian, investasi, risiko, cuaca, curah hujan, asuransi, opsi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/81438,DISTRIBUSI MAJEMUK DENGAN JUMLAHAN BOREL DAN FUNGSI REKURSIFNYA UNTUK PEMODELAN TOTAL BESAR KLAIM ASURANSI,"I G N PUTRA PRATAMA, Dr. Adhitya Ronnie Effendie, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Total besar klaim asuransi adalah jumlah dari seluruh klaim yang diajukan dalam suatu periode. Total besar klaim dapat dimodelkan sebagai distribusi majemuk yang melibatkan distribusi frekuensi dan distribusi severity. Distribusi frekuensi adalah distribusi dari banyaknya klaim yang diajukan dalam suatu periode sedangkan distribusi severity adalah  distribusi dari besarnya masing-masing klaim yang diajukan tersebut. Pada skripsi ini dibahas mengenai distribusi majemuk Poisson, Bartlett, dan Delaporte dengan jumlahan Borel yang dapat digunakan untuk memodelkan distribusi frekuensi. Pembentukan distribusi majemuk dengan jumlahan Borel ini menggunakan teori probabilitas dan kombinatorik. Dikembangkan juga suatu fungsi rekursif yang bertipe Panjer untuk menghitung fungsi massa probabilitas dari total besar klaim asuransi. Studi kasus dari skripsi ini menggunakan data klaim pada tahun 2012 dari salah satu perusahaan asuransi kendaraan bermotor di Indonesia. Estimasi parameter dari distribusi majemuk dengan jumlahan Borel menggunakan metode maksimum likelihood. Dari hasil uji kecocokan distribusi, hanya distribusi majemuk Poisson dan Bartlett dengan jumlahan Borel yang layak digunakan untuk memodelkan distribusi frekuensi. Berdasarkan perhitungan secara rekursif dengan menggunakan distribusi majemuk Poisson dan Bartlett dengan jumlahan Borel sebagai distribusi frekuensi, diperoleh bahwa distribusi probabilitas dari total besar klaim asuransi yang dihasilkan relatif sama. Nilai ekspektasi dari total besar klaim asuransi dalam satu hari adalah sebesar 9,8 juta rupiah dengan standar deviasi sebesar 11,2 juta rupiah.","Total size of insurance claims is the sum of all claims filed in a period of time. Total size of claims can be modeled as a compound distribution involving frequency distribution and severity distribution. Frequency distribution is the distribution of the number of claims filed in a period of time, while severity distribution is the distribution of the amount of each claims submitted. This thesis discussed Poisson, Bartlett, and Delaporte compound distribution with Borel summands which can be used to model frequency distribution. Formation of compound distribution with Borel summands used probability and combinatoric theory. It was developed later a Panjer type recursive function to calculate the probability mass function of total size of insurance claims. Case study of this thesis used claim data in 2012 from a motor vehicle insurance company in Indonesia. Estimation of the parameters of the compound distribution with Borel summands used maximum likelihood method. Based on the results of these estimations, it was determined the total size of insurance claim distribution with recursive function. From the result of goodness of fit test, concluded that only Poisson and Bartlett compound distributions with Borel summands are proper to used to model the frequency distribution. Based on the calculation recursively by using Poisson and Bartlett compound distributions with Borel summands as frequency distribution, obtained that the probability distribution of the total size of insurance claims produced relatively similar. Expectation value of the total size of insurance claims in one day is 9.8 million rupiahs with a standard deviation of 11.2 million rupiahs.","Kata Kunci : distribusi Borel, distribusi majemuk, distribusi Poisson, distribusi Bartlett, distribusi Delaporte, rekursif Panjer, klaim asuransi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/88357,Model Regresi Mixed Effect Zero-Inflated Poisson,"AMALIA FAJAR ARIANI, Dr. Danardono, MPH",2015 | Skripsi | S1 STATISTIKA,Model regresi mixed effect Zero-Inflated Poisson merupakan model regresi yang digunakan untuk memodelkan data pengamatan berulang atau longitudinal berupa cacah yang mengalami kelebihan nilai nol. Mixed ZIP model terdiri dari regresi Poisson dan regresi logistik dengan tambahan efek random pada model regresi Poissonnya. Estimasi parameter model regresi mixed effect Zero-Inflated Poisson diestimasi dengan metode Maximum Likelihood Estimator (MLE) yang menggunakan algoritma Ekspektasi Maksimalisasi (EM) untuk mendapatkan parameter MLE-nya. Pemilihan model terbaik dilakukan dengan cara membandingkan nilai MSE (Mean Square Error) dengan model regresi cacah lainnya. Model mixed effect Zero-Inflated Poisson diaplikasikan pada data tentang jumlah serangga silverleaf whiteflies belum dewasa pada tanaman berdaun merah. Hasilnya menunjukkan model ini memiliki MSE terkecil.,Mixed Effect Zero-Inflated Poisson regression model is a regression model which used for modeling repeated measures or longitudinal data of count responses with excess zeros. Mixed ZIP model consists of Poisson regression and logistic regression with additional random effect in its Poisson regression model. Parameter estimation for mixed effect Zero-Inflated Poisson regression model is estimated by Maximum Likelihood Estimator (MLE) which uses Expectation Maximization (EM) algorithm for getting the MLEâ€™s parameter. The best model selection is evaluated by comparing the MSE (Mean Square Error) score with other discrete regression models. Mixed effect Zero-Inflated Poisson model is applied in number of immature silverleaf whiteflies on poinsettia data. The result showed this model has the lowest MSE.,"Kata Kunci : kelebihan nol/ excess zero, model campuran/ mixed effect, mixed effect Zero-Inflated Poisson"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85545,GRAFIK PENGENDALI MULTIVARIAT ROBUST DENGAN ESTIMATOR MINIMUM COVARIANCE DETERMINANT UNTUK OBSERVASI INDIVIDU,"IKE FIANDRA AMRILIAN, Dr. Gunardi, M.Si.",2015 | Skripsi | S1 STATISTIKA,Untuk memonitor proses multivariat biasa digunakan grafik pengendali multivariat Hotellings Tsquare. Namun biasanya dalam suatu proses terdapat adanya outlier yang dapat merusak estimasi batas pengendali dan menyebabkan terjadinya masking dan swamping. Untuk mengatasi permasalahan tersebut dilakukan analisis pengendalian proses dengan grafik pengendali multivariat robust dengan estimator minimum covariance determinant (MCD) untuk observasi individu. Estimator MCD memiliki breakdown point yang tinggi dan bersifat affine equivariant sehingga kuat terhadap outlier. Skripsi ini mengaplikasikan grafik pengendali multivariat robust untuk observasi individu untuk menganalisis kualiatas produk solar yang diproduksi oleh unit Kilang Pusdiklat Migas Cepu. Dengan membandingkan grafik pengendali multivariat robust dengan grafik pengendali multivariat standar diperoleh bahwa grafik pengendali multivariat robust menghasilkan estimasi batas pengendali yang lebih baik.,"To monitor a multivariate process , a Hotellings Tsquare control chart is often used. However  it is not rarely happens that a process contains outlying observations that can destroy control limit estimation and leads to masking and swamping effect. To  solve this problem we use process control analysis using robust multivariate control chart using minimum covariance determinant estimator for individual observation. MCD estimator has high breakdown point and affine equivariant so it become highly robust estimator that can fight outlier observation without destroy limit control estimation. This thesis applies  robust multivariate control chart using minimum covariance determinant estimator for individual observation to analyze diesel fuel  production at fuel factory unit Pusdiklat Migas Cepu.  By comparing robust multivariate control chart with standar multivariate control chart, it shows that robust multivariate control chart gives  better control limit.","Kata Kunci : grafik pengendali individu, multivariat, robust, minimum covariance determinant, grafik pengendali multivariat, FAST-MCD"
http://etd.repository.ugm.ac.id/home/detail_pencarian/92974,MODEL REGRESI DIAGONAL INFLATED BIVARIATE POISSON PADA DATA OLAHRAGA,"ARISDYAN PRASETYA, Drs. Zulaela, Dipl.Med.Stats; Rianti Siswi U., S.Si., M.Sc",2015 | Skripsi | S1 STATISTIKA,"Model bivariat Poisson muncul pada konteks yang luas dalam perhitungan data berpasangan. Selain dalam bidang olahraga, model ini juga dapat diterapkan dalam berbagai bidang lain seperti bidang pemasaran, epidemiologi, kesehatan dan ekonometri serta masih banyak lagi bidang lainnya. Model distribusi Poisson bivariat digunakan untuk memodelkan data olahraga terutama olahraga sepakbola. Distribusi Poisson independen biasanya digunakan untuk memodelkan jumlah gol dari dua tim yang bersaing dalam suatu pertandingan. Salah satu masalah yang muncul adalah model bivariat Poisson hanya memodelkan data yang berkorelasi positif saja sehingga untuk data yang berkorelasi negatif masih belum dapat ditangani dengan model ini. Selain itu dalam sifat distribusi Poison tidak dapat memodelkan data yang overdispersi atau underdispersi.  Overdispersi berarti bahwa nilai variansinya melebihi nilai meannya begitu juga untuk underdispersi yang berarti bahwa nilai variansinya lebih kecil dari nilai meannya. Maka dari itu dipertimbangkan suatu model campuran dari model bivariat Poisson, yaitu model diagonal inflated bivariate Poisson (DIBP). Model DIBP juga dapat dengan mudah dilakukan komputasinya. Lebih lanjut tugas akhir ini akan membahas mengenai proses estimasi dan pembentukan model DIBP dengan menggunakan algoritma EM.","Bivariate Poisson models appear in a broad context in the calculation of the data pairs. In addition to the sports field, this model can also be applied in various other fields like marketing, epidemiology, health and econometrics and still many other fields. Bivariate Poisson distribution model is used to model the sports data especially football. Independent Poisson distribution is usually used to model the number of goals of two teams competing in a game. One problem that arises is bivariate Poisson models only model the data so as to be positively correlated but for negatively correlated data still can not be addressed by this model. Besides the Poison distribution properties can not be modeled data that is overdisperssion or underdisperssion.  Overdisperssion means that the variance value exceeds the mean value as well as for underdispersi which means that the variance value is smaller than the mean value. Therefore considered a model mixture of bivariate Poisson models, namely the model diagonal bivariate inflated Poisson (DIBP). DIBP models can also easily to computing. Furthermore, this thesis will discuss the process of estimation and model building DIBP using EM algorithm.","Kata Kunci : Overdispersi, Diagonal inflated, Regresi Poisson bivariat, Data Olahraga, Overdispersion, Diagonal inflated, Bivariate Poisson Regression, Sport data"
http://etd.repository.ugm.ac.id/home/detail_pencarian/90162,Metode Alternatif Analisis Variansi untuk Data dengan Variansi Heterogen,"PRIO ARIF BUDIMAN, Sri Haryatmi, Prof, Dr, M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Uji F klasik untuk menguji mean beberapa populasi membutuhkan asumsi data independen antar level faktor, kesamaan variansi antar level faktor dan normalitas data. Ketika asumsi-asumsi diatas khususnya untuk asumsi kesamaan variansi tidak terpenuhi, Uji F menjadi tidak robust dalam pengambilan keputusan. Hal ini menjadi masalah yang serius jika terus diterapkan, khususnya ketika digunakan untuk sampel kecil. Untuk mengatasi permasalahan ini digunakan dua metode uji alternatif untuk analisis variansi yaitu Uji Welch dan Uji Bootstrap Parametrik. Selanjutnya kedua metode alternatif ini dibandingkan kinerja dalam olah data riil dan studi simulasi perbandingan metode uji berdasarkan type I error rate dengan kombinasi parameter serta sampel yang bervariasi","The classical F-test to compare several population means depend on the assumption of equality of variance of the population and the normality. When these assumption especiality the equality of variance is dropped, the classical F-test be not robust for decision making. This can be considered a serious problem in some applications, especially when the sample is not large. To deal with this problem, a number of tests are available in the literature. In this study Welch and Parametric Bootstrap tests are alternative method of analysis variance that introduced and a simulation study is performed to compare these tests according to type I error rates in different combinations of parameters and various sample sizes.","Kata Kunci : Analisis Variansi, Variansi Heterogen, Welch, Bootstrap Parametrik, Type I Error Rate"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82229,Metode Sequential Cokriging untuk Analisis Geostatistika (Studi Kasus: Estimasi Kandungan Besi di daerah Morowali),"LUSIANA ROULINA SINAGA , Dr. Abdurakhman, S.Si., M.Si;Dr. Gunardi, M.Si",2015 | Skripsi | S1 STATISTIKA,"Kriging adalah suatu metode perhitungan untuk mengestimasi besarnya nilai yang mewakili  suatu  titik  yang  tidak  tersampel  berdasarkan  titik-titik  tersampel  yang  berada disekitarnya  dengan  mempertimbangkan  korelasi  spasial  yang  ada. Sequential kriging setara dengan simple kriging, dan sequential cokriging merupakan perluasan dari sequential kriging. Metode sequential cokriging adalah metode dengan  memperhitungkan  pengaruh  co-variable yang berada di sekitar principle variable. Dalam  tersedianya  suatu  data  tambahan,  estimator  sekuensial  memperbaiki  estimasi  sebelumnya dengan menggunakan bobot  linier dari data  yang  baru dan  estimasi sebelumnya di suatu lokasi.  Studi kasus yang digunakan yaitu data kandungan besi yang dimiliki PT. Pamapersada yang terdapat di daerah Morowali. Hasil estimasi 43 titik yang tidak tersampel pada koordinat absis 7167.41-8150, ordinat 52778.56-54900, dan kedalaman 311.71-493.26 menunjukkan bahwa rata-rata estimasi kandungan besi yaitu 23.13%.","Kriging is a computation method for estimating the value that represents a point that is unsample based on sample points  that are surrounding by considering  the  spatial correlation that exists  on  the data. Sequential  kriging  is  equivalent  to the simple  kriging and sequential cokriging is the expansion of sequential kriging. Sequential cokriging method is method that can be used to estimate by taking into calculate the influence of its co-variable that exist around the data. In  the availability  of  additional  data,  sequential  estimators  improve  previous  estimates using a linear weighting of new data and the previous estimate of a location  Case studies are used that the data content of iron contained in the area of Morowali. The estimation results of 43 points which are not sampled at coordinates abscissa, ordinate, and depth 7167.41 - 8150, 52778.56 - 54900, and 311.71 - 493.26  showed that the average content of iron is estimated 23.13% .","Kata Kunci : data spasial,geostatistika,kriging,cokriging,sequential cokriging"
http://etd.repository.ugm.ac.id/home/detail_pencarian/81722,ANALISIS TEKNIKAL SAHAM MENGGUNAKAN VARIABLE INDEX DYNAMIC AVERAGE (VIDYA) DENGAN EKSPONENTIAL WEIGHTED MOVING AVERAGE,"HANNA VAN HELLEN, Dr. Abdurakhman, S.Si., M.Si.",2015 | Skripsi | S1 STATISTIKA,"Suatu ruang lingkup keuangan selalu dikaitkan dengan perubahan yang terjadi secara kontinu dan terus menerus setiap harinya, sama halnya dengan pasar perdagangan dunia yang selalu bersifat dinamis oleh pergerakan pedagang yang menyesuaikan perubahan persepsi dan para partisipan terkait. Dengan adanya asumsi tersebut, dibutuhkan suatu indikator dinamis yang dapat mengubah periode waktu dengan cara menganalisis aksi pasar yang terjadi. Variable Index Dynamic Average (VIDYA) merupakan salah satu indikator yang secara dinamis dapat mengikuti pergerakan harga saham. VIDYA membutuhkan bobot dalam metode perhitungannya. Dalam skripsi ini, bobot EWMA digunakan untuk menentukan bobot VIDYA. Dalam perhitungan VIDYA dapat digunakan tiga metode berbeda, yaitu Standar deviasi, Chande Momentum Oscillator, dan koefisien determinasi. Metode perhitungan tersebut bertujuan menentukan indeks volatilitas yang berfungsi untuk memprediksi pergerakan harga saham dan memprediksi tren di masa yang akan datang. Selain itu, VIDYA dapat digunakan sebagai strategi trading guna menentukan sinyal jual atau beli saham dengan cara menggunakan titik breakout.","A global finance scope is always related to change-over with a continous occuring day by day, as in global trade market which has a dynamic movement fitted to a changing suspicion and a participant related. Those assumption make us find a dynamic indicator either that could change the time period by analyzing the market actions. Variable Index Dynamic Average (VIDYA) is one of the indicator which dynamically can follow the movement of the stock price. VIDYA needs weight in its calculation method. In this thesis, EWMA weight is used to determine VIDYA weight. VIDYA can be determined in three different methods, there are deviation standard, Chande Momentum Oscillator, and determination coeficient. Those methods are used to determine the volatility index whith the intention to predict the stockÃ¢ï¿½ï¿½s movement price and trend in the future. In addition, VIDYA can be used as a trading strategy to found sell signal or the buying stock condition by using breakout point.","Kata Kunci : analisis teknikal, exponential weighted moving average, indeks volatilitas, standar deviasi, koefisien determinasi, indikator momentum/technical analysis, exponential weighted moving average, volatility indeks, deviasion standard, determination coeeficien"
http://etd.repository.ugm.ac.id/home/detail_pencarian/83262,JACKKNIFED RIDGE REGRESSION ESTIMATOR UNTUK MODEL LINEAR DENGAN AUTOKORELASI PADA ERROR,"NAOMI RATNA SARI, Prof. Subanar, Ph.D",2015 | Skripsi | S1 STATISTIKA,"Analisis regresi adalah analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Dalam asumsi yang terdapat pada analisis regresi klasik diantaranya adalah tidak adanya autokorelasi pada error dan tidak adanya multikolinearitas. Jika dalam model regresi terdapat multikolinearitas dan autokorelasi, hal itu dapat menyebabkan hasil estimasi dengan menggunakan metode kuadrat terkecil menjadi tidak valid. Pada skripsi ini akan dibahas mengenai penduga regresi ridge dengan menggunakan metode jackknifed ridge regression estimator pada model linear dengan autokorelasi pada error yang dikembangkan oleh Ozkale (2008). Metode ini merupakan perluasan dari metode estimasi parameter pada regresi ridge yang dikemukakan oleh Hoerl dan Kennard (1970). Studi kasus ini menggunakan data jumlah uang beredar di Indonesia dan faktor yang mempengaruhi dari Juli 2007 sampai November 2014. Diperoleh kesimpulan bahwa metode jackknifed ridge regression estimator memberikan error yang lebih kecil daripada generalized least square estimator dan bias yang lebih kecil daripada penduga regresi ridge.","Regression analysis is a statistical analysis that used to perform model relationship between dependent variable and independen variables. One of the assumption in classical regression analysis is there is no multicollinearity and no autocorrelated errors. If there is multicollinearity and autocorrelated errors in the regression model, it could cause the result of the model that using the method of Least Squares Estimator becomes invalid. This paper will discuss about estimating ridge regression parameters using jackknifed ridge regression estimator in the linear model with autocorrelated errors developed by Ozkale (2008). This method is extension of ridge regression estimator pioneered by Hoerl and Kennard (1970). This paper case study is using the amount of money circulating in Indonesia and the factors that affecting it from July 2007 till November 2014. The conclusion is the jackknifed ridge regression estimator gives smallerMSE than the generalized least square estimator and smaller bias than the ridge regression estimator.","Kata Kunci : multikolinearitas, autokorelasi pada error, generalized least square, analisis kuadrat terkecil, analisis regresi ridge, teknik jackknife, MSE"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85567,Segmentasi dan Penentuan Klaster Terbaik Menggunakan Kriteria Elbow dan Metode Hirarki K-Means dengan Proses Skoring,"DARMAWAN RATDYA BAKTI, Yunita Wulan Sari, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Metode untuk melakukan proses klastering dibagi menjadi dua yaitu metode hirarki dan metode non-hirarki. metode non-hirarki atau k-means clustering adalah pengelompokkan data kedalam k klaster. Proses ini dimulai dengan menentukkan nilai k terlebih dahulu, data dengan karakteristik yang sama akan masuk dalam kelompok yang sama sedangkan data yang memiliki karakteristik yang berbeda akan masuk kedalam kelompok yang berbeda. Dalam proses pengelompokan menggunakan k-means ditentukan nilai k terlebih dahulu, salah satu metode yang digunakan dalam analisis ini adalah dengan menggunakan kriteria elbow. Selain itu teknik pengelompokan yang lain yaitu teknik hirarki, yang digunakan pada analisis ini adalah centroid linkage, dimana proses ini menggunakan metode aglomerasi dengan tiap data yang terbentuk menjadi klaster kemudian akan dicari nilai pusat/centroid sampai terbentuk klaster yang diinginkan. Pada skripsi ini digunakan metode hierarchical k-means penggabungan antara metode hirarki dan metode non-hirarki, proses hirarki digunakan untuk mencari inisialisasi awal untuk proses non-hirarki dan mendapatkan klaster yang optimal.","Methods to perform clustering process is divided into hierarchical method and non-hierarchical method. Non-hierarchical method  or k-means clustering is grouping data into k clusters. This process begins with determine number of cluster, data with the same characteristics will be included in the same group while the data which have different characteristics will be entered into different groups. In the process of using the k-means clustering of the number of k is determined in advance, one of the methods used in this analysis is using the elbow criterion. In addition another clustering technique is hierarichical method, in this research using centroid linkage analysis, where the agglomeration method used in this process with each of the data that is formed into a cluster and will be determined central value/centroid to cluster desired. In this paper using hierarchical k-means method, This method combines hierarchical method and non-hierarchical method, hierarichical process is used to determine the initial centroid for non-hierarchical process and obtain optimal cluster.","Kata Kunci : k-means, algoritma hirarki, kriteria elbow, clustering, inisialisasi pusat klaster"
http://etd.repository.ugm.ac.id/home/detail_pencarian/83266,Pemulusan B-Spline pada Model Hazard Proporsional Cox,"ARI KURNIAWATI, Dr. Danardono, MPH",2015 | Skripsi | S1 STATISTIKA,"Regresi Hazard Proporsional Cox memiliki asumsi bahwa resiko relatif memiliki efek linier terhadap fungsi log-hazardnya. Namun dalam praktek sesungguhnya, asumsi ini dapat dilanggar. Bentuk resiko relatif akan lebih baik jika mengikuti data yang ada. Dengan kata lain, fungsi resiko relatif tidak memiliki distribusi tertentu. Oleh karena itu, resiko relatif yang biasa dimodelkan dengan parametrik, dimodelkan dengan nonparametrik. Pemodelan nonparametrik yang ditawarkan adalah fungsi Spline dengan basis fungsi B-Spline. Metode akan diaplikasikan pada data Worcester Heart Attack Study (WHAS) oleh Dr. Robert J. Goldberg dari Departemen Kardiologi Universitas Massachusetts Medical School. Kemudian, dilakukan pembuktian bahwa model yang dihasilkan lebih baik daripada model Cox pada umumnya dengan cara membandingkan kedua model tersebut.","Cox Proportional Hazard Regression assumes that the relative risk has a linear effect on the log-hazard function. But in actual practice, this assumption may be violated. The relative risk form would be better if it follows the existing data. In other words, the relative risk function does not have a particular distribution. Therefore, the relative risk of commonly modeled with parametric, modeled with nonparametric. It aims to obtain the relative risk form a more flexible. Nonparametric modeling offered is spline function with B-Spline basis functions. The method will be applied to the Worcester Heart Attack Study (WHAS) data by Dr. Robert J. Goldberg of Cardiology Department of the University of Massachusetts Medical School. Then, by comparing both models, we will prove that the resulting model is better than Cox models in general.","Kata Kunci : Regresi Hazard Proporsional Cox, Spline, B-Spline"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85830,KLASIFIKASI MENGGUNAKAN METODE BAYESIAN BELIEF NETWORKS,"ERISA NOVIATI, Prof. Dr. Sri Haryatmi, M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Dalam era yang semakin berkembang ini, penggunaan data mining semakin banyak dalam berbagai bidang dan menjadi bagian dari perkembangan teknologi informasi yang tak terhindarkan. Data mining merupakan proses penggalian informasi yang bermakna dari sejumlah besar data historis, melalui berbagai prosedur dan metode. Dalam data mining terdapat 6 aplikasi utama, yaitu estimasi, prediksi, klasifikasi, pengelompokan, deskripsi, dan asosiasi. Klasifikasi menjadi salah satu bahasan yang paling sering dibicarakan dalam data mining karena cakupan penerapannya yang cukup luas seperti dalam bidang bisnis, keuangan, kesehatan, keamanan, ilmu pengetahuan dan teknologi. Salah satu metode klasifikasi adalah Bayesian Belief Networks (BBN). Metode BBN adalah metode yang menggunakan prinsip dan teorema Bayes dengan asumsi bahwa variabel/atribut input dapat saling terikat. Probabilitas yang dihasilkan kemudian digunakan untuk melakukan prediksi data di masa depan. Contoh penerapan klasifikasi dalam bidang kesehatan adalah proses diagnosa/prognosa/prediksi penyakit kanker payudara dilihat dari hasil foto rontgen pada data WBCD. Metode Bayesian Belief Networks dengan algoritma pencarian Tabu dan fungsi skor AIC (BBN Tabu-AIC) merupakan pendekatan pemodelan yang cukup baik untuk mengklasifikasikan data WBCD dengan tingkat akurasi sebesar 97,9502% pada data pelatihan dan 97,0588% pada data pengujian, dengan interpretasi hasil jaringan yang diperoleh mudah dipahami para dokter dan praktisi kesehatan.","In this modern world nowadays, data mining utilizing become spreading excessively at any various fields and inevitably become part of information technology development. Data mining is the process of extracting meaningful information from a large amount of historical data, through a variety of procedures and methods. There are six main tasks of data mining which is estimation, prediction, classification, clustering, description, and association. Classification become one of the most common data mining tasks because a fairly broad scope of application in such fields as business, economic, health, security, science and technology. One of the classification methods is Bayesian Belief Networks (BBN). BBN method is a method that uses the Bayes principle and theorem assuming that the input variables/attributes can be mutually bound (joint conditionally independent). Then resulted probability value will be used to future data prediction. One of the classification task example in the health field is the process of diagnosis/prognosis/prediction of breast cancer seen from the results of X-rays on the WBCD dataset. Modeling approach of Bayesian Belief Networks with Taboo search algortihm and AIC score function (BBN Taboo-AIC) method is good enough to classify WBCD dataset with the accuracy rate of the training data is 97.9502% and 97.0588% from the test data, with the interpretation of the obtained network results can be easily understood by doctors and health practitioners.","Kata Kunci : data mining, klasifikasi, Bayesian Belief Networks"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84812,PEMODELAN MULTIGROUP STRUCTURAL EQUATION MENGGUNAKAN ESTIMASI MAKSIMASI LIKELIHOOD  DENGAN ITERASI FISHER SCORING,"TRISNA HARDI SAPUTRA, Prof. Dr. rer. nat Dedi Rosadi, S.Si., M.Sc",2015 | Skripsi | S1 STATISTIKA,"Secara tradisional analisis Structural Equation Model (SEM) seringkali  memperlakukan semua variabel sebagai variabel kontinu. Beberapa peneliti mengatakan bahwa variabel ordinal seharusnya diperlakukan sebagai variabel ordinal, tidak sebagai variabel kontinu karena akan diperoleh hasil yang tidak sesuai. Multigroup SEM merupakan metode analisis yang menggunakan penerapan SEM pada beberapa kelompok sampel, dimana setiap kelompok dibangun oleh variabel-variabel yang sama baik variabel laten maupun variabel indikatornya, sehingga setiap grup akan dibangun suatu path diagram awal yang sama. Maximum Likelihood dipilih karena sampel yang digunakan dalam penelitian cukup kecil dibanding dengan estimasi lainnya. Untuk memaksimasi fungsi parameternya sendiri perlu digunakan metode numerik dengan iterasi Fisher Scoring. Objek penelitian yang digunakan adalah mahasiswa FMIPA UNY pengguna operator IM3. Peneliti mencurigai adanya perbedaan kepuasan pelanggan dalam grup yang menggunakan kartu pendamping selain IM3 dengan grup yang tidak menggunakan kartu pendamping. Hal ini sangat menarik ditinjau karena efek penggunaan kartu pendamping itu bisa saja merupakan efek tidak puas dengan fasilitas yang diberikan oleh operator IM3.","Traditionally the analysis of Structural Equation Model (SEM) is often treats all variables as continuous variables. Some researchers say that the ordinal variables should be treated as ordinal variables, not as a continuous variable because it will be obtained unappropriate results. Multigroup SEM is an analytical method that uses the application of the SEM sample groups, where each group will build a path to the beginning of the same diagram. Maximum Likelihood choosen because the samples used in the study is quite small compared with other estimates. To maximize the function of parameters themselves need to use numerical method with Fisher Scoring iteration. Object of the research is Student Faculty of FMIPA UNY that used IM3 operator. Researcher suspects that  there is difference in customer satisfaction with group who use companion card other IM3 than group who does not use a companion card. It is very interesting to be reviewed because of the companion card usage can be an effect for dissatisfaction with the facilities provided by IM3 operator.","Kata Kunci : SEM, Multigroup SEM, Path Analysis, Maximum Likelihood, Fisher Scoring, Indeks Kepuasan Pelanggan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84557,Model Aditif Tergeneralisasi untuk Analisis Desain Kasus Tunggal,"MIFTAH EL ALIMI, Dr. Herni Utami, M.Si",2015 | Skripsi | S1 STATISTIKA,"Desain kasus tunggal merupakan sebuah desain penelitian dimana subjek yang diteliti digunakan sebagai kontrolnya sendiri. Permasalahan yang dihadapi dalam analisis desain kasus tunggal adalah pemodelan bentuk fungsional antara dua variabel (sering disebut tren). Pemodelan tren tidak selalu mengikuti asumsi linearitas dan variabel respon berdistribusi normal. Metode yang akan diuraikan dalam hal ini adalah model aditif tergeneralisasi, model ini unggul dalam mendeteksi tren nonlinear. Model aditif tergeneralisasi mengganti fungsi linear pada model linear tergeneralisasi dengan fungsi aditif. Distribusi variabel respon tidak terbatas hanya pada distribusi normal saja akan tetapi distribusi lain yang termasuk dalam keluarga eksponensial. Studi kasus mengenai Evaluasi Pengaruh Kartu Respon terhadap Perilaku Mengganggu Siswa menggunakan desain kasus tunggal bertipe A-B-A-B. Variabel respon data berbentuk Binomial sehingga menggunakan fungsi hubung logit, fungsi penghalus menggunakan thin plate splines, estimasi model menggunakan Penalized Iteratively Re-weighted Least Squares (P-IRLS) dan pemilihan model terbaik mengggunakan Akaike Information Criterion (AIC). Ilustrasi diberikan menggunakan software R package mgcv. Hasil penelitian menunjukkan bahwa model aditif tergeneralisasi sangat cocok digunakan untuk menganalisis desain kasus tunggal pada pemodelan tren yang benar dan kartu respon mempunyai efek pengaruh yang signifikan terhadap perilaku mengganggu siswa.","Single case design is a design study in which subject was studied used as its own control. Problems encountered in the analysis of a single case design is modeling the functional form between two variables (often called trends). Modeling trend does not always follow the assumption of linearity and normal distribution response variables. The method will be described in this case is generalized additive models, this models excel in detecting nonlinear trends. Generalized additive models replace the linear function in generalized linear models with additive functions. The distribution of the response variable is not limited to the normal distribution but the other distributions in the exponential family. The case study about Effects of Response Cards Evaluation on Students Disruptive Behavior using single case design type A-B-A-B. Response data variable on Binomial form, then it is using logit link function, smoothing function using thin plate splines, model estimation using Penalized Iteratively Re-Weighted Least Squares (P-IRLS) and selection of the best models use Akaike Information Criterion (AIC). Illustrations are given using the R software package mgcv. The result of research shows that generalized additive models fit properly to be used for analyzing single case design and the response cards have the significant effect on students disruptive behavior.","Kata Kunci : desain kasus tunggal, regresi nonlinear, model aditif tergeneralisasi, Binomial, splines, estimasi P-IRLS"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84813,KERNEL LOGISTIK PARTIAL LEAST SQUARES (KL-PLS) UNTUK REDUKSI DIMENSI NONLINEAR DAN KLASIFIKASI BINER (Studi Kasus : Data Kanker Ovarium SELDI-TOF High Resolution),"TITIK BUNTARININGSIH, Dr. Abdurakhman, M.Si",2015 | Skripsi | S1 STATISTIKA,"INTISARI Kernel Logistik Partial Least Squares (KL-PLS) untuk Reduksi Dimensi Nonlinear dan Klasifikasi Biner (Studi Kasus : Data Kanker Ovarium SELDI-TOF High Resolution) Oleh : Titik Buntariningsih 10/305419/PA/13511 Kanker ovarium merupakan kanker yang berkembang dengan sangat cepat. Perkembangan kanker dari stadium awal hingga stadium lanjut dapat terjadi dalam kurun waktu 1 tahun. Kanker ovarium merupakan penyebab kematian terbesar kelima di Indonesia. Pertumbuhan jumlah penderita kanker ini terus bertambah setiap tahunnya. Untuk itulah, diperlukan diagnosa dini kanker dibutuhkan untuk mengurangi resiko kematian. Sebuah studi yang dilakukan oleh Tenenhaus dkk. (2007) mengusulkan sebuah model yang memanfaatkan Kernel Logistik Partial Least Square (KLPLS) untuk memprediksi adanya kanker. Skripsi ini bertujuan untuk memanfaatkan metode KLPLS untuk memprediksi penyakit kanker ovarium menggunakan data spektometri massa ovarium SELDI-TOF High Resolution seperti yang telah diusulkan oleh (Tang dkk (2010). KLPLS sendiri merupakan pengembangan dari Partial Least Square dengan melibatkan fungsi kernel di dalam prosesnya. Fungsi kernel yang digunakan dalam implementasi metode ini adalah fungsi kernel polinomial. KLPLS terbagi ke dalam dua tahap. Tahap pertama adalah tahap latih untuk mencari koefisien regresi. Tahap kedua adalah tahap uji untuk memprediksi label data uji berdasarkan koefisien regresi dari tahap latih. Uji coba dilakukan dengan memprediksi kanker pada empat dataset yang berbeda. Berdasarkan uji coba KLPLS menghasilkan rata-rata akurasi terbaik sebesar 93,53%, sensitivitas sebesar 89,81% dan spesifisitas sebesar 98,89% pada data Set A. Berdasarkan performa uji coba klasifikasi data, dapat ditarik kesimpulan bahwa model klasifikasi yang memanfaatkan KLPLS mampu melakukan klasifikasi kanker ovarium pada data ekspresi protein dalam format SELDI-TOF.","ABSTRACT Kernel Logistic Partial Least Squares (KL-PLS) for Nonlinear Dimensionalty Reduction and Binary Classification (Case Study : Ovarian Cancer SELDI-TOF High Resolution Data) By : Titik Buntariningsih 10/305419/PA/13511 Ovarian cancer is a cancer that is growing very rapidly. Development of early-stage cancer to an advanced stage can occur within a period of one year. Ovarian cancer is the fifth leading cause of death in Indonesia. Growth in the number of cancer patients is growing every year. For that, we need early diagnosis of cancer are needed to reduce the risk of death. A study conducted by Tenenhauset al. (2007) proposed a model that utilizes Logistics Kernel Partial Least Squares (KLPLS) to predict the presence of cancer. This thesis aims to utilize KLPLS method for predicting ovarian cancer using mass spectrometry data is SELDI-TOF ovarian High Resolution as proposed by Tang et al. (2010). KLPLS it self a development of the Partial Least Square involving the kernel function in the process. Kernel function used in the implementation of this method is a polynomial kernel function. KLPLS divided into two stages. The first stage is the stage trained to look for the regression coefficients. The second stage is the stage of the test to predict the test data labels based on regression coefficient of practice stage. The test is done to predict cancer in four different datasets. Based on trials KLPLS average yield of 93.53% best accuracy, sensitivity of 89.81% and a specificity of 98.89% in Data Set A. Based on the performance test data classification, it can be deduced that the classification model that utilizes KLPLS able to perform classification of ovarian cancer at the protein expression data SELDI-TOF format.","Kata Kunci : kanker ovarium, kernel, klasifikasi, KLPLS, penggalian data, reduksi dimensi, SELDI-TOF"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84559,Model Respon Bertingkat untuk Kelas Laten Multidimensional Teori Respon Butir,"ACHMAD YUSUF AFRIANSYAH ADZAN, Dr. Danardono, MPH",2015 | Skripsi | S1 STATISTIKA,"Model-model analisis butir soal pada umumnya digunakan untuk mengukur variabel laten, melalui penurunan analisis dari perangkat tes berupa kuesioner yang dibuat dengan butir-butir soal tipe respon biner atau polikotomus. Model tradisional ini berdasarkan asumsi unidimensional, yang berarti semua butir soal berkontribusi untuk mengukur variabel laten yang sama. Terlebih lagi dalam beberapa kasus, asumsi normalitas dari variabel laten ini secara eksplisit dianggap terpenuhi, namun sayang dalam beberapa praktek kedua asumsi ini terbatas. Berbagai macam pengembangan dari model tradisional di atas memungkinkan adanya multidimensionalitas dan variabel laten diskrit. Model terbaru ini juga memungkinkan perbedaan parameterisasi dari distribusi bersyarat variabel respon yang diberikan oleh variabel laten, bergantung pada tipe dari fungsi penghubung dan konstrain yang dikenakan pada parameter diskriminasi (daya pembeda) dan parameter tingkat kesulitan butir soal. Berbagai macam model tersebut dapat diestimasi dengan metode kemungkinan maksimum melalui algoritma Ekspektasi-Maksimisasi. Dalam skripsi ini dibahas analisis validitas perangkat tes Hospital Anxiety and Depression Scale (Zigmond dan Snaith, 1983) yang terdiri dari empat belas butir soal menggunakan model respon bertingkat untuk kelas laten multidimensional teori respon butir. Kuesioner ini diharapkan mampu membedakan dua sifat laten berbeda, yaitu tingkat kecemasan dan depresi. Setelah dianalisis dengan metode di atas menghasilkan kesimpulan ditolaknya asumsi bidimensionalitas sifat laten sehingga semua butir soal dalam kuesioner tersebut mengukur sifat laten yang sama, yaitu gangguan psikologis. Kemudian dari keempat belas butir soal, butir 2, 4, 6, 7, 8, 11, 13, 14 dieliminasi dari kuesioner karena tidak cukup baik dalam memberikan informasi di tingkat gangguan psikologis tertentu.","Item Response Theory (IRT) models are commonly used to measure latent traits, through the analysis of data deriving from the administration of questionaires made of items with dichotomous (binary) or polytomous. Traditional IRT models are based on the assumption of unidimensionality, which means that all items contribute to measure the same latent trait. Moreover in some cases, the assumptions of normality of this latent trait is explicitly introduced, unfortunately in several practical situations both assumptions are restrictive. The models at issue extend traditional IRT models allowing for multidimensionality and discreteness of latent traits. They also allow for different parameterizations of the conditional distribution of the response variables given the latent traits, depending on both the type of link function and constraints imposed on the discriminating and difficulty item parameters. These models may be estimated by maximum likelihood via an Expectation-Maximization algorithm. In this mini thesis discussed the analysis of the validity of the tests Hospital Anxiety and Depression Scale (Zigmond and Snaith, 1983) which consists of fourteen items using graded response model for multidimensional latent class of item response theory. The questionnaire is expected to be able to distinguish two different latent trait, namely the level of anxiety and depression. Having analyzed the above methods lead to the conclusion rejection bidimensionality assumptions latent nature so that all items in the questionnaire measuring the same latent trait, namely psychological disorders. Then on the fourteen items, item 2, 4, 6, 7, 8, 11, 13, 14 are eliminated from the questionnaire because it was not good enough in providing information at the level of certain psychological disorders.","Kata Kunci : Teori respon butir, respon polikotomus, multidimensionalitas, sifat diskrit, sifat laten, estimasi kemungkinan maksimum, algoritma ekspektasi-maksimisasi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/93778,ANALISIS MODEL TIGA FAKTOR FAMA FRENCH DALAM PEMBENTUKAN PORTOFOLIO,"NURUL HIDAYANI, Dr. Herni Utami, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Pembentukan portofolio adalah salah satu strategi untuk mengurangi risiko atau kerugian dalam berinvestasi. Berbagai teori portofolio telah banyak berkembang dalam dunia investasi. Capital Asset Pricing Model (CAPM) telah mendominasi teori keuangan lebih dari tigapuluh tahun, dikatakan bahwa hanya market beta yang dapat menjelaskan return saham. Sedangkan ada bukti lain yang menunjukkan bahwa cross-section dari return saham tidak hanya dijelaskan oleh satu faktor CAPM. Salah satu teori yang berkembang didasari dari CAPM adalah model tiga faktor fama french. Dalam model ini selain faktor market beta ada dua faktor tambahan yaitu firm size dan book to market value yang dapat menjelaskan tingkat pengembalian saham. 	Pada skripsi ini akan dilakukan pengujian untuk mengetahui pengaruh model tiga faktor fama french terhadap return saham. Data yang digunakan adalah data perusahaan yang tergabung dalam indeks LQ 45 selama periode Januari 2011- Juli 2014. Kemudian dalam studi kasus dilakukan pembentukan portofolio dengan kriteria model tiga faktor fama french. Pemilihan portofolio optimal dilakukan, terpilih portofolio optimalnya adalah portofolio dengan small firm size dan book to market high kemudian  bobot diberikan pada masing-masing sekuritas di dalam portofolio optimal tersebut.","Portfolio formation is one of strategy to reduce the risk or the loss in investment. Various theories have been developed in investment world. The Capital Asset Pricing Model (CAPM) has dominated finance theory for over thirty years, it suggests that the market beta alone is sufficient to explain stock returns. However evidence shows that the cross-section of stock return cannot described solely by one-factor CAPM. One of the theory that based on CAPM is Fama French Three Factor Model. The model adding two more factor firm size and book to market value that could explaining return in stock. In this thesis was conducted to determined the effect of fama french three factor model on return stock. The data used in this study is a data company incorporated in LQ 45 during period January 2011 â€“ July 2014. Then in case studies portofolio formation created with fama french three factor model criteria. Optimal portfolio being selected, the optimal portfolio is portfolio with small firm size and high book to market, then the weight is given for each of securities in the portfolio.","Kata Kunci : portofolio, CAPM, fama french, firm size, book to market"
http://etd.repository.ugm.ac.id/home/detail_pencarian/86101,MODEL EFEK RANDOM UNTUK PEMODELAN BERSAMA DATA TIME-TO-EVENT DAN LONGITUDINAL,"AMELIA RAHMA M, Dr. Herni Utami, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Istilah joint model mengacu pada sebuah metode berbasis model untuk analisis data dari studi longitudinal dengan setiap subyek menghasilkan dua jenis data : pengukuran berulang dengan waktu follow-up ditentukan, dan proses peristiwa pada waktunya. Model ini menjelaskan hubungan antara data pengukuran berulang Y dan data waktu peristiwa T melalui ketergantungan bersama keduanya pada efek random yang tidak teramati. Joint model menggunakan model efek campuran Gaussian linear untuk pengukuran longitudinal dan model cox hazard proporsional untuk waktu peristiwa yang bersyarat pada efek random. Secara khusus, diasumsikan model random-intercept-dan-slope, dan menggunakan algoritma EM untuk estimasi parameter. Kesalahan standar dan interval kepercayaan opsional dihitung melalui bootstrap untuk joint model yang fit. Joint model diterapkan untuk data real yang diperoleh dari recall dan catatan medis pasien dari pasien hipoalbuminemia rawat inap di Dr Sardjito Yogyakarta.","The term of joint modeling refer to model-based methods for the analysis of data from a longitudinal study in which each subject produces outcome data of two kinds: a repeated of measurements at pre-specified follow-up times, and a point process of events in time. These models attempt to explain the relationship between the measurement repeated data Y and the event time data T through their shared dependentce on unobserved random effects.  Joint model use Gaussian linear mixed-effects model for the longitudinal measurements and a Cox proportional hazards model for the event times conditional on the random effects. Specifically, we assumed a random-intercept-and-slope model, and developed an expectationâ€“maximization (EM) algorithm for maximum likelihood estimation. Standard errors and optional confidence interval calculated via bootstrap for a joint model fit. Joint model is applied to real data that obtained from recall and medical records patient of Hypoalbuminaemia Hospitalization Patients in Dr. Sardjito Yogyakarta.","Kata Kunci : joint model,kovariat time-dependent,efek random,data longitudinal,data survival,penanda biologis"
http://etd.repository.ugm.ac.id/home/detail_pencarian/86869,Regresi Linear Tersensor Student-T,"ERISKI ISNANDA, Drs. Zulaela, Dipl. Med.Stats.",2015 | Skripsi | S1 STATISTIKA,"Regresi Tobit merupakan analisis regresi yang menganani masalah kasus data tersensor dimana error diasumsikan berdistribusi normal. Tapi untuk kasus data dengan error yang tidak normal, regresi tobit ini menghasilkan estimasi yang bias. Sehingga digunakan Regresi Linear Tersensor Student-T dimana pada regresi ini error diasumsikan berdistribusi Student-T. Regresi Linear Tersensor Student-T, karena derajat parameter distribusi Student-T memberikan dimensi yang baik untuk mencapai inferensi statistik yang kuat. Regresi ini menggambarkan model tersensor menggunakan pendekatan yang berbeda dibandingkan dengan model Tobit, yang cukup rentan terhadap kehadiran outlier.","Tobit regression is a regression analysis that address the issue of data censored cases where the error is assumed to be normally distributed. But for the case of data with an error that is not normal, this tobit regression produce biased estimates. So use Censored Linear Regression Student-T where this regression is error assumed to be Student-T distributed. Censored Linear Regression Student-T, because the degree of distribution parameters Student-T provides good dimensional to achieve robust statistical inference. This illustrates censored regression models using a different approach than the Tobit models, are quite susceptible to the presence of outliers.","Kata Kunci : Regresi Tobit, Regresi Linear Tersensor Student-T, Derajat Parameter Kebebasan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/88149,PEMODELAN KURVA YIELD DENGAN OPTIMALISASI ALGORITMA DIFFERENTIAL EVOLUTION MENGGUNAKAN METODE NELSON SIEGEL SVENSSON DINAMIK,"AGUS RIYANTA, Dr. Abdurakhman, M.Si;Vemmie Nastiti L., S.Si., M.Sc",2015 | Skripsi | S1 STATISTIKA,"Obligasi adalah salah satu instrumen investasi berpendapatan tetap. keuntungan yang diterima oleh investor sampai jatuh tempo disebut dengan yield to maturity. Analisis yang menjelaskan tentang hubungan yield to maturity dengan waktu jatuh tempo disebut dengan struktur jangka waktu tingkat bunga atau term structure of interest rate. struktur jangka waktu ini digambarkan dengan grafik yang disebut dengan yield curve. Kurva yield (yield curve) ini memuat yield sebagai koordinat x dan waktu jatuh tempo sebagai koordinat y.  Pada skripsi ini akan dipelajari teori pembentukan kurva yield dengan menggunakan metode Nelson Siegel Svensson Dinamik, yang dalam pembentukannya akan dibandingkan dengan hasil pembentukan oleh Differential Evolution Algorithm, selanjutnya akan dimodelkan menggunakan metode VECM. Studi kasus penelitian ini diambil dari obligasi pemerintah Indonesia berkode FR (Fix Rate) yang diperdagangkan pada tanggal 16 Februari 2011. Hasil pemodelan diperoleh kesimpulan bahwa berdasarkan kriteria nilai RMSYE dan MAYE yang minimum dapat dipilih model Nelson Siegel Svensson Dinamik terbaik dengan optimalisasi algoritma Differential Evolution.","Bond is one of fixed-income investment instruments. The income that will returns to the investor until maturity is called by yield to maturity. An analysis that explained the relationship between yield to maturity and time to maturity is called term structure of interest rate. This term structure design into graph which called yield curve. This curve has yield in coordinate x and maturity in coordinate y.  This research learn about yield curve theory using Dynamic Nelson Siegel Svensson method, which in its formation will be compared with the results of the formation by the Differential Evolution Algorithm, the results modeled using the VECM. This research case studies taken from the Indonesian government bonds coded FR (Fix Rate) traded on February 16, 2011. The modeling results concluded that based on the criteria of the minimum value that can be selected from RMSYE and MAYE Dynamic Nelson Siegel Svensson models from optimization Differential Evolution algorithm is better than the another.","Kata Kunci : Yield Curve, Dynamic Nelson Siegel Svensson, Differential Evolution, Vector Error Correction Model."
http://etd.repository.ugm.ac.id/home/detail_pencarian/89941,REGRESI COX DENGAN ESTIMASI TERBOBOTI MENGGUNAKAN METODE KAPLAN-MEIER,"ARIF MAFAZAN SIMOHARTONO, Drs. Danardono, MPH. Ph.D. ; Gunardi, M.Si., Dr.",2015 | Skripsi | S1 STATISTIKA,"Regresi Cox adalah suatu pendekatan yang banyak digunakan untuk memodelkan suatu data survival tersensor. Bagaimanapun juga, dalam regresi Cox model yang digunakan harus memenuhi asumsi hazard proporsional yang harus diperhatikan. Pada kasus-kasus tertentu, hazard yang tidak proporsional sering kali dijumpai, khususnya pada data dengan lama waktu penelitian yang panjang. Oleh karena itu, dalam skripsi ini, akandibahas model regresi Cox dengan estimasi terboboti pada kasus data dengan hazard yang tidak proporsional. Model regresi Cox dengan estimasi terboboti ini menggunakan prosedur maksimum partial likelihood untuk mengestimasi parameternya.Fungsi pembobot yang digunakan merupakan fungsi survival dari data, yang didapatkan dengan menggunakan metode Kaplan-Meier.Jika dibandingkan dengan metode regresi Cox stratifikasi, regresi Cox dengan estimasi terboboti akan lebih mudah digunakan karena tidak perlu membentuk variabel strata. Selain itu, estimasi dari koefisien Î² dari covariate yang tidak memenuhi asumsi proporsional hazard akan tetap dapat diestimasi. Menggunakan data pecandu heroin dengan variabel clinic, prison, serta dose, akan ditunjukkan hasil estimasinya","Cox regression is a well-known approach for modeling censored survival data. However, the model has an assumption of proportional hazard which requires an attention. In certain cases,  non-proportional hazard assumption is frequently violated i.e. in the long-term study. In this studi, we examine the Cox regression with weitghted estimation model  under non-proportional hazard. This model using maximum partial likelihood for estimating the parameter. The weighting function is obtained from survival function using Kaplan-Meier method. Using Cox regression with weighted estimation, we can estimate Î² parameter from non-proportional hazard covariate, which cannot be obtained with stratified Cox regression. The proposed and existing model of Cox regression with weighted estimation are applied to a data set in heroin addict. We will show the estimate of the parameter in the data set.","Kata Kunci : analisis data survival, regresi Cox, estimasi terboboti, survival analisis, Cox regression, weighted estimation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82779,MODIFIKASI GRAFIK PENGENDALI ROBUST BERDASARKAN MEDIAN ABSOLUTE DEVIATION (MAD),"YUNA ARGADEWI, Drs. Zulaela, Dipl. Med. Stats., M.Si.",2015 | Skripsi | S1 STATISTIKA,"Grafik pengendali adalah salah satu alat yang paling kuat yang digunakan untuk mendeteksi perilaku menyimpang dalam proses produksi. Grafik pengendali Shewhart x bar dan S adalah salah satu yang paling banyak digunakan sebagai teknik pengendalian proses statistik yang dikembangkan untuk mengontrol rata-rata dan variabilitas proses berdasarkan asumsi dasar bahwa distribusi yang mendasari karakteristik kualitas adalah normal. Metode robust adalah salah satu metode statistik yang paling sering digunakan sebagai pilihan ketika asumsi normalitas yang mendasari tidak terpenuhi. Terdapat dalam jurnal ilmiah sebuah modifikasi grafik pengendali robust berdasarkan estimator skala yang sangat robust, yang disebut dengan median absolute deviation (MAD).               Metode ini menyediakan sebuah alternatif untuk grafik pengendali x bar dan  S ketika asumsi normalitas yang mendasari tidak terpenuhi dan nilai standar proses tidak diberikan oleh manajemen. Contoh numerik berdasarkan simulasi digunakan untuk menggambarkan tampilan grafik x bar MAD dan SMAD dari metode robust dan membandingkannya dengan grafik x bar dan S dari metode Shewhart untuk nilai standar proses yang tidak diberikan manajemen. Simulasi dilakukan dengan membangkitkan 300 data berdistribusi normal dan tidak normal atau berekor tebal (heavy tailed). Dari hasil simulasi, grafik x bar MAD dan SMAD dari metode robust mempunyai tampilan yang lebih baik daripada grafik x bar dan S dengan metode Shewhart  dan mempunyai sifat yang baik untuk distribusi tidak normal khususnya berekor tebal.","Control chart is one of the most powerful tools that are used to detect aberrant behavior in the production process. x bar and S Shewhart control chart  is one of the most widely used as statistical process control techniques developed to control the average and variability of the process based on the basic assumption that the underlying distribution is normal. When the underlying assumptions of normality were not met , a robust method is one of the statistical methods most often used as an option in such situations. Presented a modification of robust control chart based on a very robust scale estimator, called the median absolute deviation (MAD). This method provides an alternative x bar and  S control chart when the underlying assumptions of normality were not met and the standard values unknown or not given by the management. Numerical examples based simulation is used to describe the appearance of x bar MAD and SMAD control chart with robust method and compare it with the methods of x bar and S Shewhart. x bar MAD dan SMAD robust method looks better than x bar and S Shewhart method and has good properties for distribution is not normal especially heavy tailed . Simulations performed by generating 300 data set from normal distribution and non-normal distribution (heavy tailed). From the simulation result, x bar MAD and SMAD having a good performance than x bar and S.","Kata Kunci : Shewhart, Robust, Nilai standard proses, x bar dan S, median absolute deviation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82784,ESTIMASI PARAMETER REGRESI RIDGE BINOMIAL NEGATIF UNTUK MENANGANI MULTIKOLINEARITAS,"AWWALINA GHAIDA R., Prof. Drs. Subanar, Ph.D",2015 | Skripsi | S1 STATISTIKA,"Regresi binomial negatif merupakan suatu metode yang digunakan untuk menganalisis hubungan antara variabel respon dengan variabel penjelas, dimana variabel respon berupa data cacah. Biasanya estimator maksimum likelihood (ML) digunakan untuk mengestimasi parameter dalam model regresi binomial negatif. Namun, estimasi parameter tersebut menjadi tidak signifikan jika variabel penjelas saling berkorelasi atau dengan kata lain terjadi multikolinearitas. Oleh karena itu, estimator regresi ridge binomial negatif (NBRR) digunakan untuk mengestimasi parameter dalam model regresi binomial negatif dalam penanganan multikolinearitas.  Dalam skripsi ini akan dibahas pemodelan banyak penganggur di Kota Semarang beserta faktor-faktor yang mempengaruhinya seperti tingkat inflasi, laju PDRB, upah minimum, dan jumlah penduduk usia produktif, berdasarkan analisis regresi binomial negatif yang terdapat masalah multikolinearitas. Oleh karena itu, untuk penanganan multikolineritas tersebut akan digunakan estimator regresi ridge binomial negatif. Rata-rata kuadrat galat dari estimator maksimum likelihood dan estimator regresi ridge binomial negatif juga akan dibandingkan. Hasilnya menunjukkan bahwa estimator ridge binomial negatif lebih baik dibandingkan dengan estimator maksimum likelihood.","Negative binomial regression is a method used to analyze the relationship between response variable and explanatory variables, where response variable is count data. Commonly, maximum likelihood (ML) estimator is used to estimate parameters in negative binomial regression model. However, those estimation become insignificant if explanatory variables are collinear or multicollinearity occurred. Therefore, negative binomial ridge regression (NBRR) estimator is used to estimate parameters in negative binomial regression model in order to handling multicollinearity. 	In this thesis will be discussed about the modeling of number of unemployed in Semarang city and factors that influence it, such as inflation rate, GDP rate, minimum wage, and number of productive age population, based on negative binomial regression which involve multicollinearity problem. So that, to handling those multicollinarity will be used negative binomial ridge regression estimator. Mean squared error of maximum likelihood estimator and negative binomial ridge regression will be compared. The result indicated that the negative binomial ridge regression estimator is preferred to the maximum likelihood estimator.","Kata Kunci : regresi binomial negatif, maksimum likelihood, regresi ridge, rata-rata kuadrat galat, multikolinearitas/negative binomial regression, maximum likelihood, ridge regression, mean squared error, multicollinearity"
http://etd.repository.ugm.ac.id/home/detail_pencarian/89956,GRAFIK PENGENDALI MULTIVARIATE MODIFIED EWMA UTUK OBSERVASI BERAUTOKORELASI,"NINDA ATIKAH M, Dr. Herni Utami, S.Si., M.Si.",2015 | Skripsi | S1 STATISTIKA,"Kualitas suatu produk merupakan faktor yang berpengaruh terhadap kepuasan konsumen. Kualitas dipengaruhi oleh beberapa karakteristik atau lebih dari satu, sehingga mengontrol karakteristik-karakteristik tersebut dapat menggunakan grafik pengendali. Selain untuk mengontrol karakteristik kualitas industri, grafik pengenddali dapat digunakan juga untuk mengontrol suatu proses. Ada beberapa karakteristik yang setiap observasinya saling berautokorelasi seperti proses kimia, kertas, dan lainnya seperti proses feed platforming dari minyak bumi. Proses feed platforming adalah proses untuk mengubah minyak bumi menjadi oktan yang lebih besar. Untuk mengontrol proses tersebut dapat digunakan grafik pengendali multivariate modified EWMA (MMOEWMA). Grafik pengendali multivariate modified EWMA merupakan modifikasi dari grafik pengendali multivariat EWMA yang mempunyai fungsi untuk mengontol pergeseran mean yang kecil dan dapat mengidentifikasi perubahan yang signifikan. Pada skripsi ini digunakan ARL (Average Run Length) untuk mengidentifikasi grafik pengendali MMOEWMA yang optimal dengan nilai pergeseran mean yang sudah ditentukan. Dengan membandingkan grafik MMOEWMA dan MEWMA diperoleh bahwa grafik pengendali MMOEWMA dapat mendeteksi perubahan yang signifikan antar observasi yang dihitung secara multivariat untuk data yang memiliki autokorelasi.","The quality of product is one of factor which has big influence for consumer stratification. For maintain the quality, the products must be controlled in order to have product with good quality. The characteristics of quality consist of more than one variable which can be controlled by multivariate analysis. To control the characteristics usually used control chart. Be sides to control characteristics of industry quality, control chart can be used to control process. In many industries or processes there are some characteristics which have auto correlated observations like chemical process, paper and etc. for example is feed plat forming process. Feed plat forming process is one of process in oil industry which has purpose to change oil become big octane. To control the process can used Multivariate Modified EWMA control chart. Multivariate Modified EWMA is modification of multivariate EWMA which has function to control small shift and to identify abrupt change. In this mini thesis, use ARL (Average run length) to identify optimal Multivariate modified EWMA control chart with has definite mean shift. By comparing MMOEWMA control chart and MEWMA control chart, MMOEWMA control chart can detect abrupt change for autocorrelated observations.","Kata Kunci : MEWMA, Multivariat Modified EWMA, average run length, perubahan signifikan, autokorelasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/87145,Interpolasi Tabel Mortalita dengan Asumsi Usia Pecahan Kuadratik,"RATIH TITIN N O, Dr.Adhitya Ronnie E.,M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Asuransi jiwa adalah asuransi yang memberikan jasa dalam hal penanggulangan risiko yang dikaitkan dengan hidup atau meninggalnya seseorang yang dipertanggungkan. Dalam setiap program asuransi, perusahaan asuransi akan melakukan berbagai perhitungan berdasarkan tabel mortalitas. Tabel mortalitas ini hanya menggambarkan peluang hidup dan mati pada usia bulat saja, dan biasanya digunakan untuk perhitungan model asuransi diskrit. Padahal dalam kenyataannya tidak hanya ada asuransi diskrit, tetapi juga ada asuransi kontinu. Oleh karena itu, dikembangkanlah suatu asumsi usia pecahan untuk membantu perhitungan dalam asuransi kontinu. Bowers dkk. (1997) mulai memperkenalkan asumsi linear, asumsi eksponensial, dan asumsi hiperbolik. Jones dan Mereu (2002) membahas tiga asumsi baru, salah satunya adalah asumsi LFM. Sedangkan Hossain (2011) memperkenalkan asumsi usia pecahan kuadratik.","Life insurance is an insurance that give a service in a risk prevention about a life or a death of a customer. Every insurance program, life insurance companies have to calculate a lot of part based on mortality table or life table. Every life table just can show the probability of death and life of someone in integer age, and usually its used to do a calculation in discrete insurance. However, in fact there is not only discrete insurance but also there is continuus insurance. Thus, it starts to develop a lot of fractional age assumptions to solve the calculation in continous insurance. Bowers (1997) introduced linear assumption, exponential assumption, and hiperbolic aasumption. Jones and Mereu (2002) introduced three new assumptions, and one of them is LFM assumption. Hossain (2011) introduced  a new assumption that called quadratic fractional age assumption.","Kata Kunci : Kata kunci : Asumsi Linear, Asumsi Eksponensial, Asumsi Hiperbolik, Asumsi LFM, Asumsi Usia Pecahan Kuadratik."
http://etd.repository.ugm.ac.id/home/detail_pencarian/82541,ESTIMASI LOGNORMAL KRIGING UNTUK ANALISIS DATA KESUBURAN TANAH PADA DAERAH SUNGAI (Studi Kasus: Estimasi Kandungan Zinc di Daerah Sungai Meuse),"ADHI SUBEKTI .P, Dr. Abdurakhman, S.Si., M.Si. ; Yunita Wulan Sari, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Sungai merupakan salah satu bagian di bumi yang saat ini kebermanfaatannya telah beralih fungsi dari sumber daya yang sangat bermanfaat bagi kehidupan menjadi lahan pembuangan sampah rumah tangga maupun limbah pabrik yang dapat mengakibatkan pencemaran air serta menurunnya tingkat kesuburan tanah di sekitar sungai. Salah satu unsur hara yang sangat bermanfaat bagi tanah adalah logam zinc dengan keadaan normal (10-300 ppm). Jika kandungan zinc melebihi batas normal, kondisi tersebut dapat meningkatkan kadar keasaman tanah yang menyebabkan penurunan tingkat kesuburan tanah. Salah satu metode yang dapat digunakan untuk mengetahui kadar zinc dalam tanah pada lokasi yang tidak tersampel adalah metode estimasi kriging. Pada data lapisan tanah yang sebagian besar mengikuti distribusi lognormal, digunakan estimasi lognormal kriging. Lognormal kriging pada prinsipnya mentransformasikan data ke dalam bentuk logaritma natural, kemudian dari hasil estimasi ditransformasikan kembali ke dalam skala awal sebagai hasilnya. Studi kasus yang digunakan yaitu data kandungan zinc yang terdapat pada permukaan tanah di daerah sungai meuse pasca banjir. Hasil estimasi 3197 titik yang tidak tersampel pada koordinat absis 178912-181299 dan ordinat 329484- 333576 menunjukkan bahwa rata-rata estimasi kandungan zinc yaitu 536.254 ppm. Dari hasil tersebut menunjukkan rata-rata kandungan zinc melebihi batas normal. Hal ini dibenarkan pada kenyataan bahwa dari 3197 titik estimasi terdapat 93.65 % lokasi dengan kandungan zinc di luar batas normal, sehingga menurunkan tingkat kesuburan tanah karena bersifat asam.","River is one of the earth's part which has the benefits changing from the initial function as a natural resources into a household waste disposal and sewage plant which are can pollute water and ground around the river. One of a useful nutrient for the soil is zinc when the content has a normal limit (10-300 ppm). If the content is beyond the limit, it can caused an increase the acidity of the soil and decrease the soil fertility. The content of zinc at not sampled locations can be determined by using kriging estimation. For the data subsoil which are largely attend to lognormal distribution, it can be dealing with lognormal kriging estimation. Lognormal kriging in principle is transforming data into a logarithm natural form, then the estimation is transformed into the initial scale as a result. Case studies been used are the content of zinc contained in the surface of the soil in the area of post-flood river meuse. The estimation results of 3197 points which are not sampled at coordinates abscissa and ordinate 178912 - 181299, 329484- 333576 showed that the average content of zinc is estimated 536.254 ppm. These results showed an average content of zinc exceeds normal limits. This is justified in fact that of the 3197 point estimates are 93.6% of sites with content of zinc outside normal limits, involved decreasing slevels of soil fertility due to acidic.","Kata Kunci : data spasial, geostatistika, kriging,ordinary kriging, lognormal kriging/spatial data, geostatistics, kriging, ordinary kriging, lognormal kriging"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84608,ESTIMASI PARAMETER REGRESI LINEAR BERGANDA MENGGUNAKAN METODE JACKKNIFE,"DESI PURNAMASARI, Dr.Herni Utami, S.Si., M.Si.",2015 | Skripsi | S1 STATISTIKA,"Jackknife merupakan salah satu metode estimasi inferensi statistika yang digunakan untuk memperoleh estimasi yang sebaik-baiknya berdasarkan data yang minimal. Salah satu kelebihan dari metode Jackknife adalah tidak membutuhkan asumsi apapun mengenai distribusi dari sampel yang dimiliki. Metode Jackknife dapat digunakan pada data berpasangan pada kasus model regresi linear. 	Dalam skripsi ini, metode Jackknife diterapkan untuk mengestimasi parameter regresi linear berganda. Analisis regresi linear merupakan suatu teknik analisis untuk mengetahui hubungan antara 2 variabel atau lebih serta estimasinya menggunakan model matematika yang disebut model regresi, yakni dengan mencari model yang sangat baik untuk menggambarkan masalah kausalitas/ sebab akibat adalah metode kuadrat terkecil.","Jackknife is one inference statistical estimation methods used to obtain the best possible estimate based on minimal data. One of the advantages of the Jackknife method is not require any assumptions about the distribution of the sample owned. Jackknife method can be used on the data pairs in the case of linear regression models. 	 In this paper, Jackknife method is applied to estimate the parameters of linear regression. Linear regression analysis is analytical technique to determine the relathionship between two or more variables and estimation using mathematical model called the regression model, by searching the best model to describe the causality problem/ causal relationship is the least square method.","Kata Kunci : Metode Jackknife, Regresi Linear Ganda, Ordinary Least Square(OLS)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84357,ROBUST PRINCIPAL COMPONENT ANALYSIS (ROBPCA) UNTUK DATA BERDIMENSI BESAR DENGAN OUTLIERS,"TRI ROKHMATUN S, Prof. Dr. Sri Haryatmi, M.Sc.",2015 | Skripsi | S1 STATISTIKA,"INTISARI    Principal Component Analysis (PCA) merupakan salah satu analisis multivariate yang berguna untuk mereduksi data berdimensi besar yang memiliki korelasi sehingga membentuk data dengan variabel yang lebih sedikit tetapi tetap mampu menjelaskan keragaman dari data. PerkembanganPCAselanjutnya dipengaruhi adanya kebutuhan suatu model PCAyang robust terhadap data pencilan / outlier.Oleh karena itu dikenalkanlah suatu konsep robust PCA.Tujuan dari metode robust PCAadalah untuk memperoleh komponen utama yang tidak terlalu banyak terpengaruh dengan keberadaan outlier. Pada skripsi ini akan dipelajari mengenai ROBPCA yang dikembangkan oleh Hubert dkk (2005) dengan menggabungkan konsep Projection Pursuit (PP)dan estimator kovarian yang robust yang dalam hal ini digunakan Minimum Covariance Determinant (MCD).Hasil analisis selanjutnya dibandingkan dengan Classic Principal Component Analysis (PCA).Dalam studi kasus ini diperoleh kesimpulan bahwa ROBPCA lebih baik dari CPCA karena metode ROBPCA mampu menghasilkan jumlah komponen utama yang lebih sedikit daripada metode CPCA dantelah dapat menjelaskan 84,79% dari total variansi sampel.  outlier, CPCA, ROBPCA, komponen  utama, robust","ABSTRACT    Principal Component Analysis (PCA) is a multivariate analysis that is useful for reducing the high dimensional data of which have a correlation to form the data with less variables but still able to explain the diversity of the data. Development of PCA influenced by robust PCA models for data with outliers.Therefore introduced a robust concept of PCA. The purpose of robust PCA method is to obtain the main component that is not too much affected by the presence of outliers. In this thesis will learn about ROBPCA developed by Hubert et al (2005) to incorporate the concept of Projection Pursuit (PP) and a robust covariance estimator in this case used the Minimum Covariance Determinant (MCD). Then, the results of the analysis compared by the Classic Principal Component Analysis (PCA). In the case study, concluded that ROBPCA better than CPCA because ROBPCA method is able to produce the amount of less principal components than CPCA method and able to explain 84.79% of the total variance of the sample. Key words : outlier, CPCA, ROBPCA, principal component, robust","Kata Kunci : outlier, CPCA, ROBPCA, komponen  utama, robust"
http://etd.repository.ugm.ac.id/home/detail_pencarian/91285,ANALISIS KAPABILITAS PROSES DENGAN FUNGSI DENSITAS KERNEL,"NUNING SETIYARTI, Dr. Danardono, MPH",2015 | Skripsi | S1 STATISTIKA,"Analisis kapabilitas proses tidak terlepas dari analisis pengendalian kualitas statistik. Analisis pengendalian kualitas menggunakan metode-metode standar biasanya membutuhkan asumsi bahwa karakteristik kualitas yang bersangkutan mengikuti distribusi normal. Tidak terpenuhinya asumsi ini dapat menghasilkan penaksiran batas-batas pengendali yang kurang tepat. Pendekatan nonparametrik berdasarkan fungsi densitas kernel merupakan alternatif yang dapat digunakan untuk melakukan analisis pengendalian kualitas jika asumsi normalitas tidak terpenuhi dan terbukti menghasilkan estimasi batas pengendali yang lebih baik. Selanjutnya, analisis kapabilitas proses dapat dilakukan dengan pendekatan yang sama pula. Pendekatan berdasarkan fungsi densitas kernel tersebut digunakan untuk menganalisis data elongation (kelenturan) benang jenis 30 TR 1008 Cop dan 30 RT SIRO KEPYUR Cop. Fungsi kernel yang digunakan dispesifikasikan dalam bentuk kernel Epanechnikov dan metode pemilihan bandwidth yang digunakan adalah unbiased cross-validation. Grafik pengendali kernel dibuat berdasarkan estimasi distribusi kumulatif kernel. Estimasi kapabilitas proses dihitung dengan menganalisis tingkat kegagalan proses. Dari analisis yang dilakukan dengan batas spesifikasi yang telah ditentukan oleh perusahaan, diketahui bahwa proses produksi kedua jenis benang dengan memperhatikan karakteristik kualitas elongation memiliki kapabilitas proses yang sangat buruk. Sedangkan dari perbandingan hasil analisis kapabilitas proses berdasarkan fungsi densitas kernel, asumsi distribusi normal, serta hasil transformasi Box-Cox pada data elongation benang 30 TR 1008 Cop untuk beberapa variasi batas spesifikasi dapat disimpulkan bahwa analisis kapabilitas proses berdasarkan densitas kernel bekerja dengan lebih baik dalam menganalisis karakteristik kualitas nonnormal untuk interval spesifikasi yang lebih lebar sehingga lebih banyak pula observasi karakteristik kualitas yang berada di dalam interval tersebut.","Process capability analysis can not be separated from the statistical quality control analysis. Quality control analysis using standard methods usually requires the assumption that the quality characteristic of interest follows a normal distribution. Departures from this assumption can result in wrong control limits estimation. Nonparametric approach based on kernel density function is an alternative that can be used to perform quality control analysis when the assumption of normality is not met and has been proven to produce better control limits estimate. Furthermore, process capability analysis can be carried out with the same approach as well. The approach based on kernel density function is used to analyze elongation data of thread type 30 TR 1008 Cop and 30 RT SIRO KEPYUR Cop. The kernel function used is specified in the form of Epanechnikov kernel and bandwidth selection method used is unbiased cross-validation. Kernel control chart built based on kernel cumulative distribution estimate. Estimation of process capability is calculated by analyzing the process failure rate. From the analysis conducted with the specification limits determined by the company, it is known that the production processes of both thread types by the attention to quality characteristics elongation have very poor process capability. While the comparison of the process capability analysis results based on a kernel density function, assuming that the data follow normal distribution, as well as result of Box-Cox transformation of the elongation data of 30 TR 1008 Cop thread for several variations of specification limits can be concluded that the process capability analysis based on kernel density function performs better in analyzing the nonnormal quality characteristic for wider specification interval so that the quality characteristic observations are also a lot more within the interval.","Kata Kunci : Fungsi densitas kernel, Metode unbiased cross-validation, Grafik pengendali individu kernel, Tingkat kegagalan proses, Kapabilitas proses"
http://etd.repository.ugm.ac.id/home/detail_pencarian/86425,INTEGRASI METODE TAGUCHI DAN METODE PERMUKAAN RESPON DALAM PENINGKATAN KUALITAS PRODUK,"AKHSANUL HADITS B, Dr. Gunardi, M.Si. ; Vemmie Nastiti Lestari, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Studi ini akan membahas mengenai bagaimana menentukan sebuah desain percobaan yang kuat untuk mendapatkan hasil percobaan seperti yang diingikan dan percobaan tersebut dilakukan secara efisien. Metode tersebut adalah Metode Taguchi. Dengan menggunakan Metode Taguchi, jumlah percobaan yang harus dilakukan untuk mendapatkan hasil yang diinginkan bisa diminimalkan dibandikan dengan percobaan faktorial penuh dan didapatkan hasil baik. Hal ini dikarenakan Orthogonal Array dalam Metode Taguchi. Namun, kekurangan Metode Taguchi adalah terdapat kemungkinan terjadinya pembauran interaksi oleh faktor utama dalam percobaan, sehingga menyebabkan hasil prediksi yang tidak sesuai dengan kenyataan. Untuk menangani hal tersebut, akan dilakukan pengintegrasian dengan menggunakan Metode Permukaan Respon. Dengan menggunakan Metode Taguchi sebagai alat untuk memilih variabel yang signifikan terhadap percobaan dan Metode Permukaan Respon sebagai alat untuk melakukan perhitungan statistiknya. Metode Permukaan Respon dapat menggantikan prediksi yang dilakukan dengan Metode Taguchi.  Integrasi Metode Taguchi dan Metode Permukaan Respon diaplikasikan pada percobaan pembuatan kabel fiber. Dimana hasil percobaan dengan menggunakan integrasi kedua metode tersebut memberikan hasil yang lebih akurat dibandingkan dengan hanya menggunakan Metode Taguchi dan memberikan percobaan yang lebih efisien.","This study will discuss about how to determine a robust experimental design to get best results experiments and the experiments carried out efficiently. One method that is commonly used in designing an experiment is the Taguchi Method. By using the Taguchi method, the number of experiments to be done to get the desired results can be minimized rather then using full factorial experiments and obtained good results. This is because it uses Orthogonal Array in the Taguchi Method. However, there is a shortage in the Taguchi method, ie there is a possibility of confounding by the interaction of the main factors in the experiment, causing the prediction results sometimes do not correspond to reality. To handle this, the integration will be carried out by using Response Surface Method. By using the Taguchi Method as a tool for selecting the significant variables of the experiment and Response Surface Method as a tool to perform statistical calculations. Response Surface Method can replace the prediction made by Taguchi Method.  The integration of Taguchi Method and Response Surface Method was applied to the experimental manufacture of fiber cable. Where the results of experiments using the integration of the two methods give more accurate results compared to just using the Taguchi method and provide a more efficient trial.","Kata Kunci : Taguchi Method, Response Surface Method, Design Experiment, Orthogonal Array"
http://etd.repository.ugm.ac.id/home/detail_pencarian/91550,KULLBACK'S INFORMATION CRITERION CORRECTION (KICC) UNTUK SELEKSI MODEL REGRESI LINEAR MULTIVARIAT,"ITTA AGATHYA SARAH, Drs. Zulaela, Dipl. Med. Stats., M.Si",2015 | Skripsi | S1 STATISTIKA,"Analisi regresi linear adalah analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Apabila terdapat beberapa variabel dependen yang saling berkorelasi dan satu atau lebih variabel independen, maka dinamakan analisis regresi linear multivariat. Dalam analisis regresi pemilihan model merupakan hal yang penting, untuk mengetahui variabel prediktor mana saja yang berpengaruh signifikan terhadap variabel respon dalam model regresi. Kita telah mengetahui beberapa kriteria uji yang digunakan untuk seleksi model regresi, dan yang paling sering kita gunakan adalah kriteria AIC (Akaike Information Criterion). Tetapi apabila AIC digunakan pada sampel kecil, AIC akan bersifat bias, sehingga dibentuk kriteria uji KICC (KullbackÃ¢ï¿½ï¿½s Information Criterion Correction) sebagai penyempurnaan terhadap AIC. KICC merupakan ekspektasi dari jarak Simetris Kullback-Leibler antara densitas model sementara dengan densitas model sebenarnya.","Regression analysis is a statistical analysis that used to perform model relationship between dependent variabel and independen variables. If there are several dependen variables which are correlated and one or more independen variables, then it called multivariate linear regression analysis. In regression, model selection is a important part in order to determine which predictor (independen) variables that significantly influence the response (dependen) variable in the regression model. We already know some of the test criteria used for selection regression model , and most often we use is the criteria AIC ( Akaike Information Criterion ) . But if the AIC is used on a small sample , AIC will be biased , so that the test criteria established KICC ( Kullback 's Information Criterion Correction ) as a refinement of the AIC. KICC is an expectation of Kullback-Leibler Symmetric Divergence between probability density of candidate model and a true model.","Kata Kunci : multivariate linear regression, model selection, small samples, Kullback Leibler symmetric divergence"
http://etd.repository.ugm.ac.id/home/detail_pencarian/90530,PREDIKSI PERGERAKAN HARGA SAHAM MENGGUNAKAN EPSILON SVR DAN NU SVR,"PURWIDHIATI A, Dr. Abdurakhman, S.Si., M.Si. ; Rianti Siswi Utami, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Harga saham dapat dipengaruhi oleh kejadian politik suatu negara. Dampak dari suatu kejadian politik terhadap harga saham tidak hanya berlaku saat hari kejadian berlangsung tapi juga beberapa hari sebelum dan setelah kejadian. Metode event study mampu menganalisa peristiwa yang signifikan mempengaruhi harga saham selama beberapa hari di sekitar hari peristiwa berlangsung. Sementara untuk memprediksi harga saham selama peristiwa terjadi, dapat dilakukan dengan menggunakan dua jenis metode Support Vector Regression (SVR), yaitu epsilon SVR serta nu SVR dengan bantuan Kernel tunggal. Kedua metode SVR tersebut mampu memprediksi harga saham tanpa harus memenuhi asumsi klasik seperti asumsi stasioneritas dan asumsi linearitas. 	Pemilihan Umum 2014 merupakan salah satu kejadian politik yang dianggap mampu mempengaruhi harga Indeks Harga Saham Gabungan (IHSG). Sebanyak 25 saham yang tergabung dalam IHSG diuji dengan event study terlebih dahulu, sebelum memprediksi harga IHSG dengan menggunakan metode epsilon SVR serta  nu SVR dengan bantuan beberapa jenis Kernel yaitu Kernel Gaussian, Laplacian, Anova serta Bessel. Dari perbandingan nilai MSE (Mean Squared Error) kedua metode SVR tersebut, disimpulkan bahwa metode nu SVR dengan menggunakan Kernel Laplacian lebih baik digunakan untuk memprediksi harga IHSG selama Pemilihan Umum 2014 daripada metode epsilon SVR.","Stock price can be affected by a states political event. The impact of political events on stock price does not only apply at the day it happens but also days before and after the event takes place. Event study method is able to analyze significant events that affecting stock price for a few days around the events. Meanwhile, for predicting stock price during the event can be determined using two kinds of Support Vector Regression (SVR) methods, those are epsilon SVR and nu SVR with the help of single Kernel. Both SVR methods are able to predict the stock price without having classic assumption such as stationarity assumption and linearity assumption General Election in 2014 is one of the political events that considered capable to affect the price of Indeks Harga Saham Gabungan (IHSG). A total of 25 stocks that are members of IHSG are tested by event study before predicting the price of stock index by using epsilon SVR and  nu SVR methods with the help of several kinds of Kernel such as Gaussian, Laplacian, Anova and Bessel. From the MSE (Mean Squared Error) result comparison of both methods, we can conclude that nu SVR method using Kernel Laplacian is better to predict the price of IHSG during the General Election in 2014 than epsilon SVR method.","Kata Kunci : Support Vector Regression, event study, saham, Kernel, nu SVR, epsilon SVR"
http://etd.repository.ugm.ac.id/home/detail_pencarian/81827,PENENTUAN HARGA ASET PADA LIQUIDITY-ADJUSTED CAPITAL ASSET PRICING MODEL (LCAPM),"RIO RIZKI ARYANTO, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Dalam pasar modal tidak semua aset dapat dijual dengan cepat atau likuid. Likuiditas menyatakan mudah atau tidaknya aset dijual di pasar modal dengan cepat. Aset yang tidak likuid adalah aset yang tidak dapat dijual dengan cepat di pasar modal dikarenakan sedikitnya pembeli yang berminat terhadap aset tersebut. Aset tersebut mempunyai risiko likuiditas yaitu risiko yang dimiliki pemegang aset selama menahan aset tersebut sebelum akhirnya terpaksa menjualnya di bawah harga pasar. Likuiditas aset dinyatakan dengan biaya likuiditas dan risiko likuiditas dimana kedua faktor tersebut mempengaruhi tingkat pengembalian atau return dari aset. Liquidity-Adjusted Capital Asset Pricing Model (LCAPM) adalah suatu model yang menggunakan likuiditas aset dalam menghitung tingkat pengembalian atau return dari aset tersebut. Pada model LCAPM risiko likuiditas diwakili oleh tiga nilai kovariansi yaitu kovariansi antara likuiditas aset dengan likuditas pasar, kovariansi antara return aset dengan likuiditas pasar dan yang terakhir adalah nilai kovariansi antara likuiditas aset dengan return pasar. Dengan menggunakan model LCAPM dapat dilihat bagaimana pengaruh likuiditas dalam menentukan harga aset. LCAPM merupakan model pengembangan dari Capital Asset Pricing Model (CAPM). Dengan membandingkan portofolio dari kedua model tersebut dapat dilihat model manakah yang memiliki return lebih besar. Dari hasil perbandingan tersebut diperoleh bahwa portofolio LCAPM menghasilkan return yang lebih besar.","Not all of the assets in the capital market can be sold quickly. Liquidity is a measure that describe the ease of an assets can be sold quickly in capital market. Illiquid asset is an asset that can not be sold quickly in the market due to lack of interested buyer. Liquidity risk describe stockholders risk when holding an illiquid asset before selling it willingly below market price. Liquidity of an asset represented by liquidity cost and liquidity risk where these two factor effect the price of an asset.  Liquidity-Adjusted Capital Asset Pricing Model (LCAPM) is a model which using liquidity risk to determine expected return of an asset. In LCAPM covariance beetwen assets liquidity and markets liquidity, covariance beetwen assets return and markets liquidity and last covariance beetwen assets liquidity and markets return are used as proxy of liquidity risk. LCAPM show how  liquidity effect the price of an asset. LCAPM is a model of development from Capital Asset Pricing Model (CAPM). This work provide LCAPMs portfolio to be compared with CAPMs portfolio to show which is having higher expected return. The result show LCAPMs portfolio deliver higher expected return than CAPMs portofolio.","Kata Kunci : Likuiditas, Risiko Likuiditas, CAPM, Saham, Portofolio, Return"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84901,ESTIMATOR NADARAYA-WATSON DENGAN FUNGSI KERNEL GAUSSIAN,"NISA KHOFIFATUR R, Dr. Danardono, MPH",2015 | Skripsi | S1 STATISTIKA,"Regresi nonparametrik merupakan analisis regresi dengan pendugaan model dilakukan berdasarkan pendekatan yang tidak terikat asumsi bentuk kurva regresi tertentu, namun dibentuk sesuai dengan informasi yang ada dalam data. Salah satu jenis fungsi yang dapat digunakan untuk menduga bentuk regresi nonparametrik adalah fungsi kernel Gaussian. Pada regresi kernel, terdapat beberapa estimator yang dapat digunakan untuk memodelkan berat badan balita Kecamatan Kalasan, Kabupaten Sleman pada tahun 2014, salah satunya adalah estimator Nadaraya-Watson. Dalam melakukan analisis regresi kernel, diperlukan suatu konstanta penghalusan yang disebut dengan bandwidth. Beberapa metode yang dapat digunakan untuk mendapatkan nilai bandwidth yang sesuai dengan data adalah metode bandwidth Rule of Thumb, metode Unbiased Cross Validation (UCV), metode Biased Cross Validation (BCV) serta metode Complete Cross Validation (CCV). Untuk mengetahui metode yang lebih baik dalam mengestimasi kasus berat badan balita tersebut digunakan perbandingan nilai Mean Square Error (MSE). Nilai MSE yang paling kecil diperoleh menggunakan metode bandwidth Rule of Thumb, baik untuk data berat laki-laki maupun perempuan. Hasil estimasi menunjukkan bahwa pada usia yang sama, umumnya balita laki-laki lebih berat dibanding balita perempuan.","Nonparametric regression is a form of regression analysis in which the predictor does not take a predetermined form but is constructed according to information derived from the data. One type of function that can be used to estimate nonparametric regression is Gaussian kernel function.  In the kernel regression, there are several estimator that can be used to model children's weight at Kalasan, Sleman in 2014, one of them is Nadaraya-Watson. We need a constant smoothing called bandwidth. Several methods can be used to get the value of the bandwidth that fits the data is the bandwidth Rule of Thumb method, Unbiased Cross Validation (UCV) method, Biased Cross Validation (BCV) method and Complete Cross Validation (CCV) method. The comparison of Mean Square Error (MSE) value is used to evaluate the best method in estimating children's weight. The smallest MSE values for both male and female weight data were obtained by using bandwidth Rule of Thumb method. The estimation results indicated that at the same age, male children under five years old are generally heavier than female.","Kata Kunci : Regresi Nonparametrik, Regresi Kernel, Fungsi Gaussian,  Estimator Nadaraya-Watson, Bandwidth"
http://etd.repository.ugm.ac.id/home/detail_pencarian/92073,Metode Estimasi Robust Ganda pada Model Linear Terumumkan (MLT),"SILVIA FIFI WARDHANI , Dr. Abdurakhman, M.Si.; Rianti Siswi Utami, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Secara umum tujuan dari penelitian epidemiologi ialah untuk mengetahui hubungan antara prediktor dengan respon tertentu, dengan mengontrol sekelompok variabel kovariat tambahan. Biasanya metode yang digunakan untuk menyelesaikan masalah ini ialah model regresi yang sesuai untuk respon, dengan diketahui prediktornya dan variabel kontrol, secara umum model ini termasuk dalam kelompok Model Linear Terumumkan atau Generalized Linear Model (GLM). Parameter model tersebut dapat diestimasi melalui metode Maksimum Likelihood, jika model tersebut benar maka estimasi dari Maksimum Likelihood akan bersifat konsisten, tetapi jika tidak maka estimasi tersebut tidak konsisten. Sekarang ini, terdapat kelompok baru dari estimator yang dikenal dengan estimator Doubly Robust atau Robust Ganda telah dikembangkan. Pada estimator ini menggunakan dua model regresi yaitu satu model untuk responnya dan satu model untuk prediktornya, akan konsisten jika salah satu dari kedua model sesuai dengan data, namum tidak harus keduanya benar. Estimator Robust Ganda memberikan keuntungan kepada peneliti yaitu hanya dengan salah satu model pada regresi saja dapat membuat suatu kesimpulan yang valid.","A common aim of epidemiological research is to assess the association between a particular exposure and a particular outcome, controlling for a set of additional covariates. This is often done by using a regression model for the outcome, conditional on exposure and covariates. A commonly used class of models is the generalized linear models. The model parameters are typically estimated through maximum likelihood. If the model is correct, then the maximum likelihood estimator is consistent but may otherwise be inconsistent. Recently, a new class of estimators known as doubly robust estimators has been proposed. These estimators use two regression models, one for the outcome and one for the exposure, and are consistent if either model is correct, not necessarily both. Thus doubly robust estimators give the analyst two chances instead of only one to make valid inference.","Kata Kunci : estimator Robust Ganda, Model Linear Terumumkan, parameter gangguan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/81836,ANALISIS SENTIMEN MENGGUNAKAN METODE NAIVE BAYES CLASSIFIER DENGAN MODEL DOKUMEN BERNOULLI DAN SUPPORT VECTOR MACHINE,"TIARA GUMILANG RAMADHANI, Prof. Dr. rer. nat. Dedi Rosadi, S.Si., M.Sc.Eng. Math",2015 | Skripsi | S1 STATISTIKA,"Berkembang pesatnya sistem informasi berbasis website memberikan berbagai kemudahan untuk mendapatkan informasi dalam jumlah besar dan secara cuma - cuma. Informasi yang tersaji dalam bentuk tekstual seringkali berbentuk dokumen yang tidak terstuktur. Untuk menangani dokumen dengan pola tidak terstruktur ini analisis text mining sangat diperlukan. Salah satu analisis text mining yang sering digunakan adalah klasifikasi sentimen atau klasifikasi teks. Dari hasil klasifikasi sentimen mengenai suatu topik ini dapat disimpulkan opini tertentu dengan melihat  proporsi kelas dari keseluruhan dokumen yang disajikan. Metode yang akan dibahas kali ini adalah metode probabilistic Naive Bayes Classifier dan metode Support Vector Machine untuk menentukan kelas suatu dokumen secara biner yaitu kelas positif dan kelas negatif. Naive Bayes Classifier merupakan metode klasifikasi menggunakan aturan Bayesian dengan memanfaatkan probabilitas prior serta probabilitas bersyarat dari frekuensi kata yang muncul pada masing - masing kelas dokumen training. Nilai tersebut yang akan digunakan untuk menentukan kelas dokumen testing dengan melihat nilai maximum a posteriori masing - masing kelas. Data yang telah diklasifikasi kemudian dihitung tingkat akurasi kebenarannya menggunakan metode support vector machine dengan menentukan fungsi Kernel serta proporsi untuk data training dan data testing yang sesuai. Dari perbandingan nilai akurasi klasifikasi dengan menggunakan kedua metode diatas, didapatkan nilai akurasi paling tinggi adalah dengan menggunakan metode naive bayes classifier (NBC) dengan akurasi sebesar  62,6295%. Dari 5.412 dokumen training dan 2701 dokumen testing yang telah dilkasifikasi, didapatkan proporsi opini untuk kelas positif sebesar 44,46501% dan opini untuk kelas negatif sebesar 55,53499%.","The rise of information system base on website has given a lot of simplicity to get the information in a large number and for free. An information with textual document is usually given as an unstructured form. To dealing these unstructured document form, text mining is being required. One of analysis in text mining which is often used is sentimen classification or text classification. From the result of the classification about one topic we can specify the classes for the opinions by seeing the proportion on each class from the entire documents provided. Two methods that will be discussed are naive bayes classifier and support vector machine to determine a binary classes for each document. Naive bayes classifier is a classification method with Bayesian rule by using the prior probabilities and conditional probabilities in each class from data training. The probabilities is being used to determine the maximum a posteriori each classes for the data testing and specify the document classes. The accuracy for the classified data is being calculated by using support vector machine method with the Kernel function and data proportion determined from the data training. By comparing these two methods, the best accurate rate for the sentimen classification is by using naive bayes classifier (NBC) which is reached a percentage 62,6295%. By classifying 5.412 data training and 2701 data testing using naive bayes classifier, the result shows from the entire documents the proportion for the positive class is 44,46501% and for the negative class is 55,53499%.","Kata Kunci : Klasifikasi sentimen, opini, text mining, aturan Bayesian, Naive bayes classifier, support vector machine."
http://etd.repository.ugm.ac.id/home/detail_pencarian/82604,Analisis Teknikal Saham Menggunakan Indikator Bollinger Bands dan Average Directional Index (ADX),"DHIAJENG GENDHIS OKTARINI , Dr. Gunardi, M.Si",2015 | Skripsi | S1 STATISTIKA,"Analisis teknikal merupakan suatu metode pembacaan grafik data historis saham untuk mengetahui pergerakan harga saham. Terdapat berbagai macam indikator yang dapat digunakan dalam analisis teknikal saham. Dalam penelitian ini akan dibahas mengenai analisis teknikal saham menggunakan indikator teknikal Bollinger Bands dan Average Directional Index. Ada dua macam kategori dalam  penelitian ini, yaitu sistem yang menggunakan Bollinger Bands dan sistem yang menggunakan indikator gabungan  Bolllinger Bands  dan  Average Directional Index,  yang dilakukan pada empat saham bank  bluechip  di Indonesia. Indikator Bollinger Bands  berguna untuk mengidentifikasi harga saham relatif tinggi atau relatif rendah. Hal tersebut bermanfaat untuk strategi masuk dan keluarnya pasar.  Sedangkan indikator  Average Directional Index  berguna untuk mengukur pergerakan harga sedang dalam tren naik, turun atau  sideways. Hal tersebut bermanfaat untuk mengukur kekuatan tren yang terjadi.  Penambahan indikator Average Directional Index (ADX) pada sistem Bollinger Bands menghasilkan rata rata persentase prediksi benar lebih besar daripada hanya sistem Bollinger Bands","Technical analysis is a graphic reading method of stock history data to  define stock value activation. There are a lots of indicator can be used in stock  technical analysis. In this research focus on study about stock technical analysis  using by Bollinger Bands technic and Average Directional Index. Two categories  in research are  Bollinger Bands  system  and combined of  Bolllinger Bands- Average Directional Index system, that applying in four stocks of bluechip bank in Indonesia. Bollinger Bands used to identified high or low relative stock value. It useful to define about income and outcome market strategy.  Average Directional Index  indicator used to measured value activation in increase, decrease, or sideways trend. It can be used to measured trend power that happens.","Kata Kunci :  Saham, analisis teknikal,  Bollinger Bands,  Average Directional Index"
http://etd.repository.ugm.ac.id/home/detail_pencarian/78771,ESTIMASI BAYESIAN DARI PELUANG KEGAGALAN PADA PORTOFOLIO DENGAN KEJADIAN KEGAGALAN YANG RENDAH,"FAUZAN ABDILLAH, Prof. Subanar, Ph.D.",2015 | Skripsi | S1 STATISTIKA,"Estimasi dari peluang gagal bayar (PD) pada portofolio dengan kejadian gagal bayar yang rendah menggunakan batas atas konfidensi merupakan cara yang biasa digunakan oleh beberapa lembaga keuangan, meskipun masih sering terjadi perdebatan dalam menentukan besarnya taraf konfidensi yang digunakan untuk estimasi tersebut. Estimator Bayesian untuk PD berdasarkan prior tak informatif, distribusi prior uniform merupakan salah satu alternatif yang dapat menghindarkan dalam pemilihan taraf konfidensi. Dalam skripsi ini akan ditunjukkan pada kasus kejadian gagal bayar yang independen, batas atas konfidensi dari PD yang dapat direpresentasikan sebagai kuantil dari distribusi posterior Bayesian lebih bersifat konservatif dari pada estimator Bayesian dari PD dengan berdasarkan prior tak informatif. Selanjutnya akan dibahas mengenai implementasi estimator Bayesian dari prior neutral tak informatif dan estimator Bayesian konservatif (conservative Bayesian Estimator) pada kasus satu periode maupun multiperiode dengan data kasus kejadian gagal bayar yang dependen dan membandingkan hasil estimasi tersebut dengan estimasi menggunakan batas atas konfidensi. Perbandingan tersebut akan membawa pada versi constraine dari estimator Bayesian dengan prior neutral tak informatif sebagai alternatif untuk estimator batas atas konfidensi.","The estimation of probabilities of default (PD) for low default portfolios by means of upper confidence bounds is a well procedure that's used by financial institutions, but there are still often discussions about which confidence level to use for the estimation. The Bayesian estimator for the PD based on the uninformed prior, uniform prior distribution is an obvious alternative that avoids the choice of confidence level. In this thesis, we demonstrated that in the case of independent default events, the upper confidence bounds of the PD can be represented as quantile of a Bayesian is more conservative than Bayesian estimator for the PD based on the uninformed prior. Then in the thesis, we discribed about implemention of the Bayesian estimators based on neutral uninformed prior and conservative Bayesian estimator in the dependent one and multi-period default data cases and compare their estimates to the upper confidence bound estiamates. The comparison leads to a constrained version of the uninformed neutral Bayesian estimator as an alternative to the upper confidence bound estimators.","Kata Kunci : peluang gagal bayar, portofolio dengan kejadian gagal bayar yang rendah, batas atas konfidensi, estimator Bayesian, distribusi prior, distribusi posterior, probability of default, low default portfolio, upper confidence bound, Bayesian estimator, prior di"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84919,VALUE AT RISK MENGGUNAKAN METODE MAKSIMUM ENTROPY BOOTSTRAPPING DAN FLEX MAKSIMUM ENTROPY BOOTSTRAPPING,"AVISTA NURMAULIDYA, Dr. Abdurakhman, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Nilai Risiko adalah nilai potensi terjadinya bahaya, akibat atau konsekuensi yang dapat terjadi pada sebuah proses yang sedang berlangsung atau kejadian yang akan datang. Manajemen risiko adalah proses pengelolaan risiko yang mencakup identifikasi, evaluasi dan pengendalian risiko yang dapat mengancam kelangsungan aktivitas usaha. Permasalahannya adalah bagaimana perusahaan dapat mengukur risiko potensi terjadinya suatu peristiwa baik yang dapat diperkirakan maupun yang tidak dapat diperkirakan yang dapat menimbulkan dampak bagi pencapaian tujuan Organisasi. Kebutuhan untuk mengelola risiko, yaitu risiko kredit dan risiko pasar di perusahaan perbankan dan asuransi sudah menjadi perhatian yang serius. Penghitungan Value at Risk (VaR) yang menggunakan pendekatan central atau normal (tradisional) yaitu basic indicator approach (BIA), standardized approach (SA) dan alternative standardized approach (ASA), telah dipelajari dan dipahami menjadi tidak tepat karena menggunakan parameter yang hanya sesuai dengan business line perbankan dan tidak dapat mengakomodasi nilai risiko kejadian ekstreme. Pengamatan terkini menunjukkan bahwa (selalu) ada potensi kejadian - kejadian yang bersifat ekstrim, dimana frekuensi terjadinya memang sangat rendah namun, jika itu terjadi maka akan menimbulkan dampak kerugian yang sangat besar. Fenomena ekstrim ini tidak tercakup dalam penghitungan VaR Prinsip maksimum Entropi didasarkan pada pertimbangan bahwa ketika memperkirakan suatu distribusi probabilitas , sebaiknya memilih distribusi yang memiliki nilai ketidakpastian tetap yang terbesar (yaitu,entropi maksimum) konsisten dengan permasalahan. Entropi dapat dimaksimalkan secara analitis. Yang di kombinasikan dengan menggunakan algoritma bootstrappig utuk megatasi beberapa data tertentu. Metode Maximum Entropy Bootstrapping sangat sesuai untuk megatasi data yang memiliki nilai ekstrim.","Value of risk remains as the danger value for investor and important measures of riskfor financial assets. as a result or consequence that may occur in a process that is ongoing or upcoming events. Risk management is the process of risk management that includes the identification, evaluation and control of risks that could threaten the continuity of business activities. The problem is how companies can measure the potential risk of occurrence of an event that can both predictable and unpredictable that can have an impact for the achievement of objectives of the Organization. The need to manage risk, such as credit risk and market risk in the banking and insurance companies has become a serious concern. Calculating Value at Risk (VaR) approach that uses a central or normal (traditional) is the basic indicator approach (BIA), standardized approach (SA) and alternative standardized approach (ASA), has been studied and understood not be exact due to using only the appropriate parameters by business line banking and can not accommodate the extreme event risk value. Recent observations indicate that (always) there is potential events - events that are extreme, where the frequency of occurrence is very low but, if it happens it will result in huge losses. This extreme phenomena are not included in the calculation of VaR The maximum entropy principle is based on the consideration estimating a probability distribution, you should choose the distribution that has value remains the largest uncertainty (ie, maximum entropy) consistent with the problems. Entropy can be maximized analytically. Which in combination with the use of algorithms bootstrappig weeks to megatasi some specific data. Maximum Entropy bootstrapping method is suitable for megatasi data have extreme values.","Kata Kunci : Value at Risk, Maximum Entropy, risk management, Extreme Value Theory, Bootsrapping"
http://etd.repository.ugm.ac.id/home/detail_pencarian/79549,Estimasi Tingkat Suku Bunga Menggunakan Metode Nonparametrik Kernel,"TRI SETIANTA, Dr. Gunardi, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Tingkat suku bunga merupakan salah satu faktor yang sangat berpengaruh dalam sebuah investasi atau dalam bidang perekonomian. Setiap periode tertentu, tingkat suku bunga disetiap negara fluktuatif. Oleh sebab itu, pengestimasian tingkat suku bunga untuk periode selanjutnya sangatlah penting untuk menentukan perlakuan dari sebuah investasi. Terdapat cukup banyak metode untuk menentukan tingkat suku bunga, diantaranya model suku bunga CIR yang telah banyak digunakan dikarenakan menghasilkan tingkat suku bunga yang positif. Selain itu terdapat metode lain yang baru-baru ini dibahas oleh beberapa peneliti, yaitu metode Nonparametrik Kernel.  Dalam skripsi ini, dibahas dua metode nonparametrik kernel yaitu Gaussian Kernel dan Gamma Kernel. Sebelum melakukan estimasi tingkat suku bunga, dilakukan terlebih dahulu estimasi fungsi drift dan fungsi difusi untuk model persamaan diferensial stokastik untuk metode ini. Penurunan estimasi fungsi tersebut menggunakan metode infinitesimal generator untuk order pertama. Dan setelah dilakukan estimasi, dihasilkan bahwa metode Gaussian menghasil hasil estimasi yang lebih dekat dibandingkan dengan hasil estimasi dengan metode Gamma Kernel. Hal ini dilihat dari besar Mean Squarred Error masing-masing.","The interest rate is one of the most influential factor in an investment or in the field of economy. Any particular period, the interest rate fluctuates in each country. Therefore, estimating the interest rate for the next period it is important to determine the treatment of an investment. There are quite a lot of methods to determine the interest rates, including interest rate CIR models that have been widely used due to produce a positive interest rate. In addition there are other methods that have recently discussed by several researchers, the method of Nonparametric Kernel.  In this minithesis, discussed two methods of nonparametric kernel, there are Gaussian Kernel and Gamma Kernel. Before estimating the interest rates, estimates of the functions performed first drift function and diffusion functions for stochastic differential equation model for this method. Lower estimates of these functions using the first order of infinitesimal generator. And the result is the Gaussian Kernel methods produce estimates that are closer than the estimation results using Gamma Kernel methods. It is seen from the Mean Squarred Error respectively.","Kata Kunci : tingkat suku bunga, Nonparametrik Kernel, Gaussian Kernel, Gamma Kernel, Infinitesimal generator, fungsi drift, fungsi difusi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/83645,PEMILIHAN INDIKATOR TERBAIK DALAM ANALISIS TEKNIKAL SAHAM DENGAN MENGGUNAKAN DESCISION POINT PRICE MOMENTUM OSCILATOR DAN PRINGS KNOW SURE THING,"REVI FIRMANSYAH, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Analisis teknikal pertama kali dikembangkan oleh Homma Munehisa pada awal abad ke 18 yang menggunakan teknik grafik lilin (candlestick chart) yang merupakan perangkat analisis yang utama pada saat ini. Ketika ilmu matematika dan statistika mulai diaplikasikan dalam analisis teknikal, mulailah banyak diciptakan indikator. Descision Point Price Momentum Oscilator (PMO) merupakan salah satu indikator yang berdasarkan pada perhitungan Rate of Change (ROC) yang dihaluskan dua kali dengan exponential moving averages (EMA) yang menggunakan proses pemulusan khusus. PMO membutuhkan bobot dalam metode perhitungannya. Dalam skripsi ini, bobot EMA digunakan untuk menentukan bobot PMO namun tidak dengan menambahkan satu ke dalam banyak periode untuk membuat bobot dalam pemulusan berganda. PMO menunjukkan arah dan kecepatan perkembangan harga. Selain itu, PMO dapat digunakan sebagai strategi trading guna menentukan sinyal jual atau beli saham dengan cara menggunakan batas overbought  dan oversold. Sedangkan Know Sure Thing (KST) adalah osilator momentum yang berdasarkan pada pemulusan dari Rate of Change (ROC) selama empat frame waktu yang berbeda dengan pemulusan menggunakan simple moving average yang dikembangkan oleh Martin Pring. KST menghitung nilai akhir yang berfluktuasi antara nilai-nilai positive dan negative yang berada di atas dan di bawah garis nol. Analisis teknikal ini menggunakan informasi ini untuk melihat adanya divergensi, kondisi overbought dan oversold, dan juga sinyal crossover.","Technical analysis was developed for the first time by Homma Munehisa at the beginning of 18th century by using candlestick chart which is the primary tools analysis for current technical stock analysis. The used of math and statistics in technical stock analysis create some indocators like Descision Point Price Momentum Oscilator (PMO) which is based on Rate of Change (ROC)result that had been smoothed twice by using exponential moving averages (EMA) with particular smoothing. The custom smoothing functions are very similar to Exponential Moving Averages but instead of adding one to the time period setting to create the smoothing multiplier (as in a true EMA), the smoothing functions just uses the period by itself. Besides, PMO can be used for trading strategy to specify the sell and sold signal by using the overbought and oversold bound. Know Sure Thing (KST) is the momentum oscilator based on Rate of Change (ROC) smoothing in the different four time frames by using Simple Moving Average smoothing. KST calculates the fluctuated final result the positive and negative values which in above and below the zero point. This technical stock analysis is utilizing the information to know the divergence, oversold and overbought condition, and also the crossover signal.","Kata Kunci : analisis teknikal, simple moving average, exponential moving average, descision point price momentum oscilator, know sure thing / technical stock analysis, simple moving average, exponential moving average, descision point price momentum oscilator, know s"
http://etd.repository.ugm.ac.id/home/detail_pencarian/83136,PENGECEKAN ASUMSI PROPORTIONAL HAZARD PADA MODEL REGRESI COX,"NADIA ISMALIA, Dr. Danoardono, MPH",2015 | Skripsi | S1 STATISTIKA,"Regresi Cox proportional  hazard adalah  suatu  metode  statistika  yang digunakan  untuk  menganalisis  hubungan  antara  variabel  dependen  yang  berupa data  waktu  hidup  dan  status event,  dengan  satu  atau  lebih  kovariat  (variabel independen) yang terukur pada saat dilakukan penelitian. Ketika  model  regresi Cox  Proportional  Hazard diterapkan  pada  data survival,  hal  yang  harus  dipenuhi  adalah  bahwa  data  harus  memenuhi  asumsi proportional  hazard. Proportional  hazard yang  tidak  signifikan  dapat menyebabkan  tidak  adanya  perbedaan  dalam  interpretasi  data  terutama  untuk ukuran sampel yang besar. Terdapat  tiga  pendekatan  umum  untuk  pengecekan  asumsi proportional hazard, yaitu pendekatan dengan grafik, pendekatan uji goodness of fit (GoF) dan pendekatan  dengan  menambah  variabel  yang  berubah  menurut  waktu  (timeindependent  variable/ covariates).  Terdapat  dua  cara  melalui  pendekatan  secara grafik  yaitu  menggunakan  kurva log-log survival dan membandingkan  kurva observasi dengan ekspektasi/ prediksi (observed versus expected).","Cox  proportional  hazard  is  a  statistical method  used  to  analyze  the relationship between the dependent variable in the form survival time and event, with one or more covariates were measured at the time of the study. When  the  Cox  proportional  hazard  regression  model  applied  to the survival  data, assumption  proportional  hazard  should  be  checked  whether  the assumptions  are  met. Nonproportional  hazards  can  cause  the  absence  of differences in the interpretation of data, especially for large sample sizes. There are three general approaches to checking the proportional hazards assumption, approach  using  graph,  testing the  goodness  of  fit  (GoF)  and approach by adding variables that change over time (time independent variable/ covariates). There are two ways through approach using graph, that uses log-log survival curves and comparison of observed with expected survival curves","Kata Kunci : Survival, Hazard, Cox Proportional Hazard Regression."
http://etd.repository.ugm.ac.id/home/detail_pencarian/92864,PERBANDINGAN PERFORMANSI METODE C5.0 DAN METODE CHAID DALAM MENGKLASIFIKASI DATA PENDAPATAN PENDUDUK,"RIZTY ANGGRAINI ANUGRAH , Prof.Dr.rer.nat Dedi Rosadi, M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Data mining merupakan sebuah alat yang sangat bermanfaat untuk memperoleh informasi-informasi dari data yang sangat besar. Salah satu bagian penting pada data mining adalah pengklasifikasian data. Klasifikasi digunakan untuk menggolongkan data berdasarkan pada sifat data yang sudah dikenali masing-masing kelasnya. Ada berbagai macam teknik yang digunakan untuk mengklasifikasi data, seperti algoritma C5.0 dan algoritma CHAID. Pada skripsi ini, difokuskan pada metode klasifikasi C5.0 dan CHAID pada data pendapatan penduduk. Pendapatan penduduk per tahun ini dibagi kedalam dua jenis, yakni <=50.000 dollar dan >50.000 dollar. Dengan menggunakan kedua metode tersebut, dapat diperoleh perbandingan akurasinya sehingga dapat disimpulkan metode yang paling cocok untuk mengklasifikasi data pendapatan penduduk serta dapat dilakukan prediksi berdasarkan variabel prediktornya.","Data mining is the useful tool to discovering knowledge from large data. One of the most important thing is classification of the data. Classification is used to classify the data based on their characteristics which is recognized each classes. There are some techniques to classify the data, C5.0 algorithm and CHAID algorithm. In this minithesis, concerned to classify income dataset using C5.0 and CHAID. This income per year divided into <=$50,000 and >$50,000. Using those methods, we can compare the accuracy so we can conclude the best method to classify the income dataset and predict it consider to the predictor variables.","Kata Kunci : Kata-kata kunci : data mining, klasifikasi, metode C5.0, metode CHAID, data pendapatan penduduk"
http://etd.repository.ugm.ac.id/home/detail_pencarian/86465,PENENTUAN PREMI GROUP YIELD INSURANCE MENGGUNAKAN ESTIMASI DENSITAS BOTEV,"AYU ARDELIA P, Dr. Adhitya Ronnie E., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Salah satu alternatif untuk melindungi usaha pertanian dari risiko kerugian akibat dari berbagai fenomena bencana alam adalah dengan menerapkan suatu asuransi pertanian yang telah terbukti berhasil diterapkan oleh beberapa Negara maju. Salah satu jenis jenis asuransi pertanian yang telah terbukti berhasil diterapkan adalah jenis asuransi pertanian Group Risk Plan milik FCIC di Amerika Serikat. Dalam skripsi ini, akan dibahas perhitungan premi jenis asuransi Group Risk Plan dengan memanfaatkan estimasi densitas Botev dalam perhitungan probabilitas terjadinya indemnitas. Dengan menggunakan data historik produktivitas hasil panen di beberapa provinsi di Indonesia dan juga data historik dari harga gabah kering panen pada tingkat petani, diperoleh premi jenis asuransi Group Risk Plan dalam satuan Rupiah per hektar yang harus dibayarkan oleh petani.","An alternative to protect farming from risks due to natural disasters is to implement a crop insurance program that has been successfully applied in developed countries. Group Yield Insurance or Group Risk Plan is a type of crop insurance program that has been provided by  FCIC, a wholly owned government corporation manages the federal crop insurance program in United States. 	This undergraduate thesis discuss about premium rating of Goup Risk Plan by utilizing Botev density estimation to calculate probability of occurrence of  indemnity. By using historical data of crop yields as a measure of productivity and historical prices of rice grain at the farm level in several provinces in Indonesia, obtained pemium rates of Group Risk Plan  in units of rupiah per hectare to be paid by farmers.","Kata Kunci : Premium, crop insurance, Kernel Density Estimation, Gaussian Kernel, linear diffusion, Group Risk Plan."
http://etd.repository.ugm.ac.id/home/detail_pencarian/83394,GRAFIK PENGENDALI INDIVIDUAL BERBASIS DISTRIBUSI WEIBULL 2-PARAMETER (Studi kasus : Elongation Benang 30 TR 1004 Cone Produksi PT. Pisma Putra Textile Pekalongan),"HELMI SAFITRI, Dr. Gunardi, M.Si",2015 | Skripsi | S1 STATISTIKA,"Grafik pengendali individual adalah suatu alat yang digunakan untuk mengendalikan suatu proses produksi dengan menggunakan sampel tunggal. Asumsi yang biasa digunakan dalam membuat grafik pengendali individual adalah asumsi normalitas, namun dalam kenyataannya tidak semua data yang diperoleh berdistribusi normal. Untuk mengatasi hal tersebut terdapat beberapa langkah alternatif salah satunya adalah membuat grafik pengendali individual dengan mengikuti pola distribusi dari data. Salah satu distribusi yang dapat digunakan dalam membuat grafik pengendali individual adalah distribusi Weibull. Dalam skripsi ini distribusi Weibull yang digunakan adalah distribusi Weibull dengan dua parameter (Weibull 2-P). Batas-batas pengendali diperoleh dengan memanfaatkan dua parameter dari distribusi Weibull tersebut. Data yang digunakan dalam studi kasus adalah data Elongation benang 30 TR 1004 Cone produksi PT. Pisma Putra Textile Pekalongan. Berdasarkan studi kasus, grafik pengendali yang dihasilkan dengan mengikuti pola distribusi Weibull dengan dua parameter memiliki tingkat sensitivitas yang lebih tinggi dibandingkan dengan grafik pengendali yang dibuat dengan menganggap data berdistribusi normal.","Individual control chart is a tool used to control a production process using a single sample. The commonly used assumption to make individual control chart is normality assumption, but on the reality not all the obtained data  has normal distribution. In order to solve that, there are a few alternatives way,  one of them is by making individual control chart using distribution patterns of the data. One of the distribution which can be used to make individual control chart is Weibull distribution. The Weibull distribution that used on this thesis is 2-parameter Weibull distribution (Weibull 2-P). The controlling boundaries got by using 2 parameters of the weibull distribution itself. The data that used on the study case is the Elongation data from yarn 30 TR 1004 Cone which product of PT. Pisma Putra Textile Pekalongan. Based on the study case, the result of controlling chart by using Weibull distribution pattern with 2 parameters has higher sensitivity level than controlling chart made using normal distribution data.","Kata Kunci : grafik pengendali individual, Weibull 2-P, Elongation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/84418,INDEKS KEMAMPUAN PROSES BERDASARKAN  MEDIAN ABSOLUTE DEVIATION,"RINDANG NDARU PUSPITA, Prof. Dr. Sri Haryatmi Kartiko, M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Indeks kemampuan proses berdasarkan median absolute deviation (MAD) dirancang untuk menentukan indeks kemampuan proses dari data tidak berdistribusi normal. Kemampuan proses adalah kemampuan suatu proses dalam kondisi normal dan keadaan terkendali. Indeksnya digunakan untuk mengukur variabilitas dari suatu proses dan dapat mencerminkan kinerjanya. Untuk menghitung indeks kemampuan proses, sebagian besar industri biasanya menganggap bahwa distribusi proses adalah normal. Namun, dalam praktiknya, sebagian besar proses pengendalian kualitas tidak memenuhi uji normalitas dan dengan demikian akurasi indeks kemampuan proses berdasarkan normalitas menjadi diragukan karena tidak benar-benar mencerminkan kinerja dari proses.            Median absolute deviation adalah estimasi robust pada variabilitas ketika data sampel tidak normal atau menceng. Data proses yang sebenarnya dan data simulasi dari distribusi yang sangat menceng digunakan untuk menunjukkan penerapan indeks kemampuan proses berdasarkan MAD dan hasilnya dibandingkan dengan indeks kemampuan proses berdasarkan quantile. Sehingga diperoleh hasil bahwa indeks kemampuan proses berdasarkan MAD lebih baik digunakan pada data tidak berdistribusi normal dengan skewness positif, sedangkan indeks kemampuan proses berdasarkan quantile lebih baik digunakan pada data tidak berdistribusi normal dengan skewness negatif.","Process capability indices based on median absolute deviation (MAD) designed to determine of process capability indices for a non-normal data. Process capability is the performance of a process under normal and in-control conditions. Its indices are to measure the inherent variability of a process and thus to reflect its performance. To calculate the capability indices, most industries normally assume that the distribution of their process output is normal. However, in practice, most of process quality control fails the normality test and thus the accuracy of normal based process capability indices becomes doubtful and hence they cannot really reflect the performance of a process.  	The median absolute deviation is a robust estimate of variability when the sample data are non- normal or are skewed. Real process data and simulated process data from heavily skewed distributions are presented to demonstrate the application of the process capability indices based on median absolute deviation and the results were compared with the indices based on quantile for non-normal and skewed process data. So that the obtained results process capability indices based on MAD is better used for a non-normal data with positive skewness, while process capability indices based on quantile is better used for a non-normal data with negative skewness.","Kata Kunci : indeks kemampuan, median absolute deviation, tidak normal, data proses"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82385,Penentuan Harga Opsi Jual Amerika dengan Volatilitas Model GARCH Menggunakan Simulasi,"RAHMANIAR DWINTA KUSUMA, Prof. Dr. Sri Haryatmi, M.Sc",2015 | Skripsi | S1 STATISTIKA,"Opsi (option) merupakan salah satu bentuk investasi pada asset finansial yaitu suatu kontrak yang memberikan hak kepada pemegangnya, namun berbeda dengan obligasi, untuk membeli atau menjual saham dalam jumlah tertentu  suatu perusahaan tertentu dengan harga tertentu dalam jangka waktu tertentu (expiration date).Sampai saat ini, penentuan harga opsi pada umumnya terus mengalami perkembangan dari waktu ke waktu.  Pada mumunya penentuan harga opsi dicari menggunakan model BSM dengan volatilitas yang konstan dan return log-normal, namun nyatanya model ini memiliki kecenderungan yang kurang baik untuk data finansial.  Model GARCH secara umum sudah banyak digunakan pada data finansial yang mengalami volatility clustering. Dengan menggunakan metode simulasi dengan volatilitas model GARCH ini akan diperoleh harga opsi yang relative sama denga harga pasaran.","Option  is one form of investment in financial assets. Option  is a contract that gives the holder the right, but in contrast to bonds, to buy or sell a certain number of shares in a particular company at a specified price within a specified period (expiration date). Until today, option pricing in general continues to develop over time. In general people using BSM models with constant volatility and returns the log-normal to get option price, but in fact this model has a poor tendency to financial data. General GARCH model has been widely used in financial data that experienced volatility clustering. Option price will be obtained  by using the simulation method on GARCH models volatility, this price is similar to an option in market price.","Kata Kunci : GARCH Model, Simulation, American Option, Backward, Volatility"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82393,ANALISIS TEKNIKAL SAHAM DENGAN INDIKATOR PERCENTAGE PRICE OSCILLATOR,"ANDHIKA ADHE S, Dr. Abdurakhman, M.si.",2015 | Skripsi | S1 STATISTIKA,"Analisis teknikal saham merupakan suatu metode analisis yang menggunakan pengujian atas harga di masa lampau untuk tujuan prediksi pergerakan harga di masa yang akan datang. Analisis teknikal saham baik digunakan para pelaku trading (trader) dalam membantu memberikan prediksi kapan waktu yang tepat untuk masuk dan keluar dari pasar saham. Terdapat berbagai macam indikator yang digunakan dalam analisis teknikal, salah satunya adalah Pencentage Price Oscillator. Percentage Price Oscillator  merupakan analisis indikator yang perhitungan menggunakan 2 periode yang bisa dipilih bebas oleh penggunanya, dimana dalam skripsi ini akan menggunakan periode 12 dan periode 26 yang selanjutnya akan dibandingkan sinyal prediksi yang lebih tepat antara Stochastic Oscilator dan Percentage Price Oscillator.","Technical analysis of stock is an analytical method which tests the previous price in order to predict the future of the price movenment. The technical analysis of stock is good to help traders in predicting the right time to get in and out of the stock market. There are some indicators used in technical analysis, one of them is Pencentage Price Oscillator. Percentage Price Oscillator is an indicator that the calculation analysis will use the 2 periode can be chosen freely by the user, which in this thesis will use the period 12 and period 26, which would then be compared to a more precise prediction of the signal between the Stochastic Oscillator and Percentage Price Oscillator.","Kata Kunci : Technical Analysis, Stochastic Oscillator, Percentage Price Oscillator."
http://etd.repository.ugm.ac.id/home/detail_pencarian/78300,PERBANDINGAN OPTIMISASI PORTOFOLIO METODE MEAN-VARIANCE DENGAN METODE TAIL MEAN-VARIANCE,"ELOK ARISMA, Prof. Subanar, Ph.D",2015 | Skripsi | S1 STATISTIKA,"Portofolio merupakan kombinasi linier dari beberapa aset. Di dalam pembentukannya, tentu setiap investor berusaha untuk memaksimalkan pengembalian yang diharapkan (expected return) dari investasi dengan tingkat risiko tertentu. Dengan kata lain, portofolio yang dibentuk dapat memberikan tingkat risiko terendah dengan expected return tertentu, atau dapat memberikan expected return tertinggi dengan tingkat risiko tertentu. Portofolio yang dapat mencapai tujuan di atas disebut dengan portofolio yang efisien.  Pada skripsi ini akan dibahas mengenai pembentukan bobot portofolio menggunakan metode Tail Mean-Variance yang dikembangkan oleh Landsman (2010). Metode ini merupakan perluasan dari metode Mean-Variance yang dipelopori oleh Markowitz (1952). Studi kasus penelitian ini menggunakan data saham mingguan periode 1 Januari 2013 sampai 31 Desember 2013 dari 6 saham NASDAQ. Bobot portofolio dari metode Tail Mean-Variance dibandingkan dengan bobot portofolio dari metode Mean-Variance, peneliti melakukan trading selama 5 hari dari tanggal 8 oktober 2014 hingga 14 oktober 2014. Diperoleh kesimpulan bahwa metode Tail Mean-Variance memberikan total kerugian yang lebih kecil.","Portfolio is linear combination of some assets. In the formation, of any investors trying to maximize the expected return of an investment with a certain level of risk. In other words, the portfolio was formed to provide the lowest risk level with a certain expected return, or can provide the highest expected return with a certain level of risk. Portfolio that will achieve the goal is called the efficient portfolio. This research discuss about the formation of portfolio weights using the Tail Mean-Variance method developed by Landsman (2010). This method is extension of the Mean-Variance method pioneered by Markowitz (1952). The research case study is using weekly stocks data period January 1, 2013 until December 31, 2013 from 6 NASDAQ stocks. Portfolio weights from Tail Mean-Variance method will compared with portfolio weights from Mean-Variance, researcher trades during 5 days from October 8, 2014 until October 14, 2014. The conclusion is the Tail Mean-Variance method gives  a smaller total loss.","Kata Kunci : portfolio, Mean-Variance, Tail Mean-Variance"
http://etd.repository.ugm.ac.id/home/detail_pencarian/78301,IMPLEMENTASI METODE DIRECTED RIDGE REGRESSION DALAM PEMODELAN FAKTOR YANG MEMPENGARUHI PRODUK DOMESTIK REGIONAL BRUTO,"RAHMAT MAULIZAR, Dr. Herni Utami, S.Si., M.Si.;Yunita Wulan Sari, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Analisis regresi merupakan analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Dalam analisis regresi klasik yang menggunakan metode kuadrat terkecil terdapat beberapa asumsi yang harus terpenuhi, salah satunya adalah tidak terdapat multikolinearitas. Jika asumsi ini tidak terpenuhi, estimasi parameter dengan menggunakan metode kuadrat terkecil menjadi kurang valid serta akan memiliki variansi dan error yang besar. Seiring perkembangan zaman, ditemukan berbagai metode untuk mengatasi masalah multikolinearitas ini, salah satunya  adalah dengan regresi ridge. Konsep dari regresi ridge adalah menambahkan tetapan bias sebesar k yang merupakan matriks diagonal, ke dalam matriks korelasi X'X. Dalam skripsi ini akan dibahas mengenai salah satu metode baru yaitu Directed Ridge Regression dimana hanya akan mengganti elemen diagonal yang bersesuaian dengan nilai eigen yang relatif kecil. Metode ini memiliki kelebihan yaitu hasil estimasi akan memiliki bias yang lebih kecil daripada metode regresi ridge yang lain dimana akan mengganti semua elemen diagonal matrilks X'X.","Regression analysis is a statistical analysis that used to perform model relationship between dependent variable and independent variable. There are some assumptions should be met in classical regression analysis using Least Square Method, one of them is no multicollinearity. If this assumption is not met, parameter estimation using Least Square method become less valid and the variance and error will be large. Over the years, there are a lot of variety method for solving this multicollinearity problem, one of them is ridge regression. The concept of ridge regression is by adding a k biased constant which is a diagonal matrix, to the correlation matrix X'X. In this paper we will discuss aboout one of new mehod, it is Directed Ridge Regression which alter only diagonal elements corresponding to relatively small eigenvalues. The advantage of this method is that the resulting estimates may be less biased than other ridge regression methods that alter all diagonal elements of X'X matrix.","Kata Kunci : Metode Kuadrat Terkecil, Multikolinearitas, Regresi Ridge, Directed Ridge Regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82152,Perbandingan Model Volatilitas Return dengan Menggunakan Model Glosten-Jagannathan-Runkle GARCH (GJR-GARCH) dan Exponential GARCH (EGARCH),"FIRDAUS AL MAIDAH, Prof. Dr.rer.nat Dedi Rosadi, S.Si, M.Sc",2015 | Skripsi | S1 STATISTIKA,"Pada data finansial, model ARCH dan GARCH banyak digunakan untuk mendeskripsikan bentuk volatilitas suatu data runtun waktu. Model ARCH dan GARCH mengasumsikan bahwa error yang positif dan error yang negatif akan memberikan pengaruh yang sama terhadap volatilitas (simetris). Faktanya, asumsi ini sering dilanggar, karena data runtun waktu biasanya justru menunjukkan fenomena ketidaksimetrisan antara nilai error positif dan nilai error negatif terhadap volatilitas. Masalah ini dapat diatasi menggunakan model GJR-GARCH dan EGARCH. Penelitian ini bertujuan untuk melakukan pemodelan GJR-GARCH (1,1) dan EGARCH (1,1). Data yang digunakan adalah data saham S&P 500, NASDAQ Composite, dan NYSE ARCA Oil and Gas Index. Data tersebut tersedia mulai periode Januari 1968 sampai Desember 2002. Penelitian ini diawali dengan transformasi return lalu dipilih mean model terbaik untuk masing-masing data return. Berdasarkan mean model terbaik dari masing-masing return lalu dibentuk model volatilitas GJR-GARCH (1,1) dan EGARCH (1,1). Kemudian dilakukan perbandingan terhadap kedua model tersebut untuk mengetahui model volatilitas mana yang lebih baik. Perbandingan model GJR-GARCH (1,1) dan EGARCH (1,1) dilakukan berdasarkan nilai log likelihood yang maksimum dengan nilai BIC dan AIC yang kecil, serta diikuti dengan nilai RMSE yang minimum. Dari ketiga data tersebut dapat disimpulkan bahwa model EGARCH (1,1) merupakan model volatilitas terbaik.","In financial data, ARCH and GARCH models are widely used to describe the shape of the volatility of a time series data. ARCH and GARCH models assume that the positive errors and negative error will give the same effect on volatility (symmetrical). In fact, this assumption is often violated, because the time series data is usually just show the phenomenon of asymmetry between positive error value and negative error value to volatility. This problem can be solved using models GJR-GARCH and EGARCH. This study aims to perform modeling GJR-GARCH (1,1) and EGARCH (1,1). The data used is the stock S & P 500, NASDAQ Composite, and NYSE ARCA Oil and Gas Index. The data available from the period January 1968 to December 2002. This study begins with the transformation of return and selected the best mean model for each of the data return. Based on the mean of the best models of each return then formed GJR-GARCH (1,1) and EGARCH (1,1) volatility models. Then do a comparison of the two models to determine volatility models which one is better. Comparison of models GJR-GARCH (1,1) and EGARCH (1,1) conducted by the maximum value of the log likelihood with BIC and AIC values are small, and followed with a minimum RMSE values. Of the three data can be concluded that the model EGARCH (1,1) is the best volatility models.","Kata Kunci : return, volatilitas, asimetris, GJR-GARCH, EGARCH"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85482,"Model Peramalan Adaptive Grey (1,1) Untuk Peramalan Jangka Pendek pada Jumlah Data Kecil","AYU WIDYANINGSIH, Dr. Herni Utami, M.Si",2015 | Skripsi | S1 STATISTIKA,"Peramalan sangat diperlukan untuk membuat suatu perencanaan di waktu yang akan datang. Suatu model peramalan dikatakan baik apabila hasil peramalan mendekati data sebenarnya. Tetapi, seringkali data atau observasi yang ada sangat terbatas untuk melakukan suatu peramalan yang mengakibatkan hasil peramalan yang didapatkan tidak akurat.  Grey Forecasting Model atau biasa disebut GM(1,1) merupakan model peramalan untuk data terbatas. Model tersebut menggunakan persamaan diferensial orde satu dengan satu variabel. Adaptive Grey Forecasting Model atau AGM(1,1) adalah salah satu pengembangan dari GM(1,1) yang memiliki tingkat keakuratan peramalan lebih baik untuk peramalan jangka pendek pada jumlah data yang kecil. AGM(1,1) terdiri dari dua bagian, yang pertama perhitungan nilai metode penelusuran tren dan potensi atau biasa disebut dengan Trend and Potency Tracking Method (TPTM) dengan menggunakan inisialisasi minimal empat data pertama dan yang kedua pembentukan model peramalan grey.","Forecasting is needed to make a plan in the future. if the results of the forecasting approach the actual data, the forecasting model is said good to use. However, the data or observations sometimes is very limited that result the forecast are not accurate.  Grey Forecasting Model called GM (1,1) is a forecasting model for limited data. This model using first order differential equations with one variable. Adaptive Grey Forecasting Model or AGM (1.1) is one of the development of GM (1,1) which has a better forecasting accuracy rate for short-term forecasting on limited data. AGM (1.1) consists of two parts: the calculation of the value of the trend and potency tracking method using minimum initialization for four first data  and grey forecasting model building.","Kata Kunci : peramalan jangka pendek, data terbatas, model peramalan grey, model peramalan adaptive grey, Trend and Potency Tracking Method"
http://etd.repository.ugm.ac.id/home/detail_pencarian/92140,GRAFIK PENGENDALI MULTIVARIATE EXPONENTIALLY WEIGHTED MOVING COVARIANCE MATRIX (MEWMC) DENGAN DIAGNOSIS PERGESERAN MENGGUNAKAN REGRESSION-ADJUSTED VARIABLES,"MANDA WAHYU IUDINA, Dr. Herni Utami, M.Si.; Rianti Siswi Utami, S.Si., M.Sc.",2015 | Skripsi | S1 STATISTIKA,"Multivariate Exponentially Weighted Moving Average (MEWMA) merupakan salah satu grafik pengendali multivariat yang umum digunakan untuk mendeteksi pergeseran kecil yang terjadi pada vektor mean. Pergeseran dalam proses dapat terjadi tidak hanya pada mean tetapi juga pada variabilitas dari proses. Maka dari itu, dibutuhkan metode yang dapat digunakan untuk mendeteksi pergeseran variabilitas tersebut. Grafik Multivariate  Exponentially Weighted Moving Covariance Matrix (MEWMC) adalah suatu grafik pengendali multivariat yang merupakan modifikasi dari grafik pengendali MEWMA namun ditujukan untuk mendeteksi pergeseran proses yang kecil pada matriks kovariansi. Pada analisis grafik pengendali MEWMC digunakan data hasil transformasi multistandarisasi serta memiliki asumsi diketahuinya vektor mean dan matriks kovariansi dari proses pendahulu yang telah terkendali. Sebuah aspek penting yang seharusnya tidak dilewatkan dari sebuah analisis grafik pengendali multivariat adalah diagnosis setelah adanya sinyal proses tak terkendali. Regression-adjusted variables digunakan untuk mendiagnosis penyebab proses tidak terkendali pada grafik pengendali Multivariate EWMC. Grafik pengendali MEWMC dapat mendeteksi adanya pergeseran kecil pada matriks kovariansi untuk pengamatan individu, sementara grafik MEWMA menunjukkan bahwa proses dalam keadaan terkendali. Dengan menggunakan regression-adjusted variables maka sumber penyebab pergeseran pada matriks kovariansi proses dapat diketahui secara pasti.","Multivariate Exponentially Weighted Moving Average (EWMA) is one of the multivariate control charts are commonly used to detect small shifts that occurred in the mean vector. The shift in the process can occur not only on the mean but also on the variability of the process. Therefore needed a method that can be used to detect the variability shift. Multivariate exponentially Weighted Moving Covariance Matrix (MEWMC) is a multivariate control chart which is a modification of the MEWMA control chart but addressed to detect small shifts in the covariance matrix. MEWMC control chart analysis uses multistandardized data and MEWMC assumes that knowing mean vector and covariance matrix from the predecessor in control process. An important aspect that should not be missed from a multivariate control chart analysis is the diagnosis after the signal out of control. Regression-adjusted variables is used to diagnose the cause of the out of control process on Multivariate EWMC control chart. MEWMC control chart can detect small shifts in the covariance matrix for individual observations, while MEWMA chart indicates that the process under controlled conditions. By using regression-adjusted variables, the source causes a shift in the covariance matrix can be known certainty.","Kata Kunci : Grafik pengendali multivariat, Grafik pengendali Multivariate EWMC, Regression-adjusted variables/ Multivariate control chart, Multivariate EWMC control chart, Regression-adjusted variables"
http://etd.repository.ugm.ac.id/home/detail_pencarian/81901,ESTIMASI MODEL REGRESI BUCKLEY-JAMES DENGAN KOZIOL-GREEN UNTUK DATA TERSENSOR,"AZIS PRASETYA, Dr. Gunardi, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Regresi survival merupakan regresi dengan fungsi survival sebagai variabel dependennya. Buckley-James (1979) kemudian memperkenalkan regresi survival dengan hasil estimasi yang menunjukkan nilai waktu hingga seseorang mengalami kejadian tertentu (event).  Buckley-James merubah titik tersensor pada regresi survival biasa dengan nilai ekspektasinya dimana nilai estimasi tersebut dicari menggunakan estimator Kaplan-Meier. Namun, estimator Kaplan-Meier masih memiliki kekurangan diantaranya nilai fungsi survival pada saat data tersensor sama dengan pada waktu sebelumnya. Hal ini akan membuat perubahan titik tersensor pada regresi Buckley-James memiliki presisi yang rendah. Masalah tersebut dapat dipecahkan dengan menggunakan estimator alternatif. Salah satu yang dapat digunakan adalah estimator Koziol-Green. Berdasarkan hasil simulasi, dapat ditunjukkan bahwa regresi Buckley-James dengan Koziol-Green menghasilkan estimasi dengan presisi lebih tinggi dengan berbagai nilai rata-rata data tersensor.","Survival regression is a regression model with survival function as a dependent variable. Buckley-James (1979) introduce a survival regression that the dependent variable same as the ordinary linear regression. The dependent variable is time to event no longer the probability.   Buckley-James transformed the censored points of the ordinary survival regression to their expectation value which is determined by using Kaplan-Meier estimator. But, unfortunately Kaplan-Meier estimator still has any problem, one of the important is the survival function of censored data assumed equal to survival function of the time before. Therefore, the transformation of censored points on Buckley-James regression is less precise. That problem can be solved with alternate survival function estimator. That is using Koziol-Green model. Based on the simulation results shows that Buckley-James regression under Koziol-Green model has higher precise with various values of censored data.","Kata Kunci : linear regression, Koziol-Green model, random censorship, maximum likelihood estimator, product-limit estimator."
http://etd.repository.ugm.ac.id/home/detail_pencarian/90095,METODE RIDGE REGRESSION DALAM KASUS MULTIKOLINEARITAS PADA REGRESI POISSON,"GANANG WIDAGDO, Prof. Dr. Sri Haryatmi, M. Sc.",2015 | Skripsi | S1 STATISTIKA,"Regresi Poisson adalah suatu teknik dalam metode statistika yang digunakan untuk menganalisis hubungan antara variabel respon yang berbentuk cacah/count dan diasumsikan berdistribusi poisson dengan satu atau lebih variabel prediktor.  Salah satu asumsi pada analisis regresi adalah tidak adanya multikolinearitas (adanya korelasi antar variabel prediktor). Seiring dengan perkembangan zaman, ditemukan berbagai metode untuk mengatasi masalah multikolinearitas, salah satunya adalah dengan regresi ridge. Konsep metode ridge adalah menambahkan tetapan bias sebesar k yang merupakan matriks diagonal, ke dalam matriks korelasi X'X. Pada regresi poisson estimasi untuk parameternya menggunakan metode Maximum Likelihood (ML). Akan tetapi metode ini tidak stabil terhadap masalah multikolinearitas, sehingga digunakan metode ridge untuk mengatasi ketidakstabilan pada metode Maximum Likelihood. Untuk membandingkan estimator mana yang memiliki performa lebih baik dalam mengestimasi parameter model regresi poisson maka digunakan nilai MSE-nya sebagai pembanding. Hasil analisis tersebut akan menunjukkan bahwa estimator yang dihasilkan melalui metode ridge lebih unggul dibandingkan dengan metode Maximum Likelihood. Dalam skripsi ini akan dibandingkan beberapa parameter ridge k untuk model regresi poisson dimana parameter ridge k yang mampu menghasilkan nilai MSE estimator terkecil akan ditambahkan kedalam diagonal matriks korelasi X'X pada persamaan estimator Regresi Ridge Poisson, sehingga dapat diperoleh model regresi poisson dengan parameter yang bias dan memiliki variansi yang kecil.","Poisson Regression is one of the statistical methods that used to perform model relationship between response variable which has poisson distribution with discrete scale and prediktor variables.   There are some assumptions should be found in classical regression analysis using Least Square Method, one of them is no multicollinearity. If this assumption is not met, parameter estimation using Least Square method become less valid and the variance of error will be large. And now, there are a lot of variety methods for solving this multicollinearity problem, one of them is ridge regression. The concept of ridge is by adding a ridge parameter value of k which is diagonal matrix, to the correlation matrix X'X. poisson Regression Model is usually estimated by using maximum likelihood (ML) method but its unstable to multicollinearity. Therefore, we present a new poisson  ridge regression estimator as a remedy to the problem of instability of the traditional ML method. To investigate the performance of the Poisson Ridge Regression and the traditional Maximum Likelihood approaches for estimating the parameters of the poisson regression model, we calculate the mean squared error (MSE) as the comparison. The result from this calculation shows that Poisson Ridge Regression method is better than the traditional ML estimator.  In this paper we will discuss some ridge parameters of k for poisson regression model. The ridge parameter which could producing the smallest value of Mean Square Error will be diagonally added in correlation matrix X'X on Poisson Ridge Regression estimator. Finally, we get a Poisson Regression Model with biased estimators and has low variance.","Kata Kunci : Regresi Poisson, Maximum Likelihood, Regresi Ridge, Mean Square Error, Multikolinearitas."
http://etd.repository.ugm.ac.id/home/detail_pencarian/81904,ANALISIS REGRESI LINEAR GANDA DENGAN DATA HILANG MENGGUNAKAN IMPUTASI MULTIVARIAT CHAINED EQUATION (MICE),"ERLINA PUSPITARINI, Dr. Danardono,MPH",2015 | Skripsi | S1 STATISTIKA,"Data hilang sering terjadi dalam sebuah pengamatan. Seringkali peneliti membuang observasi yang memuat data hilang. Akan tetapi telah ditemukan berbagai metode untuk menangani masalah data hilang antara lain dengan imputasi. Imputasi terdiri dari imputasi klasik dan imputasi modern. Terkadang nilai imputasi sering tidak masuk akal. Seperti jumlah yang bernilai negatif, apalagi jika kasusnya merupakan data kategorik akan sangat mudah sekali terjadi kesalahan yang sangat fatal. Oleh karena itu muncul metode baru dalam imputasi yaitu menggunakan chained equation (MICE), karena prosedurnya berantai sehingga meminimalisir timbulnya data yang tidak masuk akal. Dalam analisis MICE setelah dilakukan iterasi sebanyak 5 kali maka akan didapat model regresi fit sebanyak 5 kali juga. Kemudian dari kelima model fit tersebut disimpulkan menadi satu model yang disebut sebagai model pool.","Missing data often occur in an observation. Often researchers discard observations containing missing data. But has found a variety of methods for dealing with missing data, among others, by imputation. Imputation consisted of imputing classical and modern imputation. Sometimes the value of imputation often unreasonable. Such amount is negative, especially if the case is categorical data will be very easy to very fatal error occurs. Therefore, it appears that a new method of imputation using chained equations (MICE), because the procedure is a chain so as to minimize the emergence of data that does not make sense. In the analysis performed iterations MICE after 5 times it will get the regression model fit as much as 5 times as well. Later than the fifth model fit inferred from giving a model called as a model pool.","Kata Kunci : missing data, imputation, chained equations, linear regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/83954,UKURAN SAMPEL MINIMAL DATA TERSENSOR KANAN BERPASANGAN DENGAN MODEL FRAILTY STABIL POSITIF SEBAGAI FUNGSI SURVIVAL GABUNGAN,"RIZKY ANGGIA NURSANTI, Dr. Gunardi, M.Si.",2015 | Skripsi | S1 STATISTIKA,"Menentukan jumlah sampel adalah hal yang paling krusial dalam sebuah penelitian. Salah satu jenis data yang muncul pada data penelitian adalah data tersensor kanan berpasangan. Dengan melibatkan tingkat korelasi antar pasangan yang dimiliki oleh data berpasangan, akan memunculkan sebuah fungsi gabungan pada kedua data berpasangan tersebut dan akan mempengaruhi perhitungan variansi ketika menentukan ukuran sampel.       Salah satu metode yang digunakan pada data tersensor kanan adalah metode non-parametrik Kaplan-Meier. Dalam skripsi ini, mean dan variansi yang digunakan dalam perhitungan sampel adalah mean dan variansi dari perbedaan estimator Kaplan-Meier pada kedua data berpasangan. Sedangkan untuk memodelkan distribusi survival gabungan digunakan model frailty stabil positif. Kemudian dengan metode yang diajukan akan dilihat pengaruh dari periode akrual dan periode follow-up dalam menentukan ukuran sampel. Hasil dari metode ini menunjukkan bahwa mengabaikan korelasi dari pasangan data akan memberikan perkiraan ukuran sampel yang terlalu tinggi.","Determining sample size is the crucial thing in designing a study. One type of data which appear on research is paired right-censored data. By involving correlation between pair owned by paired data, would bring a joint function on both paired data and would affect variance calculation when determining sample size.       One of the method used on right-censored data is Kaplan-Meier non-parametric method. In this thesis, mean and variance are used for sample calculation is mean and variance on the difference of Kaplan-Meier estimator on both paired data. Meanwhile for model joint sulvival distribution, positive stable frailty model is used. And with proposed method would be seen the impact of the accrual period and follow-up period in determining sample size. The result of this method show that ignoring the pair correlation would give an overestimating the required sample size.","Kata Kunci : Statistik Kaplan-Meier, fungsi hazard, observasi berpasangan, model frailty stabil positif/ Kaplan-Meier statistic, hazard function, paired observation, positive stable frailty model"
http://etd.repository.ugm.ac.id/home/detail_pencarian/82685,GRAFIK PENGENDALI HOTELLING T^2  DENGAN PENDEKATAN METODE BOOTSTRAP,"PURWO NUR HIDAYAT, Riani Siswi Utami, S. Si, M. Sc.",2015 | Skripsi | S1 STATISTIKA,"Pengendalian kualitas statistik multivariat digunakan dalam mengendalikan suatu proses produksi yang memiliki lebih dari dua karakteristik kualitas, agar produksi tersebut   memiliki kualitas yang baik. Dewasa ini, pengendalian kualitas statistik multivariat yang sering diperkenalkan adalah Grafik Pengendali Hotelling T2 . Terdapat beberapa asumsi yang diperlukan dalam analisis Pengendalian Kualitas Statistik, salah satunya yaitu asumsi distribusi normal.  Dengan menggunakan  metode bootstrap, data yang tidak memenuhi asumsi distribusi normal tetap dapat dianalisis  dengan standar eror  yang lebih kecil.","Multivariate statistical quality control is used for controlling a process of production that consist more than two quality characteristics, to make that production in good quality. Nowadays, Hotelling T2 control chart is the most commonly use multivariate statistical quality control. In this analysis, there is an assumption that data should be  in normal distribution. By using bootstrap, unnormally distribution data can be analyzed with minimal standart error.","Kata Kunci : grafik pengendali, bootstrap, Hotteling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/85031,SELEKSI VARIABEL DATA SURVIVAL TERSENSOR KANAN DENGAN  METODE RANDOM SURVIVAL FOREST,"ANISA ASTITIA KHOTIMAH,  Sri Haryatmi, M.Sc., Dr., Prof.",2014 | Skripsi | S1 STATISTIKA,Membangun suatu pohon ensemble dengan metode dasar pembentukan pohon kini sangatlah populer. Metode ini memberikan tingkat keakuratan yang lebih tinggi dan merupakan metode yang sangat fleksibel terhadap segala macam data tanpa memerlukan asumsi tertentu. Random Survival Forest diperkenalkan sebagai metode pohon ensemble untuk data survival tersensor kanan. Resampling data yang dikenal dengan istilah bootstrapping digunakan dalam analisis. Data asli akan dilakukan bootstrapping yang kemudian akan memecahkan data menjadi dua komponen yaitu data in-bag dan data out-of-bag. Data in-bag digunakan untuk membangun pohon survival sementara data out-of-bag digunakan untuk menghitung kesalahan prediksi. Informasi waktu dan status tersensor sangat berguna dalam penelitian untuk membangun pohon survival. Random Survival Forest memiliki tujuan analisis yang sama dengan Regresi Cox maka kedua analisis tersebut dibandingkan untuk menunjukkan keakuratan dari analisis Random Survival Forest.,"Constructing ensembles from base learners, such as trees is very popular. This method gave a higher level of accuracy and is very flexible method to all kinds of data without requiring certain assumptions. Random survival forest introduced as a method of ensemble trees for right censored survival data. Bootstrapping data known as a term used in analysis and this method  will separated the original data into two components,  in-bag and out-of-bag  data. In-bag data used to grow a survival trees and the out-of-bag data used to calculate the prediction error. The information of time and censored status is very useful to grow a survival tree. Random survival forest have the same purpose with the Cox Regression analysis then both of the analysis compared to show the accuracy of analysis of random survival forest.","Kata Kunci : Random Survival Forest, Survival, Right censored, Variable selection, Cox Regression"
http://etd.repository.ugm.ac.id/home/detail_pencarian/78130,Model Regresi Conway-Maxwell-Poisson (COM-Poisson) Dengan Tingkat Dispersi Kelompok,"KANZUL FIKRI BARZANI, Dr. Herni Utami, M.Si.",2014 | Skripsi | S1 STATISTIKA,"Analisis regresi Poisson adalah metode yang digunakan dalam pemodelan data cacah. Ketika data cacah memiliki nilai variansi yang lebih besar dari nilai rata-rata maka terjadi overdispersi. Sehingga asumsi pada regresi Poisson tidak dapat terpenuhi, di mana seharusnya nilai variansi dan rata-ratanya diasumsikan sama (equidispersi). Regresi binomial negatif merupakan alternatif pemodelan jika data mengalami overdispersi, tetapi tidak untuk underdispersi.  Akan tetapi, di dalam penulisan skripsi ini menunjukkan bahwa data cacah yang tampaknya equidispersi atau overdispersi, bisa saja merupakan hasil campuran populasi dengan tingkat dispersi yang berbeda. 	 Untuk mendeteksi dan memodelkan campuran tersebut, kami men-generalisasikan dari model regresi Conway-Maxwell-Poisson (COM-Poisson) yang memungkinkan untuk tingkat dispersi kelompok. Menggunakan metode maksimum likelihood dan Newton-Raphson untuk mendapatkan estimasi paramater dalam model tersebut.","Poisson regression analysis is a method used in modeling count data. When a count data  has greater variance value than the average value, there will be a case called overdispersion. In that case, the assumption in Poisson regression can not be fulfilled, in which the variance and average value are assumed to be equal (equidispersion). Negative binomial regression is an alternative solution for modeling data with overdispersion problem, but not for underdispersion one. However, this thesis shows that count data that seems to be equidispersion or overdispersion, could be as a result of a mixture of populations with different dispersion levels. To detect and modeling that mixture, I will describe a generalization of Conway-Maxwell-Poisson (COM-Poisson) regression which allows for a group level dispersion, using the maximum likelihood and the Newton-Raphson method to estimate the parameters in the model.","Kata Kunci : dispersi yang terlihat, regresi Conway-Maxwell-Poisson (COM-Poisson), model campuran, overdispersi, underdispersi, maksimum likelihood, bootstrap"
http://etd.repository.ugm.ac.id/home/detail_pencarian/78424,PENENTUAN HARGA UP AND OUT BARRIER CALL OPTION MODEL BLACK-SCHOLES TIPE EROPA DENGAN ESTIMASI VOLATILITAS METODE EXPONENTIAL WEIGHTED MOVING AVERAGE (EWMA),"MIA DWI PUJI WAHYUNI DARSONO, Yunita Wulan Sari, S.Si, M.Sc.",2014 | Skripsi | S1 STATISTIKA,"Penentuan harga opsi secara teoritis dapat membantu seorang investor guna mendapatkan gambaran mengenai harga opsi di pasar pada saat jatuh tempo. Dengan begitu, seorang investor dapat mengambil keputusan yang tepat apakah membeli atau menjual opsi yang dia miliki. Selain itu, jenis opsi juga berpengaruh untuk mendapatkan keuntungan yang besar. Salah satu jenis opsi yang lebih murah dari harga standar opsi biasanya adalah opsi jenis barrier. Opsi barrier tersebut, mempunyai nilai batasan yang mempengaruhi harga opsi itu sendiri. 	Penetapan harga opsi secara teoritis juga mempunyai banyak cara, yaitu salah satu diantaranya adalah model Black-Scholes. Dimana model Black-scholes tersebut mempunyai beberapa asumsi, yaitu opsi yang digunakan adalah opsi tipe Eropa, tidak ada biaya transaksi atau pajak, tidak ada pembayaran deviden selama periode opsi, tidak ada kesempatan bagi arbitrasi, dan perdagangan aset berlangsung secara kontinu. Untuk menghitung nilai opsi dengan model Black-Scholes tipe barrier diperlukan enam parameter yaitu harga asset awal (S0), harga patokan opsi (K), waktu jatuh tempo opsi (T), suku bunga bebas risiko ( r ), nilai barrier (B), dan volatilitas harga aset dasar (sigma).  	Dari keenam parameter yang digunakan untuk penghitungan nilai opsi secara teoritis tersebut hanya nilai dari volatilitas (sigma) yang tidak dapat diketahui secara langsung. Oleh karena itu, pada skripsi ini akan dibahas bagaimana cara mengestimasikan nilai volatilitas (sigma) dari harga saham tersebut untuk menghitung nilai harga opsi yang hamper mendekati harga di pasar. Di skripsi ini menggunakan metode Exponential Weighted Moving Average (EWMA) untuk pengestimasian volatilitasnya.","Theoretical option pricing can help an investor to gain insight about the options in the market price at maturity. Therefore, an investor can take an informed decision whether to buy or sell an his/her option. Moreover, this type of option is also influential to benefit greatly. One type of option that is cheaper than the standard price of the options is usually the type of barrier option. The barrier option, have limitations that affect the value of the option price itself. 	Theoretical option pricing also has a lot of ways, one of which is the Black-Scholes model. Where the Black-Scholes model has several assumptions, ie the options used is European type option, there are no transaction costs or taxes, no dividend payments during the option period, there is no opportunity for arbitrage, and asset trading takes place continuously. To calculate the value of the option with the Black-Scholes type barrier required six parameters, namely the initial asset price (S0), strike price (K), expiration date (T), risk-free interest rate ( r ), value of the barrier (B), and the volatility of the underlying asset price (sigma). 	Of the six parameters used for calculating the theoretical option value is only the value of the volatility (sigma) that can not be known directly. Therefore, in this thesis will discuss how to estimate the value of the volatility (sigma) of the stock price to calculate the value of the option price is almost close to the price in the market. In this paper using the Exponential Weighted Moving Average (EWMA) method for estimating volatility.","Kata Kunci : Barrier, Volatilitas, Moving Average, Forecasting Volatility"
http://etd.repository.ugm.ac.id/home/detail_pencarian/77233,MODEL NONLINEAR EFEK CAMPURAN DENGAN MENGGUNAKAN REGRESI ASIMTOTIK,"RAY TAMTAMA, Dr. Abdurakhman, S.Si, M.Si.",2014 | Skripsi | S1 STATISTIKA,"Model nonlinear efek campuran untuk data dalam bentuk kontinu, pengukuran berulang pada masing-masing individu, juga dikenal sebagai model nonlinear hirarkis, adalah sebuah platform yang populer untuk analisis ketika tujuan berfokus pada karakteristik individu-spesifik. Kerangka kerja ini pertama diminati oleh perhatian luas dalam komunitas riset statistik di akhir 1980-an, dan 1990-an melihat perkembangan yang kuat dari teknik metodologis dan komputasi baru untuk model ini, munculnya perangkat lunak untuk keperluan umum, dan aplikasi yang luas dari model di berbagai bidang substantif. Artikel ini menyajikan gambaran dari formulasi, interpretasi, dan pelaksanaan nonlinear efek campuran model dan survei kemajuan terbaru dan aplikasi.","Nonlinear mixed effects models for data in the form of continuous, repeated measurements on each of a number of individuals, also known as hierarchical nonlinear models, are a popular platform for analysis when interest focuses on individualspecific characteristics. This framework first enjoyed widespread attention within the statistical research community in the late 1980s, and the 1990s saw vigorous development of new methodological and computational techniques for these models, the emergence of general-purpose software, and broad application of the models in numerous substantive fields. This article presents an overview of the formulation, interpretation, and implementation of nonlinear mixed effects models and surveys recent advances and applications.",Kata Kunci :  model nonlinear efek campuran
http://etd.repository.ugm.ac.id/home/detail_pencarian/78262,SUPPORT VECTOR REGRESSION (SVR) UNTUK MENGANALISA PERGERAKAN INDEKS HARGA SAHAM GABUNGAN (IHSG) SELAMA PEMILIHAN UMUM 2014,"HAMID DIMYATI, Dr. Gunardi, M.Si.",2014 | Skripsi | S1 STATISTIKA,"Support Vector Regression (SVR) dengan bantuan kernel tunggal menjadi model yang mampu menangani kasus peramalan data saham selama masa Pemilihan Umum 2014. Pengaruh dari adanya momentum Pemilihan Umum 2014 terhadap pergerakan data saham akan dievaluasi terlebih dahulu melalui analisis event study. Selanjutnya model SVR dengan memanfaatkan peran kernel Gaussian, Laplacian, ANOVA dan Bessel akan menangani kasus peramalan data saham tersebut. Dari hasil pembandingan keempat kernel, diperoleh kesimpulan bahwa SVR dengan kernel Laplacian menjadi model yang paling baik, bahkan mengungguli hasil peramalan dengan model ARIMA melalui evaluasi mean squared error (MSE).","Support Vector Regression (SVR) with single kernel become a regression model that can deal stock forecasting problems during the Indonesia National Election 2014. Influence of the Indonesia National Election 2014 to the stock movements will be evaluated by event study analysis. Afterwards, SVR model that adopt four types of kernel: Gaussian, Laplacian, ANOVA and Bessel, will take those stock forecasting problems. Comparing the four kernels, we conclude that the Laplacian kernel SVR is the best regression model, even it outperforms ARIMA model in forescasting the same cases based on evaluating the mean squared error (MSE).","Kata Kunci : saham, event study, support vector regression, kernel"
http://etd.repository.ugm.ac.id/home/detail_pencarian/77260,Model Logistik Aditif Tergeneralisasi,"FATMA NURUL HIDAYAH, Dr. Herni Utami, M.Si",2014 | Skripsi | S1 STATISTIKA,"Model aditif tergeneralisasi merupakan perluasan dari regresi linear, yaitu pemodelan yang sesuai untuk mengatasi kenonlinearan dalam hubungan antara variabel respon dan prediktor serta tidak membatasi distribusi variabel respon hanya pada distribusi normal saja akan tetapi distribusi-distribusi lain dalam keluarga eksponensial dapat dipergunakan dalam model ini. Model logistik aditif tergeneralisasi merupakan bagian dari model aditif tergeneralisasi dengan respon bertipe biner, yaitu dengan mengganti fungsi linear yang ada pada model dengan jumlahan fungsi yang diestimasi menggunakan local scoring. Penggunaan model logistik aditif tergeneralisasi untuk variabel prediktor kuantitatif dengan estimasi fungsi penghalus cubic smoothing spline. Komponen aditif dari model logistik aditif tergeneralisasi merupakan jumlahan fungsi tunggal yang dimiliki oleh setiap prediktor sehingga dapat diketahui kontribusi dari setiap prediktor terhadap respon.","Generalized additive model is an extension of the linear regressio, is a modeling to solve non-linearity problem in the relation between response variable and predictor, and not just on the limit distribution of the normal response variable but other distributions in exponential family can be used in this model. Generalized additive logistic model is part of generalized additive model with binary-type response, which is replacing linear function on the model by summing the function which be estimated using the local scoring. Generalized additive logistic model is used to quantitative predictor by estimating smoothing function of cubic smoothing spline. Additive component of generalized additive logistic models is sum of single function of each predictor that can be known the contribution of each predictor to response.","Kata Kunci :  model logistik aditif tergeneralisasi, biner, local scoring, cubic smoothing spline"
http://etd.repository.ugm.ac.id/home/detail_pencarian/78034,ESTIMASI PARAMETER PADA REGRESI LOGISTIK DENGAN NILAI KOVARIAT HILANG MENGGUNAKAN ESTIMATOR INVERS PROBABILITAS TERBOBOT DAN IMPUTASI BERGANDA,"RENINTA DEWI .N., Prof. Dr. Sri Haryatmi, M.Sc",2014 | Skripsi | S1 STATISTIKA,"Data merupakan salah satu poin penting dalam setiap analisis data, karena tidak akan mungkin analisis dilakukan tanpa data. Data yang digunakan diharapkan merupakan data yang baik. Namun pada kenyataannya, seringkali data tidak sesuai dengan yang kita harapkan. Data yang tidak lengkap menyebabkan proses penarikan kesimpulan menjadi lebih sulit. Jika data yang hilang diabaikan, maka menyebabkan kesimpulan yang bias atau tidak valid. Oleh karena itu, muncullah berbagai metode untuk mengestimasi nilai yang hilang tersebut. Metode-metode tersebut adalah Inverse Probability Weighted, Multiple Imputation dan kita dapat mengkombinasikan keduanya, metode itu adalah Inverse Probability Weighted combining Multiple Imputation.","Data is one of the important points in every data analysis as it is impossible to conduct data analysis without data. The data used is expected to be a good data. In fact, it is commonly found that the data doesnâ€™t meet the expectation. Incomplete data causes the difficulty in drawing the conclusion. If missing data are ignored, it causes the conclusion are bias or invalid. Therefore, there are various methods for estimating the missing value. The methods are Inverse Probability Weighted, Multiple Imputation and we can combine both of them as Inverse Probability Weighted combining Multiple Imputation.","Kata Kunci : Missing Covariate, Inverse Probability Weighted, Multiple Imputation, Logistic Regression Inverse Probability Weighted combining Multiple Imputation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/77542,Estimasi Generalized Moment pada Model Linear Panel Efek Random dengan Komponen Error Spasial,"RATNA WULANSARI, Prof. Dr. Sri Haryatmi, M. Sc.",2014 | Skripsi | S1 STATISTIKA,"Skripsi ini akan membahas estimasi model panel linear efek random dengan komponen error spasial. Model panel linear efek random digunakan untuk mengatasi ketidakpastian model pada model efek tetap. Sedangkan, komponen spatial error menunjukkan variabel dependennya tergantung pada karakteristik-karakteristik lokal yang diamati dengan komponen galat yang berkorelasi dalam ruang.  Estimasi pada model linear panel efek random dengan komponen spatial error ini difokuskan pada estimasi generalized moment. Uji lagrange multiplier untuk melihat apakah dalam model terdapat efek spasial autokorelasi pada cross section. Data public capital productivity di 48 negara bagian Amerika dari tahun 1970-1986 dijadikan sebagai studi kasusnya.","This final project discussed about estimation panel linear random effect model with spatial error. Panel linear random effect model is used to overcome uncertainty model in the fixed effect model. Meanwhile, the spatial error indicates the dependent variabel depens on a set of observed  local characteristics with  error components are correlated in space. Estimation panel linear random effect model with spatial error is focused on the estimation of generalized moment. Lagrange multiplier test applied to testing existence of spatial autocorrelation in the cross section. Public capital productivity data in 48 state of America from 1970 until 1986 is applied as case studies in this thesis.","Kata Kunci : spatial error, model efek random, estimasi generalized moment, uji lagrange multiplier"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150798,ESTIMASI LOGNORMAL KRIGING UNTUK ANALISIS DATA KESUBURAN TANAH PADA DAERAH SUNGAI (Studi Kasus: Estimasi Kandungan Zinc di Daerah Sungai Meuse); SOIL FERTILITY DATA ANALYSIS ON THE RIVER AREA USING LOGNORMAL KRIGING ESTIMATION (Case Study: Zinc Content Estimation on the Meuse River Area),"ADHI SUBEKTI PURWANINGTYAS, Abdurakhman",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Sungai merupakan salah satu bagian di bumi yang saat ini kebermanfaatannya telah beralih fungsi dari sumber daya yang sangat bermanfaat bagi kehidupan menjadi lahan pembuangan sampah rumah tangga maupun limbah pabrik yang dapat mengakibatkan pencemaran air serta menurunnya tingkat kesuburan tanah di sekitar sungai. Salah satu unsur hara yang sangat bermanfaat bagi tanah adalah logam zinc dengan keadaan normal (10-300 ppm). Jika kandungan zinc melebihi batas normal, kondisi tersebut dapat meningkatkan kadar keasaman tanah yang menyebabkan penurunan tingkat kesuburan tanah. Salah satu metode yang dapat digunakan untuk mengetahui kadar zinc dalam tanah pada lokasi yang tidak tersampel adalah metode estimasi kriging. Pada data lapisan tanah yang sebagian besar mengikuti distribusi lognormal, digunakan estimasi lognormal kriging. Lognormal kriging pada prinsipnya mentransformasikan data ke dalam bentuk logaritma natural, kemudian dari hasil estimasi ditransformasikan kembali ke dalam skala awal sebagai hasilnya. Studi kasus yang digunakan yaitu data kandungan zinc yang terdapat pada permukaan tanah di daerah sungai meuse pasca banjir. Hasil estimasi 3197 titik yang tidak tersampel pada koordinat absis 178912-181299 dan ordinat 329484- 333576 menunjukkan bahwa rata-rata estimasi kandungan zinc yaitu 536.254 ppm. Dari hasil tersebut menunjukkan rata-rata kandungan zinc melebihi batas normal. Hal ini dibenarkan pada kenyataan bahwa dari 3197 titik estimasi terdapat 93.65 % lokasi dengan kandungan zinc di luar batas normal, sehingga menurunkan tingkat kesuburan tanah karena bersifat asam.",,"Kata Kunci : data spasial, geostatistika, kriging,ordinary kriging, lognormal kriging"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150803,ANALISIS TEKNIKAL SAHAM MENGGUNAKAN VARIABLE INDEX DYNAMIC AVERAGE (VIDYA) DENGAN EKSPONENTIAL WEIGHTED MOVING AVERAGE; STOCK TECHNICAL ANALYSIS USING VARIABLE INDEX DYNAMIC AVERAGE (VIDYA) WITH EXPONENTIAL WEIGHTED MOVING AVERAGE,"Hanna Van Hellen, Abdurakhman",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Suatu ruang lingkup keuangan selalu dikaitkan dengan perubahan yang terjadi secara kontinu dan terus menerus setiap harinya, sama halnya dengan pasar perdagangan dunia yang selalu bersifat dinamis oleh pergerakan pedagang yang menyesuaikan perubahan persepsi dan para partisipan terkait. Dengan adanya asumsi tersebut, dibutuhkan suatu indikator dinamis yang dapat mengubah periode waktu dengan cara menganalisis aksi pasar yang terjadi. Variable Index Dynamic Average (VIDYA) merupakan salah satu indikator yang secara dinamis dapat mengikuti pergerakan harga saham. VIDYA membutuhkan bobot dalam metode perhitungannya. Dalam skripsi ini, bobot EWMA digunakan untuk menentukan bobot VIDYA. Dalam perhitungan VIDYA dapat digunakan tiga metode berbeda, yaitu Standar deviasi, Chande Momentum Oscillator, dan koefisien determinasi. Metode perhitungan tersebut bertujuan menentukan indeks volatilitas yang berfungsi untuk memprediksi pergerakan harga saham dan memprediksi tren di masa yang akan datang. Selain itu, VIDYA dapat digunakan sebagai strategi trading guna menentukan sinyal jual atau beli saham dengan cara menggunakan titik breakout.",,"Kata Kunci : analisis teknikal, exponential weighted moving average, indeks volatilitas, standar deviasi, koefisien determinasi, indikator momentum"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150806,VALUE AT RISK MENGGUNAKAN METODE MAKSIMUM ENTROPY BOOTSTRAPPING DAN FLEX MAKSIMUM ENTROPY BOOTSTRAPPING; VALUE AT RISK USING MAXIMUM ENTROPY BOOTSTRAPPING AND FLEX MAXIMUM ENTROPY BOOTSTRAPPING METHOD,"Avista Nurmaulidya, Abdurakhman",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Nilai Risiko adalah nilai potensi terjadinya bahaya, akibat atau konsekuensi yang dapat terjadi pada sebuah proses yang sedang berlangsung atau kejadian yang akan datang. Manajemen risiko adalah proses pengelolaan risiko yang mencakup identifikasi, evaluasi dan pengendalian risiko yang dapat mengancam kelangsungan aktivitas usaha. Permasalahannya adalah bagaimana perusahaan dapat mengukur risiko potensi terjadinya suatu peristiwa baik yang dapat diperkirakan maupun yang tidak dapat diperkirakan yang dapat menimbulkan dampak bagi pencapaian tujuan Organisasi. Kebutuhan untuk mengelola risiko, yaitu risiko kredit dan risiko pasar di perusahaan perbankan dan asuransi sudah menjadi perhatian yang serius. Penghitungan Value at Risk (VaR) yang menggunakan pendekatan central atau normal (tradisional) yaitu basic indicator approach (BIA), standardized approach (SA) dan alternative standardized approach (ASA), telah dipelajari dan dipahami menjadi tidak tepat karena menggunakan parameter yang hanya sesuai dengan business line perbankan dan tidak dapat mengakomodasi nilai risiko kejadian ekstreme. Pengamatan terkini menunjukkan bahwa (selalu) ada potensi kejadian - kejadian yang bersifat ekstrim, dimana frekuensi terjadinya memang sangat rendah namun, jika itu terjadi maka akan menimbulkan dampak kerugian yang sangat besar. Fenomena ekstrim ini tidak tercakup dalam penghitungan VaR Prinsip maksimum Entropi didasarkan pada pertimbangan bahwa ketika memperkirakan suatu distribusi probabilitas , sebaiknya memilih distribusi yang memiliki nilai ketidakpastian tetap yang terbesar (yaitu,entropi maksimum) konsisten dengan permasalahan. Entropi dapat dimaksimalkan secara analitis. Yang di kombinasikan dengan menggunakan algoritma bootstrappig utuk megatasi beberapa data tertentu. Metode Maximum Entropy Bootstrapping sangat sesuai untuk megatasi data yang memiliki nilai ekstrim.",,"Kata Kunci : Value at Risk, Maksimum Entropy, Manajemen Risiko, Nilai ekstrim , Bootsrapping"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150807,PENENTUAN HARGA ASET PADA LIQUIDITY-ADJUSTED CAPITAL ASSET PRICING MODEL (LCAPM); ASSET PRICING ON LIQUIDITY-ADJUSTED CAPITAL ASSET PRICING MODEL (LCAPM),"Rio Rizki Aryanto, Dedi Rosadi",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam pasar modal tidak semua aset dapat dijual dengan cepat atau likuid. Likuiditas menyatakan mudah atau tidaknya aset dijual di pasar modal dengan cepat. Aset yang tidak likuid adalah aset yang tidak dapat dijual dengan cepat di pasar modal dikarenakan sedikitnya pembeli yang berminat terhadap aset tersebut. Aset tersebut mempunyai risiko likuiditas yaitu risiko yang dimiliki pemegang aset selama menahan aset tersebut sebelum akhirnya terpaksa menjualnya di bawah harga pasar. Likuiditas aset dinyatakan dengan biaya likuiditas dan risiko likuiditas dimana kedua faktor tersebut mempengaruhi tingkat pengembalian atau return dari aset. Liquidity-Adjusted Capital Asset Pricing Model (LCAPM) adalah suatu model yang menggunakan likuiditas aset dalam menghitung tingkat pengembalian atau return dari aset tersebut. Pada model LCAPM risiko likuiditas diwakili oleh tiga nilai kovariansi yaitu kovariansi antara likuiditas aset dengan likuditas pasar, kovariansi antara return aset dengan likuiditas pasar dan yang terakhir adalah nilai kovariansi antara likuiditas aset dengan return pasar. Dengan menggunakan model LCAPM dapat dilihat bagaimana pengaruh likuiditas dalam menentukan harga aset. LCAPM merupakan model pengembangan dari Capital Asset Pricing Model (CAPM). Dengan membandingkan portofolio dari kedua model tersebut dapat dilihat model manakah yang memiliki return lebih besar. Dari hasil perbandingan tersebut diperoleh bahwa portofolio LCAPM menghasilkan return yang lebih besar.",,"Kata Kunci : Likuiditas, Risiko Likuiditas, CAPM, Saham, Portofolio, Return"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150808,PENENTUAN HARGA ASET PADA LIQUIDITY-ADJUSTED CAPITAL ASSET PRICING MODEL (LCAPM); ASSET PRICING ON LIQUIDITY-ADJUSTED CAPITAL ASSET PRICING MODEL (LCAPM),"Tiara Gumilang Ramadhani, Dedi Rosadi",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Berkembang pesatnya sistem informasi berbasis website memberikan berbagai kemudahan untuk mendapatkan informasi dalam jumlah besar dan secara cuma – cuma. Informasi yang tersaji dalam bentuk tekstual seringkali berbentuk dokumen yang tidak terstuktur. Untuk menangani dokumen dengan pola tidak terstruktur ini analisis text mining sangat diperlukan. Salah satu analisis text mining yang sering digunakan adalah klasifikasi sentimen atau klasifikasi teks. Dari hasil klasifikasi sentimen mengenai suatu topik ini dapat disimpulkan opini tertentu dengan melihat proporsi kelas dari keseluruhan dokumen yang disajikan. Metode yang akan dibahas kali ini adalah metode probabilistic Naïve Bayes Classifier dan metode Support Vector Machine untuk menentukan kelas suatu dokumen secara biner yaitu kelas positif dan kelas negatif. Naïve Bayes Classifier merupakan metode klasifikasi menggunakan aturan Bayesian dengan memanfaatkan probabilitas prior serta probabilitas bersyarat dari frekuensi kata yang muncul pada masing – masing kelas dokumen training. Nilai tersebut yang akan digunakan untuk menentukan kelas dokumen testing dengan melihat nilai maximum a posteriori masing – masing kelas. Data yang telah diklasifikasi kemudian dihitung tingkat akurasi kebenarannya menggunakan metode support vector machine dengan menentukan fungsi Kernel serta proporsi untuk data training dan data testing yang sesuai. Dari perbandingan nilai akurasi klasifikasi dengan menggunakan kedua metode diatas, didapatkan nilai akurasi paling tinggi adalah dengan menggunakan metode naïve bayes classifier (NBC) dengan akurasi sebesar 62,6295%. Dari 5.412 dokumen training dan 2701 dokumen testing yang telah dilkasifikasi, didapatkan proporsi opini untuk kelas positif sebesar 44,46501% dan opini untuk kelas negatif sebesar 55,53499%.",,"Kata Kunci : Klasifikasi sentimen, opini, text mining, aturan Bayesian, Naïve bayes classifier, support vector machine."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150809,ANALISIS SENTIMEN MENGGUNAKAN METODE NAÏVE BAYES CLASSIFIER DENGAN MODEL DOKUMEN BERNOULLI DAN SUPPORT VECTOR MACHINE; SENTIMENT ANALYSIS USING NAÏVE BAYES CLASSIFIER WITH BERNOULLI DOCUMENT MODEL AND SUPPORT VECTOR MACHINE,"Tiara Gumilang Ramadhani, Dedi Rosadi",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Berkembang pesatnya sistem informasi berbasis website memberikan berbagai kemudahan untuk mendapatkan informasi dalam jumlah besar dan secara cuma – cuma. Informasi yang tersaji dalam bentuk tekstual seringkali berbentuk dokumen yang tidak terstuktur. Untuk menangani dokumen dengan pola tidak terstruktur ini analisis text mining sangat diperlukan. Salah satu analisis text mining yang sering digunakan adalah klasifikasi sentimen atau klasifikasi teks. Dari hasil klasifikasi sentimen mengenai suatu topik ini dapat disimpulkan opini tertentu dengan melihat proporsi kelas dari keseluruhan dokumen yang disajikan. Metode yang akan dibahas kali ini adalah metode probabilistic Naïve Bayes Classifier dan metode Support Vector Machine untuk menentukan kelas suatu dokumen secara biner yaitu kelas positif dan kelas negatif. Naïve Bayes Classifier merupakan metode klasifikasi menggunakan aturan Bayesian dengan memanfaatkan probabilitas prior serta probabilitas bersyarat dari frekuensi kata yang muncul pada masing – masing kelas dokumen training. Nilai tersebut yang akan digunakan untuk menentukan kelas dokumen testing dengan melihat nilai maximum a posteriori masing – masing kelas. Data yang telah diklasifikasi kemudian dihitung tingkat akurasi kebenarannya menggunakan metode support vector machine dengan menentukan fungsi Kernel serta proporsi untuk data training dan data testing yang sesuai. Dari perbandingan nilai akurasi klasifikasi dengan menggunakan kedua metode diatas, didapatkan nilai akurasi paling tinggi adalah dengan menggunakan metode naïve bayes classifier (NBC) dengan akurasi sebesar 62,6295%. Dari 5.412 dokumen training dan 2701 dokumen testing yang telah dilkasifikasi, didapatkan proporsi opini untuk kelas positif sebesar 44,46501% dan opini untuk kelas negatif sebesar 55,53499%.",,"Kata Kunci : Klasifikasi sentimen, opini, text mining, aturan Bayesian, Naïve bayes classifier, support vector machine."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150810,ESTIMASI MODEL REGRESI BUCKLEY-JAMES DENGAN KOZIOL-GREEN UNTUK DATA TERSENSOR; BUCKLEY-JAMES REGRESSION ESTIMATION UNDER KOZIOLGREEN OF CENSORED DATA,"Azis Prasetya, Gunardi",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi survival merupakan regresi dengan fungsi survival sebagai variabel dependennya. Buckley-James (1979) kemudian memperkenalkan regresi survival dengan hasil estimasi yang menunjukkan nilai waktu hingga seseorang mengalami kejadian tertentu (event). Buckley-James merubah titik tersensor pada regresi survival biasa dengan nilai ekspektasinya dimana nilai estimasi tersebut dicari menggunakan estimator Kaplan-Meier. Namun, estimator Kaplan-Meier masih memiliki kekurangan diantaranya nilai fungsi survival pada saat data tersensor sama dengan pada waktu sebelumnya. Hal ini akan membuat perubahan titik tersensor pada regresi Buckley-James memiliki presisi yang rendah. Masalah tersebut dapat dipecahkan dengan menggunakan estimator alternatif. Salah satu yang dapat digunakan adalah estimator Koziol-Green. Berdasarkan hasil simulasi, dapat ditunjukkan bahwa regresi Buckley-James dengan Koziol-Green menghasilkan estimasi dengan presisi lebih tinggi dengan berbagai nilai rata-rata data tersensor.",,"Kata Kunci : regresi linier, model Koziol-Green, random censorship, maximum likelihood estimator, product-limit estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150677,ESTIMASI BAYESIAN DARI PELUANG KEGAGALAN PADA PORTOFOLIO DENGAN KEJADIAN KEGAGALAN YANG RENDAH; BAYESIAN ESTIMATION OF PROBABILITIES OF DEFAULT FOR LOW DEFAULT PORTFOLIOS,"FAUZAN ABDILLAH, Subanar",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Estimasi dari peluang gagal bayar (PD) pada portofolio dengan kejadian gagal bayar yang rendah menggunakan batas atas konfidensi merupakan cara yang biasa digunakan oleh beberapa lembaga keuangan, meskipun masih sering terjadi perdebatan dalam menentukan besarnya taraf konfidensi yang digunakan untuk estimasi tersebut. Estimator Bayesian untuk PD berdasarkan prior tak informatif, distribusi prior uniform merupakan salah satu alternatif yang dapat menghindarkan dalam pemilihan taraf konfidensi. Dalam skripsi ini akan ditunjukkan pada kasus kejadian gagal bayar yang independen, batas atas konfidensi dari PD yang dapat direpresentasikan sebagai kuantil dari distribusi posterior Bayesian lebih bersifat konservatif dari pada estimator Bayesian dari PD dengan berdasarkan prior tak informatif. Selanjutnya akan dibahas mengenai implementasi estimator Bayesian dari prior neutral tak informatif dan estimator Bayesian konservatif (conservative Bayesian Estimator) pada kasus satu periode maupun multiperiode dengan data kasus kejadian gagal bayar yang dependen dan membandingkan hasil estimasi tersebut dengan estimasi menggunakan batas atas konfidensi. Perbandingan tersebut akan membawa pada versi constraine dari estimator Bayesian dengan prior neutral tak informatif sebagai alternatif untuk estimator batas atas konfidensi.",,"Kata Kunci : peluang gagal bayar, portofolio dengan kejadian gagal bayar yang rendah, batas atas konfidensi, estimator Bayesian, prior distribution, posterior distribution."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150720,ESTIMASI TINGKAT SUKU BUNGA MENGGUNAKAN METODE NONPARAMETRIK KERNEL; ESTIMATION OF INTEREST RATES USING NONPARAMETRIC KERNEL METHOD,"Tri Setianta, Gunardi",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Tingkat suku bunga merupakan salah satu faktor yang sangat berpengaruh dalam sebuah investasi atau dalam bidang perekonomian. Setiap periode tertentu, tingkat suku bunga disetiap negara fluktuatif. Oleh sebab itu, pengestimasian tingkat suku bunga untuk periode selanjutnya sangatlah penting untuk menentukan perlakuan dari sebuah investasi. Terdapat cukup banyak metode untuk menentukan tingkat suku bunga, diantaranya model suku bunga CIR yang telah banyak digunakan dikarenakan menghasilkan tingkat suku bunga yang positif. Selain itu terdapat metode lain yang baru-baru ini dibahas oleh beberapa peneliti, yaitu metode Nonparametrik Kernel. Dalam skripsi ini, dibahas dua metode nonparametrik kernel yaitu Gaussian Kernel dan Gamma Kernel. Sebelum melakukan estimasi tingkat suku bunga, dilakukan terlebih dahulu estimasi fungsi drift dan fungsi difusi untuk model persamaan diferensial stokastik untuk metode ini. Penurunan estimasi fungsi tersebut menggunakan metode infinitesimal generator untuk order pertama. Dan setelah dilakukan estimasi, dihasilkan bahwa metode Gaussian menghasil hasil estimasi yang lebih dekat dibandingkan dengan hasil estimasi dengan metode Gamma Kernel. Hal ini dilihat dari besar Mean Squarred Error masingmasing.",,"Kata Kunci : tingkat suku bunga, Nonparametrik Kernel, Gaussian Kernel, Gamma Kernel, Infinitesimal generator, fungsi drift, fungsi difusi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150740,REGRESI RELATIVE SURVIVAL DENGAN METODE GENERALIZED LINEAR MODEL POISSON (GLMP); Regression of Relative survival using Generalized Linear Model Poisson (GLMP) method,"FATHONI FAWZI IDRIS, Zulaela",2015 | Skripsi | PROGRAM STUDI STATISTIKA,"Secara teoritis, nilai relative survival didefinisikan sebagai rasio dari proporsi tahan hidup observasi (observed survival) dengan tahan hidup harapan (expected survival) dalam sebuah populasi. Fungsi relative survival digunakan ketika informasi penyebab kematian tidak akurat atau tidak ada. Untuk mengestimasi relative survival bisa menggunakan metode life table yaitu metode Ederer I, Ederer II, dan Hakulinen, selain itu bisa menggunakan model regresi relative survival, yaitu dengan metode Generalized Linear Model poisson, Esteve et al. full likelihood approach, dan Hakulinen-Tenkanen. Pada skripsi ini akan digunakan metode Generalized Linear Model poisson untuk mengestimasi relative survival pada data pasien Acute Myocardial Infarction (AMI) di Slovenia selama tahun 1982 sampai 1986.",,"Kata Kunci : relative survival, generalized linear model poisson,"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150273,PERBANDINGAN ESTIMATOR CENSORED LEAST ABSOLUTE DEVIATIONS (CLAD) DAN SYMMETRICALLY CENSORED LEAST SQUARES (SCLS) UNTUK MODEL REGRESI TOBIT (Studi Kasus : Analisis Faktor-Faktor yang Mempengaruhi Partisipasi Perempuan dalam Perekonomian Rumah Tangga di Provinsi Daerah Istimewa Yogyakarta); A COMPARISON BETWEEN CENSORED LEAST ABSOLUTE DEVIATIONS (CLAD) AND SYMMETRICALLY CENSORED LEAST SQUARES (SCLS) FOR TOBIT MODEL (Case Study: Factors Analyzes of Women’s Participant in Domestic Economy of Yogyakarta province),"VANIA PRIMA AMELINDA, Subanar",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam tugas akhir ini, dibahas suatu metode alternatif untuk estimator Maximum Likelihood untuk data tersensor atau yang sering disebut dengan Model Tobit. Ada dua metode alternatif yang akan dibahas dan dibandingkan yaitu Censored Least Absolute Deviations (CLAD) dan Symmetrically Censored Least Squares (SCLS). Tidak seperti metode Maximum Likelihood, metode CLAD konsisten dan asimtotis normal serta robust digunakan untuk data yang tidak memenuhi asumsi normalitas dan homoskedastisitas. Sementara metode SCLS masih mengasumsikan kesimetrisan (dan independensi) dari distribusi error, namun tetap konsisten walaupun residual tidak berdistribusi identik dan tetap robust untuk data heterokedastik. Untuk studi kasus, digunakan data Survei Angkatan Kerja Nasional (SAKERNAS) 2013 untuk memodelkan faktor-faktor yang mempengaruhi partisipasi perempuan dalam perekonomian rumah tangga di provinsi Daerah Istimewa Yogyakarta",,"Kata Kunci : Regresi Semi Parametrik Tersensor; Model Tobit; Censored Least Absolute Deviations, Simmetrically Censored Least Squares; Bootstrap; Algoritma ILPA; Algoritma Newton Type"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149256,ANALISIS KOVARIANSI PADA RANCANGAN TERSARANG; ANALYSIS COVARIANCE IN NESTED DESIGN,"KHOIRUL UMAM, Sri Haryatmi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Variabel konkomitan adalah variabel lain yang muncul dalam suatu percobaan dan mempengaruhi variabel respon yang diamati. Keberadaan variabel konkomitan dapat mempengaruhi tingkat ketelitian suatu percobaan dikarenakan berhubungan secara linier dengan variabel respon, sehingga variabel ini tidak bisa diabaikan begitu saja. Penyelesaian terhadap adanya variabel konkomitan ini dilakukan dengan analisis kovariansi. Analisis kovariansi sendiri adalah suatu teknik analisis statistik yang mengkombinasikan analisis variansi dan analisis regresi. Analisis kovariansi merupakan salah satu teknik yang dapat digunakan untuk perbaikan ketelitian pada suatu percobaan. Analisis kovariansi bertujuan untuk mengurangi variansi suku error yang besar yang kadang muncul dalam analisis variansi. Untuk tujuan ini, analisis kovariansi memanfaatkan hubungan variabel dependen dengan satu atau lebih variabel independen kuantitatif. Pada kasus ini Analisis kovariansi yang digunakan adalah analisis kovariansi pada rancangan tersarang (nested design). Rancangan tersarang sendiri adalah rancangan dengan sifat bahwa taraf faktor yang satu tersarang dalam fakor yang lain.",,Kata Kunci : Variabel konkomitan; analisis kovariansi; rancangan tersarang
http://etd.repository.ugm.ac.id/home/detail_pencarian/149258,OPTIMISASI PORTOFOLIO CAMPURAN; OPTIMIZATION MIXED PORTFOLIO,"Chandika Handayani, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Saham merupakan salah satu instrumen yang sering dipakai dalam investasi. Tingkat pengembalian (return) saham dan besarnya risiko yang ditanggung investor merupakan hal yang perlu diperhatikan. Untuk mengoptimalkan return dan meminimalkan risiko dapat dibentuk portofolio saham. Return dihitung dari harga penutupan saham bulanan pada masing-masing aset yang terdaftar pada NASDAQ-100. Optimisisasi portofolio campuran merupakan kombinasi aset berisiko dan sebuah aset bebas aset risiko dengan sharpe ratio yang tinggi yang dikenal dengan portofolio tangency. Aset berisiko adalah aset-aset yang tingkat return aktualnya dimasa depan masih mengandung ketidakpastian, misalnya saham. Aset bebas risiko adalah aset yang tingkat return dimasa depan sudah bisa ditentukan pada saat ini, misalnya Treasury Bill.",,Kata Kunci : Aset Berisiko; Aset Bebas Risiko; Portofolio Tangency
http://etd.repository.ugm.ac.id/home/detail_pencarian/149520,APLIKASI GENERALIZED RIDGE REGRESSION UNTUK MENGATASI MASALAH MULTIKOLINEARITAS; (APPLICATION OF GENERALIZED RIDGE REGRESSION FOR SOLVING MULTICOLINEARITY PROBLEM),"VALENDRA GRANITHA SHANDIKA PURI, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode kuadrat terkecil adalah salah satu metode penaksiran parameter yaitu metode untuk menduga koefisien regresi. Jika salah satu asumsi regresi klasik yaitu asumsi no multikolinearitas tidak terpenuhi, estimasi parameter dengan menggunakan metode kuadrat terkecil menjadi kurang valid, bahkan jika terjadi multikolinearitas sempurna dapat menyebabkan parameter beta tidak dapat diestimasi. Hubungan linier antar variabel independen ini menyebabkan variansi parameter beta menjadi besar, errornya pun besar. Padahal nilai estimasi yang diinginkan adalah yang memiliki nilai variansi dan error yang kecil. Salah satu penanganan multikolinearitas ini adalah dengan regresi ridge. Konsep dari regresi ridge adalah menambahkan tetapan bias sebesar k yang merupakan matriks diagonal, ke dalam matriks korelasi ' X X . Dalam skripsi ini akan dibahas mengenai salah satu metode estimasi parameter k yaitu dengan metode Generalized Ridge Regression. Dengan metode ini, nilai parameter ridge k yang dihasilkan tidak hanya satu macam, nilai k yang didapat berbeda untuk tiap variabel independennya",,Kata Kunci : Metode Kuadrat Terkecil; Multikolinearitas; Regresi Ridge; Generalized Ridge Regression
http://etd.repository.ugm.ac.id/home/detail_pencarian/149521,"MANAJEMEN PERSEDIAAN MENGGUNAKAN MODEL Q,r DENGAN TIMEWEIGHTED BACKORDERS; INVENTORY MANAGEMENT USING Q,r MODEL WITH TIME-WEIGHTED BACKORDERS","Ida Nurfaizah Rahmah, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Persediaan adalah merupakan salah satu unsur paling aktif dalam operasi perusahaan yang secara kontinue diperoleh, diubah, yang kemudian dijual kembali. Pada dasarnya persediaan juga merupakan sumber daya yang menganggur (idle resources), yang berarti jika persediaan berlebih menyebabkan investasi sia-sia, akan tetapi bila tidak ada persediaan akan sulit mengantisipasi fluktuasi permintaan atau hal-hal lain yang menyebabkan terjadinya kekurangan. Ketika dihadapkan dengan kehabisan persediaan, reaksi pelanggan berbeda-beda, tergantung pada bagaimana hal itu mempengaruhi bisnis masing-masing. Beberapa peka terhadap frekuensi stockout sementara yang lain menganggap jumlah backorder lebih penting. Dalam jenis usaha tertentu seperti mesin atau elemen penting, bagaimanapun durasi stockout merupakan elemen penting. Dengan demikian time-weighted backorders adalah tindakan tepat dari stockout dalam situasi seperti ini. . Keputusan yang menyangkut “berapa banyak dan kapan harus melakukan pemesanan” merupakan masalah utama dalam manajemen persediaan. Model persediaan Q,r dengan time-weighted backordes diyakini mampu menyelesaikan masalah itu, sistem ini akan menentukan titik posisi persediaan kapan harus memesan barang (R), dan jumlah barang yang harus selalu dipesan (Q) ketika persediaan berada tepat pada titik itu menggunakan metode HMMS multi-item.",,Kata Kunci : manajemen persediaan; time-weighted backorders; HMMS model; persediaan multi-item
http://etd.repository.ugm.ac.id/home/detail_pencarian/149524,OPTIMISASI PORTOFOLIO MENGGUNAKAN PENDEKATAN TELSER BERBASIS VALUE AT RISK; PORTFOLIO OPTIMIZATION USING VAR BASED TELSER APPROACH,"SASHA MEGAWATI PASUHUK, Adhitya Ronnie Effendie",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam menanamkan modalnya di pasar modal khususnya saham, investor memerlukan suatu metode optimisasi portofolio. Salah metode optimisasi dikembangkan oleh Markowitz (1952) yaitu metode mean-variance. Metode ini berkonsentrasi pada upside dan downside risk yang menggunakan pendekatan statistika standar deviasi sebagai ukuran risiko nya. Padahal beberapa investor menganggap bahwa risiko sebenarnya adalah risiko return negatif (dowside risk) sehingga dibutuhkan metode lain yang hanya berkonsentrasi pada dowside risk. Salah satunya dengan menerapkan prinsip safety first. Metode pendekatan Telser berbasis Value at Risk merupakan salah satu teknik optimisasi portofolio yang menerapkan prinsip safety first. Metode ini bertujuan untuk membentuk portofolio yang memaksimalkan expected return sekaligus memenuhi kendala Value at Risk. Kendala Value at Risk digunakan untuk meminimumkan risiko portofolio. Dalam penghitungannya return diasumsikan berdistribusi eliptikal. Kemudian dibawah asumsi tersebut dihitung formula optimisasi Telser dengan kendala VaR yang nantinya akan menghasilkan alokasi aset optimal untuk masing-masing saham. Pada studi kasus dibentuk portofolio yang terdiri dari saham-saham pada Bursa efek Indonesia, yaitu CTRP, TLKM, MNCN, BBRI, LSIP, SMCB, MEDC menggunakan metode pendekatan Telser, metode pendekatan Telser dengan kendala value at risk, dan mean variance. Kemudian kinerja ketiga portofolio tersebut dibandingkan dengan ukuran kinerja besarnya kentungan/ kerugian, tingkat pengembalian (rate of return) dan Sharpe ratio. Hasilnya, portofolio VaR Telser menunjukkan kinerja portofolio yang baik dengan risiko yang paling rendah di antara ketiga portofolio tersebut. Metode optimisasi portofolio dengan pendekatan Telser berbasis Value at Risk menghasilkan portfolio dengan risiko yang lebih rendah dibandingkan dengan optimisasi portofolio pendekatan Telser, dan mengahasilkan keuntungan yang lebih tinggi dibandingkan dengan metode optimisasi mean-variance. Oleh karena itu, optimisasi portofolio menggunakan pendekatan Telser berbasis Value at Risk ini bisa menjadi alternatif pilihan bagi investor",,Kata Kunci : Portofolio; prinsip safety first; optimisasi Telser; Value at Risk; Optimisasi Telser berbasis Value at Risk; optimisasi mean-variance; distribusi eliptikal
http://etd.repository.ugm.ac.id/home/detail_pencarian/150303,PERBANDINGAN UJI T STATISTIK RATA-RATA TERBOBOT DENGAN RATA-RATA TIDAK TERBOBOT PADA DATA KEUANGAN (Studi Kasus Yield Obligasi Muncipal dan Korporasi Amerika Serikat dengan Rating AAA); COMPARISON t-STATISTIC WEIGHTED MEANS and UNWEIGHTED MEANS in DATA of FINANCE Diajukan untuk memenuhi salah satu syarat memperoleh derajat Sarjana Program Studi Statistika Jurusan Matematika,"INDAH MENTARI, Gunardi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Rata-rata adalah teknik penjelasan kelompok atau ukuran pusat dari sekumpulan data. Biasanya suatu pertanyaan statistik akan muncul saat dua sampel random, diambil dari dua populasi normal, yaitu apakah rata-rata kedua populasi ini sama atau tidak. Untuk menguji kesamaan rata-rata tersebut, biasanya digunakan uji-t mean dua sampel. Pada skripsi ini akan dijelaskan perluasan dari uji-t mean dua sampel yaitu dengan menggunakan pembobotan. Metode ini menyajikan pengembangan dari dua sample t-test untuk kasus di mana nilai-nilai sampel harus diberi bobot yang tidak setara . Ini merupakan hal umum dalam pemodelan keuntungan di mana beberapa sampel dianggap lebih dapat diandalkan dibandingkan yang lain dalam memprediksi rata-rata populasi. Disini ditunjukkan dengan contoh data pengembalian yang diterima oleh investor yang menggunakan uji-t tidak terbobot dapat mengarah pada kesimpulan yang salah dibandingkan uji-t terbobot",,Kata Kunci : Uji rata-rata; Analisis normalitas
http://etd.repository.ugm.ac.id/home/detail_pencarian/149538,ESTIMASI NILAI DATA HILANG MENGGUNAKAN IMPUTASI GANDA DENGAN METODE REGRESI; Value Estimation of Missing Data using Multiple Imputation with Regression Method,"MONICA RINDAYU GALIH KUSUMAARUM, Subanar",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Data merupakan salah satu poin penting dalam setiap analisis data, karena tidak akan mungkin analisis dilakukan tanpa data. Data yang digunakan diharapkan merupakan data yang baik. Namun pada kenyataannya, seringkali data tidak sesuai dengan yang kita harapkan. Data yang tidak lengkap menyebabkan proses penarikan kesimpulan menjadi lebih sulit. Jika data yang hilang diabaikan, maka menyebabkan kesimpulan yang bias atau tidak valid. Oleh karena itu, muncullah berbagai metode untuk mengestimasi nilai yang hilang tersebut. Salah satunya adalah imputasi ganda dengan menggunakan metode regresi. Metode ini digunakan untuk mengestimasi nilai data yang hilang pada variabel dependen. Pembahasan diakhiri dengan studi kasus mengenai estimasi nilai data hilang pada variabel persentase penduduk miskin.",,Kata Kunci : regresi linear; data hilang; imputasi ganda; imputasi ganda menggunakan metode regresi
http://etd.repository.ugm.ac.id/home/detail_pencarian/150308,PENGKLASTERAN DATA RUNTUN WAKTU BERBASIS DENSITAS PERAMALAN (Studi Kasus : Data Bulanan Indeks Produksi Industri Per Negara periode Januari 1990- Januari 2014); CLUSTERING TIME SERIES BASED ON FORECAST DENSITIES (Case Studies : Monthly Industrial Production Indices (seasonally adjusted) January 1990 to January 2014),"ORIEZA FEBRIANDHANI, Subanar",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Suatu metode pengklasteran data runtun waktu diperkenalkan, dengan berbasis pada densitas probabilitas dari peramalan pada titik atau rentang waktu tertentu. Pertama, prosedur bootstrap dikombinasikan dengan estimator nonparametrik kernel untuk memperoleh estimasi dari densitas peramalan. Hasil estimasi densitas peramalan ini kemudian digunakan untuk membentuk matriks ketakmiripan yang selanjutnya dipakai untuk melakukan pengklasteran. Terakhir, aplikasi metode ini pada dataset riil juga akan dibahas.",,Kata Kunci : Pengklasteran; Runtun Waktu; Autoregression Bootstrap; Estimator Kernel; Indeks Produksi Industri
http://etd.repository.ugm.ac.id/home/detail_pencarian/150311,PEMILIHAN PORTOFOLIO DENGAN MENGGUNAKAN PEMODELAN MIXTURE OF MIXTURE; PORTFOLIO SELECTION USING MIXTURE OF MIXTURE MODELING,"MIDIAN RAJAGUKGUK, Gunardi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Perkembangan teknologi, informasi dan komunikasi yang sangat cepat, secara tidak disadari telah mewarnai kemajuan penciptaan instrumen investasi secara global dalam pasar modal. Berbagai macam instrumen telah diperdagangkan secara terbuka dan mampu memberikan fasilitas serta kesempatan kepada investor yang cukup leluasa untuk memperbesar macam cara investasi. Keuntungan yang diperoleh bisa sangat besar dan cepat dengan melakukan investasi secara simultan atau bersamaan dengan tepat. Hal ini akan memaksa para investor untuk sadar akan perlunya manajemen resiko yang dapat mewarnai perjalanan pengambilan keputusannya. Metode yang cukup tua dan populer serta sederhana secara statistik dalam penghitungan resiko investasi melalui portofolio adalah Value at Risk (VaR). Metode ini memberikan cara perhitungan nilai kerugian minimum atas portofolio yang dipilih pada tingkat kepercayaan tertentu. Dengan membagi ke dalam beberapa segmen berdasarkan waktu dan aktivitas perekonomian dunia dan kemudian melakukan pemodelan portofolio berdasarkan data dari beberapa saham (BBNI.JK, BNGK.JK, BMRI.JK, AALI.JK, emas) menggunakan mixture of mixture akan merepresentatifkan dalam menjelaskan pola return dan sekaligus menggunakan hasilnya untuk menghitung besarnya proposi besaran dana yang akan dialokasikan.",,Kata Kunci : pemodelan mixture of mixture; portofolio
http://etd.repository.ugm.ac.id/home/detail_pencarian/150315,MODEL RUNTUN WAKTU UNTUK MEMODELKAN DATA DERET BERKALA JANGKA PANJANG; TIME SERIES MODEL TO MODELIZE LONG MEMORY DATA,"DEWINTA PUTRI, Yunita Wulan Sari",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Terdapat dua jenis data yang dikenal dalam analisis runtun waktu yaitu data short memory dan long memory. Short memory adalah data yang memiliki ciri proses jangka pendek sedangkan long memory adalah data yang memiliki ciri proses jangka panjang. Data long memory hanya mampu dianalisis secara akurat menggunakan metode ARFIMA (Autoregressive Fractionally Integrated Moving Average). Tujuan penulisan skripsi ini untuk menjelaskan bagaimana melakukan pemodelan data dengan metode ARFIMA secara tepat dengan langkah-langkah analisis data dengan metodologi Box Jenkins dan mampu mengaplikasikan metode peramalan tersebut pada data real. Data runtun waktu yang bersifat long memory ditandai dengan plot Fungsi Autokorelasi (Autocorrelation Function (ACF)) yang tidak turun secara eksponensial melainkan menurun secara lambat atau hiperbolik. Model ARFIMA merupakan pengembangan dari model Autoregressive Moving Average (ARIMA), dengan nilai pembedaan (differencing) bernilai pecahan. Dalam penulisan skripsi ini, identifikasi orde parameter dilakukan secara eksploratori dengan melihat pada correlogram ACF dan PACF. Langkah-langkah pemodelan ARFIMA sebagai berikut: 1. menguji normalitas data long memory; 2. membuat plot runtun waktu, plot ACF, plot PACF, menguji ketidakstasioneran data; 3. melakukan transformasi data menggunakan transformasi logaritma jika data tidak stasioner dalam variansi dan melakukan differencing jika data tidak stasioner dalam mean; 4. mengestimasi parameter model; 5. melakukan diagnostic checking dengan model yang telah memenuhi asumsi residual white noise dan pemilihan model ARFIMA terbaik dengan kriteria memiliki nilai AIC terkecil diantara kemungkinan model",,Kata Kunci : runtun waktu; data deret berkala jangka panjang; ARFIMA; estimasi GPH.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149050,DETEKSI OUTLIER MULITVARIAT MENGGUNAKAN JARINGAN SYARAF TIRUAN SELF-ORGANIZING MAP; MULTIVARIATE OUTLIER DETECTION USING ARTIFICIAL NEURAL NETWORK SELF-ORGANIZING MAP,"ARYA ANDIKA DUMANAUW, Subanar",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Adanya outlier dalam data dapat mengganggu analisis statistik. Oleh karena itu, diperlukan deteksi outlier pada data sebeluim dianalisis. Metode Self-Organizing Map (SOM) dalam Jaringan Saraf Tiruan dapat digunakan untuk mendeteksi outlier dengan cara mengonstruksi u-matriks dan boxplot kesalahan kuantisasi dari jarngan yang terbentuk. Dari u-matriks, neuron dengan jarak yang sangat jauh dibanding kebanyakan neuron lain diidentifikasi sebagai neuron outlier. Outlier merupakan gabungan dari pengamatan di dalam neuron outlier pada u-matrix dan juga pengamatan outlier yang diperoleh sesuai kriteria outlier boxplot kesalahan kuantisasi. Secara umum, SOM dapat mendeteksi 60% outlier dalam data serta hanya mendeteksi kurang dari 20% data sehat yang dideteksi sebagai outlier. Ketepatan klasifikasi dari SOM adalah sekitar 70%. Trial error dalam penentuan parameter jaringan dan pendekatan lain dalalm intepretasi umatrix mungkin dilakukan untuk mendapatkan hasil yang lebih baik dan akurat.",,Kata Kunci : deteksi outlier; SOM; u-matrix; kesalahan kuantisasi; ketepatan akurasi; masking; swamping; boxplot
http://etd.repository.ugm.ac.id/home/detail_pencarian/150591,ESTIMASI GENERALIZED MOMENT PADA MODEL LINEAR PANEL EFEK RANDOM DENGAN KOMPONEN ERROR SPASIAL; GENERALIZED MOMENT ESTIMATION FOR PANEL LINEAR RANDOM EFFECT MODEL WITH SPATIAL ERROR,"RATNA WULANSARI, Sri Haryatmi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Skripsi ini akan membahas estimasi model panel linear efek random dengan komponen error spasial. Model panel linear efek random digunakan untuk mengatasi ketidakpastian model pada model efek tetap. Sedangkan, komponen spatial error menunjukkan variabel dependennya tergantung pada karakteristikkarakteristik lokal yang diamati dengan komponen galat yang berkorelasi dalam ruang. Estimasi pada model linear panel efek random dengan komponen spatial error ini difokuskan pada estimasi generalized moment. Uji lagrange multiplier untuk melihat apakah dalam model terdapat efek spasial autokorelasi pada cross section. Data public capital productivity di 48 negara bagian Amerika dari tahun 1970-1986 dijadikan sebagai studi kasusnya.",,"Kata Kunci : spatial error, model efek random, estimasi generalized moment, uji lagrange multiplier"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148804,METODE BOOTSTRAP DALAM CADANGAN KLAIM IBNR (Incurred But Not Reported); BOOTSTRAP METHOD IN IBNR CLAIM RESERVES,"Mustika Rizky Amalia, Adhitya Ronnie Effendie",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Incurred but not reported (IBNR) adalah jenis klaim pada asuransi nonjiwa yang sudah terjadi namun belum dilaporkan kepada perusahaan asuransi, hal ini dapat disebabkan karena waktu yang dibutuhkan dalam menjalankan prosedurprosedur dalam melengkapi berkas pengajuan klaim seperti prosedur hukum dan beberapa prosedur administratif lainnya. Adanya efek periode kejadian dan periode pengembangan membuat prediksi IBNR sulit untuk dimodelkan Menurut Pinheiro (2001), Kaas (2008), Shapland dan Leong (2010), distribusi yang cocok untuk memodelkan cadangan klaima adalah over dispersed Poisson (ODP) dimana mean data tidak lebih besar dari variansinya. Pada skripsi ini penulis mengaplikasikan metode bootstrap untuk memperoleh prediksi cadangan IBNR dalam model ODP. Bootstrap adalah suatu metode resampling yang digunakan untuk mengestimasi atau menaksir suatu parameter. (Efron (1979)). Studi kasus yang digunakan adalah data klaim IBNR periode 1999-2008 untuk kompensasi yang diberikan kepada karyawan dari seluruh perusahaan yang bergerak di bidang industri di Amerika Serikat dalam juta dolar US. Hasil dari pemodelan bootstrap diperoleh total cadangan klaim IBNR sebesar 155477,9 juta dollar US dengan standar error prediksi yaitu 7366,258",,Kata Kunci : cadangan klaim; IBNR; generalized linear model; over dispersed poisson; bootstra
http://etd.repository.ugm.ac.id/home/detail_pencarian/150095,PEMODELAN KREDIBILITAS RATEMAKING MENGGUNAKAN ELLIPTICAL COPULA; CREDIBILITY RATEMAKING MODELLING USING ELLIPTICAL COPULA,"WAHYU HIDAYAT, Danardono",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Program asuransi sebenarnya bukan hal baru. Berbagai kalangan memahami, asuransi sebagai lembaga keuangan pengelola risiko memang diperlukan keberadaannya. Tetapi, kesadaran untuk mengikuti program asuransi belum dimiliki semua orang. Dalam kehidupan manusia, faktor risiko adalah suatu yang mungkin terjadi. Mulai dari risiko kehilangan aset, risiko sakit, cacat total hingga risiko kehilangan jiwa atau meninggal dunia. Dengan sering terjadinya bencana alam, kecelakaan membuat orang ataupun pelaku usaha mulai melirik asuransi sebagai pilihan utama untuk mengurangi kerugian atas risiko yang mungkin terjadi di kemudian hari. Salah satu cara untuk mengukur risiko adalah dengan kredibilitas ratemaking menggunakan copula. Kredibilitas ratemaking adalah teknik untuk memprediksi ekspektasi klaim di masa depan berdasarkan kelas risiko, dengan berdasarkan klaim masa lalu dan kelas risiko yang terkait",,Kata Kunci : klaim; kredibilitas ratemaking; copula
http://etd.repository.ugm.ac.id/home/detail_pencarian/150097,MODEL ADITIF CAMPURAN TERGENERALISASI; GENERALIZED ADDITIVE MIXED MODELS,"JAMILATUZZAHRO, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Generalized additive models (GAM) merupakan perluasan dari regresi linier biasa, dengan mengganti fungsi linier menjadi fungsi aditif sehingga model ini dapat digunakan meskipun hubungan variabel respon dan variabel prediktor tidak linier. Serta, variabel responpada GAM merupakan keluarga eksponensial. Namun GAM tidak dapat digunakan jika ada dua efek dalam suatu model yaitu efek tetap dan efek acak. Generalized additive mixed models (GAMM) ini diharapkan lebih efisien dalam mengidentifikasi sebaran pengaruh komponenen acak sehingga mampu menerangkan lebih tepat pengaruh komponen acak tersebut dalam suatu model. Penggunaan generalized additive mixed models untuk data variabel kuantitatif dengan estimasi fungsi penghalus menggunakan smoothing spline, dan estimasi parameter menggunakan Maximum Likelihood Estimation (MLE) tidak dapat diselesaikan secara analitik, sehingga estimator dihitung dengan memaksimumkan fungsi log-likelihood secara numerik menggunakan metode Newton-Raphson dengan menggunakan ekspektasi turunan kedua dari fungsi log-likelihood yang dinamakan teknik fisher scoring",,Kata Kunci : Generalized additive mixed models; maximum likelihood estimation; smoothing spline; fisher scoring
http://etd.repository.ugm.ac.id/home/detail_pencarian/149591,EKSPEKTASI ONGKOS GARANSI DUA DIMENSI MENGGUNAKAN KEBIJAKAN NON- RENEWING KOMBINASI FRW DAN PRW; THE EXPECTATION OF TWO- DIMENSIONAL WARRANTY COST USING NON- RENEWING FRW AND PRW COMBINATION POLICY,"Irma Yuniar Pangesti Rahayu, Danardono",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Garansi adalah sebuah obligasi yang diberikan pada produk yang mewajibkan perusahaan untuk menetapkan ganti rugi kepada konsumen jika produk tersebut gagal berfungsi dari penggunaan normalnya setelah pembelian selama periode garansi yang telah ditentukan. Dengan adanya penawaran garansi akan menimbulkan biaya tambahan yang disebut ongkos garansi sehingga estimasi ongkos garansi sangat penting untuk dilakukan. Dalam tugas akhir ini akan dilakukan studi garansi dua dimensi dengan kebijakan Non- Renewing Kombinasi Free Replacement Warranty dan Pro Rata Warranty pada produk nonrepairable, dimana garansi dibatasi oleh umur dan tingkat pemakaian. Perhitungan ekspektasi biaya garansi dua dimensi menggunakan distribusi gabungan dari distribusi variabel waktu dan pemakaian, yaitu distribusi bivariat. Pemodelan kegagalan mengikuti distribusi bivariat pareto, dengan parameter a, Q1, Q2 yang diperoleh melalui metode MLE. Pada penghitungan ekspektasi ongkos garansi kebijakan FRW, digunakan jenis integral konvolusi dan transformasi laplace yang memerlukan bantuan software untuk mendapatkan solusinya, begitu pula dalam penghitungan ekspektasi refund pada kebijakan PRW.",,Kata Kunci : garansi 2 dimensi; kombinasi FRW dan PRW; non- renewing; nonrepairable; bivariat pareto; transformasi laplace; integral konvolusi
http://etd.repository.ugm.ac.id/home/detail_pencarian/150107,COPULA BERSYARAT UNTUK MENGESTIMASI VALUE at RISK (Studi Kasus Saham Nasdaq dan S&P500); CONDITIONAL COPULA to ESTIMATES VALUE at RISK,"MELVINA OCHTORA DAMANIK, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Value at Risk (VaR) memerankan peran penting dalam bidang risiko manajemen sekarang ini.Banyak metode yang digunakan dalam perhitungan VaR seperti metode Variansi-Kovariansi, simulasi historis dan sebagainya. Umumnya pendekatan-pendekatan tersebut mengasumsikan data return berdistribusi normal dan ukuran dependensi diantara saham-saham portofolio menggunakan korelasi linear. Dalam dunia finansial asumsi normalitas jarang terpenuhi dan sesuai dengan namanya, korelasi linear tidak dapat mendeteksi hubungan dependensi yang non linear sehinggaestimasi VaR kurang akurat.Konsep Copula merupakan alat yang sangat powerful dalam memodelkan distribusi gabungan untuk berbagai bentuk distribusi marginal data.Ukuran dependensi yang digunakan juga berbasis copula seperti tail dependensi. Penulisan skripsi ini membahas tentang konsep copula dan aplikasinya dalam mengestimasi Value at Risk suatu portofolio yang terdiri dari 2 indeks saham. Data yang digunakan adalah return-return indeks saham Nasdaq dan S&P500 pada periode 13 Agustus 2012 hingga 10 Maret 2014 dimana masingmasing return saham dimodelkan dengan ARMA-GARCH untuk didapatkan residualnya yang untuk selanjutnya digunakan untuk pemodelan Copula dan estimasi VaR.",,Kata Kunci : COPULA BERSYARAT; ESTIMASI; VALUE at RISK
http://etd.repository.ugm.ac.id/home/detail_pencarian/150619,IMPLEMENTASI METODE DIRECTED RIDGE REGRESSION DALAM PEMODELAN FAKTOR YANG MEMPENGARUHI PRODUK DOMESTIK REGIONAL BRUTO; (IMPLEMENTATION OF DIRECTED RIDGE REGRESSION METHOD IN MODELLING OF FACTORS AFFECTING THE GROSS DOMESTIC REGIONAL PRODUCT),"Rahmat Maulizar, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi merupakan analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Dalam analisis regresi klasik yang menggunakan metode kuadrat terkecil terdapat beberapa asumsi yang harus terpenuhi, salah satunya adalah tidak terdapat multikolinearitas. Jika asumsi ini tidak terpenuhi, estimasi parameter dengan menggunakan metode kuadrat terkecil menjadi kurang valid serta akan memiliki variansi dan error yang besar. Seiring perkembangan zaman, ditemukan berbagai metode untuk mengatasi masalah multikolinearitas ini, salah satunya adalah dengan regresi ridge. Konsep dari regresi ridge adalah menambahkan tetapan bias sebesar",,"Kata Kunci : Metode Kuadrat Terkecil, Multikolinearitas, Regresi Ridge, Directed Ridge Regression."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150108,PENENTUAN HARGA OPSI EROPA MODEL TRINOMIAL DENGAN TEKNIK EKSTRAPOLASI; TRINOMIAL EUROPEAN OPTION PRICING MODEL WITH EXTRAPOLATION TECHNIQUE,"Ikha Fitria Herdyanti, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Penelitian mengenai bagaimana penentuan harga opsi terus mengalami perkembangan dari waktu ke waktu. Salah satunya dengan menggambarkan pergerakan harga saham yang mengikuti model pohon trinomial. Dengan model ini diasumsikan bahwa pergerakan harga saham untuk periode ke depan mengikuti 3 kondisi yaitu harga saham akan naik, cenderung tetap, atau justru akan menurun. Namun, hasil yang diperoleh memiliki kekonvergenan yang lambat sehingga diperlukan suatu metode untuk mempercepat konvergensi dengan ekstrapolasi. Metode ekstrapolasi yang sering digunakan adalah teknik ekstrapolasi Richardson yang merupakan teknik ekstrapolasi yang cukup populer. Ide awal dari penggunaan teknik ekstrapolasi Richardson adalah dengan melakukan eliminasi pada beberapa bagian awal dari ekspansi asimtotis fungsi pendekatan yang bergantung pada barisan stepsize untuk mendapatkan pendekatan yang lebih baik.",,Kata Kunci : trinomial; ekstrapolasi Richardson; stepsize
http://etd.repository.ugm.ac.id/home/detail_pencarian/150621,MODEL REGRESI CONWAY-MAXWELL-POISSON (COM-POISSON) DENGAN TINGKAT DISPERSI KELOMPOK; CONWAY-MAXWELL-POISSON (COM-POISSON) REGRESSION MODEL WITH GROUP LEVEL-DISPERSION,"Kanzul Fikri Barzani, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi Poisson adalah metode yang digunakan dalam pemodelan data cacah. Ketika data cacah memiliki nilai variansi yang lebih besar dari nilai rata-rata maka terjadi overdispersi. Sehingga asumsi pada regresi Poisson tidak dapat terpenuhi, di mana seharusnya nilai variansi dan rata-ratanya diasumsikan sama (equidispersi). Regresi binomial negatif merupakan alternatif pemodelan jika data mengalami overdispersi, tetapi tidak untuk underdispersi. Akan tetapi, di dalam penulisan skripsi ini menunjukkan bahwa data cacah yang tampaknya equidispersi atau overdispersi, bisa saja merupakan hasil campuran populasi dengan tingkat dispersi yang berbeda. Untuk mendeteksi dan memodelkan campuran tersebut, kami men-generalisasikan dari model regresi Conway-Maxwell-Poisson (COM-Poisson) yang memungkinkan untuk tingkat dispersi kelompok. Menggunakan metode maksimum likelihood dan Newton-Raphson untuk mendapatkan estimasi paramater dalam model tersebut.",,"Kata Kunci : dispersi yang terlihat, regresi Conway-Maxwell-Poisson (COM-Poisson), model campuran, overdispersi, underdispersi, maksimum likelihood, bootstrap"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150371,KLASIFIKASI MENGGUNAKAN METODE BOOSTING; CLASSIFICATION WITH BOOSTING METHOD,"RINA PUSPITA SARI, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Di dalam tugas akhir ini dibahas metode Boosting yang digunakan dalam klasifikasi. Dewasa ini, kecermatan dan ketepatan dalam hal klasifikasi data merupakan hal yang sangat penting. Hasil klasifikasi data dengan beberapa metode yang telah dikembangkan akan mempengaruhi sejumlah keputusan, misalnya dalam hal kedokteran (emergency medicine), cuaca, bioinformatika dan bidang-bidang lainnya. Salah satu metode yang diperkenalkan adalah metode Boosting. Metode boosting adalah metode yang mengkombinasikan pengklasifikasi lemah menjadi pengklasifikasi yang kuat. Di dalam metode boosting diperkenalkan adanya iterasi. Dalam tiap iterasi learning-nya, dibangun model hasil prediksi data latih dan kemudian disampel ulang untuk masuk pada itrasi berikutnya. Pada umumnya hal ini akan meningkatkan tingkat keakurasian dalam pengklasifikasi. Metode Boosting juga baik digunakan pada data yang persebarannya tidak seimbang atau sering dikatakan imbalance. Pada kesimpulannya diperoleh bahwa penggunaan metode Boosting pada klasifikasi memberikan hasil yang lebih baik dengan tingkat keakurasian yang tinggi.",,"Kata Kunci : boosting, adaboost, adabag"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150631,ESTIMASI PARAMETER PADA REGRESI LOGISTIK DENGAN NILAI KOVARIAT HILANG MENGGUNAKAN ESTIMATOR INVERS PROBABILITAS TERBOBOT DAN IMPUTASI BERGANDA; ESTIMATION PARAMETER OF MODEL LOGISTIC REGRESSION WITH MISSING VALUES COVARIATE USING INVERSE PROBABILITY WEIGHTED ESTIMATOR AND MULTIPLE IMPUTATION,"RENINTA DEWI NUGRAHENI, Sri Haryatmi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Data merupakan salah satu poin penting dalam setiap analisis data, karena tidak akan mungkin analisis dilakukan tanpa data. Data yang digunakan diharapkan merupakan data yang baik. Namun pada kenyataannya, seringkali data tidak sesuai dengan yang kita harapkan. Data yang tidak lengkap menyebabkan proses penarikan kesimpulan menjadi lebih sulit. Jika data yang hilang diabaikan, maka menyebabkan kesimpulan yang bias atau tidak valid. Oleh karena itu, muncullah berbagai metode untuk mengestimasi nilai yang hilang tersebut. Metode-metode tersebut adalah Inverse Probability Weighted, Multiple Imputation dan kita dapat mengkombinasikan keduanya, metode itu adalah Inverse Probability Weighted combining Multiple Imputation.",,"Kata Kunci : kovariat hilang, imputasi ganda, invers probabilitas terbobot, regresi logistik, invers probabilitas terbobot kombinasi imputasi ganda"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150632,SUPPORT VECTOR REGRESSION (SVR) UNTUK MENGANALISA PERGERAKAN INDEKS HARGA SAHAM GABUNGAN (IHSG) SELAMA PEMILIHAN UMUM 2014,"Hamid Dimyati, Gunardi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Support Vector Regression (SVR) dengan bantuan kernel tunggal menjadi model yang mampu menangani kasus peramalan data saham selama masa Pemilihan Umum 2014. Pengaruh dari adanya momentum Pemilihan Umum 2014 terhadap pergerakan data saham akan dievaluasi terlebih dahulu melalui analisis event study. Selanjutnya model SVR dengan memanfaatkan peran kernel Gaussian, Laplacian, ANOVA dan Bessel akan menangani kasus peramalan data saham tersebut. Dari hasil pembandingan keempat kernel, diperoleh kesimpulan bahwa SVR dengan kernel Laplacian menjadi model yang paling baik, bahkan mengungguli hasil peramalan dengan model ARIMA melalui evaluasi mean squared error (MSE).",,"Kata Kunci : saham, event study, support vector regression, kernel"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149614,ESTIMASI PARAMETER REGRESI ROBUST DENGAN FUNGSI HUBER DAN FUNGSI BISQUARE; (PARAMETER ESTIMATION OF ROBUST REGRESSION BY MEANS OF HUBER FUNCTION AND BISQUARE FUNCTION,"PRADHANA RAMESHWARI, Sri Haryatmi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,Di dalam tugas akhir ini dibahas estimasi parameter regresi robust dengan menggunakan fungsi Huber dan fungsi Bisquare. Penggunaan regresi robust dimaksudkan untuk mengatasi pengaruh adanya data outlier. Pembahasan dimulai dari konsep regresi linear ?= = + + n i i i Y X 1 0 ? ? ? ; asumsi-asumsi yang perlu dipenuhi dalam regresi linear; estimasi parameter,,Kata Kunci : regresi robust; fungsi Huber; fungsi Bisquare; estimasi-M; outlier
http://etd.repository.ugm.ac.id/home/detail_pencarian/149616,PEMODELAN TOPIK UNTUK MEDIA SOSIAL MENGGUNAKAN LATENT DIRICHLET ALLOCATION; (TOPIC MODELS FOR SOCIAL MEDIA USING LATENT DIRICHLET ALLOCATION,"RUSKE ILLA KENGKEN, Dedi Rosadi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,Berkembangnya analisis media sosial saat ini memberikan suatu kebutuhan baru. Kita dituntut untuk dapat menyimpulkan opini atau argumen dalam kumpulan dokumen yang sangat besar seperti pada media sosial secara cepat dan efisien. Dari opini yang didapat kita dapat menyimpulkan sebuah informasi utama yang tersembunyi dan dapat digunakan untuk analisis lebih lanjut. Pemodelan topik atau topic models merupakan perkembangan dari analisis teks yang bermanfaat dalam pemodelan data tekstual dengan tujuan menemukan topik yang tersembunyi didalamnya. Salah satu model yang akan dibahas adalah model probabilitas Latent Dirichlet Allocation (LDA). Model Latent Dirichlet Allocation (LDA) merupakan sebuah model probabilitas dari data tekstual dimana dapat menjelaskan korelasi antara kata-kata dengan tema semantik yang tersembunyi didalam dokumen tersebut. Estimasi parameter yang digunakan dalam model adalah metode Bayesian. Metode Bayesian adalah sebuah metode yang memberikan nilai estimasi melalui distribusi posterior. Untuk model ini perhitungan estimasi dari distribusi posterior sangat kompleks sehingga digunakan estimasi Gibbs sampling. Dalam skripsi ini diterapkan model probabilitas Latent Dirichlet Allocation (LDA) untuk data yang bersumber dari salah satu platform media sosial yaitu Twitter. Tujuannya adalah untuk mengetahui berita apa yang dominan dibicarakan masyarakat di Twitter dalam periode tertentu. Hasil dari pemodelan topik ini adalah berupa topik utama dari seluruh opini masyarakat yang diinterpretasikan menjadi berita yang paling dominan dibicarakan masyarakat,,Kata Kunci : pemodelan topik; latent Dirichlet allocation; Bayesian; Gibbs sampling; text mining; analisis teks; twitter
http://etd.repository.ugm.ac.id/home/detail_pencarian/150641,PERBANDINGAN OPTIMISASI PORTOFOLIO METODE MEAN-VARIANCE DENGAN METODE TAIL MEAN-VARIANCE; COMPARISON PORTFOLIO OPTIMIZATION MEAN-VARIANCE METHOD WITH TAIL MEAN-VARIANCE METHOD,"ELOK ARISMA, Subanar",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Portofolio merupakan kombinasi linier dari beberapa aset. Di dalam pembentukannya, tentu setiap investor berusaha untuk memaksimalkan pengembalian yang diharapkan (expected return) dari investasi dengan tingkat risiko tertentu. Dengan kata lain, portofolio yang dibentuk dapat memberikan tingkat risiko terendah dengan expected return tertentu, atau dapat memberikan expected return tertinggi dengan tingkat risiko tertentu. Portofolio yang dapat mencapai tujuan di atas disebut dengan portofolio yang efisien. Pada skripsi ini akan dibahas mengenai pembentukan bobot portofolio menggunakan metode Tail Mean-Variance yang dikembangkan oleh Landsman (2010). Metode ini merupakan perluasan dari metode Mean-Variance yang dipelopori oleh Markowitz (1952). Studi kasus penelitian ini menggunakan data saham mingguan periode 1 Januari 2013 sampai 31 Desember 2013 dari 6 saham NASDAQ. Bobot portofolio dari metode Tail Mean-Variance dibandingkan dengan bobot portofolio dari metode Mean-Variance, peneliti melakukan trading selama 5 hari dari tanggal 8 oktober 2014 hingga 14 oktober 2014. Diperoleh kesimpulan bahwa metode Tail Mean-Variance memberikan total kerugian yang lebih kecil.",,"Kata Kunci : portofolio, Mean-Variance, Tail Mean-Variance."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149619,VALUE AT RISK NONPARAMETRIK UNTUK CLAIM SEVERITY PADA ASURANSI KERUGIAN MENGGUNAKAN ESTIMASI KERNEL BERTRANSFORMASI GANDA; NONPARAMETRIC VALUE AT RISK FOR CLAIM SEVERITY ON NONLIFE INSURANCE USING DOUBLE TRANSFORMED KERNEL ESTIMATION,"Diah Putri Ramadhani, Dedi Rosadi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Asuransi melibatkan dua pihak yaitu pihak penanggung dan pihak tertanggung. Pihak penanggung berkewajiban membayar pertanggungan sedangkan pihak tertanggung berkewajiban membayar sejumlah uang sebagai kompensasinya. Hal ini membuat perusahaan asuransi harus menentukan harga premi. Salah satu ukuran yang digunakan sebagai patokan adalah ukuran resiko, dan salah satu cara menghitung resiko adalah metode Value at Risk. Value at Risk dimodelkan sesuai dengan bentuk dari distribusi kerugian. Untuk data data asuransi yang bersifat heavy tailed dapat digunakan metode Estimasi Kernel Bertransformasi Ganda",,"Kata Kunci : Klaim Asuransi; Estimator Kernel; Value at Risk; Distribusi Kerugian; Disribusi Modified Champernowne; Distribusi Beta(3,3)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150390,KONSTRUKSI KURVA YIELD MENGGUNAKAN METODE NELSON SIEGEL SVENSSON ALGORITMA DIFFERENTIAL EVOLUTION; CONSTRUCTION YIELD CURVE USING THE NELSON SIEGEL SVENSSON METHOD WITH DIFFERENTIAL EVOLUTION ALGORITHM,"NUR ALIFAH, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Penelitian ini mempelajari tentang teori kurva yield metode Nelson Siegel Svensson algoritma Differential Evolution kemudian dibandingkan dengan kajian empiris kurva yield yang menggunakan metode Nelson Siegel Svensson. Pada studi kasus penelitian ini, data diambil dari transaksi obligasi Pemerintah Indonesia tanggal 16 Februari 2011. Dari analisis MSE (Mean Square Error) didapatkan bahwa metode Nelson Siegel Svensson algoritma Differential Evolution memberikan hasil empiris lebih baik dalam memodelkan kurva yield dibandingkan metode Nelson Siegel Svensson.",,"Kata Kunci : kurva yield, Nelson Siegel Svensson, algoritma Differential Evolution"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150394,MODEL AUTOREGRESI SPASIAL; SPATIAL AUTOREGRESSIVE MODEL,"Novan Dwi Atmaja, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Model regresi spasial adalah model yang terbentuk dari regresi umum yang mendapatkan pengaruh spasial (lokasi). Anselin (1988). Beberapa model dependensi spasial yang dapat terbentuk berdasarkan dependensi yang terdapat pada model yaitu Spatial Autoregressive Model (SAR) dependensi nilai respon antar lokasi, Spatial Error Model (SEM) dependensi nilai galat antar lokasi. Pengujian Spatial Dependence dilakukan untuk melihat data setiap variabel memiliki pengaruh spasial pada lokasi. Pemodelan didasarkan pada pengaruh dependensi spasial, sehingga sebelum dilakukan pemodelan perlu dilakukan pengujian pengaruh spasial yang terkandung dalam data menggunakan statistik uji Lagrange Multiplier (LM). Pada tugas akhir ini, berdasarkan hasil pemodelan Spatial Autoregressive Model (SAR) diketahui bahwa faktor-faktor yang berpengaruh terhadap angka putus sekolah usia SMP di Jawa Tengah adalah rata-rata anggota rumah tangga dan persentase desa terpencil di tiap kabupaten/kota.",,"Kata Kunci : Spasial, Lagrange Multiplier, SAR, SEM."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150140,REGRESI NONPARAMETRIK KERNEL DAN METODE THEIL DALAM MEMODELKAN HUBUNGAN KURS RUPIAH DENGAN IHSG; USING NONPARAMETRIC REGRESSION KERNEL AND THEIL'S METHOD IN MODEL THE RELETIONSHIP OF EXCHANGE RATE AND JCI,"I GUSTI BAGUS YOGISWARA PATRA, Zulaela",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi adalah suatu analisis yang sering digunakan untuk memodelkan hubungan antara dua variabel atau lebih. Ada regresi parametrik dan regresi nonparametrik. Pada regresi nonparametrik, ada banyak metode untuk mengestimasi model persamaan, seperti metode Theil dan regresi kernel. Keduanya akan digunakan untuk memodelkan data nilai tukar rupiah terhadap US Dollar dan IHSG yang terindikasi memiliki outlier dan tidak memenuhi asumsi regresi parametrik. Akan dibandingkan pula dengan nilai MSE, MAE dan MAPE untuk melihat metode mana yang lebih baik pada kasus ini",,Kata Kunci : regresi nonparamterik; metode Theil; kernel; outlier
http://etd.repository.ugm.ac.id/home/detail_pencarian/150398,GENERALIZED ESTIMATING EQUATIONS MENGGUNAKAN METODE BOOTSTRAP BERPASANGAN; GENERALIZED ESTIMATING EQUATIONS USING PAIRED BOOTSRAP,"SAWITRI, Danardono",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode bootstrap merupakan metode yang digunakan untuk mengestimasi suatu distribusi populasi yang tidak diketahui. Metode bootstrap dapat digunakan untuk mengestimasi inferensi statistik seperti bias dan standar error. Metode bootstrap berpasangan adalah suatu metode resampling dengan mempertahankan korelasi pasangan variabel dependen dan variabel independennya. Metode bootstrap berpasangan digunakan dalam Generalized Estimating Equations. Generalized Estimating Equations merupakan salah satu metode dalam masalah regresi untuk data longitudinal. Studi kasus yang dilakukan menggunakan data real, yaitu pada penurunan jarak gigi marmut.",,"Kata Kunci : data longitudinal, Generalized Estimating Equations, bootstrap berpasangan, resampling."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150402,KONSTRUKSI KURVA YIELD DAN PENETAPAN HARGA ZERO COUPON BOND BERDASARKAN METODE CUBIC B-SPLINE; YIELD CURVE CONSTRUCTION AND ZERO COUPON BOND PRICING USING CUBIC B-SPLINE METHOD,"Rizki Evitasari, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Obligasi adalah salah satu instrumen investasi berpendapatan tetap. Keuntungan yang diterima oleh investor sampai jatuh tempo disebut dengan yield to maturity. Analisis yang menjelaskan tentang hubungan yield to maturity dengan waktu jatuh tempo disebut dengan struktur jangka waktu tingkat bunga atau term structure of interest rate. Struktur jangka waktu ini digambarkan dengan grafik yang disebut dengan yield curve. Kurva yield ini memuat yield sebagai koordinat x dan waktu jatuh tempo sebagai koordinat y. Pada skripsi ini akan dipelajari teori pembentukan kurva yield dengan menggunakan metode cubic B-spline yang kembangkan oleh Baki (2006). Metode ini perluasan dari metode cubic spline yang dipelopori oleh McCulloch (1975). Studi kasus penelitian ini diambil dari obligasi pemerintah Indonesia berkupon nol dengan seri SPN pada saat Bahan Bakar Minyak langka, yaitu tanggal 26-29 Agustus 2014. Hasil pemodelan diperoleh kesimpulan bahwa berdasarkan kriteria nilai MSYE yang minimum dapat dipilih model cubic B-spline terbaik dengan tiga titik knot. Titik knot optimum yang diperoleh adalah",,"Kata Kunci : Bond pricing, Kurva yield, Cubic B-Spline, MSYE, titik knot."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150405,ANALISIS FLEKSIBEL BAYESIAN UNTUK REGRESI KUANTIL TERPENALTI DENGAN MENGGUNAKAN ALGORITMA MCMC GIBBS SAMPLING; PENALIZED FLEXIBLE BAYESIAN QUANTILE REGRESSION BY USING MCMC GIBBS SAMPLING ALGORITHM,"Afifka Fitri Nugrahwati, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi kuantil terpenalti dapat digunakan untuk mengatasi keterbatasan regresi linear dalam menganalisis data yang bentuknya tidak simetris, terdapat pencilan dan distribusi data yang tidak homogen. Regresi kuantil terpenalti dengan LASSO (Least Absolute Shrinkage and Selection Operator) dan Adaptive Lasso penalty dapat diestimasi menggunakan metode fleksibel bayesian yakni suatu metode analisis berdasarkan pada informasi yang berasal dari sampel dan informasi prior. Gabungan informasi sampel dan informasi prior ini dinamakan posterior. Dalam mencari distribusi posterior untuk parameter yang cukup banyak sering kali mengalami kesulitan. Teknik khusus yang dapat digunakan untuk mempermudah yaitu dengan menggunakan simulasi MCMC (Marcov Chain Monte Carlo) Gibbs sampling yang juga dapat meningkatkan model fit dan mereduksi variansi dari distribusi posterior. Pada software R terdapat paket bayesQR untuk analisis regresi kuantil terpenalti dengan metode bayesian secara lengkap. Studi kasus dalam skripsi ini membahas hubungan antara tingkat serum antigen prostat spesifik dengan sejumlah tindak klinis pada pria yang hendak menerima prostatektomi radikal. Hasil estimasi regresi kuantil terpenalti dengan metode fleksibel bayesian dibandingkan dengan metode OLS, regresi kuantil dan regresi kuantil bayesian. Dengan menggunakan nilai R2 dan MSE diperoleh kesimpulan bahwa regresi kuantil terpenalti dengan metode fleksibel bayesian menghasilkan estimasi yang lebih akurat dan presisi daripada estimasi dengan metode lainnya.",,"Kata Kunci : Regersi Kuantil Terpenalti, Lasso Penalty, Adaptive Lasso Penalty, Fleksibel Bayesian, Marcov Chain Monte Carlo, Gibbs sampling, bayesQR"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150151,REGRESI RIDGE DENGAN BENTUK Q DAN FAKTOR SHRINKAGE MENGGUNAKAN SINGULAR VALUE DECOMPOSITION; RIDGE REGRESSION WITH Q-SHAPE AND SHRINKAGE FACTORS USING SINGULAR VALUE DECOMPOSITION,"Rizki Arista Sonata Pusparani, Zulaela",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Salah satu asumsi yang harus dipenuhi dalam regresi linier berganda adalah tidak ada multikolinieritas atau tidak ada hubungan linier diantara variabel bebas. Multikolinearitas menyebabkan MSE dari penduga kuadrat terkecil menjadi sangat besar sehingga estimator yang diperoleh dari metode kuadrat terkecil tidak tepat. Pada kenyataannya, yang diharapkan pada sebuah penelitian adalah model yang memiliki ragam minimum, meskipun berbias. Metode regresi ridge dengan bentuk Q dan faktor shrinkage merupakan salah satu cara untuk mengatasi masalah multikolinieritas karena menghasilkan MSE yang kecil meskipun bias yang dihasilkan relatif kecil. Metode ini menyusutkan koefisien regresi linier menggunakan faktor shrinkage dan mengontrol penyusutannya menggunakan bentuk Q. Metode ini merupakan modifikasi dari metode regresi ridge biasa dengan menggunakan singular value decomposition yaitu menguraikan matriks dari variabel bebas X menjadi 3 komponen tanpa mengubah karakteristik variabel-variabel bebasnya.",,Kata Kunci : Regresi ridge; Singular value
http://etd.repository.ugm.ac.id/home/detail_pencarian/150156,ANALISIS TEKNIKAL SAHAM MENGGUNAKAN INDIKATOR TRUE STRENGTH INDEX (TSI); STOCKS TECHNICAL ANALYSIS WITH TRUE STRENGTH INDEX (TSI) INDICATOR,"NABILA KENCANA, Sri Haryatmi Kartiko",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis teknikal saham menggunakan grafik sebagai medianya, digunakan untuk memprediksi pergerakan harga saham sehingga kita dapat mengetahui kapan harus membeli dan menjual saham untuk memperoleh keuntungan yang maksimal. Terdapat banyak sekali indikator yang digunakan dalam analisis teknikal saham. Indikator True Strength Index (TSI) merupakan indikator teknikal yang berbasis momentum yang membantu trader menjelaskan kondisi jenuh beli dan jenuh jual dari saham dengan cara menggabungkan momentum jangka pendek dari sekuritas dengan lag keuntungan dari moving average. Exponential Moving Average (EMA) dengan periode 25 dihitung dari selisih dari dua harga, dan kemudian EMA dengan periode 13 dihitung dari hasil EMA sebelumnya, hal ini membuat indikator lebih sensitif untuk kondisi pasar yang berlaku. Setelah data dihaluskan, digunakan angka -20 dan +20 digunakan untuk mengidentifikasi dimana saham jenuh beli dan jenuh jual. Akan dibuktikan juga bahwa TSI dengan menggunakan EMA adalah yang terbaik daripada menggunakan SMA maupun WMA. Indikator TSI adalah variasi dari indikator Relative Strength Index (RSI).",,Kata Kunci : Saham; analisis teknikal; SMA; EMA; WMA; TSI; RSI
http://etd.repository.ugm.ac.id/home/detail_pencarian/148877,ESTIMASI HARGA OBLIGASI MENGGUNAKAN PENDEKATAN DURASI DAN KONVEKSITAS HEATH JARROW MORTON DARI UKURAN SENSITIVITAS HARGA OBLIGASI TERHADAP YIELD (Studi Kasus : Obligasi Pemerintah Indonesia); BOND PRICE ESTIMATE USING APPROACH DURATION AND CONVEXITY HEATH JARROW MORTON FROM SENSITIVITY MEASURES OF BOND FOR YIELD,"Febtio Adi Wibawanto, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Obligasi sebagai instrumen investasi yang dikelompokkan pada sekuritas berpendapatan tetap memiliki risiko akibat dari perubahan tingkat bunga pasar. Dalam hal ini, investor membutuhkan manajemen risiko obligasi untuk meminimalkan risiko akibat dari perubahan tingkat bunga (yield) dan mendapatkan imbal hasil (return) sesuai yang diinginkan. Durasi dan konveksitas merupakan kombinasi yang cocok digunakan dalam manajemen risiko. Pendekatan Durasi dan konveksitas Heath Jarrow Morton untuk obligasi berkupon merupakan salah satu cara untuk mengukur sensitivitas harga obligasi yang diperkenalkan oleh Manfred Fruhwirth (2001) dengan dua contoh model HJM yang populer dengan struktur volatilitas deterministik yaitu model volatilitas konstan (Ho/Lee, 1986) dan model volatilitas eksponensial (Hull/White, 1990). Dalam skripsi akan dibahas bagaimanakah menghitung durasi dan konveksitas Heath Jarrow Morton menggunakan model volatilitas konstan untuk mengetahui estimasi perubahan harga obligasi yang lebih akurat akibat dari perubahan yield dengan membandingkan estimasi harga obligasi menggunakan pendekatan tradisional dan eksponensial dalam yield waktu kontinu. Selanjutnya, Mengkonstruksi bobot/proporsi obligasi yang optimal untuk membentuk portofolio obligasi dengan menggunakan metode Moment.",,Kata Kunci : harga obligasi; durasi; konveksitas; heath jarrow morton; return; yield; metode moment
http://etd.repository.ugm.ac.id/home/detail_pencarian/150163,KLASIFIKASI DENGAN METODE RANDOM FOREST DAN ANALISIS DISKRIMINAN LINEAR; CLASSIFICATION USING RANDOM FOREST AND LINEAR DISCRIMINANT ANALYSIS,"CUT ASYRAF ANZILA, Adhitya Ronnie Effendie",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Pohon Keputusan telah banyak digunakan pada berbagai macam masalah karena melihat efisiensi waktu dalam menganalisa dan keakuratan klasifikasinya. Salah satu pengembangan dari pohon keputusan adalah metode klasifikasi Random Forest. Penggunaan metode random forest untuk menghasilkan pohon gabungan telah memberikan dugaan yang lebih tinggi akurasinya dibandingkan dengan pohon tunggal. Analisis klasifikasi klasik yang biasa digunakan adalah Analisis Diskriminan Linear. Namun biasanya Analisis Diskriminan akan menghasilkan prediksi dibawah metode Random Forest. Untuk meningkatkan akurasi dari Analisis Diskriminan Linear ini, akan digunakan seleksi fitur dari Random Forest. Seleksi fitur merupakan sebuah tahapan penting dalam proses klasifikasi, karena fitur yang terseleksi sangat mempengaruhi tingkat akurasi dari klasifikasi. Pada dataset yang memiliki banyak fitur membutuhkan proses untuk mereduksi fitur yang dianggap kurang penting. Setelah menyeleksi fitur tahap selanjutnya adalah mengklasifikasikan data",,Kata Kunci : Klasifikasi; Random Forest; Analisis Diskriminan Linear; seleksi fitur.
http://etd.repository.ugm.ac.id/home/detail_pencarian/150420,SEGMENTASI KARAKTERISTIK DEBITUR MENGGUNAKAN METODE K-MEANS CLUSTER DAN MULTI-CLASS SUPPORT VECTOR MACHINES; DEBTOR CHARACTERISTIC SEGMENTATION USING K-MEANS CLUSTER AND MULTI-CLASS SUPPORT VECTOR MACHINES METHOD,"Afina Nurseha, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis kredit menggunakan teknik statistika telah mengalami perkembangan dan mendapat perhatian yang besar. Salah satu teknik analisa kredit untuk mengantisipasi adanya kredit macet yaitu dengan mengelompokkan karakteristik debitur. Salah satu metode untuk mengelompokkan sejumlah data debitur yaitu menggunakan metode K-means Cluster (clustering non-hierarki). Setelah data dikelompokkan, selanjutnya dilakukan prediksi pada dataset (testing data) dan dihitung tingkat akurasi kebenarannya menggunakan metode multi-class support vector machines. Metode multi-class support vector machines merupakan pengembangan dari metode support vector machines yang dapat digunakan untuk mengklasifikasikan data yang memiliki lebih dari dua kelas (multi kelas). Salah satu metode yang dapat digunakan untuk mengimplementasikan metode multiclass support vector machines adalah metode one-against-one (satu lawan satu). Penelitian ini dilakukan dengan berbagai variasi proporsi training data dan testing data serta nilai sigma dari fungsi Kernel berupa Radial Basis Function.",,"Kata Kunci : Segmentasi debitur, K-means cluster, Support Vector Machines, Multi-class Support Vector Machines, One-Against-One"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150165,MENENTUKAN OPSI BELI BARRIER DOWN AND OUT TIPE EROPA DENGAN VOLATILITAS MODEL GARCH (Studi Kasus Saham Melco Crown Entertainment Limited);Determine European Down and Out Barrier Call Option with GARCH Model Volatility,"LARAS NUR NOVIANI, Yunita Wulan Sari",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Produk derivatif merupakan kontrak finansial turunan dari produk acuan seperti saham, obligasi, atau suku bunga. Salah satu contoh produk derivatif yang populer ialah opsi beli barrier down and out tipe Eropa. Opsi ini merupakan opsi beli yang letak barrier-nya berada di bawah harga beli saham dan opsi akan dinonaktifkan jika harga beli saham menembus atau keluar dari nilai barrier. Opsi jenis ini digunakan untuk memanfaatkan kecenderungan perilaku harga saham yang selalu di atas. Harga opsi beli barrier down and out tipe Eropa dapat ditentukan dengan formula Black-Scholes. Semua parameter dalam formula Black-Scholes, yaitu harga saham di pasar, harga pelaksanaan (K) , waktu sampai jatuh tempo (T) , nilai barrier (B), tingkat bunga bebas resiko (r) dapat diketahui kecuali volatilitas (",,Kata Kunci : Opsi call Eropa; Opsi barrier down and out ; Volatilitas;GARCH
http://etd.repository.ugm.ac.id/home/detail_pencarian/149655,ESTIMASI MAXIMUM LIKELIHOOD DAN RESTRICTED MAXIMUM LIKELIHOOD UNTUK MODEL LINEAR EFEK CAMPURAN; MAXIMUM LIKELIHOOD AND RESTRICTED MAXIMUM LIKELIHOOD ESTIMATION FOR LINEAR MIXED EFFECTS MODEL,"ADIANTO PANGARIBUAN, Gunardi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Model linear efek campuran merupakan salah satu metode regresi parametrik linear untuk data berkelompok, longitudinal, atau data pengamatan yang berulang untuk mengetahui hubungan variabel dependen dan independen. Model linear efek campuran terdiri dari parameter efek tetap dan efek random. Estimasi parameter dari model linear efek campuran dapat diperoleh menggunakan metode estimasi maximum likelihood dan restricted maximum likelihood dengan distribusi multivariat normal dari model marginal. Koefisien efek random pada model dapat diprediksi menggunakan ekspektasi bersyarat. Pemilihan model terbaik menggunakan uji rasio likelihood beserta Akaike Information Criterion (AIC) dan Bayesian Information Criterion (BIC). Pemilihan metode estimasi terbaik menggunakan mean square error. Aplikasi data untuk model linear efek campuran yaitu registrasi pemeriksaan ibu hamil di salah satu Puskesmas di Yogyakarta",,Kata Kunci : model linear efek campuran; model marginal; efek random; maximum likelihood; restricted maximum likelihood; ekspektasi bersyarat; uji rasio likelihood; AIC dan BIC
http://etd.repository.ugm.ac.id/home/detail_pencarian/150426,MODEL AVERAGING BAYESIAN PADA REGRESI LINIER; BAYESIAN MODEL AVERAGING (BMA) FOR LINEAR REGRESSION,"YULIA INDAH PERMATA SARI, Zulaela",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Statistika banyak digunakan dalam berbagai bidang. Salah satunya adalah bidang kesehatan. Permasalahan dalam bidang kesehatan sangat banyak, contohnya kematian bayi. Kematian bayi merupakan permasalahan serius yang memerlukan perhatian sendiri. Di Indonesia Angka Kematian Bayi (AKB) masih cukup tinggi dibandingkan di beberapa negara ASEAN lainnya. Padahal AKB sebagai tolak ukur keberhasilan pembangunan kesehatan di suatu negara. Pada skripsi ini akan dianalisis AKB menggunakan model averaging bayesian dengan occam’s window karena kasus tersebut melibatkan ketidakpastian model. Model averaging bayesian akan mempertimbangkan model yang tidak pasti dalam pemilihan variabel dengan mengkombinasikan model-model yang terbentuk dari variabel prediktor. Studi yang digunakan adalah AKB di Indonesia pada tahun 2012. Data yang digunakan terdiri dari satu variabel dependen dan lima variabel independen. Hasil dari estimasi dengan metode model averaging bayesian akan dibandingkan dengan analisis regresi berganda. Model averaging bayesian memiliki nilai SSE yang lebih rendah dan tingkat ketepatan prediksi yang lebih tinggi dibandingkan dengan analisis regresi berganda.",,"Kata Kunci : model averaging bayesian, occam’s window, analisis regresi beganda, angka kematian bayi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150171,DISTRIBUSI BETA WEIBULL UNTUK ANALISIS DATA SURVIVAL BETA; WEIBULL DISTRIBUTION FOR THE SURVIVAL DATA ANALYSIS,"Vinie Francisca, Rianti Siswi Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam skripsi ini dijelaskan tentang estimasi parameter dari model distribusi beta Weibull.Distribusi beta Weibull adalah sebuah distribusi hasil modifikasi antara fungsi beta dan distribusi Weibull yang memiliki empat parameter. Distribusi beta Weibull memiliki beberapa submodel khusus untuk beberapa parameter yang diberi nilai tertentu. Distribusi beta Weibull memiliki fungsi hazard yang berbentuk bathtup, unimodal, turun, dan naik. Untuk mengestimasi parameter dari model distribusi beta Weibull, digunakan metode maksimum likelihood dan BFGS.Dapat ditunjukkan bahwa distribusi beta Weibull lebih baik dibandingkan dengan distribusi Weibull.",,Kata Kunci : beta Weibull; maksimum likelihood; BFGS; bathtup; unimodal
http://etd.repository.ugm.ac.id/home/detail_pencarian/150429,ESTIMASI-S ROBUST UNTUK REGRESI SPLINE TERPENALTI; ROBUST S-ESTIMATION FOR PENALIZED REGRESSION SPLINES,"Henni Pratika, Adhitya Ronnie Effendie",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi spline terpenalti adalah salah satu metode yang saat ini sering digunakan untuk smoothing noisy data. Model regresi spline terpenalti adalah alat statistik yang popular untuk masalah fitting kurva karena fleksibilitas dan efisiensi dalam kumputasinya. Pada regresi spline, estimasi kurva regresi dapat diselesaikan dengan kuadrat terkecil terpenalti atau Penalized Least Square. Namun metode estimasi ini rentan terhadap kehadiran pencilan. Sehingga diperkenalkan metode estimasi-S robust terpenalti. Oleh karena itu metode estimasi kuadrat terkecil untuk regresi spline terpenalti diganti dengan metode estimasi-S robust yang mampu menangani kehadiran pencilan dalam data. Dengan tetap menjaga pembentukan model spline dan menjaga bentuk penalti, meskipun menggunakan estimator-S daripada estimator kuadrat terkecil, didapatkan metode estimasi yang robust dan cukup fleksibel untuk menangkap trend non-linear dalam data. Dalam skripsi ini juga mempelajari bagaimana memilih secara robust parameter penalti ketika kemungkinan terdapat outlier pada data. Diberikan kriteria pemilihan parameter penalti robust berdasarkan generalized cross-validation yang juga didapat dari gambaran weighted penalized least square dari estimator S-regresi terpenalti. Contoh data simulasi dan data riil digunakan untuk menggambarkan efektivitas prosedur.",,"Kata Kunci : estimasi-S robust, regresi spline terpenalti, estimasi-S terpenalti, parameter pemulus, regresi nonparametrik, rgcv"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149664,ANALISIS KELOMPOK UNTUK DATA KATEGORIK BERDASARKAN ALGROITMA VEKTOR HAMMING DISTANCE (Studi Kasus : Klasifikasi Hewan Berdasarkan Morfologi dan Karakteristik Umum); CLUSTER ANALYSIS FOR CATEGORICAL DATA BASED ON HAMMING DISTANCE VECTOR ALGORITHM (Case Studies : Animal Classification Based on Their Morphology and Common Characteristics),"SHIDDIQ SUGIONO, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,Analisis kelompok menggunakan metode K-Means Clustering yang menggunakan rata-rata sebagai pusat dari kelompok tidak lagi berarti jika digunakan dalam kumpulan data yang bersifat kategorik. Analisis kelompok menggunakan algoritma vektor Hamming distance dikhususkan untuk kumpulan data yang bersifat kategorik. Teori dalam algoritma ini banyak berdasar pada coding theory. Algoritma ini tidak diperlukan model ataupun kriteria konvergensi dalam jalannya algortima. Algoritma ini merupakan alternatif dari algoritma yang sebelumnya telah diajukan seperti K-modes dan Autoclass. Studi kasus yang dilakukan adalah pengklasifikasian hewan menurut morfologi dan karakteristik umumnya. Pengklasifikasian ini bertujuan untuk mengelompokan hewan pada kelompok/sub-populasi yang memiliki kemiripan tinggi. Diharapkan hasil ini bisa digunakan untuk mempelajari hewan berdasarkan kemiripan sifatnya.,,Kata Kunci : Data Kategorik; Analsis kelompok; Vektor Hamming distance; statistik uji Modified chi-squared
http://etd.repository.ugm.ac.id/home/detail_pencarian/150179,Estimasi Model Spatial Autoregressive dan Model Spatial Error dengan Matriks Pembobot Spasial Tipe Rook; Estimation for Spatial Autoregressive Model and Spatial Error Model with Rook Spatial Weight Matrices,"NOVIANA, Zulaela",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi klasik merupakan suatu metode dalam analisis statistik yang dapat mengetahui bagaimana suatu variabel yang mempengaruhi dapat menduga variabel yang lain yang dipengaruhi. Analisis data menggunakan regresi klasik terkadang efek wilayah kurang diperhatikan sehingga dapat menghasilkan kesimpulan yang kurang akurat bahkan dapat menghasilkan estimasi yang bias sehingga hasil analisis kurang baik digunakan untuk mengambil suatu keputusan, oleh karena itu diperlukan analisis yang mampu mangatasi masalah efek wilayah ini. Regresi spasial yang merupakan generalisasi dari analisis regresi klasik yang memuat unsur autoregressive atau error di dalam model untuk mendapatkan kesimpulan yang lebih tepat dengan data yang mengandung efek wilayah. Pada kasus Crude Birth Rate (CBR) signifikan mengandung efek error namun tidak mengandung efek autoregressive sehingga Spatial Error Model lebih tepat digunakan daripada model yang mengandung efek autoregressive pada Spatial Autoregressive Model.",,Kata Kunci : regresi klasik; autoregressive; error; Spatial Error Model; Spatial Autoregressive Model
http://etd.repository.ugm.ac.id/home/detail_pencarian/150441,RANCANGAN BUJUR SANGKAR GRAECO LATIN; GRAECO LATIN SQUARE DESIGN,"Romie Priyastama, Suryo Guritno",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Rancangan percobaan adalah salah satu cara terbaik untuk mendapatkan hasil yang lebih baik pada sebuah percobaan. Salah satu bentuk rancangan percobaan adalah rancangan bujursangkar Graeco Latin yang merupakan bagian dari rancangan kelompok lengkap. Pada dasarnya rancangan bujursangkar graeco latin merupakan gabungan dari dua rancangan bujursangkar yang saling ortogonal yang mana rancangan bujursangkar yang satu terdiri dari huruf Latin sedangkan rancangan bujursangkar yang lain terdiri dari huruf Yunani. Rancangan ini bertujuan untuk mengendalikan tiga sumber keragaman dari tiga arah. Parameter-parameter yang terdapat pada model rancangan bujursangkar Graeco Latin diestimasi dengan menggunakan metode kuadrat terkecil yaitu metode yang meminimumkan jumlah kuadrat kesalahan. Pada penelitian kali ini digunakan analisis variansi untuk mengetahui perbedaan pencampuran bensin dan uji lanjut perbandingan ganda untuk memperoleh nilai perbedaan rata-rata antar perlakuan. Ada empat faktor yang digunakan pada percobaan untuk mengetahui keefektifan penggunaan bahan bakar antara lain hari percobaan, merk mobil, pengemudi dan pencampuran bensin. Dengan menggunakan rancangan bujur sangkar graeco latin ini dapat diketahui bahwa hanya faktor pencampuran bensin yang berpengaruh secara signifikan dalam mengefisienkan penggunaan bahan bakar yang diukur melalui jarak tempuh per liter.",,"Kata Kunci : rancangan percobaan, graeco latin, ortogonal"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150188,PEMBENTUKAN PORTOFOLIO OBLIGASI BERKUPON TETAP DENGAN MODEL INDEKS TUNGGAl  (Studi Kasus Pada Obligasi Korporasi Periode 2010-2011),"WINDU PRAMANA PUTRA BARUS, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Tujuan dari makalah ini adalah untuk menetukan portofolio optimal dengan menggunakan model indeks tunggal. Model indeks tunggal merupakan satu dari banyak model lain untuk menentukan portofolio optimal. Pembentukan portofolio optimal dapat mengurangi risiko investasi menjadi pertimbangan investor untuk menolak. Disisi lain juga untuk membentuk pengembalian yang diharapkan untuk batas tertinggi portofolio. Obligasi merupakan investasi alternatif untuk mendapatkan keuntungan bagi investor dengan risiko di dalam area toleransi. Ketentuan dasar dalam menentukan portofolio berdasarkan model indeks tunggal adalah dengan ERB ? C* akan diterimas sebagai portofolio optimal. Setelah diketahui saham mana yang termasuk kedalam portofolio optimal, Kemudian dapat ditentukan proporsi dana yang diinvestasikan pada setiap obligasi. Hasil analisis terdapat 3 obligasi yang masuk kedalam poertofolio optimal yakni : PPGD 12B, JMPD14JM10 dan PPGD13B. Proporsi dana untuk masing-masing obligasi adalah 7,22%, 9,01%, dan 83,77%. Dari portofolio optimal yang telah terbentuk memberikan pengembalian yang diharapkann sebesar 0,09288 dan risiko portofolio yang terbentuk sebesar 0,00147 %.",,Kata Kunci : Model Indeks Tunggal; ERB dan Cut-off Point (C*).
http://etd.repository.ugm.ac.id/home/detail_pencarian/149421,ALOKASI OPTIMAL UKURAN SAMPEL DATA LONGITUDINAL DUA GRUP DENGAN RESPON KONTINU DAN MATRIKS KOVARIAN COMPOUND SYMMETRY; OPTIMAL ALLOCATION SAMPLE SIZE FOR TWO-GROUPS LONGITUDINAL STUDY WITH CONTINOUS RESPONSE AND COMPOUND SYMMETRY COVARIANCE MATRIX,"CAMELIA FALISA, Zulaela",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Ukuran sampel adalah banyaknya individu yang diamati dalam suatu penelitian atau percobaan. Penentuan ukuran sampel yang tidak tepat akan mengakibatkan sampel tersebut tidak dapat merepresentasikan populasinya dengan baik. Estimasi ukuran sampel dan alokasi optimum harus sesuai dengan hipotesis desain studinya. Begitu juga dengan desain studi longitudinal yang merupakan pengamatan berulang atas suatu individu atau observasi terhadap waktu. Skripsi ini membahas alokasi optimal ukuran sampel untuk sata longitudinal dua grup dengan respon kontinu dan matriks kovarian compound symmetry. Tujuan menentukan alokasi optimal adalah untuk memberikan acuan estimasi ukuran sampel dan ukuran pengulangan optimal kepada peneliti, yang meminimumkan biaya apabila power ditentukan oleh peneliti atau memaksimumkan power jika biaya ditentukan oleh peneliti. Sehingga peneliti mendapatkan ukuran sampel yang efektif dan efisien",,Kata Kunci : ukuran sampel; data longitudinal; alokasi optimal; matriks kovarian compound symmetry
http://etd.repository.ugm.ac.id/home/detail_pencarian/149424,PENENTUAN HARGA UP AND OUT BARRIER CALL OPTION MODEL BLACK-SCHOLES TIPE EROPA DENGAN ESTIMASI VOLATILITAS METODE EXPONENTIAL WEIGHTED MOVING AVERAGE (EWMA); Determination Option Pricing of Up and Out Barrier Call Option Black-Scholes Model European Type With Estimation Volatility Exponential Weighted Moving Average (EWMA) Method,"MIA DWI PUJI WAHYUNI DARSONO, Yunita Wulan Sari",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Penentuan harga opsi secara teoritis dapat membantu seorang investor guna mendapatkan gambaran mengenai harga opsi di pasar pada saat jatuh tempo. Selain itu, jenis opsi juga berpengaruh untuk mendapatkan keuntungan yang besar. Salah satu jenis opsi yang lebih murah dari harga standar opsi biasanya adalah opsi jenis barrier. Penetapan harga opsi secara teoritis juga mempunyai banyak cara, yaitu salah satu diantaranya adalah model Black-Scholes. Dimana model Black-scholes tersebut mempunyai enam parameter yaitu harga aset awal (S0), harga patokan opsi (K), waktu jatuh tempo opsi (T), suku bunga bebas risiko ( r ), nilai barrier (B), dan volatilitas harga aset dasar (?). Dari keenam parameter tersebut hanya nilai dari volatilitas (?) yang tidak dapat diketahui secara langsung. Oleh karena itu, pada skripsi ini akan dibahas bagaimana cara mengestimasikan nilai volatilitas (?) menggunakan metode Exponential Weighted Moving Average (EWMA).",,Kata Kunci : Barrier; Volatilitas; Moving Average; Forecasting Volatility
http://etd.repository.ugm.ac.id/home/detail_pencarian/150193,COPULA DOUBLE EXPONENSIAL UNTUK PREDIKSI MODEL KERUGIAN AGGREGATE; DOUBLE EXPONENTIAL COPULA FOR PREDICTION AGGREGATE LOSS MODELS,"PUJI LESTARI, Adhitya Ronnie Effendi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Aggregate loss atau kerugian aggregate adalah total kerugian harus ditanggung oleh perusahaan asuransi dalam suatu periode waktu tertentu dalam suatu kontrak. Hal yang menarik dalam model kerugian aggregate adalah mengenai prediksi banyak klaim maupun besar klaim. Skripsi ini mengembangkan prediktor dari kerugian aggregate dengan menggunakan data longitudinal. Pada data longitudinal, salah satu pertemuan data dari data crossection kelas risiko dengan data klaim asuransi terdahulu yang tersedia untuk masing-masing kelas risiko Untuk membantu menjelaskan dan memprediksi banyak klaim dan besar klaim kita membutuhkan variabel penjelas. Model distribusi marginal klaim dalam skripsi ini menggunakan model linear tergeneralisasi (GLM). Banyak klaim direpresentasikan menggunakan model regresi Poisson yang bersyarat pada variabel latent. Variabel laten mempunyai hubungan antara banyak klaim, sehingga distribusi bersama antara keduanya dimodelkan dengan menggunakan copula elliptical. Skripsi ini menampilkan ilustrasi dengan mengambil contoh data klaim kendaraan dikota Massachusetts. Estimasi parameter dari variabel laten dari proses klaim di peroleh dan didapat simulasi prediksi.",,Kata Kunci : Kerugian aggregate; data longitudinal; copula elliptical.
http://etd.repository.ugm.ac.id/home/detail_pencarian/150709,SELEKSI VARIABEL DATA SURVIVAL TERSENSOR KANAN DENGAN METODE RANDOM SURVIVAL FOREST; VARIABLE SELECTION FOR RIGHT CENSORED SURVIVAL DATA USING RANDOM SURVIVAL FOREST METHOD,"ANISA ASTITIA KHOTIMAH, Sri Haryatmi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,Membangun suatu pohon ensemble dengan metode dasar pembentukan pohon kini sangatlah populer. Metode ini memberikan tingkat keakuratan yang lebih tinggi dan merupakan metode yang sangat fleksibel terhadap segala macam data tanpa memerlukan asumsi tertentu. Random Survival Forest diperkenalkan sebagai metode pohon ensemble untuk data survival tersensor kanan. Resampling data yang dikenal dengan istilah bootstrapping digunakan dalam analisis. Data asli akan dilakukan bootstrapping yang kemudian akan memecahkan data menjadi dua komponen yaitu data in-bag dan data out-of-bag. Data in-bag digunakan untuk membangun pohon survival sementara data out-of-bag digunakan untuk menghitung kesalahan prediksi. Informasi waktu dan status tersensor sangat berguna dalam penelitian untuk membangun pohon survival. Random Survival Forest memiliki tujuan analisis yang sama dengan Regresi Cox maka kedua analisis tersebut dibandingkan untuk menunjukkan keakuratan dari analisis Random Survival Forest.,,"Kata Kunci : Random Survival Forest, Survival, Tersensor kanan, Seleksi variabel, Regresi Cox."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150215,Model Marginal Data Longitudinal Biner menggunakan Rantai Markov;Marginal Model of Binary Longitudinal Data using Markov Chain,"Rochyati, Danardono",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Data longitudinal adalah data yang pengumpulannya dilakukan berkali-kali selama jangka waktu tertentu. Data longitudinal biasanya akan berkorelasi serial dalam subyek. Jelasnya, jika yit merepresentasikan observasi subyek ke -i waktu ke -t , maka subyek i memuat respon berulang yit. Karena observasinya diambil dari subyek yang sama akibatnya respon berulang ini berkorelasi. Analisis data longitudinal dimana variabel responnya biner itu perlu diperhatikan dari sudut pandang inferensi likelihood, dimana mengharuskan spesifikasi yang lengkap dari model stokastik untuk individu. Dalam skripsi ini digunakan rantai Markov biner –yang merupakan mekanisme stokastik dasar- untuk menggambarkan model marginal dalam data longitudinal dengan memperhatikan efek random. Dalam skripsi ini, analisis model marginal data longitudinal biner menggunakan rantai Markov dengan efek random diaplikasikan untuk menganalisis penelitian pada 577 bayi yang terkena penyakit Infeksi Saluran Pernafasan Akut (ISPA) di Purworejo. Analisis ini menghasilkan estimasi yang tidak jauh berbeda dengan analisis regresi logistik, tetapi dengan analisis ini dapat dihitung estimasi parameter dependensi dan etimasi variansi.",,Kata Kunci : data longitudinal biner; model marginal; rantai Markov; odds ratio
http://etd.repository.ugm.ac.id/home/detail_pencarian/150216,ANALISIS DISKRIMINAN FLEKSIBEL; FLEXIBLE DISCRIMINANT ANALYSIS,"RASITA NINGRUM, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis diskriminan fleksibel adalah pengembangan dari analisis diskriminan linear, yang digunakan untuk menyelesaikan masalah klasifikasi pada regresi non-parametrik. Flexibel discriminant analysis (FDA) menggantikan langkah regresi linear dengan regresi non-parametrik, salah satu regresi nonparametrik yang dapat digunakan adalah Multivariate Adaptive Regression Splines (MARS). MARS bertujuan untuk memprediksi. Model MARS yang terbaik adalah yang menghasilkan nilai Generalized Cross Validation (GCV) minimum. Asumsi yang harus dipenuhi dalam pemodelan analisis diskriminan fleksibel adalah tidak terdapat outlier, kesamaan matriks variansi-kovariansi. Dalam studi kasus dilakukan pengelompokkan observasi, yakni kadar dissolved oxygen (DO) dalam air berdasarkan 5 variabel prediktornya, yaitu: fosfat, COD,BOD, TSS dan Ph.",,Kata Kunci : Analisis diskriminan linear; analisis diskriminan fleksibel; Multivariate Adaptive Regression Splines (MARS).
http://etd.repository.ugm.ac.id/home/detail_pencarian/149706,ANALISIS TEKNIKAL SAHAM MENGGUNAKAN VARIABLE INDEX DYNAMIC AVERAGE (VIDYA); STOCK TECHNICAL ANALYSIS USING VARIABLE INDEX DYNAMIC AVERAGE (VIDYA),"ROSELINA YOLANDA, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Keuangan kita dan aksi pasar berubah setiap menit dalam setiap harinya. Pasar ini bersifat dinamis karena pedagang terus-menerus menyesuaikan dengan perubahan persepsi dan para partisipan. Oleh karena itu, kita membutuhkan indikator dinamis yang dapat mengubah periode waktu dengan cara menganalisis aksi pasar. Variable Index Dynamic Average (VIDYA) merupakan salah satu indikator yang mengadaptasi indikator Exponential Moving Average (EMA) yang secara dinamis dapat mengikuti pergerakan harga saham. Sama seperti Exponential Moving Average (EMA), VIDYA juga membutuhkan bobot dalam metode perhitungannya. Dalam skripsi ini, bobot EMA digunakan untuk menentukan bobot VIDYA. VIDYA dapat dihitung dengan menggunakan tiga metode yang berbeda, seperti standar deviasi, Chande Momentum Oscillator, dan koefisien determinasi. Metode perhitungan tersebut digunakan untuk menentukan indeks volatilitas yang berfungsi untuk memprediksi pergerakan harga saham dan memprediksi tren di masa yang akan datang. Selain itu, VIDYA dapat digunakan sebagai strategi trading dan dapat menentukan sinyal jual atau beli saham dengan cara menggunakan titik breakout.",,Kata Kunci : analisis teknikal; exponential moving average; indeks volatilitas; standar deviasi; koefisien determinasi; indikator momentum
http://etd.repository.ugm.ac.id/home/detail_pencarian/150226,PEMODELAN LOKASI-SKALA UNTUK DATA ORDINAL BERTINGKAT,"LUAILI NURUL HUSNA, Zulaela",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Kasus data yang memuat variabel respon diskrit, khususnya ordinal sering ditemukan dalam penelitian. Untuk data yang mempunyai variabel respon berskala ordinal, dapat digunakan analisis regresi ordinal. Ketika sampel data mempunyai struktur hierarki, analisis regresi ordinal tidak dapat digunakan lagi karena asumsi independensi antar pengamatan tidak terpenuhi. Selain itu, model ordinal mengasumsikan bahwa pengaruh variabel prediktor sama sepanjang logit kumulatif. Namun, pada kenyataannya, asumsi ini tidak mudah dipenuhi. Untuk mengatasi permasalahan tersebut, dapat digunakan model lokasi-skala untuk data ordinal. Model ini memuat parameter lokasi dan parameter skala sehingga dapat digunakan untuk membagi derajat variansi di dalam subjek dan variansi antar subjek.",,Kata Kunci : Model ordinal; Asumsi proportional odds; Kluster; Logit kumulatif; Variansi di dalam-subjek; Variansi antar-subjek.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149974,ANALISIS SURVIVAL WAKTU DISKRIT; DISCRETE TIME SURVIVAL ANALYSIS,"DJati Sampurna, Danardono",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Model hazard proposional waktu kontinu didasarkan pada asumsi yang tidak realistis bahwa efek prediktor pada saat terjadinya event adalah konstan sepanjang waktu. Ada beberapa kasus bahwa prediktor nilainya bisa bervariasi dari waktu ke waktu. Maka dari itu analisis survival waktu diskrit digunakan sebagai pendekatan umum analisis survival untuk mengatasi masalah ini. Prediktor yang nilainya bervariasi dari waktu ke waktu dapat dimodelkan seperti time-invariant. Singer dan Willet (1993) menunjukkan bahwa model ini bisa sesuai menggunakan analisis regresi logistik dan menggunakan prosedur maksimum likelihood untuk mengestimasi parameternya. Menggunakan data penderita AIDS dengan prediktor STRATUM dan CD4, akan dijelaskan perbedaan kedua prediktor tersebut dalam menginterpretasikannya.",,Kata Kunci : analisis survival; data longitudinal; regresi logistik
http://etd.repository.ugm.ac.id/home/detail_pencarian/149987,ESTIMASI HARGA OBLIGASI MENGGUNAKAN PENDEKATAN DURASI DAN KONVEKSITAS HEATH JARROW MORTON DARI UKURAN SENSITIVITAS HARGA OBLIGASI TERHADAP YIELD (Studi Kasus : Obligasi Pemerintah Indonesia),"FEBTIO ADI WIBAWANTO, Abdurakhman",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Obligasi sebagai instrumen investasi yang dikelompokkan pada sekuritas berpendapatan tetap memiliki risiko akibat dari perubahan tingkat bunga pasar. Dalam hal ini, investor membutuhkan manajemen risiko obligasi untuk meminimalkan risiko akibat dari perubahan tingkat bunga (yield) dan mendapatkan imbal hasil (return) sesuai yang diinginkan. Durasi dan konveksitas merupakan kombinasi yang cocok digunakan dalam manajemen risiko. Pendekatan Durasi dan konveksitas Heath Jarrow Morton untuk obligasi berkupon merupakan salah satu cara untuk mengukur sensitivitas harga obligasi yang diperkenalkan oleh Manfred Fruhwirth (2001) dengan dua contoh model HJM yang populer dengan struktur volatilitas deterministik yaitu model volatilitas konstan (Ho/Lee, 1986) dan model volatilitas eksponensial (Hull/White, 1990). Dalam skripsi akan dibahas bagaimanakah menghitung durasi dan konveksitas Heath Jarrow Morton menggunakan model volatilitas konstan untuk mengetahui estimasi perubahan harga obligasi yang lebih akurat akibat dari perubahan yield dengan membandingkan estimasi harga obligasi menggunakan pendekatan tradisional dan eksponensial dalam yield waktu kontinu. Selanjutnya, Mengkonstruksi bobot/proporsi obligasi yang optimal untuk membentuk portofolio obligasi dengan menggunakan metode Moment.",,"Kata Kunci : harga obligasi, durasi, konveksitas, heath jarrow morton, return, yield, metode moment"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150504,PENYUSUTAN KOEFISIEN DAN SELEKSI VARIABEL REGRESI DENGAN ELASTIC NET; COEFFICIENT SHRINKAGE AND VARIABLE SELECTION OF REGRESSION USING ELASTIC NET,"Fitri Ramadhini, Adhitya Ronnie Effendie,",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Elastic Net merupakan metode seleksi varibel yang inovatif untuk estimasi regresi linear. Elastic Net meminimumkan jumlah kuadrat sisa dengan beberapa batasan. Batasan ini menghasilkan beberapa koefisien regresi menyusut hingga tepat 0, sehingga menyebabkan seleksi variabel dan memudahkan dalam interpretasi model. Dugaan kuadrat terkecil (Ordinary Least Squares, OLS) kurang efisien ketika banyak prediktor yang digunakan besar, hal ini karena OLS memasukkan semua variabel tanpa menyeleksinya sehingga tidak dapat memberikan model yang mudah untuk diinterpretasikan. Seleksi variabel tidak hanya bermanfaat untuk banyak prediktor besar, akan tetapi juga bermanfaat ketika terjadi masalah multikolinearitas pada prediktor. Elastic Net sangat berguna terutama ketika banyaknya prediktor lebih besar dari banyaknya observasi (",,"Kata Kunci : Regresi, Penyusutan, Elastic Net, Kuadrat Terkecil, LASSO, Seleksi Variabel, Microarray, Multikolinearitas, Data Dimensi Tinggi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150505,MODEL LOGISTIK ADITIF TERGENERALISASI; GENERALIZED ADDITIVE LOGISTIC MODELS,"FATMA NURUL HIDAYAH, Herni Utami",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Model aditif tergeneralisasi merupakan perluasan dari regresi linear, yaitu pemodelan yang sesuai untuk mengatasi kenonlinearan dalam hubungan antara variabel respon dan prediktor serta tidak membatasi distribusi variabel respon hanya pada distribusi normal saja akan tetapi distribusi-distribusi lain dalam keluarga eksponensial dapat dipergunakan dalam model ini. Model logistik aditif tergeneralisasi merupakan bagian dari model aditif tergeneralisasi dengan respon bertipe biner, yaitu dengan mengganti fungsi linear yang ada pada model dengan jumlahan fungsi yang diestimasi menggunakan local scoring. Penggunaan model logistik aditif tergeneralisasi untuk variabel prediktor kuantitatif dengan estimasi fungsi penghalus cubic smoothing spline. Komponen aditif dari model logistik aditif tergeneralisasi merupakan jumlahan fungsi tunggal yang dimiliki oleh setiap prediktor sehingga dapat diketahui kontribusi dari setiap prediktor terhadap respon.",,"Kata Kunci : model logistik aditif tergeneralisasi, biner, local scoring, cubic smoothing spline"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149231,MODIFIKASI CADANGAN PREMI METODE FULL PRELIMINARY TERM PADA ASURANSI JIWA DWIGUNA MODEL DISKRIT; MODIFICATION OF PREMIUM RESERVE BY USING FULL PRELIMINARY TERM METHOD ON DISCRETE ENDOWMENT LIFE INSURANCE,"MH ROESANGGIT PRABU, Gunardi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Asuransi jiwa merupakan usaha seseorang untuk mengurangi resiko yang ditimbulkan jika seseorang meninggal dunia. Dalam asuransi jiwa, pemegang polis atau tertanggun membayar sejumlah uang kepada perusahaan asuransi jiwa pada periode tertentu dengan niali tertentu yang disebut premi. Sementara itu, Perusahaan Asuransi berjanji akan memberikan sejumlah uang jika terjadi sesuatu seperti yang telah diperjanjikan misalnya kematian atau tetap hidup di akhir kontrak polis yang disebut manfaat. Oleh karena kewajiban memenuhi janji tersebut, maka perusahaan asuransi melakukan perhitungan berapa dana yang dibutuhkan jika terjadi klaim dimasa mendatang. Dana yang harus ada itulah yang disebut cadangan premi. Perhitungan cadangan ada beberapa macam, diantara metode prospektif dan Full Preliminary Term. Pada perhitungan cadangan prospektif cadangan premi kotor yang dihasilkan lebih kecil dari cadangan premi bersihnya. Dengan metode full preliminaty term, cadangan premi kotor yang dihasilkan besarnya sama dengan cadangan premi kotornya. Jika premi kotor lebih kecil dari premi kotor full preliminary term, maka cadangan full preliminary term menghasilkan nilai yang lebih kecil dari cadangan metode prospektif",,Kata Kunci : Asuransi Jiwa; Cadangan Premi; Full Preliminary Term
http://etd.repository.ugm.ac.id/home/detail_pencarian/150271,CREDIT SCORING ADAPTIF MENGGUNAKAN KERNEL LEARNING METHODS; ADAPTIVE CREDIT SCORING WITH KERNEL LEARNING METHODS,"Muhamad Rashif Hilmi, Dedi Rosadi",2014 | Skripsi | PROGRAM STUDI STATISTIKA,"Credit scoring merupakan suatu metode berbasis analisis statistika yang digunakan untuk mengukur besaran resiko kredit. Metode klasifikasi yang paling populer diadopsi di industri credit scoring adalah analisis diskriminan linier dan regresi logistik. Namun, metode tersebut mempunyai beberapa keterbatasan. Yaitu memerlukan seleksi variabel untuk regresi logistik dan data harus mengikuti distribusi tertentu untuk analisis diskriminan linear. Berdasarkan informasi tersebut, sulit untuk mengotomatisasi proses pemodelan data ketika lingkungan atau populasi terjadi perubahan. Metode Kernel adalah salah satu solusi dari permasalahan tersebut. Metode ini tidak memerlukan upaya pemilihan variabel dan dapat selalu konvergen ke solusi yang optimal dan memberikan hasil yang sama tanpa menghadapi masalah numerik atau harus kehilangan informasi. Hal ini memungkinkan pemodel untuk merancang proses penilaian kredit secara dinamis dalam praktek di mana model keputusan dapat diperbarui dan diperbaiki dengan kedatangan informasi baru",,Kata Kunci : Manajemen Resiko; Penilaian Kredit; Metode Kernel; Support Vector Machines
http://etd.repository.ugm.ac.id/home/detail_pencarian/148484,PERHITUNGAN HARGA PREMI DALAM ASURANSI KESEHATAN MENGGUNAKAN METODE HYBRID; PREMIUM PRICING IN HEALTH INSURANCE BY HYBRID METHOD,"Ari Suprapto, Adhitya Ronnie Effendie",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Di dalam penelitian ini, estimasi model multi status dilakukan dengan menentukan intensitas transisi dari suatu status ke status yang lain menggunakan estimator Aalen dan Johansen. Hasil yang di dapat dibuat kedalam suatu matriks intensitas transisi yang kemudian akan digunakan untuk mencari peluang transisi. Peluang transisi ini ditentukan dengan cara hybrid yaitu dengan menggabungkan nilai yang diestimasi dan nilai yang diambil dari tabel mortalita 2011. Dalam proses penggabungan perhitungan tersebut digunakan proporsi untuk setiap nilainya. Hasil dari peluang transisi tersebutdimanfaatkan dalamperhitunganpremi dwigunaasuransi kesehatan.",,Kata Kunci : multi status; estimator Aalen dan Johansen; intensitas transisi; probabilitas transisi; TMI 2011; premiasuransi kesehat
http://etd.repository.ugm.ac.id/home/detail_pencarian/149515,OPTIMALISASI PORTOFOLIO MODEL BLACK-LITTERMAN DAN CAPITAL ASSET PRICING MODEL; OPTIMIZATION PORTFOLIO BLACK LITTERMAN MODEL AND CAPITAL ASSET PRICING MODEL,"RIZKA NUR ASFARINA, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Saham merupakan salah satu instrumen yang sering dipakai dalam investasi. Tingkat pengembalian (return) saham dan besarnya resiko yang ditanggung investor merupakan hal yang perlu diperhatikan. Untuk mengoptimalkan return dan meminimalkan resiko dapat dibentuk portofolio saham. Diasumsikan bahwa return saham tunggal dan portofolio berdistribusi normal. Return dihitung dari harga penutupan saham harian pada masing-masing aset yang terdaftar dalam bursa Indeks LQ-45. Bobot portofolio dengan model Black-Litterman memberikan hasil portofolio yang optimal. Teori pembentukan portofolio diawali oleh Markowitz dengan meanvariancenya di tahun 50an. Selanjutnya bermunculan teori tentang portofolio seperti CAPM dan Single index model. Hingga pada tahun 90an muncul model portofolio yang dikenal dengan Model Black-Litterman oleh Robert Litterman dan Fischer Black. Formula return model Black-Litterman dapat ditelusuri melalui berbagai pendekatan. Selain dengan pendekatan Bayes, formula Black- Litterman dijelaskan oleh Mankert (2003) melalui pendekatan teori sampling. Sebagai simulasi nilai return akan diamati 4 saham dari indeks LQ45 yaitu AALI, JSMR, INDF, PGAS akan dibentuk suatu portofolio yang diharapkan dapat memberikan keuntungan yang optimal dengan menggunakan model Black- Litterman.",,Kata Kunci : Model CAPM; Model Black Litterman; Teori Sampling
http://etd.repository.ugm.ac.id/home/detail_pencarian/149771,VALUASI PREMI SYARIAH MENGGUNAKAN DEFLATOR DENGAN PENDEKATAN MODEL HULL-WHITE; PREMIUM VALUATION SHARIA USING DEFLATOR BY HULL-WHITE MODEL APPROACH,"Risma Aryani, Adhitya Ronnie Effendie",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam prinsip ekonomi islam tidak mengenal adanya konsep nilai waktu terhadap uang (time-value of money) dan pelarangan riba dalam berbagai bentuknya. Prinsip ini tidak menghilangkan faktor acak dalam perhitungan return pada suatu rencana investasi yang berbeda pada prinsip ekonomi konvensional. Maksudnya, tidak menghilangkan faktor ketidakpastian yang ada dalam perolehan keuntungan atau kerugian suatu usaha. Pengamatan perlakuan perubahan secara acak ini dilakukan pada pergerakan fluktuasi equivalen rate. Fluktuasi equivalen rate tersebut diambil dari historical data periode sebelumnya pada suatu lembaga dinar. Untuk memperlihatkan pergerakan fluktuasi equivalen rate tersebut digunakan adanya pendekatan model Hull-White yang sebelumnya dilakukan proses estimasi terhadap parameter-parameternya. Selanjutnya pergerakan fluktuasi equivalen rate dan taksiran nilai equivalen rate periode ke depan tersebut digambarkan dengan menggunakan regresi Ordinary Least Square. Dalam skripsi ini juga diperkenalkan diskretisasi persamaan model Hull-White sebagai deflator yang digunakan untuk faktor diskon syariah.",,Kata Kunci : ekonomi syariah; time-value of money; equivalen rate; deflator; model Hull-White
http://etd.repository.ugm.ac.id/home/detail_pencarian/149262,SEQUENTIAL KRIGING DALAM GEOSTATISTIKA; SEQUENTIAL KRIGING IN GEOSTATISTICAL,"MUHAMAD AJI IKHSANTO, Yunita Wulan Sari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Kriging adalah suatu teknik perhitungan untuk menghitung estimasi dari suatu variabel teregional yang menggunakan pendekatan bahwa data yang dianalisis dianggap sebagai suatu realisasi dari suatu variable acak, dan keseluruhan variable acak yang dianalisis akan membentuk suatu fungsi acak dengan menggunakan model structural variogram. Semivariogram isotropy adalah semivariogram yang dipengaruhi oleh jarak antar titik sampel. Sequential kriging setara dengan simple kriging. Kumpulan data (data set) dibagi ke dalam beberapa subset dan tiap subset memungkinkan data tunggal. Dalam tersedianya suatu data tambahan, estimator sekuensial memperbaiki estimasi sebelumnya dengan menggunakan bobot linier dari data yang baru dan estimasi sebelumnya di suatu lokasi.",,Kata Kunci : semivariogram isotropy; simple kriging; sequential kriging
http://etd.repository.ugm.ac.id/home/detail_pencarian/149775,OPTIMASI PERMUKAAN RESPON KUADRATIK MENGGUNAKAN ANALISIS; RIDGE OPTIMIZATION OF QUADRATIC RESPONSE SURFACE USING RIDGE ANALYSIS,"Yufan Putri Astrini, Yunita Wulan Sari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode permukaan respon adalah suatu metode yang bertujuan untuk menentukan kondisi operasi yang optimal dalam suatu percobaan. Karakteristik respon dari kondisi operasi dapat berupa respon yang maksimum, minimum, atau saddle. Pada beberapa situasi dimana titik stasioner tidak memiliki nilai, misalnya titik stasioner berada pada titik saddle atau titik stasioner yang menyebabkan titik respon minimum atau maksimum berada di luar daerah percobaan, perlu dilakukan suatu analisis khusus untuk menanganinya. Analisis ini disebut analisis ridge. Analisis ridge adalah suatu prosedur yang sangat berguna untuk mengoptimasi prediksi respon model kuadratik pada bentuk hyperspherical terpusat. Prosedur ini menghitung estimasi ridge pada respon optimum untuk meningkatkan jari-jari dari pusat percobaan sebenarnya. Studi kasus yang digunakan dalam skripsi ini adalah optimasi kekuatan torque lampu TL terhadap tekanan low air, low air cooling, dan natural gas. Rancangan percobaan menggunakan central composite design (CCD) dengan jumlah percobaannya yaitu 23 run. Hasil analisis menunjukkan bahwa kondisi optimal diperoleh ketika tekanan low air 3,80125 Kpa, low air cooling 7,3515 Kpa, dan natural gas 4,97 Kpa, yaitu pada saat kekuatan torque mencapai 3,11125 Newton meter",,Kata Kunci : metode permukaan respon; analisis ridge dalam metode permukaan respon; central composite design (CCD)
http://etd.repository.ugm.ac.id/home/detail_pencarian/149265,REGRESI CONWAY-MAXWELL-POISSON (COM-POISSON) TERPOTONG KIRI; LEFT TRUNCATED CONWAY-MAXWEL-POISSON (COM-POISSON) REGRESSION MODEL,"NOVITA RESTUPUSPA, Herni Utami",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Model regresi Poisson digunakan untuk menganalisis data diskrit (count data) dengan asumsi data mengikuti distribusi Poisson. Model regresi ini mengasumsikan mean dan varian dari variabel respon adalah sama. Pada kejadian riilnya, data sangat dimungkinkan mempunyai penyebaran (dispersion) yang luas dan tidak tepat apabila digunakan model regresi Poisson. Untuk mengatasi masalah ini dapat digunakan model regresi COM-Poisson yang merupakan perluasan dari metode regresi Poisson. Model regresi COM-Poisson memiliki dua parameter, yaitu parameter regresi (?) dan dispersi ( ). Pada beberapa kasus diperlukan pembatasan pada variabel dependen biasa disebut variabel dependen terpotong. Dalam skripsi ini digunakan metode estimasi maksimum likelihood untuk mencari nilai estimasi dari regresi COM-Poisson untuk data terpotong kiri. Dengan metode ini akan dicari nilai ? yang memaksimumkan fungsi likelihood. Selanjutnya dalam studi kasus akan membahas menganai hubungan antara banyaknya penyakit komplikasi yang muncul pada pasien diabetes mellitus dengan karaktereistik pasien tersebut.",,Kata Kunci : dispersi; COM-Poisson; terpotong kiri; maksimum likelihood
http://etd.repository.ugm.ac.id/home/detail_pencarian/149781,APLIKASI MODEL KREDIBILITAS HIRARKI TIGA LEVEL PADA INDUSTRI ASURANSI UMUM; APPLICATION OF THREE LEVEL HIERARCHICAL CREDIBILITY MODEL IN NON-LIFE INSURANCE INDUSTRY,"Rida Perwitasari, Adhitya Ronnie Effendie",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam inferensi statistika, teori kredibilitas adalah ilmu yang mengkombinasikan gagasan tentang stabilitas, presisi, dan kemampuan reaksi terhadap data-data yang baru saja terjadi. Teori kredibilitas ini mempelajari proses penghitungan premi berdasarkan pengalaman masa lampau, serta mengkombinasikan antara pengalaman individual dan pengalaman kolektif. Pada kehidupan sehari-hari, seringkali ditemui struktur hirarki dalam berbagai bidang, seperti pada bidang industri, bisnis, asuransi, dan lain sebagainya. Model kredibilitas hirarki dapat dipandang sebagai alat untuk mendistribusikan premi secara fair pada portofolio heterogen yang diklasifikasikan secara hirarki. Ide fundamental dari teori kredibilitas hirarki adalah dengan membagi suatu portofolio yang besar menjadi beberapa subportofolio yang lebih homogen berdasarkan kriteria-kriteria tertentu. Dalam skripsi ini akan dianalisis cara mendapatkan estimator kredibilitas hirarki dan parameter struktural yang melibatkan sifat matematik ekpektasi bersyarat dan kovariansi bersyarat. Studi kasus dalam skripsi ini menganalisis estimasi rasio kerugian kredibilitas di antara 12 negara yang tergabung dalam asuransi perlindungan hukum (legal protection insurance) RIAD yang diklasifikasikan menurut kelompok pendapatan berdasarkan pengklasifikasian dari Bank Dunia",,Kata Kunci : kredibilitas hirarki; ekspektasi bersyarat; kovariansi bersyarat
http://etd.repository.ugm.ac.id/home/detail_pencarian/148257,PORTOFOLIO OPTIMAL DENGAN CONSTANT CORRELATION MODEL; OPTIMAL PORTFOLIO BY CONSTANT CORRELATION MODEL,"KURNIAWATI ROSA UPADI, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Mean-Variance method is a method commonly used in modelling portfolio introduced by Markowitz in 1952. Mean-Variance method using variancecovariance matrix of securities in the calculation so that in the cases involving multiple securities, using the Mean-Variance calculations will take longer. Elton, Gruber, and Padberg (1976) presented Constant Correlation Model with simple ranking procedure to simplify the calculation. This method is also included riskfree asset in the calculation. Constant Correlation Model is based on the assumption of a co-movement between securities. This final assignment will explain how to construct optimal portfolio by Constant Correlation Model, which will be compared with Mean-Variance method to determine which method has better performance. The comparator measure that used is Sharpe Ratio.",,Kata Kunci : Portofolio; Model Korelasi Konstan; Sharpe Ratio
http://etd.repository.ugm.ac.id/home/detail_pencarian/148773,REGRESI LINEAR UNTUK DATA TERSENSOR MENGGUNAKAN ESTIMATOR BUCKLEY-JAMES; LINEAR REGRESSION FOR CENSORED DATA USING BUCKLEY-JAMES ESTIMATOR,"Muhammad Bayu Nirwana, Danardono",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi linear merupakan salah satu analisis dalam statistika yang bertujuan untuk mengetahui pengaruh variabel independen terhadap variabel dependen. Dalam kasus di mana variabel dependen mengandung data yang tersensor, model Cox’s proportional hazard atau model AFT dapat digunakan. Regresi Buckley-James merupakan model linear yang dapat digunakan sebagai alternatif dari model Cox’s proportional hazard atau model AFT. Keunggulan regresi Buckley-James yang berupa model linear, membuat regresi Buckley-James lebih mudah diinterpretasikan secara langsung. regresi Buckley-James diestimasi dengan mengubah censored point pada data tersensor ke nilai ekspektasinya untuk selanjutnya menggunakan modifikasi metode least square yang dibobot dengan estimator Kaplan-Meier.",,Kata Kunci : Regresi Linear; Data Tersensor; Estimator Kaplan-Meier; Least Square.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149293,REGRESI LOGISTIK ORDINAL UNTUK ESTIMASI RESPON DALAM ANALISIS CONJOINT; ORDINAL LOGISTIC REGRESSION FOR RESPONSE ESTIMATION IN THE CONJOINT ANALYSIS,"DESY CIPTANINGRUM, Herni Utami",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis conjoint merupakan salah satu metode statistika yang penggunaannya banyak digunakan dalam bidang riset pemasaran. Dalam perkembangan analisis ini, biasanya responden diminta memberikan penilaian berupa ranking atau rating. Penilaian tersebut merupakan data bertingkat (ordinal), maka model yang digunakan untuk estimasinya lebih sesuai dengan model untuk data ordinal. Regresi logistik ordinal adalah model regresi logistik untuk data dengan variabel respon yang bertingkat lebih dari dua kategori. Dalam skripsi ini akan dibahas mengenai analisis conjoint dengan estimasi responnya menggunakan model regresi logistik ordinal. Model yang diperoleh berupa satu fungsi respon yang tentunya lebih mudah dan praktis untuk digunakan dan diinterpretasikan daripada menggunakan model conjoint tradisional.",,Kata Kunci : analisis conjoint; regresi logistik ordinal; fungsi respon
http://etd.repository.ugm.ac.id/home/detail_pencarian/148783,PEBANDINGAN PORTOFOLIO METODE MEAN-VARIANCE DENGAN METODE INVESTASI SAHAM LINDUNG NILAI INFLASI; COMPARISON OF MEAN-VARIANCE PORTFOLIO WITH INFLATION HEDGE STOCK INVESTMENT PORTFOLIO MODEL,"Eko Suprihanto Prasetyo, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode Mean-Variance merupakan salah satu metode yang digunakan untuk melakukan pembobotan portofolio. Namun masih diragukan kemampuan metode ini dalam mengahasilkan komposisi pembobotan pada pembentukan portofolio optimal. Salah satu keraguan metode Mean-Variance Markowitz adalah mengenai optimalnya portfolio dalam masalah yang nantinya ditemui saat melakukan investasi. Salah satu masalah yang nanti ditemui tersebut adalah inflasi (Inflation). An Inflation Hedging Portfolio Selection atau yang lebih dikenal dengan portofolio IHSI, menunjukkan strategi investasi yang memungkinkan pada investor untuk mendapatkan return yang optimal dengan tingkat laju inflasi yang terjadi.",,Kata Kunci : portofolio; lagrange; inflasi
http://etd.repository.ugm.ac.id/home/detail_pencarian/149295,MODEL REGRESI INVERSE GAUSSIAN; INVERSE GAUSSIAN REGRESSION MODEL,"BUDI LESTARI, Herni Utami",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam regresi linear klasik, terdapat asumsi-asumsi yang harus dipenuhi seperti normalitas respon dan homoskedastisitas residual. Apabila asumsi-asumsi tersebut tidak terpenuhi, estimasi untuk model regresi linear tidak akurat. Regresi Inverse Gaussian adalah metode alternatif untuk memodelkan hubungan antar variabel respon dan variabel prediktor dimana respon diasumsikan berdistribusi Inverse Gaussian dan tidak mensyaratkan homoskedastisitas. Regresi ini menggunakan fungsi hubung link power tetapi lebih sering menggunakan fungsi hubung log. Analisis ini sangat bermanfaat pada kondisi dimana variabel respon kontinu positif (0,?) seperti rate, proporsi, dan sering digunakan di bidang asuransi untuk memodelkan besarnya klaim asuransi. Estimasi parameter menggunakan metode Maximum Likelihood Estimation (MLE) tidak dapat diselesaikan secara analitik sehingga estimator dihitung dengan memaksimumkan fungsi log-likelihood secara numerik dengan menggunakan metode Newton Raphson dengan menggunakan ekspektasi turunan kedua dari fungsi loglikelihood yang dinamanakan teknik fisher",,Kata Kunci : regresi Inverse Gaussian; Maximum Likelihood Estimation; Fisher Scoring; link function
http://etd.repository.ugm.ac.id/home/detail_pencarian/148784,ESTIMASI BAYESIAN SEEMINGLY UNRELATED REGRESSION DENGAN ALGORITMA MCMC DAN PMC; BAYESIAN ESTIMATION OF SEEMINGLY UNRELATED REGRESSION WITH MCMC AND PMC ALGORITHM,"Susi Utami, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Model Seemingly Unrelated Regression terdiri dari beberapa persamaan regresi yang saling berhubungan. Persamaan regresi tersebut saling berhubungan karena error antar persamaan saling berkorelasi secara contemporaneously. Metode estimasi OLS dapat digunakan untuk mengestimasi parameter model SUR, tetapi metode ini tidak melibatkan korelasi contemporaneously error dalam perhitungannya. Metode OLS juga merupakan metode frequentist, tidak melibatkan informasi prior dalam perhitungannya. Dalam skripsi ini digunakan metode estimasi Bayesian untuk mengestimasi parameter model SUR. Metode estimasi ini melibatkan informasi prior dari parameter yang digunakan. Metode Bayesian seringkali menghasilkan penghitungan yang tidak dapat diselesaikan secara analitis. Satu-satunya cara adalah melalui integrasi numerik yang penggunaannya secara langsung masih tidak dapat dilakukan karena besarnya dimensi integral yang dihitung. Tekhnik khusus yang dapat digunakan adalah melalui simulasi Markov Chain Monte Carlo (MCMC). Software R menyediakan package Laplace?s Demon untuk analisis Bayesian secara lengkap. Algoritma Population Monte Carlo dapat meningkatkan model fit dan mereduksi variansi dari distribusi posterior. Studi kasus dalam skripsi ini membahas faktor-faktor yang mempengaruhi total nilai ekspor Migas dan Nonmigas. Uji Lagrange Multiplier digunakan untuk menguji adanya korelasi contemporaneosly pada error estimasi. Pada studi kasus disimpulkan bahwa secara umum estimasi Bayesian lebih baik dari estimasi klasik OLS dan GLS.",,Kata Kunci : Seemingly Unrelated Regression; contemporaneously; Frequentist; Bayesian; Markov Chain Monte Carlo; Laplace?s Demon; Population Monte Carlo; Uji Lagrange Multiplier.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149297,ESTIMASI MAXIMUM LIKELIHOOD PADA MODEL LINEAR PANEL RANDOM EFFECT DENGAN KOMPONEN SPATIAL ERROR; MAXIMUM LIKELIHOOD ESTIMATION FOR PANEL LINEAR RANDOM EFFECT MODEL WITH SPATIAL ERROR,"TRYA MUSTIKA NURITA SARI, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Model spasial dengan komponen galat (spatial error model) merupakan model yang variabel dependennya tergantung pada karakteristik-karakteristik lokal yang diamati dan komponen galatnya yang disebut error term berkorelasi dalam ruang. Error term dalam model tersebut secara spasial berautokorelasi sehingga terdapat koefisien autokorelasi spasial. Model ini merupakan kasus spesial dari matriks varian-kovarian yang nonspherical. Dalam skripsi ini membahas model panel linear random effect, dengan komponen spatial error untuk mengatasi ketidakpastian model dan hilangnya derajat kebebasan dari model fixed effect, serta memfokuskan pada estimasi parameter dengan metode Maximum Likelihood. Uji spatial Hausmann diterapkan untuk melihat ada tidaknya efek random, uji Lagrange Multiplier untuk menguji apakah terdapat spasial autokorelasi dalam model, serta uji Wald untuk melihat hubungan antar kategori kali-silang. Studi kasus dalam skripsi ini adalah model panel linear random effect dengan komponen spatial error menggunakan data Produc yang terdiri atas 48 negara bagian di USA dengan periode waktu dari tahun 1970-1986.",,Kata Kunci : Spatial error model; random effect; error term; autokorelasi spasial; nonspherical; Uji spatial Haussman; Uji Lagrange Multiplier; uji Wald; Maximum Likelihood
http://etd.repository.ugm.ac.id/home/detail_pencarian/148788,EKSPEKTASI BIAYA GARANSI SEUMUR HIDUP POLIS FREE RECTIFICATION LIFETIME WARRANTY SATU DIMENSI; EXPECTED COST OF ONE- DIMENTIONAL COST FREE RECTIFICATION LIFETIME WARRANTY POLICY,"Sri Rahayu Rakhmaningsih, Adhitya Ronnie Effendie",2013 | Skripsi | PROGRAM STUDI STATISTIKA,Free rectification Lifetime Warranty merupakan salah satu polis garansi seumur hidup. Dalam hal ini perusahaan bertanggung jawab melakukan perbaikan untuk setiap kegagalan produk yang dijualnya selama masa hidup dari produk tersebut. Free Rectification Lifetime Warranty biasanya diterapkan pada produk yang komponen penyusunnya lebih dari satu dan dapat diperbaiki (repairable). Kegagalan produk dimodelkan pada level sistem yang tergantung waktu dan disebut sebagai non-stationary Poisson process (Non-Homogeneous Poisson Process) dengan perbaikan minimal repair. Pada polis garansi ini batas atas dari masa garansi tidak pasti dan mengikuti distribusi eksponensial terpotong. Biaya untuk setiap perbaikan berdistribusi eksponensial. Adapun perhitungan biaya garansi pada tugas akhir ini merupakan perkalian dari ekspektasi biaya untuk setiap kegagalan dengan banyaknya kegagalan selama seumur hidup.,,Kata Kunci : Ekspektasi; Free Rectification Lifetime Warranty
http://etd.repository.ugm.ac.id/home/detail_pencarian/149557,REGRESI SPLINES BENTUK-TERBATAS MONOTON; MONOTONE SHAPE-RESTRICTED REGRESSION SPLINES,"ESTRI PURWANI, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi merupakan analisis statistika yang sering digunakan untuk menyelidiki hubungan antara variabel prediktor dengan variabel respon. Jika asumsi bentuk parametrik diketahui, maka regresi parametrik dapat dilakukan. Tetapi jika asumsi bentuk parametriknya tidak diketahui maka estimasi fungsi regresi dapat dilakukan dengan regresi nonparametrik. Metode regresi nonparametrik yang sering digunakan adalah regresi splines karena menggunakan lebih sedikit parameter dalam proses estimasi. Regresi splines mampu memodelkan data yang mempunyai karakteristik berbeda dalam interval ???, ??? . Proses estimasi dilakukan dengan membagi ???, ??? menjadi beberapa sub interval yang mempunyai kesamaan karakteristik. Regresi splines sangat sensitif terhadap penentuan jumlah dan lokasi titik knot sehingga diperlukan kriteria yaitu Generalized Cross-Validation untuk menentukan jumlah dan lokasi titik knot yang optimal. Dalam aplikasi nyata, variabel prediktor dan respon diketahui mempunyai bentuk tertentu seperti monotonisitas. Asumsi monotonisitas ini dapat diterapkan ke dalam proses estimasi regresi splines yang kemudian dinamakan regresi splines bentuk-terbatas monoton. Estimasi fungsi regresi ini dapat diperoleh menggunakan kombinasi linier dari basis fungsi yaitu I-splines dan membatasi koefisiennya agar bernilai positif. Regresi splines dengan pembatasan bentuk ini memiliki Mean Squared Error (MSE) yang lebih kecil dan R-square yang lebih besar daripada regresi splines tanpa pembatasan bentuk. Dalam skripsi ini, analisis regresi splines bentuk terbatas monoton diaplikasikan untuk menganalisis hubungan umur dan tinggi badan balita di posyandu Sakura, kelurahan Caturharjo, kecamatan Pandak, kabupaten Bantul. Kemudian hasil estimasi regresi splines bentuk terbatas monoton dibandingkan dengan regresi splines dan regresi linier sederhana. Dengan melihat hasil estimasi kurva regresi, MSE dan R-square diperoleh kesimpulan bahwa regresi splines bentuk-terbatas monoton merupakan model yang terbaik dibandingkan dengan model lainnya.",,Kata Kunci : regresi nonparametrik; regresi splines; knot; regresi splines bentukterbatas monoton; I-splines
http://etd.repository.ugm.ac.id/home/detail_pencarian/149558,ANALISIS REGRESI RIDGE DUA TAHAP UNTUK PERMASALAHAN MULTIKOLINEARITAS; TWO STAGES RIDGE REGRESSION ANALYSIS FOR MULTICOLLINEARITY PROBLEM,"Estira Woro Astrini, Subanar",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi adalah analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Dalam asumsi yang terdapat pada analisis regresi klasik salah satunya adalah tidak terdapat multikolinearitas. Jika terdapat multikolinearitas dalam model regresi, hal itu dapat menyebabkan hasil estimasi menggunakan metode kuadrat terkecil menjadi tidak valid. Seiring perkembangan waktu, mulailah ditemukan berbagai analisis regresi modern. Dan salah satu analisis regresi modern yang dapat mengatasi permasalahan multikolinearitas adalah analisis regresi Ridge yang pertama kali diperkenalkan oleh A.E Hoerl dan Kennard pada tahun 1970. Seperti halnya analisis regresi klasik yang berkembang menjadi analisis regresi modern, regresi Ridge pun mengalami perkembangan. Salah satunya adalah analisis regresi Ridge dua tahap yang baru diperkenalkan oleh Hussein Eledum dan Mostafa Zahri pada tahun 2013. Analisis regresi Ridge dua tahap ini merupakan gabungan antara metode regresi kuadrat terkecil dua tahap dengan metode regresi Ridge biasa. Dalam skripsi ini, analisis regresi Ridge dua tahap diaplikasikan pada analisis faktor-faktor yang mempengaruhi jumlah uang beredar di Amerika sehingga memperoleh model yang tepat dan bebas dari multikolinearitas.",,Kata Kunci : analisis regresi kuadrat terkecil dua tahap; analisis regresi Ridge; analisis regresi Ridge dua tahap.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149053,PARTIAL LEAST SQUARE PATH MODELING MODEL REFLEKTIF UNTUK DATA NON METRIK; PARTIAL LEAST SQUARE PATH MODELING REFLECTIVE MODEL FOR NON-METRIC DATA,"AMBAR KUSUMAWATI, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode Partial Least Square pendekatan Structural Equation Modeling atau yang biasa disebut dengan Partial Least Square Path Modeling (PLS-PM) adalah alternatif dari Covariance Based SEM. Metode PLS tidak memerlukan asumsi baik mengenai distribusi maupun jumlah sampel yang harus dipenuhi, sehingga metode ini disebut soft modeling dan banyak diaplikasikan. Namun pada prakteknya di berbagai bidang penelitian juga menganalis variabel kategorik yang tidak dapat dianalisis secara langsung seperti variabel numerik. Pada skripsi ini akan memberikan modifikasi dari teknik PLS pendekatan Structural Equation Modeling yang mampu menangani baik metrik maupun non metrik variabel sekaligus dengan konsep optimal scaling tanpa pre-handling untuk menangani variabel non metrik, yang disebut dengan Non Metric Partial Least Square Path Modeling (NM-PLSPM). Pendekatan Non Metrik Partial Least Square memaksimalkan kriteria yang sama dimana parameter model diestimasi, yakni memaksimalkan korelasi indikator terukur dengan estimasi inner dari variabel laten. Pada studi kasus skripsi ini, akan dibandingkan hasil estimasi dan indeks kualitas model PLS-PM dengan model NM-PLSPM. Didapat kesimpulan bahwa model NM-PLSPM mampu menghasilkan estimasi model dengan indeks kualitas yang lebih optimal.",,"Kata Kunci : Structural Equation Modeling, Partial Least Square, Partial Least Square Path Modeling; optimal scaling; indikator kategorik; data non metri"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149309,ANALISIS TEKNIKAL SAHAM DENGAN INDIKATOR COMMODITY CHANNEL INDEX ( CCI ); STOCKS TECHNICAL ANALYSIS WITH COMMODITY CHANNEL INDEX (CCI) INDICATOR,"CENDITIA HAKIKI JAMIN, Sri Haryatmi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Di zaman yang semakin modern ini banyak cara yang dilakukan orang untuk berinvestasi, salah satunya lewat trading saham. Salah satu analisis yang digunakan dalam trading saham yaitu analisis teknikal. Analisis teknikal merupakan analisa untuk memprediksi pergerakan harga saham dengan menggunakan grafik sehingga kita dapat mengetahui kapan harus membeli dan menjual saham untuk memperoleh keuntungan dan membatasi kerugian. Terdapat berbagai macam indikator yang dapat digunakan dalam analisis teknikal saham, salah satu diantaranya adalah CCI (Commodity Channel Index). Indikator CCI beguna untuk menunjukkan trend harga saham yang sedang terjadi dan membantu menentukan daerah jenuh jual dan daerah jenuh beli sebagai sinyal untuk trader untuk masuk dan keluar pasar saham. Dalam penulisan skripsi ini, perhitungan indikator CCI dengan berbasis SMA (Simple Moving Average), EMA (Exponential Moving Average), dan WMA (Weighted Moving Average) yang kemudian akan dibandingkan kinerjanya. Periode yang digunakan adalah sepertiga siklus komplit dari pergerakan suatu harga saham yaitu dari puncak ke puncak atau dari lembah ke lembah sebagai acuan waktu yang digunakan untuk menghitung CCI.",,Kata Kunci : Saham; analisis teknikal; SMA; EMA; WMA; CCI
http://etd.repository.ugm.ac.id/home/detail_pencarian/149054,GRAFIK PENGENDALI CUMULATIVE SUM (CUSUM) dan GRAFIK PENGENDALI EXPONENTIALLY WEIGHTED MOVING AVERAGE (EWMA); CUMULATIVE SUM (CUSUM) CONTROL CHART and EXPONENTIALLY WEIGHTED MOVING AVERAGE (EWMA) CONTROL CHART,"DIAH PRATIWI, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Salah satu alat yang bantu yang digunakan dalam proses pengendalian kualitas adalah grafik pengendali. Salah satu grafik pengendali yang efisien digunakan untuk mendeteksi pergeseran yang kecil (kurang dari 1,5",,Kata Kunci : Grafik Pengendali; CUSUM; EWMA
http://etd.repository.ugm.ac.id/home/detail_pencarian/149056,PENGAMBILAN KEPUTUSAN DALAM KONDISI KETIDAKPASTIAN DAN DALAM KONDISI BERESIKO; MAKING DECISION UNDER UNCERTAINTY CONDITION AND UNDER RISK CONDITION,"MUHAMMAD ALAWIDO, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam pengambilan keputusan, diperlukan suatu cara atau teknik yang dapat memberikan hasil keputusan yang optimal. Keputusan optimal diperoleh apabila hasil keputusan sesuai dengan yang diharapkan dan sesuai dengan syarat yang ditentukan oleh pengambil keputusan. Suatu keputusan dikatakan dalam kondisi berisiko apabila pengambil keputusan mengetahui besarnya nilai peluang mengenai hasil keputusan. Sebaliknya, suatu keputusan dikatakan dalam kondisi ketidakpastian apabila nilai peluang hasil keputusan tidak diketahui. Dalam skripsi ini dibahas mengenai metode pengambilan keputusan dalam kondisi ketidakpastian dengan empat kriteria yaitu kriteria Laplace, kriteria Minimaks, kriteria Savage Minimaks Regret, kriteria Hurwicz, dan dalam kondisi beresiko dengan tiga kriteria yaitu kriteria nilai harapan, kriteria kecenderungan maksimum, dan kriteria rasionalitas, sehingga memperoleh hasil keputusan yang optimal.",,"Kata Kunci : Pengambilan keputusan, ketidakpastian, beresiko, Laplace, Minimaks, Savage Minimaks Regret; Hurwicz; nilai harapan; kecenderungan maksimum; rasionalitas."
http://etd.repository.ugm.ac.id/home/detail_pencarian/148801,PENENTUAN RETENSI OPTIMAL DAN HARGA PREMI DARI ASURANSI KE REASURANSI DENGAN VALUE AT RISK; DETERMINATION OF OPTIMAL RETENTION AND PREMIUM PRICING OF INSURANCE TO REINSURANCE WITH VALUE AT RISK,"Dian Saraswati, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Pada asuransi umum, asuransi menanggung risiko kerugian dari benda yang diasuransikan oleh pemegang polis jika terjadi klaim. Kerugian tersebut bisa tinggi bisa juga rendah. Asuransi menemui kendala jika risiko kerugian yang ditanggung bernilai tinggi dalam hal ini berarti dana yang dibutuhkan untuk mangganti kerugian tersebut besar nilainya. Solusi dari kendala tersebut adalah dengan mengasuransikan kembali pada reasuransi. Hal itu berarti bahwa asuransi akan membagi risiko yang ditanggung dengan reasuransi. Asuransi berkewajiban membayar sejumlah dana kepada reasuransi yang disebut premi reasuransi. Reasuransi berkewajiban menanggung risiko sebagian risiko yang telah dilimpahkan kepadanya. Dalam pembagian risiko tersebut ditentukan batas yang mana mengacu pada kemampuan maksimal asuransi menanggung risiko yang disebut retensi. Batas ini ditentukan berdasarkan optimisasi Value at Risk. Kriteria optimisasi ditetapkan berdasarkan nilai minimal VaR dari risiko total asuransi, untuk menurunkan retensi optimal pada reasuransi stop loss. Hasil solusi optimal pada kriteria optimisasi memiliki beberapa karakteristik penting, antara lain: retensi optimal mempunyai analisis yang sangat sederhana; retensi optimal hanya bergantung pada asumsi distribusi kerugian dan faktor loading reasuransi. Dengan demikian asuransi dapat mengatasi kendala dari risiko kerugian yang bernilai tinggi, karena asuransi hanya menanggung kerugian maksimal sebesar retensi dan reasuransi menanggung kerugian sisanya",,Kata Kunci : Asuransi Umum; Retensi Optimal; Value at Risk (VaR)
http://etd.repository.ugm.ac.id/home/detail_pencarian/149318,OPTIMISASI PORTOFOLIO ROBUST MENGGUNAKAN SECOND-ORDER CONE PROGRAMMING (SOCP); ROBUST PORTFOLIO OPTIMIZATION USING SECOND-ORDER CONE PROGRAMMING (SOCP),"Dessy Paramita, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Optimisasi portofolio merupakan salah satu metode seleksi portofolio yang terkenal dan paling banyak digunakan. Teknik optimisasi portofolio pertama kali dikembangkan oleh Markowitz (1952), yaitu model mean-variance. Walaupun model ini didukung oleh teori yang kuat dan memiliki kemudahan dalam komputasi, mean-variance menunjukkan beberapa kelemahan, salah satunya sangat sensitif terhadap perubahan parameter input. Untuk mengurangi sensitivitas model ini, dikenalkan teknik optimisasi portofolio robust. Dalam optimisasi portofolio robust, parameter inputnya dianggap tidak pasti, dalam hal ini terletak dalam sebuah interval konfidensi yang selanjutnya disebut himpunan ketidakpastian (uncertainty set). Hal ini disebabkan karena pada realita mengestimasi nilai kedua parameter ini tidak mudah, selain itu nilainya selalu berubah setiap saat. Setelah menentukan himpunan ketidakpastian, masalah optimisasi akan diselesaikan untuk kasus terburuk yakni kondisi dengan nilai pengembalian (expected return) portofolio minimum dan risiko portofolio maksimum. Masalah optimisasi dalam analisis portofolio robust ini akan dibawa ke dalam bentuk second-order cone programming (SOCP) yang dapat diselesaikan dengan metode titik interior primal-dual. Studi kasus dilakukan dengan membentuk portofolio yang terdiri dari saham-saham yang terdaftar di Bursa Efek Indonesia, yaitu AALI, ADRO, ASRI, BBRI, CPIN, TLKM dan UNVR, menggunakan teknik SOCP dan mean-variance. Kedua metode ini dibandingkan dengan mengamati kinerja portofolio yang diukur dari tingkat pengembalian (rate of return) portofolio dan nilai indeks Sharpe. Hasilnya, portofolio robust SOCP menunjukkan kinerja yang lebih baik daripada portofolio mean-variance.",,Kata Kunci : Portofolio; optimisasi robust; optimisasi mean-variance; second-order cone programming.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149319,PERMODELAN DAERAH TERTINGGAL MENGGUNAKAN GEOGRAPHICALLY WEIGHTED REGRESSION (Studi Kasus Data Sensus Potensi Desa dan Survei Ekonomi Nasional 2006 di Kabupaten Banjarnegara); POVERTY MODELLING WITH GEOGRAPHICALLY WEIGHTED REGRESSION (Case Study Sensus Potensi Desa and Survei Ekonomi Nasional 2006 data in Banjarnegara),"ADHIARSA RAKHMAN, Sri Haryatmi Kartiko",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Geographically Weighted Regression (GWR) adalah salah satu metode yang efektif digunakan untuk mengestimasi data yang memiliki heterogenitas spasial. Penulis akan menggunakan metode GWR untuk memprediksi rata-rata pengeluaran per kapita sehari penduduk Banjarnegara menggunakan data sensus PODES dan Survei SUSENAS 2006. Setelah itu untuk menunjukkan GWR lebih baik dari regresi linear biasa, akan dibandingkan nilai Mean Square Error (MSE) dari kedua model tersebut.",,Kata Kunci : Geographically Weighted Regression; Weighing Matrix; Spatial Analysis
http://etd.repository.ugm.ac.id/home/detail_pencarian/148808,SEGMENTASI KARAKTERISTIK DEBITUR MENGGUNAKAN ALGORITMA X-MEANS; DEBTOR CHARACTERISTIC SEGMENTATION USING X-MEANS ALGORITHM,"NURLITA KUSUMA DEWI, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,Kejadian debitur gagal membayar atau menunggak pembayaran diistilahkan “default”. Salah satu cara meminimalisir default kita dapat mengenali ciri-ciri debitur yang biasanya mengalami default menggunakan algoritma xmeans. Clustering menggunakan algoritma x-means merupakan pengembangan dari k-means cluster. X-means membentuk cluster awal menggunakan k-means. Setiap cluster awal yang terbentuk dibagi menjadi dua cluster berdasarkan kreiteria BIC. Proses ini berulang hingga tidak ada lagi cluster yang dapat dibagi. X-means membutuhkan komputasi yang lebih sedikit daripada k-means dan mampu mengoptimalkan jumlah cluster yang terbentuk,,Kata Kunci : resiko kredit; segmentasi; x-means; k-means; BIC
http://etd.repository.ugm.ac.id/home/detail_pencarian/149320,PORTOFOLIO OPTIMAL DENGAN MULTI GROUP MODEL (SHORTSALES ALLOWED) (Studi Kasus Pada Saham-Saham LQ-45 di IDX Periode 2012-2013);OPTIMAL PORTFOLIO BY MULTI GROUP MODEL (SHORTSALES ALLOWED),"FREDY ARI PURWANDONO, Adhitya Ronnie Effendie",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Mean Variance Method is a classical portfolio method which was introduced by Harry Markowitz on his article titled Portfolio Selection. This method is using variance and co variance matrix from securities, so as in certain cases which have many securities will take longer time in its solution. Meanwhile, this methode will not include risk free assets as portfolios construction. Then Elton, Gruber, Padberg on 1976 adviced Multi Group Model which divided stocks into certain groups based on its sectors. This new method, will include risk free assets into its solution. Multi Group Model Method is based on the correlation within the group are same for all the pairs in its group and the correlation for all pairs of stocks between group is the same. In this essay, will explanation about optimal portfolios construction by Multi Group Model Method, where will be compared by Mean Variance Method for specifying which method has better solution. Parameter is used by Sharpe Ratio.",,Kata Kunci : Portofolio; Multi Group Model; Sharpe Ratio
http://etd.repository.ugm.ac.id/home/detail_pencarian/148553,GENERALIZED LINEAR MIXED MODEL (GLMM) UNTUK DATA GEOSTATISTIK; GENERALIZED LINEAR MIXED MODEL (GLMM) FOR GEOSTATISTICAL DATA,"Ulung Wibowo, Sardjono",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Geostatistik mengandung pengertian Ilmu statistika yang diterapkan dalam ilmu geologi dam ilmu bumi secara umum, data geostatistik tidak hanya berbatasan pada lingkup bumi saja, tetapi mencakup pada wilayah yang lebih universal. Yaitu data-data yang berhubungan dengan teori statistika dan aplikasinya dengan indeks spasial kontinu yang membentuk suatu permukaan. Metode konvensional yang digunakan pada data geostatistik umumnya hanya bisa digunakan untuk masalah dengan data yang berdistribusi gaussian. Untuk data yang berdistribusi non-Gaussian digunakan model GLMM, dengan memandang variabel spasial sebagai variabel latent. Inferensi bayesian digunakan untuk mengestimasi parameter model. Markov Chain Monte Carlo (MCMC) digunakan untuk membantu digunakan untuk membantu menyelesaikan analisis bayesian serta melakukan prediksi mengguanakn algoritma Metropolos-Hasting. Metodologi ini diimplementasikan untuk memetakan pesebaran penyakit gizi buruk di Provinsi Jawa Timur pada tahun 2008.",,Kata Kunci : Geostatistik; MCMC; Bayesian; Generalized linear mixed model; Metropolis-Hasting
http://etd.repository.ugm.ac.id/home/detail_pencarian/149066,METODE ORDINARY COKRIGING DENGAN MENGGUNAKAN SEMIVARIOGRAM ANISOTROPY; ORDINARY COKRIGING METHOD WITH SEMIVARIOGRAM ANISOTROPY,"ATIYA PRAHYUNINGTYAS, Yunita Wulan Sari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Pada penelitian ini dipelajari metode perluasan dari ordinary kriging. ordinary kriging tidak mampu mengestimasi cadangan mineral dengan memperhitungkan pengaruh dari variabel lain yang disebut co-variable. Sehingga perlu dipelajari metode yang mampu digunakan untuk mengestimasi dengan memperhitungkan pengaruh co-variable nya yaitu metode ordinary cokriging. Metode ordinary cokriging digunakan pada kasus data kandungan mineral tersample tidak memiliki trend tertentu, dan rerata tidak diketahui. Dalam metode ordinary cokriging terdapat komponen yang dinamakan semivariogram, yaitu komponen untuk mengamati korelasi antar data sampel dan cross semivariogram, yakni semivariogram yang menggambarkan korelasi variabel yang berbeda. Semivariogram anisotropy adalah semivariogram yang dipengaruhi oleh arah dan jarak antar titik sampel. Dalam semivariogram anisotropy dibutuhkan toleransi arah dan toleransi jarak yang tepat agar jumlah pasangan data sesuai dengan model semivariogram teoritis. Hasil estimasi yang diperoleh dengan metode ordinary cokriging menghasilkan the best linear unbiased estimator (BLUE). Hasil akhir yang diperoleh adalah estimasi cadangan mineral dan peta yang menggambarkan zona potensial mineral tersebut.",,Kata Kunci : ordinary kriging ; ordinary cokriging; semivariogram; cross semivariogram; anisotropy.
http://etd.repository.ugm.ac.id/home/detail_pencarian/150096,PENGAMBILAN SAMPEL KLASTER DUA TAHAP DENGAN ESTIMATOR HANSEN-HURWITZ; (PPS SAMPLING TANPA PENGEMBALIAN) TWO STAGE CLUSTER SAMPLING WITH HANSEN-HURWITZ ESTIMATOR (PPS SAMPLING WITHOUT REPLACEMENT),"SARI TITIS WAHYUNINGTYAS, Sri Haryatmi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode pengambilan sampel klaster dua tahap merupakan metode yang sangat baik untuk mengambil sampel dengan populasi dan tingkat keheterogenan yang besar. Pada pengambilan sampel klaster dua tahap, pengambilan di awali dengan membagi wilayah penelitian menjadi unit-unit primer, kemudian masing-masing unit primer dibagi menjadi unit-unit sampling. Pengambilan sampel dilakukan dengan pps sampling tanpa pengembalian. Dalam skripsi ini, estimator Hansen-Hurwitz digunakan untuk mengestimasi mean dan total populasi. Dalam pengambilan klaster ini, estimator Hansen-Hurwitz juga dapat digunakan untuk berbagai jumlah tahapan dalam pengambilan sampel tergantung pada probabilitas terpilihnya sampel dalam populasi. Pada skripsi ini akan diberikan studi kasus pengambilan sampel klaster dua tahap untuk mengestimasi rata-rata dan jumlah puskesmas di Indonesia",,Kata Kunci : Pengambilan Sampel Klaster Dua Tahap; Estimator Hansen-Hurwitz; PPS sampling
http://etd.repository.ugm.ac.id/home/detail_pencarian/149601,TREND ANALYSIS PADA DESIGN SATU ARAH; TREND ANALYSIS IN ONE WAY DESIGN,"ANJANIA RAYI SAPUTRI, Yunita Wulansari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Prosedur trend analysis dirancang untuk membantu menilai apakah ada hubungan fungsional antara level faktor dan rata-rata sel. Hubungan fungsional menggambarkan kecenderungan umum atau sifat hubungan antara level faktor dan rata-rata sel. Menggunakan prosedur ini, justru dapat menggambarkan trend pada data dalam hal bagian komponennya. Kita menguraikan hubungan antara level faktor dan rata-rata sel menjadi beberapa komponen trend ketika mempunyai derajat kebebasan untuk level faktor. Tujuan dari analisis ini adalah mengestimasi rata-rata sel pada tiap level faktor supaya menghasilkan kurva yang halus. Untuk melakukan trend analysis, level faktor harus kuantitatif dan harus ada interval yang sama antara level faktor. Nilai sudah disediakan pada tabel koefisien orthogonal polinomial. Langkah dari trend analysis pada design satu arah adalah menghitung rata-rata sel tiap level faktor kemudian menghitung masing-masing trend ke-k dan trend tingkat tinggi. Apabila trend tingkat tinggi sudah diperoleh dan signifikan masuk model trend analysis pada design satu arah, maka estimasi rata-rata sel pada tiap level faktor dapat dihitung dan dibuat plot untuk melihat bentuk trend yang diperoleh.",,Kata Kunci : Trend analysis; Design satu arah
http://etd.repository.ugm.ac.id/home/detail_pencarian/149350,MODEL HAZARD PROPORSIONAL UNTUK DATA UJI HIDUP TERSENSOR INTERVAL;PROPORTIONAL HAZARDS MODEL FOR INTERVAL-CENSORED FAILURE TIME DATA,"Dunie Anaton, Danardono",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Model proporsional hazard adalah model yang dapat diterima dan digunakan secara luas untuk analisis tahan hidup, karena hasil estimasinya bermanfaat serta mudah dipahami oleh peneliti kesehatan. Secara esensi keuntungan dari metode ini adalah tanpa asumsi distribusi dan hasil estimasinya natural untuk resiko dari kegagalan dihubungkan dengan vektor kovariat. Sebagian besar aplikasi, data mungkin tersensor interval. Dengan data tersensor interval, berarti bahwa variabel random dari yang menjadi perhatian hanya diketahui terletak pada interval. Pada kasus ini, informasi yang dimiliki untuk masing-masing individu hanya waktu kejadian (event) yang terjadi dalam interval, waktu terjadinya kejadian (event) pasti tidak diketahui. Fungsi survival merupakan fungsi yang sangat penting dalam studi medis dan kesehatan. Menggunakan metode Turnbull, estimasi MLE nonparametrik dapat dicari dalam keadaan tersensor interval. Hasil yang diberikan untuk menguji hipotesis dari koefisien regresi nol mengarahkan untuk melakukan generalisasi dari uji log-rank untuk membandingkan beberapa kurva survival menggunakan metode skor.",,Kata Kunci : Model Hazard Proporsional; Data Tersensor Interval; MLE Nonparametrik; Uji Skor
http://etd.repository.ugm.ac.id/home/detail_pencarian/148338,MANAJEMEN PERSEDIAAN DENGAN KETIDAKPASTIAN PERMINTAAN DAN LEAD TIME; INVENTORY MANAGEMENT INVOLVING RANDOM LEAD TIME AND DEMAND,"Muhammad Farkhan Novianto, Yunita Wulan Sari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Persediaan merupakan sumber daya yang menganggur (idle resources), yang berarti jika persediaan berlebih menyebabkan investasi sia-sia, akan tetapi bila terlalu sedikit persediaan yang ada, akan sulit mengantisipasi fluktuasi permintaan, waktu tunggu barang yang dipesan (lead time) atau hal-hal lain yang menyebabkan terjadinya kekurangan persediaan. Keputusan yang menyangkut “berapa banyak dan kapan harus melakukan pemesanan” merupakan masalah utama dalam manajemen persediaan. Sistem continuous review yang banyak dibahas para ahli diyakini mampu menyelesaikan masalah itu, sistem ini akan menentukan titik posisi persediaan kapan harus memesan barang (R), dan jumlah barang yang harus selalu dipesan (Q) ketika persediaan berada tepat pada titik itu. Akan tetapi, masih jarang perusahaan yang menerapkan sistem ini. Salah satunya adalah PT. United Tractors, Tbk., yang merupakan salah satu distributor spare parts terbesar di Indonesia yang masih menggunakan sistem Periodic Review, yang kebanyakan nilai-nilainya ditentukan oleh perusahaan sendiri. Dengan data manajemen persediaan yang ada pada PT. United Tractors, Tbk., akan dihitung nilai Q dan R yang optimal untuk meminimalkan biaya persediaan. Selanjutnya disimulasikan manajemen persediaan dengan sistem continuous review yang telah dihitung dibandingkan dengan sistem periodic review yang digunakan PT. United Tractors, Tbk dengan menggunakan simulasi monte carlo.",,Kata Kunci : manajemen persediaan; random demand; random lead time; continuous review; periodic review; newton raphson; simulasi monte carlo
http://etd.repository.ugm.ac.id/home/detail_pencarian/149117,ASURANSI JIWA UNIT LINK DENGAN JAMINAN MINIMUM MANFAAT KEMATIAN MENGGUNAKAN PENDEKATAN OPSI TIPE EROPA; UNIT-LINKED LIFE INSURANCE CONTRACT WITH MINIMUM DEATH BENEFIT GUARANTEE USING EUROPEAN OPTION APPROACH,"ADHA ROZAK, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Kontrak asuransi jiwa unit-linked merupakan proteksi sekaligus investasi. Selain akan mendapat perlindungan dari suatu sebab tertentu yang merugikan, juga kita akan mendapat hasil investasi dari instrumen keuangan tertentu. Dewasa ini , perusahaan asuransi telah kreatif dalam mengkreasikan berbagai macam jaminan untuk ditambahkan ke dalam asuransi unit-linked murni, seperti pengembalian premi, asset garansi, asuransi renter, ratchet. Pada skripsi ini digunakan jaminan minimum kematian sebesar harga kontrak saham dengan pendekatan opsi tipe eropa. Sehingga jika harga saham ketika tertanggung jatuh, akan mendapatkan nilai kontrak dan sebaliknya jika harga saham melejit tinggi maka akan mendapatkan harga saham tersebut.",,Kata Kunci : N
http://etd.repository.ugm.ac.id/home/detail_pencarian/149377,PERBANDINGAN MODEL REGRESI NONPARAMETRIK SPLINE DAN REGRESI NONPARAMETRIK KERNEL; A COMPARISON OF MODELS NONPARAMETRIC REGRESSION SPLINE AND NONPARAMETRIC REGRESSION KERNEL,"Yuni Kurnia Purnamasari, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis Regresi merupakan salah satu alat statistika yang banyak digunakan untuk mengetahui hubungan antara sepasang variable atau lebih. Analisis regresi dibagi dua yaitu regresi parametrik dan regresi nonparametrik. Regresi nonparametric memiliki beberapa metode smoothing, seperti regresi spline dan kernel. Tujuan utamanya adalah membandingkan kedua metode tersebut untuk mengestimasi model regresi nonparametrik. Data yang digunakan untuk membandingkan kedua metode tersebut yaitu data pertuumbuhanbalita.",,Kata Kunci : Regresi nonparametrik; regresi spline; regresi kernel.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149122,APLIKASI MODEL ASYMMETRIC POWER GARCH (APGARCH) PADA KOMODITI EMAS (EMAS KAS DAN EMAS BERJANGKA); ASYMMETRIC POWER GARCH (APGARCH) MODEL APPLICATION ON GOLD COMMODITIES (GOLD CASH AND GOLD FUTURES),"Rani Nurindah R, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Model asymmetric power GARCH (APGARCH) yang dikembangkan oleh Ding et al (1993) menjadi versi baru dalam keluarga ARCH di mana transformasi power digunakan pada data yang tidak berdistribusi normal. Model APGARCH memungkinkan adanya transformasi power dari semua harga positif sehingga memungkinkan adanya transformasi dengan ruang yang tak terbatas. Pada tugas akhir ini, model APGARCH yang diperkenalkan oleh Ding, Granger dan Engle (1993) dapat diterapkan pada enam permodelan pada data emas (emas kas 1984-2003, emas berjangka 1984-2003, data krisis emas kas 1987, data krisis emas berjangka 1987, data krisis emas kas 2001, dan data krisis emas berjangka 2001) di mana hampir semua variabel pada data tersebut tidak berdistribusi normal. Pergerakan fluktuasi harga emas di pasaran yang dipengaruhi variabel-variabel yang signifikan ini bisa dilihat melalui model asymmetric power GARCH (APGARCH).",,Kata Kunci : APGARCH
http://etd.repository.ugm.ac.id/home/detail_pencarian/149124,ESTIMASI EFISIENSI TEKNIS PADA FUNGSI PRODUKSI DENGAN PENDEKATAN STOKASTIK FRONTIER; ESTIMATION OF TECHNICAL EFFICIENCY IN PRODUCTION FUNCTION WITH STOCHASTIC FRONTIER APPROACH,"Endah Putrihadia, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Efisiensi teknis merupakan kondisi yang cukup penting dalam sebuah perusahaan atau industri. Hal ini dikarenakan efisiensi teknis mencerminkan keberhasilan proses produksi dalam memaksimalkan output dengan diberikan sejumlah input. Dalam perkembangannya, banyak peneliti yang melakukan estimasi efisiensi teknis dengan berbagai macam metode. Salah satu diantaranya yang akan dibahas dalam skripsi ini adalah metode Stokastik Frontier. Berbeda dengan metode sebelumnya yaitu metode Ordinary Least Square dan Corrected Ordinary Least Square, Stokastik Frontier memiliki dua komponen galat yaitu galat yang berasal dari kesalahan acak dan galat yang berasal dari faktor-faktor lain yang masih bisa dikendalikan yang disebut galat inefisiensi. Berdasarkan studi kasus yang telah dilakukan, dengan mengambil data dari 12 jenis industri di Indonesia, didapatkan kesimpulan bahwa estimasi efisiensi teknis dengan menggunakan Stokastik Frontier memiliki hasil yang tidak berbeda dengan metode sebelumnya. Namun jika dilihat dari nilai jumlahan residual kuadratnya, Stokastik Frontier bisa dikatakan lebih baik. Disamping itu dengan menggunakan Stokastik Frontier bisa diidentifikasi ada tidaknya efek waktu dalam model.",,Kata Kunci : Stokastik Frontier; Efisiensi Teknis; Industri Indonesia
http://etd.repository.ugm.ac.id/home/detail_pencarian/149130,ANALISIS TEKNIKAL SAHAM DENGAN INDIKATOR GABUNGAN STOCHASTIC OSCILLATOR DAN EXPONENTIAL MOVING AVERAGE; STOCK TECHNICAL ANALYSIS USING COMBINED INDICATORS OF STOCHASTIC OSCILLATOR AND EXPONENTIAL MOVING AVERAGE,"Purry Nurina Oktaviany, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis teknikal saham merupakan suatu metode analisis yang menggunakan pengujian atas harga di masa lampau untuk tujuan prediksi pergerakan harga di masa yang akan datang. Analisis teknikal saham baik digunakan para pelaku trading (trader) dalam membantu memberikan prediksi kapan waktu yang tepat untuk masuk dan keluar dari pasar saham. Terdapat berbagai macam indikator yang digunakan dalam analisis teknikal, salah satunya adalah Stochastic Oscillator. Stochastic Oscilator merupakan gabungan dari simple moving average yang memiliki sifat sensitivitas tinggi sehingga dapat menjadikannya suatu kekurangan, yaitu munculnya sinyal palsu. Skripsi ini akan membahas indikator gabungan stochastic oscillator dan exponential moving average yang dapat meminimalisir sinyal palsu tersebut dengan periode yang digunakan adalah 9, 14 25. Performa gabungan stochastic oscillator dan exponential moving average berdasar pada ketepatan sinyal lebih baik dibandingkan dengan stochastic oscillator biasa yang menggunakan simple moving average.",,Kata Kunci : Analisis teknikal; Stochastic Oscillator; Simple Moving Average; Exponential Moving Average
http://etd.repository.ugm.ac.id/home/detail_pencarian/148875,ANALISIS DATA MINING CUACA MENGGUNAKAN ATURAN ASOSIASI DAN REGRESI LOGISTIK KERNEL; WEATHER DATA MINING ANALYSIS USING ASSOCIATION RULES AND KERNEL LOGISTICS REGRESSION,"HEBNU PRIYAMBODO, Gunard",2013 | Skripsi | PROGRAM STUDI STATISTIKA,Pengetahuan tentang pola dan hubungan memegang peranan sangat penting dalam pengambilan kebijakan pada bidang pertanian. Tingkat curah hujan merupakan salah satu faktor yang mempengaruhi produktifitas tanaman komoditas pangan. Aturan asosiasi (association rule) dan Regresi logistik kernel (Kernel Rgresson Logistik) merupakan teknik data mining untuk membantu dalam pengambilan suatu keputusan. Pemanfaatan model dalam mengggambarkan suatu probabilitas serta menemukan aturan-aturan yang terjadi dalam data jumlah besar. Penulis melakukan penelitian terhadap data yang diperoleh dari Badan Meteorologi Klimatologi dan Geofisika (BMKG) serta Badan Pusan Statistik (BPS). Tugas akhir ini memberikan kesimpulan bahwa regresi logistik kernel dengan bantuan grafik dapat menghasilkan pola tingkat curah hujan serta gambaran pada bidang pertanian. Sedangkan association rule menghasilkan aturan-aturan tersembunyi yang terdapat dalam suatu data set.,,Kata Kunci : Curah hujan; Regresi logistik kernel; Aturan sosiasi; Data mining; Komoditas pangan; data set
http://etd.repository.ugm.ac.id/home/detail_pencarian/149387,PENENTUAN HARGA OBLIGASI CALLABLE DENGAN SUKU BUNGA BLACK DERMAN TOY MENGGUNAKAN POHON BINOMIAL; CALLABLE BOND PRICING WITH BLACK DERMAN TOY INTEREST RATE MODEL USING BINOMIAL TREE,"HELIDA HAERINI, Yunita Wulan Sari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Suku bunga mempunyai peran penting dalam penentuan harga aset finansial, salah satunya adalah obligasi. Obligasi callable merupakan obligasi yang memberikan hak kepada penerbit untuk membeli kembali obligasi pada harga tertentu sepanjang umur obligasi tersebut. Digunakan metode pohon binomial dalam megestimasi harga obligasi. Untuk tingkat bunga yang konstan, tidaklah sulit untuk menentukan harga obligasi. Akan tetapi pada kenyataannya, pergerakan tingkat bunga berubah-ubah secara tidak pasti dan merupakan proses stokastik sehingga untuk mengamatinya diperlukan suatu model tingkat bunga stokastik. Dalam skripsi ini, digunakan suku bunga acuan model faktor tunggal dengan asumsi no-arbitrage yang dikembangkan pada tahun 1990 oleh Fischer Black, Emanuel Derman, dan William Toy dan diasumsikan bahwa suku bunga berdistribusi lognormal, yaitu suku bunga Black Derman Toy (BDT). Dalam suku bunga BDT digunakan bisection method untuk mencari nilai drift. Suku bunga BDT juga memasukkan unsur mean reversion di dalamnya. Pada penentuan pohon binomial, dilakukan pemodelan suku bunga terlebih dulu secara maju dan selanjutnya secara mundur dalam menentukan harga obligasi.",,Kata Kunci : suku bunga; obligasi callable; Black Derman Toy; bisection method; pohon binomial
http://etd.repository.ugm.ac.id/home/detail_pencarian/149388,PEMBENTUKAN PORTOFOLIO OPTIMAL MENGGUNAKAN METODE MEAN-EXTENDED-GINI; (OPTIMUM PORTFOLIO USING MEAN-EXTENDED-GINI METHOD),"Febrian Dwi Rahmawati, Herni Utami",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode Mean-Gini adalah alternative dari Mean-Variance klasik yang distribusi return-nya tidak harus berdistribusi normal seperti asumsi pada Mean- Variance. Pembentukan portofolio optimal dengan Mean-Gini telah terbukti lebih baik jika dibandingkan dengan Mean-Variance. Namun, metode Mean-Gini tidak memperhatikan preferensi investor. Investor dianggap memiliki tanggapan yang sama terhadap risiko. Metode Mean-Extended-Gini adalah perluasan dari Mean-Gini. Metode ini menarik bagi investor karena metode ini memperhatikan preferensi investorMean Extended Gini merupakan metode pembobotan portofolio yang cocok untuk investor yang menghindari risiko (risk averse). Investor diberikan beberapa pilihan besarnya Extended-Gini yang merupakan ukuran risiko pada berbagai tingkat (parameter risk averse). Kemudian nilai Extended-Gini yang merupakan fungsi tujuan diperoleh dengan teknik pemrograman non-linear yang dilakuan iterasi sedemikian hingga didapat bobot portofolio yang optimal pada berbagai tingkat . Pengalokasin dana didasarkan pada bobot optimal yang diperoleh pada berbagai tingkat Besarnya parameter yang dipilih adalah yang menghasilkan bobot yang memberikan keuntungan terbesar.",,Kata Kunci : portofolio; risk aversion; Extended-Gini; Generalized Reduced Gradient; Excel solver
http://etd.repository.ugm.ac.id/home/detail_pencarian/149134,MODEL JOINT FRAILTY UNTUK KEJADIAN BERULANG DAN KEJADIAN AKHIR (STUDI KASUS: RAWAT INAP KEMBALI PASIEN KANKER KOLOREKTAL ); JOINT FRAILTY MODEL FOR RECURRENT EVENTS AND TERMINAL EVENT (CASE STUDY: REHOSPITALIZATION OF PATIENTS WITH COLORECTAL CANCER),"Annisa Tiara Dewi, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam studi klinis atau epidemiologi, subjek dapat berpotensi mengalami kejadian berulang. Jangka waktu untuk proses kejadian berulang individu tergantung pada kejadian akhir, seperti kematian. Seringkali terulangnya kejadian yang serius, dikaitkan dengan peningkatan risiko kematian. Model joint frailty adalah konsep pemodelan statistik yang tujuannya untuk menghitung heterogenitas yang disebabkan oleh kovariat-kovariat yang tidak teramati untuk kejadian berulang dan kejadian akhir. Model joint frailty ini juga merupakan model alternatif dari model regresi Cox. Efek random i? (frailties) diasumsikan independen. Densitas frailty gamma di sini diadopsi dengan mean 1 dan variansi ?. Nilai frailty yang lebih tinggi akan menghasilkan risiko perulangan yang lebih tinggi dan risiko kematian yang lebih tinggi pula.",,Kata Kunci : model joint frailty; kejadian berulang; kejadian akhir.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149390,METODE FUZZY ANALYTICAL HIERARCHY PROCESS DALAM PENGAMBILAN KEPUTUSAN PENYALURAN KREDIT (Studi Kasus Penyaluran Kredit pada PD. BPR BKK Kebumen Cabang Puring) FUZZY ANALYTICAL HIERARCHY PROCESS METHOD IN DECISION MAKING CREDIT DISTRIBUTION (Case Study of Credit Distribution in PD. BPR BKK Kebumen Cabang Puring);,"EKA SETYANINGSIH, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Bank merupakan badan usaha yang menghimpun dana dari masyarakat dalam bentuk simpanan dan menyalurkannya kepada masyarakat dalam bentuk kredit ataupun bentuk-bentuk lainnya. Seiring dengan perjalanan waktu sesudah kredit direalisasikan, tidak dapat dipungkiri bank akan dihadapkan pada permasalahan risiko, yaitu risiko kredit bermasalah. Misalnya saja ketidakmampuan untuk membayar bunga dan ketidakmampuan mengembalikan kreditnya pada saat jatuh tempo. Oleh sebab itu bank harus bisa mengambil keputusan yang tepat dan efektif dalam penyaluran kredit kepada calon debitur. Terkait dengan hal di atas, maka metode Analytical Hierarchy Process (AHP) dapat digunakan untuk membantu menyelesaikan masalah tersebut. AHP digunakan manakala keputusan yang diambil melibatkan banyak faktor, dimana pengambil keputusan mengalami kesulitan dalam membuat bobot setiap faktor tersebut. Dengan demikian AHP dapat memecahkan suatu situasi yang kompleks, tidak terstruktur ke dalam beberapa komponen dalam susunan yang hirarki, dengan memberi nilai subjektif tentang pentingnya setiap variable secara relative, dan menetapkan variable mana yang memiliki prioritas paling tinggi guna mempengaruhi hasil pada situasi tersebut. Meskipun demikian penggunaan AHP dalam permasalahan Multi Criteria Decision Making (MCDM) sering dikritisi suhubungan dengan kurang mampunya pendekatan AHP untuk mengatasi faktor ketidakpresisian yang dialami oleh pengambil keputusan ketika harus memberikan nilai yang pasti dalam matriks perbandingan berpasangan. Oleh karena itu, untuk mengatasi kelemahan AHP yang ada maka dikembangkan suatu metode yang disebut Fuzzy AHP. Metode Fuzzy AHP merupakan penggabungan antara metode AHP dengan pendekatan Fuzzy",,Kata Kunci : Analytic Hierarchy Process; Himpunan Fuzzy; Fuzzy Analytic Hierarchy Process
http://etd.repository.ugm.ac.id/home/detail_pencarian/149140,CREDIT SCORING MENGGUNAKAN ANALISIS REGRESI COX; CREDIT SCORING USING COX REGRESSION ANALYSIS,"WIRDA ARDANTI, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,Kredit merupakan hal penting pada negara yang sedang berkembang. Kredit yang bermasalah dapat mengganggu keuangan bank maupun perusahaan tersebut. Bank dapat menyaring debiturnya agar tidak terjadi kredit macet. Hal ini dapat dilakukan dengan mengukur kemungkinan debitur akan mengembalikan pinjamannya dengan lancar atau tidak. Kejadian debitur gagal membayar kembali pinjamannya dikenal dengan istilah default. Salah satu cara untuk mengantisipasi terjadinya default dapat menggunakan metode yang disebut credit scoring. Alat yang digunakan untuk memodelkan credit scoring yaitu Analisis Regresi Cox. Kelebihan memodelkan credit scoring dengan menggunakan analisis regresi cox adalah mengetahui waktu dimana debitur akan mengalami default.,,Kata Kunci : Kredit; Analisis Risiko Kredit; Credit Scoring; Analisis Survival; Analisis Regresi Cox
http://etd.repository.ugm.ac.id/home/detail_pencarian/148890,PEMODELAN PERSAMAAN REGRESI SPLINE KUADRATIK DENGAN MENENTUKAN TITIK - TITIK KNOT YANG OPTIMAL (Studi Kasus Pertambahan Persentase Penduduk dengan Persentase Penerimaan Tenaga Kerja Baru);,"TRI EFENDI, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi digunakan untuk melihat pengaruh variabel prediktor terhadap variabel respon dengan sebuah kurva regresi. Pendekatan yang digunakan untuk menentukan kurva regresi yaitu pendekatan parametrik dan nonparametrik. Regresi spline merupakan salah satu model pendekatan kearah pengepasan data dengan tetap memperhitungkan kemulusan kurva regresi, yang merupakan modifikasi dari fungsi polynomial tersegmen. Bentuk estimator spline sangat di pengaruhi oleh nilai parameter penghalus ? yang pada hakekatnya adalah penentuan lokasi titik – titik knot. Pemilihan ? optimal merupakan persoalan yang sangat penting dalam estimasi regresi spline. Model regresi spline kuadratik diterapkan pada suatu study kasus mengenai persentase pertumbuhan penduduk Indonesia dan persentase penerimaan tenaga kerja baru di Indonesia dengan memilih nilai MSE dan GCV( Generalized Cross Validation ) yang optimum",,Kata Kunci : PEMODELAN; PERSAMAAN REGRESI; SPLINE KUADRATIK; TITIK - TITIK KNOT
http://etd.repository.ugm.ac.id/home/detail_pencarian/149146,PORTOFOLIO OPTIMAL MENGGUNAKAN CAPITAL ASSET PRICING MODEL DENGAN KEYAKINAN HETEROGEN;,"ATHIKAH DIAN ALFARIZY, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Investasi merupakan suatu bentuk penanaman modal dengan harapan nantinya akan mendapatkan keuntungan. Semakin tinggi pengaharapan investor akan keuntungan, semakin tinggi pula risiko yang dihadapi. Untuk dapat meminimalkan risiko dalam investasi, investor dapat melakukan portofolio (diversifikasi) saham yaitu dengan melakukan investasi pada banyak saham sehingga risiko kerugian pada satu saham dapat ditutup dengan keuntungan pada saham yang lainnya. Salah satu model faktor tunggal yang dapat digunakan untuk mencari bobot masingmasing saham pada portofolio adalah Capital Asset Pricing Model (CAPM) yang didasarkan pada asumsi bahwa investor memiliki keyakinan yang homogen tentang mean dan kovariansi aset berisiko. Asumsi ini telah diperdebatkan oleh banyak pihak dari tahun ke tahun, karena dinilai tidak realistis dan tidak sesuai dengan pasar yang sebenarnya, di mana setiap investor memiliki keyakinan yang berbeda-beda akan keuntungan dan risiko di periode mendatang. Maka, dicari portofolio saham dengan menggunakan CAPM dengan asumsi keyakinan investor yang heterogen untuk dibandingkan tingkat keuntungan dan risikonya dengan portfolio yang dicari dengan menggunakan CAPM pada asumsi homogen. Sehingga didapatkan bahwa portofolio yang didapatkan saat mengasumsikan pasar heterogen, menghasilkan pengembalian yang lebih besar dan risiko yang lebih kecil.",,Kata Kunci : Saham; Portofolio; CAPM; Keyakinan Heterogen; Return; Aset Bebas Risiko
http://etd.repository.ugm.ac.id/home/detail_pencarian/149405,REGRESI POISSON DENGAN EFEK RANDOM UNTUK DATA BERKELOMPOK; POISSON REGRESSION WITH EFFECT RANDOM FOR GROUPED DATA,"SHINTA DWI RATNASARI, Sardjono",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi Poisson adalah suatu teknik analisis regresi dimana variabel dependen adalah cacah (count, frequency) sedangkan untuk variabel independen berbentuk kuantitatif (kontinu atau cacah) dan kualitatif (dummy atau kategorik). Pada umumnya, regresi Poisson memiliki ekspektasi (mean) dan variansi yang sama (equidispersi). Namun, seringkali dilapangan ditemukan data dengan variansi lebih besar dari pada ekspektasi (mean) yang disebut dengan overdispersi (Var(Y) > E(Y)). Selain itu, beberapa kasus memiliki tipe data berkelompok dan memiliki korelasi dalam kelompok. Jika dalam kondisi tersebut diaplikasikan regresi Poisson standar maka akan menghasilkan analisis yang tidak tepat. Sehingga untuk mengatasi permasalahan overdispersi dan korelasi didalam kelompok diperlukan metode alternatif lain untuk menyelesaikan masalah tersebut. Untuk mengatasi kesulitan yang ada pada data, akhirnya muncul metode yang dilakukan oleh Seco dan Aubyn (2003) dalam jurnalnya A Random – Effect Log – Linear Model with Poisson Distributions. Skripsi ini mencoba mengimplementasikan metode yang terdapat pada jurnal untuk memecahkan masalah overdispersi dan korelasi di dalam kelompok.Pada jurnal tersebut dijelaskan model untuk mengatasi masalah overdispersi dan masalah korelasi didalam kelompok. Untuk mengestimasi parameter dalam model, digunakan metode IRGLS (Iteratively reweighted Generalized Least Square).",,Kata Kunci : regresi poisson; efek random; data berkelompok; Iteratively reweighted Generalized Least Square.
http://etd.repository.ugm.ac.id/home/detail_pencarian/148898,ESTIMASI NILAI DATA HILANG PADA REGRESI LINEAR SEDERHANA MENGGUNAKAN ALGORITMA EKSPEKTASI MAKSIMISASI; Estimate Value of Missing Data in a Simple Linear Regression using Expectation Maximization Algortihm,"ANASTASIA SEKAR NATALIA, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam analisis regresi tentu saja data adalah elemen penting, karena data tersebut nantinya akan dijadikan bahan yang diolah untuk penarikan kesimpulan. Sehingga data tersebut diharapkan adalah data yang baik, yakni dapat menggambarkan keadaan yang sebenarnya dari objek pengamatan. Namun tidak jarang data yang tersedia tidak lengkap. Jika data yang hilang hanya sebagian kecil dari jumlah data yang ada, data hilang tersebut dapat diabaikan. Namun, jika data yang hilang cukup banyak dan tidak memungkinkan untuk mengulang proses pengumpulan data, harus dilakukan cara lain untuk dapat mengurangi resiko tersebut. Alternatif yang dapat dilakukan adalah dengan memperkirakan nilai dari data yang hilang tersebut. Sehingga pada skripsi kali ini akan diperkenalan metode estimasi dengan menggunakan iterasi algoritma Ekspektasi Maksimisasi. Pembahasan akah diakhiri dengan studi kasus mengenai perkiraan nilai data hilang pada variabel penjelas atau promosi.",,Kata Kunci : regresi linear; data hilang; ekspektasi maksimisasi
http://etd.repository.ugm.ac.id/home/detail_pencarian/148390,ANALISIS REGRESI ROBUST menggunakan PEMBOBOT WELSCH dan PEMBOBOT HAMPEL; ANALYSIS of ROBUST REGRESSION using WEIGHTED WELSCH and WEIGHTED HAMPEL,"RICKY JOSUA SIMANJUNTAK, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam beberapa studi kasus yang ditemukan saat melakukan penelitian, kita sering menjumpai adanya data yang memiliki pencilan. Untuk melakukan analisis regresi dengan data tersebut, maka kita dapat melakukan analisis regresi robust. Analisis ini memiliki beberapa metode dan banyak fungsi yang dapat digunakan untuk mencari parameter koefisien untuk variabel independen dalam menentukan variabel dependen. Salah satu metode yang dapat digunakan adalah Estimasi-S. Metode estimasi-S memiliki breakdown point yang tinggi (50%) dan fungsi yang digunakan adalah fungsi dengan pembobot welsch dan pembobot hampel. Laporan skripsi kali ini bertujuan untuk membandingkan kedua fungsi tersebut dengan menggunakan estimasi S serta membandingkan pula dengan metode kuadrat terkecil (dengan/ tanpa pencilan). Data yang digunakan dalam studi kasus laporan skripsi ini adalah data kualitas air dari Laporan Hasil Uji di Balai Laboratorium Kesehatan Yogyakarta dengan periode pada tahun 2012.",,Kata Kunci : Pencilan; Analisis Regresi Robust; Estimasi-S; Pembobot Welsch; Pembobot Hampel
http://etd.repository.ugm.ac.id/home/detail_pencarian/149415,MODEL REGRESI ZERO INFLATED NEGATIVE BINOMIAL; ZERO INFLATED NEGATIVE BINOMIAL REGRESSION MODEL,"TRI MARTINI, Suryo Guritno",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi Poisson digunakan untuk menganalisis hubungan antara variabel independen dengan variabel dependen yang berupa data diskrit atau cacah. Regresi Poisson mengasumsikan bahwa nilai mean dan variansi pada variabel dependen mempunyai nilai yang sama. Akan tetapi, dalam penerapannya sering terjadi overdispersi. Overdispersi adalah kondisi dimana nilai variansi pada variabel dependen lebih besar dari nilai mean pada variabel dependen. Overdispersi dapat terjadi karena banyaknya jumlah observasi yag bernilai nol (excess zeros) pada variabel dependen. Salah satu penanganan overdispersi yang disebabkan oleh banyaknya jumlah observasi yang bernilai nol (excess zeros) pada variabel dependen adalah dengan menggunakan model regresi Zero Inflated Negative Binomial (ZINB). Estimasi parameter regresi ZINB dapat dilakukan dengan menggunakan EM algoritma. Uji kelayakan model dilakukan dengan menggunakan Likelihood Ratio Test. Dari hasil penelitian menunjukkan bahwa pemodelan regresi ZINB lebih baik dibandingkan menggunakan regresi Poisson dan ZIP.",,Kata Kunci : regresi poisson; overdispersi; excess zeros; ZINB; EM algoritma; ZIP
http://etd.repository.ugm.ac.id/home/detail_pencarian/149169,REGRESI TERBOBOTI GEOGRAFIS BAYES; BAYESIAN GEOGRAPHICALLY WEIGHTED REGRESSION,"MAULIA DARMASTUTI, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Bayesian Geographically Weighted Regression merupakan pendekatan regresi spasial dimana masing-masing variabel diberikan bobot sesuai dengan jarak antar lokasi pengamatan. Selain itu, analisis Bayesian Geographically Weighted Regression dapat menangani masalah pencilan atau ragam tidak konstan antar amatan yang belum tertangani oleh analisis Geographically Weighted Regression. Dengan memasukkan beberapa nilai prior, akan didapatkan model terbaik untuk data spasial yang memiliki efek heteroskedastik. Bobot yang digunakan untuk analisis regresi geografis ini antara lain Gaussian Kernel, Bisquare Kernel, Adaptive Bisquare Kernel, dan Adaptive Gaussian Kernel. Untuk menghitung bobot, diperlukan bandwidth yang merupakan ukuran jarak fungsi pembobot dan sejauh mana pengaruh lokasi terhadap lokasi lain, dimana nilai bandwidth optimal dapat diperoleh dengan Cross Validation (CV).",,"Kata Kunci : Gaussian Kernel; Bisquare Kernel; Adaptive Bisquare Kernel; dan Adaptive Gaussian Kernel, Bandwidth, Geographically Weighted Regression, Bayesian Geographically Weighted Regression; prior; Gibbs Sampling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148661,ANALISIS CHURN PADA PELANGGAN TELEKOMUNIKASI MENGGUNAKAN ALGORITMA C4.5; TELECOMMUNICATIONS CUSTOMERCHURN ANALYSIS USING C4.5 ALGORITHM,"Lintang Gustika Paratu, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Industri telekomunikasi seluler telah berkembang pesat, persaingan antar provider menjadi sangat ketat. Dibutuhkan alat analisis yang akurat untuk mempertahankan pelanggan lama bersamaan dengan mendapatkan pelanggan yang baru. Pada prakteknya mendapatkan pelanggan baru membutuhkan cara yang lebih susah daripada mempertahankan, membutuhkan biaya yang lebih besar, dan cara yang lebih atraktif. Pada skripsi ini akan dibahas bagaimana analisis untuk mengetahui pelanggan yang bagaimana yang akan pindah (churn) dari provider tersebut, sehingga lebih lanjut akan ditentukan mana pelanggan yang akan dipertahankan oleh provider tersebut. Dalam skripsi ini akan menggunakan alat analisis pengklasifikasian dengan nama Algoritma C4.5. Algoritma C4.5 ini dirasa cukup akurat untuk mengklasifikasikan apakah pelanggan akan pindah (churn) atau tidak dengan tampilan pohon keputusan agar mudah dipahami kebanyakan orang pada umumnya.",,Kata Kunci : Telco-Churn; Algoritma C4.5; pohon keputusan; data mining; model klasifikasi
http://etd.repository.ugm.ac.id/home/detail_pencarian/149174,REGRESI ROBUST dengan ESTIMASI - S; ROBUST REGRESSION with S - ESTIMATION,"Muhammad Helmi Hakim, Sri Haryatmi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Pencilan dapat memengaruhi estimator kuadrat terkecil, maka kita dapat menggunakan regresi robust dan salah satu metodenya adalah estimasi-S. Metode ini menemukan suatu garis yang meminimalkan estimasi robust dari sakala residualnya. Metode ini mempunyai breakdown point yang tinggi (50%), dan robust terhadap pencilan pada respon. Studi kasus diberikan untuk mengilustrasikan pendeteksian pencilan, menemukan estimator kuadrat terkecil dan estimator-S, dan membandingkan standard error dari model regresi untuk menemukan model regresi terbaik antara estimasi kuadrat terkecil dan estimasi-S. Data yang digunakan pada studi kasus adalah kualitas air dari Laporan Hasil Uji di Balai Laboratorium Kesehatan Yogyakarta dengan periode pada tahun 2012.",,Kata Kunci : Pencilan; Regresi robust ; Estimasi-S; Breakdown point; Standard error.
http://etd.repository.ugm.ac.id/home/detail_pencarian/148932,REGRESI KUANTIL DENGAN METODE ESTIMASI BAYESIAN; BAYESIAN REGRESSION QUANTILE,"LATIFAH RATNAWATI, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi kuantil merupakan teknik statistika yang digunakan untuk menduga hubungan antara variabel respon dengan variabel prediktor dengan cara mengestimasi kuantil kondisional pada berbagai nilai proporsi dari variabel respon. Pendekatan Bayesian dalam regresi kuantil digunakan karena Asymptotic inference tidak diperlukan, sehingga regresi kuantil dapat digunakan untuk semua ukuran sampel. Pendekatan Bayesian untuk regresi kuantil dengan membentuk fungsi likelihood didasarkan pada distribusi Laplace asimetris. Dengan menggunakan Markov chain Monte Carlo (MCMC) metode Bayesian relative lebih mudah mendapatkan distribusi posterior, bahkan dalam situasi yang kompleks.",,Kata Kunci : Regresi Kuantil; Inferensi Bayesian; Distribusi Laplace Asimetris; Markov chain Monte Carlo
http://etd.repository.ugm.ac.id/home/detail_pencarian/149447,ANALISIS BAYESIAN UNTUK REGRESI KUANTIL DENGAN MENGGUNAKAN ALGORITMA GIBBS SAMPLING; BAYESIAN ANALYSIS FOR QUANTILE REGRESSION USING GIBBS SAMPLING ALGORITHM,"ANNISA’ HANIF, Subanar",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi kuantil mendapatkan perhatian yang tinggi baik dari segi teoritis maupun dari sudut pandang empiris. Ini adalah suatu prosedur statistik dengan meminimalkan jumlahan dari asymmetrically weighted absolute dan dapat digunakan untuk memeriksa hubungan antara kuantil dari distribusi variabel dependen. Regresi kuantil dapat digunakan untuk mengatasi keterbatasan regresi linear dalam menganalisis sejumlah data yang berbentuk lonceng tidak simetris dan regresi kuantil sangat berguna jika distribusi data tidak homogen. Regresi Kuantil dapat diestimasi menggunakan metode Bayesian. Metode Bayesian adalah metode analisis yang berdasarkan pada informasi yang berasal dari sampel dan informasi prior. Gabungan informasi ini disebut posterior. Untuk mencari distribusi posterior seringkali menghasilkan perhitungan yang tidak dapat diselesaikan secara analitis sehingga digunakan pendekatan Gibbs sampling. Estimasi parameter dari model adalah mean dari distribusi posterior yang diperoleh dari proses Gibbs sampling tersebut. Dalam skripsi ini dibahas regresi kuantil menggunakan asymmetric Laplace distribution dari sudut pandang Bayesian. Digunakan algoritma Gibbs sampling untuk mencari estimator dari model regresi kuantil berdasarkan parameter lokasi dan skala dari mixture representation asymmetric Laplace distribution. Studi kasus dalam skripsi ini membahas faktor apa saja yang mempengaruhi harga emas. Hasil estimasi regresi kuantil dengan metode Bayesian akan dibandingkan dengan regresi linear menggunakan metode OLS, dan dibandingkan dengan metode regresi kuantil. Lalu didapatkan kesimpulan bahwa metode Bayesian lebih baik daripada estimasi yang lainnya.",,Kata Kunci : Regresi Kuantil; Bayesian; Gibbs sampling; Asymmetric Laplace Distribution
http://etd.repository.ugm.ac.id/home/detail_pencarian/149192,ANALISIS PARTIAL LEAST SQUARES REGRESI (PLS-R); PARTIAL LEAST SQUARES REGRESSION (PLS-R) ANALYSIS,"INGGRIT RABERTA, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode Partial Least Squares Regresi (PLS-R) adalah teknik regresi linear antara variabel prediktor X yang bersifat multivariat dengan variabel respon Y. Model linear optimal PLS-R dibangun dari hasil reduksi variabel prediktor X. Proses reduksi tersebut menghasilkan variabel baru yang disebut sebagai komponen utama. Proses estimasi parameter-parameter yang ada pada model regresi ini, digunakan Algoritma Nonlinear Iterative Partial Least Squares (NIPALS). Metode PLS-R tersebut akan digunakan untuk studi kasus mengenai faktor-faktor yang mempengaruhi Pendapatan Asli Daerah (PAD) Kabupaten Sleman. Data menunjukkan adanya multikolinearitas, sehingga pada kondisi ini regresi OLS tidak cocok digunakan, dan digunakan PLS-R sebagai suatu alternatif dalam mengatasi multikolinearitas data. Hasil analisis studi kasus disimpulkan bahwa metode PLS-R mampu menangani masalah multikolinearitas.",,Kata Kunci : Partial Least Square Regresi; PLS-R; komponen utama; NIPALS; regresi berganda; Pendapatan Asli Daerah; multikolinearitas.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149448,PORTOFOLIO OPTIMAL DENGAN METODE BEST BETA CAPM ( Studi Kasus Pada Saham – Saham LQ-45 Periode 2010 – 2013 );OPTIMAL PORTFOLIOS WITH BEST BETA CAPM,"ASTRIANI KUSUMANINGRUM, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Masalah 'best-beta' muncul yaitu pada potensi kesalahan pada model asset pricing Sharpe-Lintner-Hitam (CAPM) yang telah diakui. Dengan memasukkan variable target kedalam preferensi investor, diperoleh suatu best beta CAPM yang membahas perbandingan teori CAPM dan analisis sederhana dalam meningkatkan akurasi penetapan harga. Pengamatan empiris menunjukkan bahwa BCAPM diharapkan memprediksi hasil yang lebih baik dibandingkan dengan CAPM yaitu sekitar 20% sampai 30% per tahun. Kita tidak dapat menemukan, kita bisa setidaknya memperbaiki, kita dapat memberikan sedikit perkembangan.",,Kata Kunci : Resiko; Capital Asset Pricing Model ( CAPM ); Beta; Portofolio
http://etd.repository.ugm.ac.id/home/detail_pencarian/149452,REGRESI ROBUST DENGAN ESTIMASI GENERALIZED M; ROBUST REGRESSION WITH GENERALIZED M ESTIMATION,"M. SULTONI ARDHI, Subanar",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi merupakan suatu analisis statistik untuk mempelajari hubungan antara variabel dependen dengan variabel independen. Metode kuadrat terkecil merupakan salah satu metode yang sering digunakan untuk mengestimasi parameter dalam analisis regresi linear. Namun apabila terdapat pencilan pada data, dapat menyebabkan asumsi dalam regresi linear tidak terpenuhi. Salah satu cara untuk mengatasi masalah tersebut adalah tetap menggunakan seluruh data, tetapi dengan memberikan bobot yang rendah untuk observasi yang terindikasi sebagai pencilan, metode ini dikenal dengan nama metode regresi robust. Terdapat beberapa metode estimasi dalam regresi robust, salah satunya adalah estimasi GM (Generalized M). Estimasi GM merupakan pengembangan dari estimasi M untuk mengatasi kekurangan dari estimasi M ketika pencilan terdapat pada variabel independen, karena estimasi M hanya dapat mengatasi pencilan pada variabel dependen. Estimasi GM dapat digunakan untuk mengatasi pencilan pada variabel dependen dan variabel independen. Di dalam Tugas Akhir ini, koefisien determinasi dari metode estimasi GM akan dibandingkan dengan metode kuadrat terkecil dan estimasi M. Hasilnya, estimasi GM dapat menghasilkan model yang lebih baik dibandingkan dengan metode kuadrat terkecil dan estimasi M.",,Kata Kunci : Regresi Linear; MKT; Regresi Robust; Pencilan; Identifikasi Pencilan; Estimasi GM; Estimasi M
http://etd.repository.ugm.ac.id/home/detail_pencarian/149709,ANALISIS BAYESIAN UNTUK REGRESI SPLINER TERPENALTI; BAYESIAN ANALYSIS FOR PENALIZED SPLINE REGRESSION,"RIKA FITRIANI, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,N,,Kata Kunci : Regresi spline terpenalti; Regresi sline terpenalti dengan metode bayesian gibbs sampling
http://etd.repository.ugm.ac.id/home/detail_pencarian/149455,ESTIMASI MAKSIMUM LIKELIHOOD PADA MODEL GROWTH CURVE DAN APLIKASINYA PADA MODEL GROWTH CURVE LINEAR; MAXIMUM LIKELIHOOD ESTIMATION MODEL OF GROWTH CURVE MODEL AND APPLICATION OF LINEAR GROWTH CURVE,"FAUZIA FATMANINGRUM, Sri Haryatmi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Model growth curve merupakan model Generalized Multivariate Analysis of variance (GMANOVA) yang khusus digunakan untuk menganalisis masalah pertumbuhan pada runtun waktu yang pendek. Untuk kasus pertumbuhan subjek secara linear, digunakan model growth curve linear. Tujuan dari analisis model growth curve adalah pembandingan parameter untuk mengetahui variabel respon yang diamati pada suatu runtun waktu tertentu, pada beberapa grup yang ada. Dimana pada beberapa bidang ilmu seperti ilmu hewan, hortikultura, uji klinis, ilmu kedokteran, psikologi, eksperimen psikologi, biologi, dan sosial diperlukan analisis mengenai masalah pembandingan pertumbuhan suatu subjek yang diamati ulang dalam runtun waktu tertentu untuk kasus lebih dari 2 grup atau perlakuan. Masalah tersebut mempunyai kemiripan dengan masalah pada MANOVA, hanya saja variabel responnya ditetapkan sebagai pertumbuhan subjek runtun waktu tertentu atau yang biasa disebut kasus repeated measure. Repeated measure mengacu pada situasi di mana pengukuran beberapa variabel respon yang diperoleh, selama beberapa periode waktu, dari setiap unit eksperimental. Biasanya, respon yang diambil dari waktu ke waktu, dalam waktu mingguan / bulanan. Dalam skripsi ini langkah yang digunakan untuk menganalisis growth curve adalah pengujian asumsi model growth curve, estimasi parameter model growth curve menggunakan metode Maximum likelihood Estimator (MLE) untuk mengestimasi parameter model growth curve, dan pembandingan parameter model growth curve.",,Kata Kunci : Model Growth Curve Linear; Maximum likelihood Estimator; Generalized Multivariate Analysis of Variance; Multivariate Analysis of Variance; Repeated Measure
http://etd.repository.ugm.ac.id/home/detail_pencarian/148949,PENENTUAN HARGA OPSI BELI TIPE EROPA DENGAN MENGESTIMASI NILAI IMPLIED VOLATILITY MENGGUNAKAN METODE NEWTON-RAPHSON DAN BISECTION;,"FAHMI NURHUDA, Sardjono",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Opsi call tipe Eropa adalah salah satu contoh dari Sekuritas derivatif. Model revolusioner untuk menghitung nilai opsi call Eropa adalah formula Black- Scholes. Untuk menghitung nilai opsi dengan menggunakan formula Black- Scholes diperlukan lima parameter, yaitu: harga aset saat awal (S0), harga patokan opsi (K), waktu jatuh tempo opsi (T), suku bunga bebas risiko (r), dan volatilitas harga asset dasar (?). Semua parameter dalam formula Black-Scholes dapat diketahui secara langsung, kecuali volatilitas (?). Dalam skripsi ini akan dibahas bagaimana mengestimasi volatilitas dengan menggunakan implied volatility, yaitu nilai volatilitas yang di dapat dengan menggunakan bantuan nilai opsi yang didapat dari pasar dengan metode numerik, yaitu metode Bisection dan metode Newton-Raphson. Hasil dari nilai implied volatility ini akan digunakan untuk menentukan harga wajar opsi call tipe Eropa pada periode selanjutnya",,Kata Kunci : Black-Scholes; Implied Volatility; Bisection; Newton-Raphson; Opsi call Eropa
http://etd.repository.ugm.ac.id/home/detail_pencarian/149461,APLIKASI SUKU BUNGA MODEL COX-INGERSOLL-ROSS (CIR) DALAM PERHITUNGAN PREMI ASURANSI JIWA DWIGUNA; THE APPLICATION OF COX – INGERSOLL - ROSS (CIR) INTEREST RATE MODEL IN PREMIUM CALCULATION OF ENDOWMENT LIFE INSURANCE,"HERMAWATI SETIYANINGSIH, Yunita Wulan Sari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,Asuransi jiwa dwiguna adalah kombinasi antara asuransi jiwa berjangka dan asuransi jiwa dwiguna murni. Polis ini menjanjikan pembayaran manfaat kepda ahli waris tertanggung bila tertanggung meninggal dalam jangka waktu mengikuti polis atau pembayaran manfaat kepada tertanggung bila ia hidup sampai akhir masa kontrak asuransi. Jadi artinya polis asuransi jiwa dwiguna memiliki dua elemen yaitu perlindungan jiwa dan tabungan sehingga tertanggung dan ahli waris dapat memperoleh manfaat. Perhitungan premi asuransi jiwa biasanya dengan asumsi suku bunga bergerak tetap sepanjang waktu. Asumsi ini tidak sesuai dengan kenyataan bahwa suku bunga bergerak secara fluktuatif. Sehingga diperlukan model tingkat suku bunga stokastik yang telah mempertimbangkan pergerakan tingkat suku bunga secara fluktuatif. Pada skripsi ini akan dihitung harga premi tahunan asuransi jiwa dwiguna untuk tingkat bunga mengikuti model CIR. Model Cox-Ingersoll-Ross (CIR) merupakan salah satu model stokastik yang menggambarkan perubahan tingkat bunga untuk jangka waktu yang pendek. Model suku bunga CIR merupakan perluasan dari model suku bunga Vasicek. Model suku bunga CIR mengasumsikan suku bunga selalu positif. Parameterparameter model CIR akan diestimasi dengan metode estimasi kuadrat terkecil bersyarat.,,Kata Kunci : suku bunga; asuransi jiwa dwiguna; Cox-Ingersoll- Ross (CIR); estimasi kuadrat terkecil bersyarat
http://etd.repository.ugm.ac.id/home/detail_pencarian/149208,KLASIFIKASI DAN PREDIKSI KEPUTUSAN CREDIT SCORING BERDASARKAN KLASIFIER NAIVE BAYES; CLASSIFICATION AND PREDICTION OF DECISION CREDIT SCORING BASED ON NAIVE BAYES CLASSIFICATION,"DYAH RETNA WULANDARI, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Data mining adalah suatu ilmu yang bertujuan untuk menganalisis dan mencari pola atau hubungan tertentu dari sekumpulan data dalam ukuran besar melalui data-data historis. Hasil keluaran data mining ini adalah berupa pola atau hubungan yang bisa memperbaiki keputusan di masa depan. Salah satu pola atau hubungan tersebut adalah pengklasifikasian credit scoring. Credit scoring ini bertujuan untuk mengetahui karakteristik seorang debitur apakah Ia termasuk golongan Good atau Bad dengan melihat data historis. Berbagai cara dapat digunakan untuk memodelkan credit scoring, namun ada juga metode yang memberikan nilai keakuratan kecil. Pada tugas akhir kali ini akan dibahas Naive Bayes Classification sesuai dengan literatur yang dikumpulkan. Metode ini dapat memberikan keakuratan yang cukup baik dengan data training kecil. Selain itu, metode ini juga mampu mendeteksi eror lebih cepat. Naive Bayes Classification ini adalah suatu metode klasifikasi dengan mengasumsikan variabel-variabelnya independen. Selain itu Naive Bayes Classification ini menggunakan teori dasar bayes. Hasil analisis tersebut berupa probabilitas masing-masing variabel yang akan digunakan untuk memprediksi keputusan masa depan. .",,Kata Kunci : data mining; Naive Bayes Classification; Bayessian; credit scoring.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149467,ANALISIS BAYESIAN PADA REGRESI LOGISTIK MULTIVARIAT DENGAN ALGORITMA MCMC RANDOM WALK METROPOLIS; BAYESIAN ANALYSIS OF MULTIVARIATE LOGISTIC REGRESSION WITH MCMC RANDOM WALK METROPOLIS ALGORITHM,"ARIF MARJUKI, Herni Utami",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Di banyak area aplikasi, seperti epidemiologi dan penelitian biomedis, regresi logistik merupakan pendekatan standar untuk analisis data biner maupun data kategorik. Pendekatan umum untuk data jenis ini dapat digunakan pendekatan generalized estimating equation (GEE, Zeger dan Liang, 1986). Meskipun pendekatan GEE memecahkan masalah data biner ataupun data kategorik, namun pendekatan ini bergantung pada asumsi sampel besar.Dalam skripsi ini menggunakan pendekatan metode Bayesian untuk mengestimasi data biner. Pendekatan Bayesian sering kali menghasilkan perhitungan yang rumit dimana melalui integrasi numerik dengan dimensi integral yang cukup besar. Dengan menggunakan algoritma Markov Chain Monte Carlo (MCMC) didapatkan perkiraan distribusi posterior yang tepat, algoritma ini juga tidak memerlukan pembenaran asumsi sampel besar. Algoritma ini juga menghasilkan perhitungan yang cepat dan efisien. Metode estimasi Bayesian melibatkan informasi prior dari parameter yang digunakan. Skripsi ini termotivasi oleh kebutuhan untuk mengembangkan metode Bayesian pada regresi logistik multivariat dengan distribusi prior noninformatif",,Kata Kunci : Multivariate binary data; Logistic regression; Bayesian statistic; MCMC algorithm; Metropolis-Hasting algorithm.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149472,ANALISIS REGRESI HAZARD ADITIF DENGAN MODEL LIN DAN YING; (ADDITIVE HAZARD REGRESSION ANALYSIS WITH LIN AND YING MODEL),"RAHMASARI NUR AZIZAH, Danardono",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Data antar kejadian (data survival) merupakan data yang berupa lama waktu hingga suatu kejadian terjadi. Apabila waktu kejadian dipengaruhi oleh satu atau beberapa variabel yang lain, maka dapat digunakan analisis regresi untuk memodelkan pengaruh dari variabel independen tersebut. Salah satu analisis regresi yang dapat digunakan adalah analisis regresi hazard aditif dengan model Lin dan Ying. Pada model hazard aditif Lin dan Ying, koefisien regresi bersifat konstan, nilainya tidak bergantung pada waktu. Metode yang digunakan untuk mengestimasi koefisien regresi pada model ini menyerupai maximum partial likelihood pada regresi Cox. Estimasi dari koefisien regresi dapat diperoleh dari persamaan score equation yang didapat dengan meniru score equation model Cox. Score equation model Cox merupakan turunan dari log partial likelihoodnya. Dalam skripsi ini, analisis regresi hazard aditif dengan model Lin dan Ying digunakan untuk menganalisis variabel-variabel yang mempengaruhi kegagalan pengobatan pasien penderita penyakit TBC di Puskesmas Mantang, Lombok Tengah. Risk difference juga dihitung untuk menjelaskan pengaruh masing-masing variabel tersebut. Diberikan pengerjaan alternatif menggunakan analisis regresi hazard dengan model Aalen, dimana grafik dari fungsi regresi kumulatifnya digunakan untuk menginterpretasikan pengaruh dari variabelvariabel independen terhadap kegagalan pengobatan. Dapat terlihat bahwa analisis regresi hazard aditif dengan menggunakan model Lin dan Ying memiliki kemudahan dalam hal interpretasi pengaruh dari masing-masing variabel, jika dibandingkan dengan analisis regresi hazard aditif dengan model Aalen.",,Kata Kunci : data antar kejadian; analisis regresi hazard aditif; model Lin dan Ying; maximum partial likelihood; regresi Cox; analisis regresi hazard dengan model Aalen
http://etd.repository.ugm.ac.id/home/detail_pencarian/149986,ANALISIS DATA MINING CUACA MENGGUNAKAN ATURAN ASOSIASI DAN REGRESI LOGISTIK KERNEL,"HEBNU PRIYAMBODO, Gunardi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,Pengetahuan tentang pola dan hubungan memegang peranan sangat penting dalam pengambilan kebijakan pada bidang pertanian. Tingkat curah hujan merupakan salah satu faktor yang mempengaruhi produktifitas tanaman komoditas pangan. Aturan asosiasi (association rule) dan Regresi logistik kernel (Kernel Rgresson Logistik) merupakan teknik data mining untuk membantu dalam pengambilan suatu keputusan. Pemanfaatan model dalam mengggambarkan suatu probabilitas serta menemukan aturan-aturan yang terjadi dalam data jumlah besar. Penulis melakukan penelitian terhadap data yang diperoleh dari Badan Meteorologi Klimatologi dan Geofisika (BMKG) serta Badan Pusan Statistik (BPS). Tugas akhir ini memberikan kesimpulan bahwa regresi logistik kernel dengan bantuan grafik dapat menghasilkan pola tingkat curah hujan serta gambaran pada bidang pertanian. Sedangkan association rule menghasilkan aturan-aturan tersembunyi yang terdapat dalam suatu data set.,,"Kata Kunci : Curah hujan, Regresi logistik kernel, Aturan sosiasi, Data mining, Komoditas pangan, data set."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149475,MODEL STOKASTIK BERDASARKAN TEKNIK CHAIN-LADDER; A STOCHASTIC MODEL UNDERLYING THE CHAIN-LADDER TECHNIQUE,"GALANG YUNAWAN, Danardono",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Skripsi ini menyajikan pembentukan model statistik berdasarkan teknik chainladder. Proses pembentukan model statistik ini memanfaatkan beberapa asumsi dan pendekatan, baik yang bersifat deterministik maupun stokastik, dalam menentukan estimasi cadangan klaim Incurred but Not Reported (IBNR). Model ini dituangkan dalam bentuk generalized linear model yang menggunakan asumsi awal distribusi Poisson untuk incremental claim amounts. Selain itu, metode ini juga dapat mengatasi masalah negative incremental claims dengan memanfaatkan algoritma Verbeek yang merupakan pendekatan model Poisson. Teknik ini memberikan alternatif bagi perusahaan asuransi untuk menentukan secara tepat estimasi cadangan klaim di waktu yang akan datang.",,Kata Kunci : MOdel stokastik; Teknik Chain-Ladder
http://etd.repository.ugm.ac.id/home/detail_pencarian/149991,PEMODELAN PERSAMAAN REGRESI SPLINE KUADRATIK DENGAN MENENTUKAN TITIK - TITIK KNOT YANG OPTIMAL (Studi Kasus Pertambahan Persentase Penduduk dengan Persentase Penerimaan Tenaga Kerja Baru),"TRI EFENDI, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi digunakan untuk melihat pengaruh variabel prediktor terhadap variabel respon dengan sebuah kurva regresi. Pendekatan yang digunakan untuk menentukan kurva regresi yaitu pendekatan parametrik dan nonparametrik. Regresi spline merupakan salah satu model pendekatan kearah pengepasan data dengan tetap memperhitungkan kemulusan kurva regresi, yang merupakan modifikasi dari fungsi polynomial tersegmen. Bentuk estimator spline sangat di pengaruhi oleh nilai parameter penghalus ? yang pada hakekatnya adalah penentuan lokasi titik – titik knot. Pemilihan ? optimal merupakan persoalan yang sangat penting dalam estimasi regresi spline. Model regresi spline kuadratik diterapkan pada suatu study kasus mengenai persentase pertumbuhan penduduk Indonesia dan persentase penerimaan tenaga kerja baru di Indonesia dengan memilih nilai MSE dan GCV( Generalized Cross Validation ) yang optimum.",,"Kata Kunci : PERSAMAAN REGRESI SPLINE KUADRATIK, TITIK - TITIK KNOT"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149482,PERBANDINGAN OPTIMISASI PORTOFOLIO METODE MEANVARIANCE DENGAN METODE MEAN-SEMIVARIANCE; COMPARISON PORTFOLIO OPTIMIZATION MEAN-VARIANCE METHOD WITH MEAN-SEMIVARIANCE METHOD,"Septi Wahyuni, Yunita Wulansari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Pada tahun 1952 Markowitz memelopori penggunaan metode Mean-Variance untuk permasalahan optimisasi portofolio, yang hingga saat ini metode Mean- Variance sangat populer untuk digunakan. Namun, metode Mean-Variance ini memiliki kekurangan bahwa data return harus berdistribusi normal. Faktanya, sangat sulit mendapatkan data saham yang memiliki return berdistribusi normal. Markowitz (1959) berpendapat bahwa “analisis berdasarkan semivariansi cenderung menghasilkan portofolio yang lebih baik dibandingkan portofolio berdasarkan variansi”. Walaupun begitu, mengapa analisis portofolio dengan Mean-Variance lebih sering digunakan daripada Mean-Semivariance? Hal ini dikarenakan, tidak seperti matriks variansi-kovariansi yang bersifat simetrik dan eksogen, matriks semivariansi-semikovariansi bersifat tidak simetrik dan endogen. Sehingga dalam penghitungan bobot harus digunakan algoritma numerik yang jarang digunakan oleh para praktisi dan akademisi. Oleh karena itu, digunakanlah pendekatan heuristik yang berfungsi untuk mengubah matriks semivarian-semikovarian menjadi simetrik dan eksogen. Sehingga penghitungan bobot portofolio Mean-Semivariance bisa menggunakan metode yang sama dengan Mean-Variance. Optimisasi portofolio menggunakan Mean-Semivariance tidak memerlukan asumsi distribusi apapun, sehingga lebih mudah penggunaannya dibandingkan Mean-Variance. Penghitungannya pun mudah dan dengan pendekatan heuristik dihasilkan matriks semivarian-semikovarian yang memiliki bentuk dan penyelesaian yang sama dengan matriks varian-kovarian milik metode Mean- Variance. Pada skripsi ini akan dilakukan perbandingan empiris antara optimisasi portofolio Mean-Semivariance dengan optimisasi portofolio Mean-Variance. Lalu dalam studi kasus dilakukan pembentukan portofolio Mean-Variance dan juga portofolio Mean-Semivariance dengan kombinasi dari beberapa aset finansial yang berupa saham.",,Kata Kunci : portofolio; Mean-Semivariance; pendekatan heuristik
http://etd.repository.ugm.ac.id/home/detail_pencarian/149996,PORTOFOLIO OPTIMAL MENGGUNAKAN CAPITAL ASSET PRICING MODEL DENGAN KEYAKINAN HETEROGEN,"ATHIKAH DIAN ALFARIZY, Dedi Rosadi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Investasi merupakan suatu bentuk penanaman modal dengan harapan nantinya akan mendapatkan keuntungan. Semakin tinggi pengaharapan investor akan keuntungan, semakin tinggi pula risiko yang dihadapi. Untuk dapat meminimalkan risiko dalam investasi, investor dapat melakukan portofolio (diversifikasi) saham yaitu dengan melakukan investasi pada banyak saham sehingga risiko kerugian pada satu saham dapat ditutup dengan keuntungan pada saham yang lainnya. Salah satu model faktor tunggal yang dapat digunakan untuk mencari bobot masingmasing saham pada portofolio adalah Capital Asset Pricing Model (CAPM) yang didasarkan pada asumsi bahwa investor memiliki keyakinan yang homogen tentang mean dan kovariansi aset berisiko. Asumsi ini telah diperdebatkan oleh banyak pihak dari tahun ke tahun, karena dinilai tidak realistis dan tidak sesuai dengan pasar yang sebenarnya, di mana setiap investor memiliki keyakinan yang berbeda-beda akan keuntungan dan risiko di periode mendatang. Maka, dicari portofolio saham dengan menggunakan CAPM dengan asumsi keyakinan investor yang heterogen untuk dibandingkan tingkat keuntungan dan risikonya dengan portfolio yang dicari dengan menggunakan CAPM pada asumsi homogen. Sehingga didapatkan bahwa portofolio yang didapatkan saat mengasumsikan pasar heterogen, menghasilkan pengembalian yang lebih besar dan risiko yang lebih kecil.",,"Kata Kunci : Saham, Portofolio, CAPM, Keyakinan Heterogen, Return, Aset Bebas Risiko"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149485,ANALISIS TEKNIKAL SAHAM DENGAN INDIKATOR STOCHASTIC RSI; (RELATIVE STRENGTH INDEX) STOCK TECHNICAL ANALYSIS USING INDICATOR OF STOCHASTIC RSI (RELATIVE STRENGTH INDEX),"MUHAMMAD RIO NUGRAHA, Abdurakhman",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis teknikal saham merupakan suatu metode analisis yang menggunakan pengujian atas harga di masa lampau untuk tujuan prediksi pergerakan harga di masa yang akan datang. Analisis teknikal saham baik digunakan para pelaku trading (trader) dalam membantu memberikan prediksi kapan waktu yang tepat untuk masuk dan keluar dari pasar saham. Terdapat berbagai macam indikator yang digunakan dalam analisis teknikal, salah satunya adalah Stochastic RSI (Relative Strength Index). Stochastic RSI (Relative Strength Index) merupakan analisi indikator yang perhitungan mirip dengan Stochastic Oscilator namun untuk data lowest dan highest biasanya diganti dengan data tertinggi dan terendah dari RSI (Relative Strenth Index) sehingga diprediksi dapat menghasilkan prediksi yang lebih tepat dari indikator lainnya, dimana dalam skripsi ini akan menggunakan periode 25 yang selanjutnya akan dibandingkan sinyal prediksi yang lebih tepat antara Stochastic Oscilator dan Stochastic RSI.",,Kata Kunci : Analisis teknikal; Stochastic Oscillator; RSI (Relative Strength Index); Stochastic RSI (Relative Strength Index).
http://etd.repository.ugm.ac.id/home/detail_pencarian/149745,SIMULASI PELUANG KEBANGKRUTAN UNTUK DISTRIBUSI KLAIM INVERSE GAUSSIAN PADA PROSES SURPLUS POISSON MAJEMUK; SIMULATION OF RUIN PROBABILITIES FOR INVERSE GAUSSIAN CLAIMSIZE DISTRIBUTION ON COMPOUND POISSON SURPLUS PROCESS,"DIONISIA SEKAR ROSARI, Adhitya Ronnie Effendi",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Suatu perusahaan asuransi mempunyai risiko kebangkrutan apabila cadangan dana yang dimiliki tidak mencukupi untuk membayar klaim, sebagai akibat dari pembayaran klaim-klaim yang melampaui besarnya total premi yang diterima oleh perusahaan tersebut. Laporan Tugas Akhir ini fokus membahas tentang estimasi probabilitias dimana pengeluaran suatu perusahaan asuransi melebihi pendapatannya. Laporan ini dimulai dengan beberapa definisi yang sering digunakan dalam Asuransi. Bagian pertama pada laporan ini berkonsentrasi pada proses surplus poisson majemuk dimana dalam model risiko tersebut didasari pada sifat penting dari proses Poisson ,yaitu waktu antar klaim merupakan distribusi eksponensial yang independen dan identik. Dari beberapa metode yang dapat digunakan untuk mengestimasi besar peluang kebangkrutan, dalam laporan ini akan digunakan metode simulasi. Metode simulasi digolongkan sebagai metode sampling karena input dibangkitkan secara acak dari suatu distribusi probabilitas untuk proses sampling dari populasi nyata. Dalam aplikasinya, akan ditentukan besar peluang kebangkrutan suatu perusahaan asuransi dimana besar klaim berdistribusi Inverse Gaussian pada proses surplus poisson majemuk dengan batas waktu tertentu",,Kata Kunci : Peluang Kebangkrutan; Proses Surplus Poisson Majemuk; Simulasi; Distribusi Inverse Gaussian; Proses Poisson
http://etd.repository.ugm.ac.id/home/detail_pencarian/148986,PENGARUH STRUKTUR KORELASI DALAM GENERALIZED ESTIMATING EQUATIONS; EFFECT OF CORRELATION STRUCTURE IN GENERALIZED ESTIMATING EQUATIONS,"WAHYU KARTIKA, Zulaela",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi merupakan alat statistik yang bermanfaat untuk mengetahui hubungan antara dua variabel atau lebih, sehingga salah satu variable dapat diduga dari variabel lainnya. Metode regresi cukup beragam, begitu pula metode estimasi parameter regresi. Generalized Estimating Equations (GEE) merupakan salah satu metode estimasi untuk regresi pada data longitudinal. Terdapat beberapa struktur korelasi dalam GEE, yakni: independence, exchangeable, autoregressive order 1 (AR1), dan unstructured. Pengaruh struktur korelasi dalam GEE adalah terdapat perbedaan nilai estimasi koefisien pada masing-masing struktur korelasi. Pemilihan struktur korelasi terbaik memiliki nilai QIC terkecil. Metode GEE diaplikasikan pada data real, yakni pada pemeriksaan ibu hamil di klinik bidan praktek swasta. Software yang digunakan untuk mengestimasi GEE adalah software R dengan package geepack.",,Kata Kunci : GEE; korelasi; independence; exchangeable; AR(1); unstructured
http://etd.repository.ugm.ac.id/home/detail_pencarian/148733,STRESS TESTING VALUE at RISK dengan SIMULASI MONTE CARLO; STRESS TESTING VALUE at RISK with MONTE CARLO SIMULATION,"Delly Novia Lestary, Dedi Rosad",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Value at Risk (VaR) adalah standar yang ditetapkan untuk mengukur risiko pasar. VaR mengukur kerugian yang diperkirakan terburuk dalam kondisi pasar normal selama suatu interval waktu tertentu pada tingkat kepercayaan yang diberikan. Namun dengan perkembangan kondisi pasar keuangan selama dua tahun terakhir akibat dari krisis subprime mortgage, pelaku pasar yakin bahwa pengukuran risiko pasar nilai VaR masih harus dilengkapi dengan analisis Stress Testing nilai VaR. Penelitian ini akan melakukan stress testing atas nilai VaR dari berbagai saham yang dihitung dengan simulasi monte carlo. Simulasi Monte Carlo dianggap metode yang baik sebab nilai acak yang dibangun diyakini selalu konvergen ke keadaan riilnya. Atas perhitungan nilai VaR, akan dilakukan analisis Scenario Stress Testing yang menggunakan kejadian historis sebagai kondisi ekstrim (‘stress event’).",,Kata Kunci : Value at Risk; Simulasi Monte Carlo; Stress Testing; StressVaR
http://etd.repository.ugm.ac.id/home/detail_pencarian/149502,PENDEKATAN FUNGSI DESIRABILITY SEBAGAI METODE OPTIMASI RESPON GANDA PADA METODOLOGI PERMUKAAN RESPON; DESIRABILITY FUNCTION APPROACH AS METHOD OF MULTIRESPONSE OPTIMIZATION ON RESPONSE SURFACE METHODOLOGY,"NISA NUNGNURFATHMA, Yunita Wulansari",2013 | Skripsi | PROGRAM STUDI STATISTIKA,"Metodologi Permukaan Respon merupakan suatu metode yang diterapkan dalam proses optimasi suatu variabel respon. Respon yang akan dioptimalkan dalam analisa permukaan respon pada umumnya adalah respon tunggal. Namun pada kenyataan, percobaan pada berbagai kasus dan penelitian melibatkan banyak respon penting yang juga harus dioptimalkan (respon ganda). Selain itu, respon – respon tersebut juga memiliki batasan – batasan atau nilai kendala yang harus dipenuhi. Salah satu metode untuk mengoptimalkan respon ganda dimana setiap respon memiliki batasan-batasan atau nilai kendala yang harus dipenuhi adalah dengan menggunakan pendekatan fungsi desirability. Fungsi ini bertujuan agar perlakuan yang diberikan menghasilkan nilai respon yang dapat mendekati atau mencapai nilai yang diharapkan (target) serta masih berada pada batasan nilai rentang yang diinginkan. Secara garis besar langkah-langkah dalam menganalisa response surface yaitu : merancang percobaan, membuat model dan melakukan optimalisasi. Dalam skripsi ini akan digunakan pendekatan fungsi desirability dalam optimasi respon dengan rancangan yang digunakan adalah rancangan Box- Behnken.",,Kata Kunci : Metodologi Permukaan Respon; Fungsi Desirability; Box-Behnken
http://etd.repository.ugm.ac.id/home/detail_pencarian/149762,"STRESS TESTING DENGAN PENDEKATAN VALUE-AT-RISK (Studi Kasus Pada Saham ATBL, BIIB, dan SNE Periode 2007-2009); STRESS TESTING USING VALUE-AT-RISK (VAR) APPROACH (Case Study On ATBL, BIIB, and SNE for The Period 2007-2009)","TIO ANTA WIBAWA, Abdurakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Perkembangan kondisi pasar keuangan selama dua tahun terakhir akibat dari krisis subprime mortgage berdampak sangat signifikan. Sehingga pelaku pasar yakin bahwa pengukuran risiko pasar nilai VaR masih harus dilengkapi dengan analisis Stress Testing nilai VaR. Analisis stress testing dilakukan dengan menghitung nilai VaR sesuai skenario yang ditetapkan sebagai kondisi ekstrim. Estimasi nilai stress testing VaR dengan model dalam penelitian menghasilkan metode perhitungan VaR yang disebut StressVaR. StressVaR didasarkan pada asumsi normalitas data, akan tetapi dalam kenyataanya pada saat krisis data cenderung berdistribusi fat tails sehingga diakomodasi dengan menggunakan distribusi student’s t. Pengintegrasian distribusi dalam StressVaR didapatkan estimasi VaR baru yang disebut StressVaR-X.",,"Kata Kunci : Stress testing, Value at Risk, StressVaR, StressVaR-x"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149769,ANALISIS TEKNIKAL SAHAM DENGAN INDIKATOR MOVING AVERAGE CONVERGENCE DIVERGENCE (MACD); STOCKS TECHNICAL ANALYSIS WITH MOVING AVERAGE CONVERGENCE DIVERGENCE (MACD) INDICATOR,"TINE ANGGRAINI RAMADHAN, Abdurakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis teknikal saham merupakan suatu metode pembacaan grafik data historis saham untuk mengetahui pergerakan harga saham. Analisis teknikal saham ini baik digunakan untuk para pelaku trading (trader) dalam menentukan kapankah mereka harus masuk dan keluar dari pasar saham. Terdapat berbagai macam indikator yang dapat digunakan dalam analisis teknikal saham, salah satu diantaranya adalah MACD (Moving Average Convergence Divergence). Indikator MACD beguna untuk menunjukkan trend harga saham yang sedang terjadi dan membantu menentukan daerah jenuh jual dan beli sebagai sinyal untuk trader untuk masuk dan keluar pasar saham. Perhitungan indikator MACD ini berbasis EMA (Exponential Moving Average) dimana EMA merupakan Moving Average (Pergerakan Rata-Rata) yang terboboti secara eksponensial, dengan periode standar yang dipakai adalah 9, 12, dan 26.",,"Kata Kunci : Saham, analisis teknikal, MACD, EMA."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149033,PENENTUAN PREMI TUNGGAL BERSIH ASURANSI JIWA DWIGUNA MURNI UNIT LINK DENGAN GARANSI FLEKSIBEL; NET SINGLE PREMIUM PRICING OF UNIT LINKED PURE ENDOWMENT LIFE INSURANCE WITH FLEXIBLE GUARANTEE,"SHINTA CANDRA SARI, Abdurakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Asuransi unit link merupakan suatu produk asuransi yang menggabungkan antara asuransi dan investasi. Manfaat dari asuransi unit link bervariasi karena sesuai dengan nilai aset investasinya. Untuk itu diperlukan garansi agar kerugian yang ditimbulkan tidak terlalu besar. Asuransi unit link dengan garansi itu sendiri pun terdiri dari berbagai macam diantaranya adalah asuransi jiwa unit link dengan garansi tetap dan asuransi jiwa unit link dengan garansi fleksibel. Jenis asuransi yang akan dibahas di sini adalah asuransi jiwa unit link dengan garansi fleksibel yang dibatasi pada jenis dwiguna murni. Pengertian dari asuransi dwiguna murni itu sendiri merupakan asuransi yang dalam kontraknya dikatakan akan memberikan sejumlah uang (disebut manfaat) kepada tertanggung apabila tertanggung masih hidup sampai kontrak berakhir dan tidak akan memberikan apaapa jika tertanggung meninggal sebelum kontrak berakhir. Asuransi jiwa unit link dengan garansi fleksibel merupakan asuransi unit link yang diinvestasikan pada 2 aset (yang dipakai dalam skripsi ini adalah saham) dimana aset pertama berperan sebagai aset yang beresiko, sedangkan aset kedua berperan sebagai garansinya. Manfaat dari asuransi ini adalah maksimal dari aset pertama dan kedua. Sifat-sifat dari asuransi jenis ini adalah semakin tinggi usia tertanggung maka harga premi akan semakin murah, sedangkan semakin panjang jangka waktu kontraknya maka harga premi akan semakin naik dan pada titik tertentu akan semakin turun.",,"Kata Kunci : Asuransi unit link, garansi fleksibel, dwiguna murni, premi tunggal bersih."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149810,THRESHOLD VECTOR ERROR CORRECTION MODEL (Studi Kasus : Penerapan Threshold Vector Error Correction Model dalam membantu penentuan kebijakan moneter bagi Bank Indonesia); THRESHOLD VECTOR ERROR CORRECTION MODEL (Case Study : Implementation of Threshold Error Correction Model to help the determination of monetary policy for Central Bank of Indonesia,"RIZAL YUDHA TAMA, Herni Utama",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Pada umumnya sebagian besar data pada bidang ekonomi, ialah data yang berkecenderungan memiliki efek tren ataupun musiman, atau sering disebut data yang non stasioner. Vector Autoregressive (VAR) merupakan model ekonometri time series yang digunakan untuk menjelaskan perubahan dinamis dalam data ekonomi, namun pemodelan dengan VAR harus mengisyaratkan data bersifat stasioner. Disamping itu terdapat Vector Error Correction Model (VECM) yang merupakan model VAR terbatas yang dapat mencover data non stasioner dan terkointegrasi. Threshold Error Correction Model (TVECM) ialah suatu model yang digunakan untuk menciptakan dua kondisi atau lebih yang dibatasi oleh nilai ambang (theshold) dari model VECM. Estimasi parameter dari TVECM menggunakan MLE dan dengan pendekatan algoritma Hansen-Seo (2002) untuk memperoleh nilai ambang (threshold). Pengujian adanya efek threshold dari model VECM digunakan Sup LM test yang dikembangkan oleh Hansen dan Seo (2002)",,"Kata Kunci : Vector Autoregressive, Kointegrasi, Vector Error Correction Model,Threshold Vector Error Correction Model"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149057,MIXED KUPIEC BACKTESTING UNTUK VALIDASI VALUE AT RISK,"TITIK WAHYUNINGSIH, Abdurakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,Value at Risk merupakan salah satu ukuran resiko yang populer dan memiliki peranan penting dalam manajemen resiko. Value at risk adalah nilai estimasi besarnya kerugian maksimum yang mungkin terjadi pada periode tertentu dengan tingkat kepercayaan tertentu dan dalam kondisi pasar yang normal untuk suatu investasi. Value at Risk perlu diuji kembali keakuratannya untuk melihat sejauh mana hasil perhitungan VaR dapat dipercaya keakuratannya yaitu dengan menggunakan metode yang dinamakan backtesting. Tugas akhir ini membahas konsep dari Mixed Kupiec Backtesting untuk validasi Value at Risk yang telah dicari dengan metode Normal VaR dan metode simulasi historis. Data yang digunakan adalah data saham Bank BRI. Untuk kasus data saham Bank BRI tanggal 2 Februari 2009 – 31 Mei 2010 disimpulkan bahwa metode simulasi historis dianggap lebih akurat daripada metode normal VaR setelah dilakukan uji Likelihood Ratio Mixed Kupiec Backtesting.,,"Kata Kunci : Backtesting, Likelihood Ratio, Manajemen Resiko, Mixed Kupiec Backtesting , Normal VaR, Simulasi Historis, Value at Risk"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149073,PENENTUAN PORTOFOLIO OPTIMAL MENGGUNAKAN MULTI-INDEX MODELS (Studi Kasus Pada Saham-Saham LQ-45 di BEI Periode 2009-2011),"RIDHO ZENI ARIEF, Suryo Guritno",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Multi-Index Models merupakan pengembangan dari Single-Index Model. Model ini berusaha untuk menjelaskan adanya beberapa pengaruh di luar pasar yang menentukan pergerakan harga sekuritas. Kelebihan dari model ini adalah penggunaan lebih dari satu indeks faktor yang dapat meningkatkan akurasi dari estimasi return, sedangkan kelemahan terjadi apabila terdapat kesalahan pemilihan indeks seperti adanya indeks faktor yang ternyata tidak cukup menjelaskan pergerakan return sekuritas. Skripsi ini akan membahas tentang pembentukan portofolio optimal menggunakan Multi-Index Models dan juga dilakukan studi kasus pada saham-saham LQ-45 di BEI Periode 2009- 2011 dengan indeks yang akan dimasukkan ke dalam Multi-Index Model adalah indeks IHSG, indeks Hang Seng, dan indeks S&P 500. Dalam studi kasus performa antara Multi- Index Models dan Single-Index Model akan dibandingkan secara langsung dengan menggunakan data yang sama, sehingga dapat diketahui metode mana yang memiliki performa yang baik.",,"Kata Kunci : portofolio optimal, multi-index models"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149845,"ANALISIS TEKNIKAL SAHAM MENGGUNAKAN BOLLINGER BANDS DAN TRIPLE EXPONENTIAL MOVING AVERAGE (TRIX) (Studi Kasus: Saham Sektor Minyak, Indeks AS); STOCK TECHNICAL ANALYSIS USING BOLLINGER BANDS AND TRIPLE EXPONENTIAL MOVING AVERAGE (TRIX) (Case Study: Stocks in Oil Sector, U.S. Index)","SITI ROSIANA, Abdurakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis teknikal merupakan studi tentang informasi internal pasar saham untuk meramalkan pergerakan harga saham dan kecenderungan pasar di masa mendatang dengan cara mempelajari grafik dan volume perdagangan saham serta memberikan rekomendasi mengenai saat yang tepat untuk membeli atau menjual saham. Skripsi ini membahas tentang analisis teknikal saham menggunakan indikator teknikal Bollinger Bands dan Triple Exponential Moving Average. Penelitian ini dibagi menjadi dua kategori, yaitu sistem yang menggunakan indikator Bollinger Bands dan sistem yang menggunakan dua indikator gabungan, yaitu Bollinger Bands dan Triple Exponential Moving Average. dilakukan pada lima saham sektor ninyak, indeks AS. Hasil penelitian yang didapat bahwa rata-rata transaksi sukses yang dihasilkan sistem gabungan dua indikator Bollinger Bands dan Triple Exponential Moving Average (BB_TRIX) lebih besar dibandingkan dengan sistem yang hanya menggunakan sistem Bollinger Bands (BB). Namun rata-rata laba yang dihasilkan sistem BB jauh lebih besar dibanding dengan sistem BB_TRIX. Hal ini dikarenakan sinyal transaksi yang dihasilkan sistem gabungan dua indikator lebih sedikit dibanding sistem yang menggunakan Bollinger Bands, sehingga berpengaruh terhadap laba yang dihasilkan. Modifikasi sistem yang tepat, seperti manajemen modal terbukti meningkatkan laba yang dihasilkan sistem BB_TRIX. Kedua sistem, baik BB maupun BB_TRIX sangat baik digunakan dalam analisis teknikal saham karena memberikan persentase prediksi benar lebih besar dari 50% dan menghasilkan laba yang lebih besar dibanding investasi dengan sistem beli dan memegang saham (buy and hold). Sistem BB cocok untuk investor yang menganut paham high risk, high return, sedangkan sistem BB_TRIX lebih cocok untuk investor yang menganut paham low risk, low return",,"Kata Kunci : Analisis Teknikal, Bollinger Bands, Triple Exponential Moving Average."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149859,OPTIMASI VARIABEL RESPON MENGGUNAKAN METODOLOGI PERMUKAAN RESPON – RANCANGAN BOX BEHNKEN (BBD); OPTIMIZATION OF RESPONSE VARIABLE USING RESPONSE SURFACE METHODOLOGY – BOX BEHNKEN DESIGN (BBD),"IMAM HUSNI TAMRIN, Yunita Wulan Sari",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Metodologi permukaan respon merupakan suatu metode yang digunakan untuk melihat hubungan antara variabel independen terhadap variabel dependen kemudian mengoptimalkan variabel dependen tersebut. Secara garis besar yang menjadi inti dari metodologi permukaan respon adalah merancang percobaan, membuat model, dan melakukan optimalisasi. Banyak aplikasi dari metodologi permukaan respon yaitu digunakan untuk berbagai macam kebutuhan seperti kontrol kualitas suatu produk, desain produk, dan digunakan di banyak penelitian. Metodologi permukaan respon memiliki banyak jenis rancangan, seperti Central Composite Design (CCD), Box-Behnken design (BBD), dan lain-lain. Dalam skripsi ini hanya akan di fokuskan kepada rancangan box behnken (BBD) dengan 3 variabel independen dimana nantinya variabel independen tersebut akan mengoptimalkan variabel dependen.",,"Kata Kunci : Metodologi Permukaan Respon, Rancangan Box Behnken."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149860,PENDUGA PENALTI GANDA LIKELIHOOD DALAM MODEL REGRESI LOGISTIK; DOUBLE PENALIZED LIKELIHOOD ESTIMATION IN LOGISTIC REGRESSION MODEL,"AULIA ISNADIA, Danardono",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi logistik merupakan metode yang sering digunakan pada model data biner. Akan tetapi separation dan multikolinieritas dapat terjadi pada model regresi logistik secara bersamaan. Firth (1993) menyarankan penalti Firth untuk menangani adanya separation dan le Cessie dan van Houwelingen (1992) menyarankan regresi ridge digunakan untuk menangani adanya mutikolinieritas pada regresi logistik. Akan tetapi kedua metode tersebut digunakan hanya untuk masing-masing permasalahan. Shen dan Gao (2008) menyarankan untuk memasukkan penalti kedua yaitu parameter ridge ke penalti Firth. Metode tersebut kemudian disebut dengan double penalized likelihood estimation. Dalam skripsi ini akan dilakukan analisis terhadap data penentuan pemenang penghargaan Cy Young. Pada analisis regresi logistik yang dilakukan terhadap data tersebut menghasilkan variansi yang sangat besar dan setelah dilakukan pengecekan diketahui adanya separation dan multikolinieritas pada data. Untuk menanganinya dilakukan dengan menggunakan metode double penalized likelihood estimation. Metode tersebut menghasilkan estimasi yang lebih stabil dan variansi yang lebih kecil,sehingga metode ini cocok untuk menangani permasalahan tersebut.",,"Kata Kunci : regresi logistik, separation, multikolinieritas, penalti Firth, parameter ridge, double penalized."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149871,MANAJEMEN PORTOFOLIO DENGAN INCREMENTAL VALUE AT RISK (IVaR) DAN COMPONENT RISK (CVaR); PORTFOLIO MANAGEMENT WITH INCREMENTAL VALUE AT RISK (IVaR) AND COMPONENT RISK (CVaR),"GEMPUR SAFAR, Suryo Guritno",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Portofolio pada dasarnya berkaitan dengan bagaimana mengalokasikan sejumlah saham ke dalam suatu invetasi untuk mencapai keuntungan yang optimal. Dengan melakukan diversifikasi, investor dapat mengurangi tingkat risiko dan sekaligus mengoptimalkan tingkat pengembalian yang diharapkan. Portofolio optimal yang terbentuk menggunakan Model Indeks Tunggal yang menggunakan nilai ERB dan nilai C* untuk mendapatkan saham pembentuk portofolio optimal dan proporsi masing-masing aset pada portofolio. Value at Risk (VaR) adalah nilai yang paling banyak digunakan untuk mengukur risiko di bidang keuangan dalam memperkirakan risiko kerugian dari suatu portofolio. Namun, terkadang VaR tidak cukup untuk menentukan risiko yang diberikan oleh setiap saham yang termasuk dalam portofolio optimal dan ketika komposisi dari portofolio berubah (yaitu satu aset ditambahkan atau dihilangkan). Berdasarkan hal tersebut, tugas akhir ini akan menyajikan tentang pengukuran risiko yang diberikan oleh masing-masing aset dalam portofolio yang membawa kita pada definisi Risiko Komponen (CVaR) dan perubahan VaR ketika satu atau lebih saham ditambahkan atau dikeluarkan dari portofolio optimal yang selanjutnya kita sebut sebagai Risiko Tambahan (IVaR). IVaR diestimasi dengan menggunakan pendekatan Brute-Force , Pendekatan Garman delVaR, Pendekatan IVaR berdasarkan VaR Portofolio dan Pendekatan VaR berdasarkan Posisi VaR",,"Kata Kunci : portofolio optimal, Value at Risk, CVaR, IVaR, Garman delVaR, Brute Force, Posisi VaR, VaR Portofolio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149885,OPTIMASI PORTOFOLIO MENGGUNAKAN MODEL MAXIMUM ENTROPY (Studi Kasus Saham Indeks LQ-45); PORTFOLIO OPTIMIZATION USING MAXIMUM ENTROPY MODEL (Case Study Stock of Index LQ-45),"SITI ALAWIYAH, Dedi Rosadi",2012 | Skripsi | PROGRAM STUDI STATISTIKA,Portofolio model Markowitz (Mean Variance) merupakan metode yang sudah umum digunakan dalam pembentukan portofolio sejak tahun 1952. Pada model portofolio Markowitz asumsi yang digunakan adalah return berdistribusi normal sedangkan pada model Maximum Entropy ini tidak membutuhkan asumsi distribusi normal dari return. Sehingga model Maximum Entropy sangat cocok untuk menentukan bobot portofolio karena pada kenyataanya sangat jarang ditemukan data return saham yang berdistribusi normal. Fungsi entropy pada model ini akan dimaksimalkan berdasarkan return dan resiko tertentu. Portofolio model Maximum Entropy akan memberikan return yang lebih tinggi daripada model Markowitz pada tingkat resiko yang sama dengan model Markowitz dan juga memberikan resiko yang lebih rendah pada tingkat return yang sama dengan Markowitz. Dalam skripsi ini akan dijelaskan pembentukan portofolio optimal menggunakan model Maximum Entropy. Studi kasus yang dilakukan adalah membandingkan performa portofolio model Maximum Entropy dengan portofolio model Markowitz pada tingkat resiko yang sama dengan portofolio model Markowitz.,,"Kata Kunci : model Maximum Entropy, model Mean Variance Markowitz, Optimasi portofolio."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149119,ANALISIS REGRESI POLINOMIAL KUADRATIK TANPA VARIABELVARIABEL INTERAKSI; QUADRATIC POLYNOMIAL REGRESSION ANALYSIS WITHOUT INTERACTION VARIABLES,"Mochamad Kautzar Ichramsyah, Subanar",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam analisis regresi, bentuk hubungan antara variabel dependen dan independen sangat menentukan langkah kita selanjutnya dalam menentukan model regresi yang paling tepat dan akurat untuk digunakan, tentunya dengan tingkat kesalahan serendahrendahnya. Untuk kasus yang sederhana, memang analisis regresi linier sederhana sudah cukup untuk membuat model estimasi apabila hubungan antara variabel dependen dan independen mulus dan membentuk garis lurus. Tapi, apabila hubungannya tidak membentuk garis lurus, walaupun terlihat mulus, analisis regresi linier sederhana tidaklah cukup untuk membuat model regresi yang baik. Karena padakenyataannya, sangat sulit ditemukan di dunia nyata terdapat data yang hubungan antara variabel dependen dan independennya mulus dan membentuk garis lurus. Skripsi ini menunjukkan analisis regresi polinomial khususnya untuk orde kedua atau kuadratik merupakan metode yang sangat efektif apabila kita menemukan kasus seperti itu. Hasil analisis menunjukkan bahwa model regresi yang terbentuk menggunakan analisis regresi polinomial lebih baik digunakan pada kasus tersebut dibandingkan dengan analisis regresi linier sederhana walaupun sudah menggunakan transformasi pada data.",,"Kata Kunci : regresi, polinomial, kuadratik, transformasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149905,MULTIVARIATE GENERALIZED AUTOREGRESSIVE CONDITIONAL HETEROSCEDASTICITY DENGAN MODEL MATRIKS KONDISIONAL KOVARIANSI; MULTIVARIATE GENERALIZED AUTOREGRESSIVE CONDITIONAL HETEROSCEDASTICITY WITH CONDITIONAL COVARIANCE MATRICE MODEL,"ANA ANDRIYATI, Dedi Rosadi",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Model dengan volatilitas telah menjadi salah satu penelitian yang sebagian besar sukses dan diminati dalam bidang time series keuangan dan peramalan di bidang ekonomi pada akhir-akhir ini salah satunya adalah model heteroskedastik, yakni model Generalized Autoregressive Conditional Heteroscedasticity. Skripsi ini membahas tentang model multivariat dari model GARCH, terutama pemodelan dengan matriks kondisional kovariansi, yang nantinya digunakan untuk forecasting data di bidang keuangan, misalnya saham. Model VECH-GARCH merupakan generalisasi langsung dari model univariat GARCH ke multivariat GARCH. Akan tetapi, selain banyaknya parameter dalam model ini, kesulitan menjamin kondisi positif definit untuk matriks kovariansi juga menjadi salah satu pertimbangan. Model yang menjamin kondisi positif definit pada estimasi matriks kovariansi bersyarat salah satunya adalah model BEKK-GARCH. Akan tetapi, model ini juga mempunyai sesuatu yang disebut “curse of dimensionality” karena tidak sedikitnya jumlah parameter.",,"Kata Kunci : Multivariate GARCH, model kondisional kovariansi, VECHGARCH, BEKK-GARCH"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149912,ALGORITMA INTERACTION MINER SEBAGAI UPAYA UNTUK MENINGKATKAN AKURASI REGRESI LOGISTIK DALAM PEMBUATAN MODEL CREDIT SCORING (Studi Kasus Data Konsumen Kredit PT. Bank Mandiri Tbk.); IMPROVING LOGISTIC REGRESSION WITH INTERACTION MINER FOR CREDIT SCORING (Case Study : PT. Bank Mandiri Tbk. Consumer Credit Data),"YOGA DIMAS PRASETYA, Abdurrakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis risiko kredit memegang peranan yang sangat penting pada analisis kredit macet. Credit Scoring merupakan salah satu langkah untuk membantu dalam pengambilan keputusan kredit. Meskipun terdapat banyak teknik pengolahan kredit scoring yang lebih powerfull, bank menghadapi aturan dimana metode yang digunakan tidak boleh mengandung unsur yang black-box. Akibatnya, meskipun banyak sekali teknik yang lebih powerfull bank masih lebih memilih menggunakan Regresi Logistik sebagai metode pengolahan datanya. Dalam tugas akhir ini dibahas bagaimana mengubah teknik yang black-box tersebut sehingga dapat digunakan dalam praktek di dunia perbankan. Metode yang dimaksud dalam tugas akhir ini adalah metode Interaction Miner. Metode ini memanfaatkan metode random forest sebagai seleksi variabel dan pendeteksian interaksi sehingga dapat dilakukan regresi logistik. Dalam rangka melihat dan membandingkan metode Interaction Miner dan Regresi Logistik, penulis melakukan studi kasus terhadap data yang didapat dari PT. Bank Mandiri. Tbk. Tugas akhir ini berakhir dengan kesimpulan bahwa Interaction Miner terbukti mampu mengungguli metode regresi logistik berdasarkan ukuran asosiasi yang ada. Selain itu, tingkat akurasi yang dimiliki metode ini lebih besar dibanding metode regresi logistik dengan selisih tingkat akurasi sebesar 3.08%.",,"Kata Kunci : Kredit, Analisis Risiko Kredit, Credit Scoring, Interaction Miner, Random Forest, Regresi Logistik, Maksimisasi Akurasi, Klasifikasi,"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149914,REGRESI ZERO ADJUSTED INVERSE GAUSSIAN (ZAIG) DALAM MEMODELKAN KLAIM ASURANSI KESEHATAN; ZERO ADJUSTED INVERSE GAUSSIAN REGRESSION FOR MODELLING HEALTH INSURANCE CLAIMS,"TRI BEKTI UTAMI, Adhitya Ronnie Effendie",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Di bidang asuransi, perusahaan asuransi (penanggung) memberikan jaminan kepada tertanggung terhadap risiko yang terjadi sesuai kesepakatan berupa sejumlah uang yang disebut dengan klaim. Pada beberapa kasus sering ditemukan adanya nilai nol dalam data klaim asuransi. Nilai nol ini menyatakan tidak adanya klaim asuransi yang dilaporkan tertanggung kepada penanggung dalam periode tertentu. Dalam skripsi ini akan dibahas regresi Zero Adjusted Inverse Gaussian (ZAIG) yang dapat mengatasi adanya nilai nol dalam data asuransi ini. Model ZAIG merupakan model campuran diskrit dan kontinu dengan model kontinunya merupakan distribusi Inverse Gaussian. Model ZAIG sangat cocok untuk data yang mempunyai extreme right-skewness serta adanya nilai nol dalam data. Model ZAIG di sini akan diestimasi menggunakan metode Maximum Likelihood Estimation (MLE) dengan algoritma RS.",,"Kata Kunci : regresi Zero Adjusted Inverse Gaussian, extreme right-skewness, Maximum Likelihood Estimation, algoritma RS."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149923,SEGMENTASI MENGGUNAKAN KRITERIA ELBOW DAN METODE KERNEL K-MEANS; SEGMENTATION USING ELBOW CRITERION AND KERNEL K-MEANS METHOD,"LULUK MUSLIMAH, Yunita Wulan Sari",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode k-means clustering merupakan salah satu analisis multivariat untuk melakukan segmentasi atau pengelompokan. Secara umum, kasus-kasus di dunia nyata adalah kasus yang menggambarkan ketidaklinearan sehingga data sulit dipisahkan secara linear. Karena itu, bila suatu kasus klasifikasi memperlihatkan ketidaklinearan, salah satu cara untuk mengatasinya yaitu dengan menggunakan metode kernel. Metode kernel memberikan pendekatan alternatif dengan cara melakukan mapping data dari input space ke feature space dengan dimensi yang lebih tinggi. Dalam skripsi ini digunakan kriteria elbow untuk menentukan jumlah klaster dan menggunakan metode Kernel K-Means untuk melakukan segmentasi. Dengan penggunaan metode kernel k-means ini diharapkan data bisa dipisahkan dengan lebih baik karena data yang nonlinear bisa menjadi linear di ruang dimensi baru, dalam hal ini pencarian jarak antara tiap titik terhadap pusat klaster dilakukan di feature space. Studi kasus yang dibahas mengenai segmentasi peserta KB aktif per mix kontrasepsi berdasarkan tujuh jenis alat kontrasepsi sehingga bisa dilakukan profilling dari masing-masing segmen yang terbentuk dan menyimpulkannya.",,"Kata Kunci : segmentasi, feature space, kriteria elbow, Kernel K-Means"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149926,"PENENTUAN HARGA OBLIGASI CALLABLE DENGAN SUKU BUNGA COX, INGERSOLL, DAN ROSS (CIR) MENGGUNAKAN METODE POHON BINOMIAL; CALLABLE BOND PRICING WITH COX, INGERSOLL, AND ROSS (CIR) INTEREST RATE MODEL USING BINOMIAL TREE","IRENA ANANDARI, Yunita Wulan Sari",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Obligasi callable adalah obligasi yang memberikan hak kepada penerbit untuk membeli kembali obligasi pada harga tertentu sepanjang umur obligasi tersebut. Penentuan harga obligasi callable sangat dibutuhkan untuk memutuskan apakah akan terus memegang atau menjual obligasinya dan menilai apakah obligasi tersebut termasuk murah atau mahal agar investor memperoleh keuntungan. Untuk menentukan harga obligasi callable pada skripsi ini digunakan pohon binomial. Perhitungan harga obligasi menggunakan pohon binomial biasanya didasarkan pada asumsi bahwa suku bunga bergerak secara tetap sepanjang waktu. Asumsi ini tidak sesuai pada kenyataan bahwa suku bunga bergerak secara fluktuatif, sehingga diperlukan model suku bunga stokastik yang telah mempertimbangkan pergerakan suku bunga secara fluktuatif. Model suku bunga yang digunakan pada skripsi ini adalah model Cox, Ingersoll, dan Ross (CIR). Parameter-parameter model CIR akan diestimasi dengan metode estimasi kuadrat terkecil bersyarat. Pada penentuan pohon binomial, mula-mula suku bunga acuan dimodelkan dengan pohon binomial yang bekerja maju kemudian akan bekerja mundur dalam menentukan harga obligasi.",,"Kata Kunci : suku bunga, obligasi callable, Cox, Ingersoll, dan Ross (CIR), estimasi kuadrat terkecil bersyarat, pohon binomial"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148648,PORTOFOLIO DINAMIS DENGAN MENGGUNAKAN MAXIMUM ABSOLUTE; DEVIATION (MAD) DYNAMIC OPTIMAL PORTFOLIO WITH MAXIMUM ABSOLUTE DEVIATION,"Ginanjar Bachtiar, Gunardi",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Skripsi ini mengulas tentang Pemilihan Portofolio Multiperiode, dengan Menggunakan Maximum Absolute Deviation. Investor diasumsikan mencoba strategi investasi untuk memaksimalkan kekayaan pada akhir periode investasi dan sebaliknya meminimalkan total resiko pada semua periode investasi. Berbeda pada penentuan ukuran resiko yang menggunakan variansi (matriks variansi kovariansi), dalam tulisan ini, Total resiko didefinisikan sebagai rata-rata dari penjumlahan ukuran maximum absolute deviation dari semua asset di semua periode. Di waktu yang sama, mengingat jika resiko selama periode investasi terlalu tinggi bisa menyebabkan kebangkrutan, Level resiko maksimum diberikan pada masing-masing periode investasi.",,Kata Kunci : Optimisasi Portofolio; dynamic programming; maximum absolute deviation (MAD)
http://etd.repository.ugm.ac.id/home/detail_pencarian/149929,SEMIVARIOGRAM ANISOTROPY DALAM ANALISIS ORDINARY INDICATOR KRIGING,"INA AYU SUKMAWATI, N",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Kriging merupakan metode untuk mengestimasi besarnya nilai yang mewakili suatu titik yang tidak tersampel berdasarkan titik-titik tersampel yang berada di sekitarnya dengan mempertimbangkan korelasi spasial yang ada dalam data tersebut. Dalam metode kriging terdapat satu komponen yang dinamakan semivariogram, yaitu komponen untuk mengamati korelasi antar data sampel. Semivariogram anisotropy adalah semivariogram yang dipengaruhi oleh arah dan jarak antar titik sampel. Dalam semivariogram anisotropy dibutuhkan toleransi arah dan toleransi jarak yang tepat agar jumlah pasangan data sesuai dengan model semivariogram teoritis. Ordinary indicator kriging adalah indicator kriging yang dijalankan berdasarkan prinsip ordinary kriging. Metode ini tidak membutuhkan asumsi normalitas dan dapat mengatasi data outlier. Pada studi kasus akan digunakan data kandungan nikel daerah Morowali sebanyak 127 data. Kemudian diestimasi nilai probabilitas dari suatu lokasi untuk mempunyai kandungan nikel ? 0,79% dengan ordinary indicator kriging.",,"Kata Kunci : semivariogram, anisotropy, indicator kriging, ordinary kriging"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149934,PEMODELAN SUKU BUNGA DAN HARGA CALLABLE BOND MODEL COX-INGERSOLL-ROSS MENGGUNAKAN KERANGKA POHON TRINOMIAL; INTEREST RATE AND CALLABLE BOND PRICE MODELLING WITH COX-INGERSOLL-ROSS MODEL USING TRINOMIAL TREE FRAMEWORK,"YUNIAWATI TRISNA MAWAR DANI, Abdurakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Callable bond merupakan salah satu tipe obligasi dimana obligasi tersebut dapat ditebus kembali oleh penerbitnya sebelum jatuh tempo. Nilai yang harus dibayar oleh penerbit jika obligasi tersebut dilunasi sebelum jatuh tempo disebut dengan call price. Investor yang ingin berinvestasi pada callable bond sering dihadapkan pada masalah memilih obligasi mana yang dapat mendatangkan keuntungan. Oleh karena itu, penentuan harga obligasi sangat diperlukan untuk menilai apakah obligasi tersebut termasuk murah atau tidak. Salah satu cara untuk menentukan harga callable bond adalah dengan mengkonstruksi suku bunga dalam kerangka pohon trinomial yang mengikuti model Cox-Ingersoll-Ross. Model Cox-Ingersol-Ross (CIR) merupakan salah satu model stokastik yang menggambarkan perubahan tingkat bunga untuk jangka waktu yang pendek. Model ini mempunyai sifat mean reversion. Untuk jangka waktu yang lama, diperoleh bahwa mean dan variansi dari suku bunga pada saat jatuh tempo mendekati suatu nilai. Karena obligasi merupakan sekuritas yang bergantung pada suku bunga, maka suku bunga dimodelkan secara forward. Setelah itu, pohon trinomial akan bekerja backward berdasarkan suku bunga tadi menentukan harga straight bond-nya. Harga callable bond-nya adalah nilai minimal dari harga straight bond dan call price",,"Kata Kunci : Harga callable bond, suku bunga, Model Cox-Ingersoll-Ross, mean reversion, pohon trinomial"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149935,PERPADUAN ANALISIS FAKTOR DAN METODE MINIMUM COVARIANCE DETERMINANT ROBUSTIFIED MEAN VARIANCE DALAM PENENTUAN PORTOFOLIO OPTIMAL; COMBINATION OF FACTOR ANALYSIS AND MINIMUM COVARIANCE DETERMINANT ROBUSTIFIED MEAN VARIANCE IN PORTFOLIO OPTIMIZATION,"ANA MELASARI, Dedi Rosadi",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Secara umum, portofolio yang optimal adalah portofolio yang dapat mengurangi risiko. Portofolio yang ideal adalah portofolio yang mampu memberikan return optimal dengan risiko seminimal mungkin. Namun, terlebih dulu investor seringkali dihadapkan pada masalah bagaimana memilih aset untuk portofolio ketika ingin membuat keputusan investasi. Untuk mengatasinya, diterapkan penggunaan analisis faktor untuk melihat kinerja individual saham. Analisis faktor adalah suatu teknik statistik yang tujuan utamanya adalah untuk mendeteksi dan menggambarkan struktur antar variabel dalam analisis. Analisis faktor dengan sepuluh indikator keuangan perusahaan anggota LQ45 sebagai variabelnya dapat digunakan untuk melihat kinerja individual saham. Setelah terpilih sembilan saham kemudian digunakan dalam penentuan portofolio optimal. Dalam portofolio mean-variance, digunakan mean sampel dan estimator kovarian untuk mengestimasi expected return dan kovariansi antar asset. Namun, terdapat salah satu kelemahan dari estimator standar yaitu sensitif terhadap outlier. Untuk mengatasi kasus ini, digunakan estimasi robust untuk mean dan kovariansi. Salah satunya adalah minimum covariance determinant (MCD) estimator. Selanjutnya akan dibandingkan portofolio metode Mean Variance dengan metode Minimum Covariance Determinant robustified Mean Variance dalam penentuan portofolio optimal.",,"Kata Kunci : analisis faktor, MCD, Fast-MCD, Mean Variance, MCD Robustified mean variance"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149937,"Perbandingan Metode Kausal Step dan Product of Coefficient untuk Variabel Intervening serta Perbandingan Uji MRA, Uji Nilai Mutlak Selisih, dan Uji Residual untuk Variabel Moderating; The Comparison of Causal Step Method and Product of Coefficient for Intervening Variable, and the Comparison between Moderated Regression Analysis (MRA), the Value of Difference-Absolute Test, and Residual Test for Moderating Variable","ROSSANTI KUMALASARI, Gunardi",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Variabel intervening adalah suatu variabel yang memediasi hubungan antara variabel independen dengan variabel dependen. Pengaruh mediasi dari variabel intervening dapat dianalisis menggunakan metode Kausal Step dan Product of Coefficient. Variabel moderating berperan sebagai moderate yaitu dapat memperkuat atau memperlemah hubungan antara variabel independen terhadap variabel dependen. Analisis variabel moderating akan dikerjakan dengan tiga metode berbeda yaitu uji interaksi atau MRA (Moderated Regression Analysis), uji nilai mutlak selisih, dan uji residual. Metode untuk transformasi data berskala ordinal menjadi data berskala interval dalam tugas akhir ini menggunakan MSI (Method Succesive Interval). Dalam studi kasus data diperoleh melalui metode survei dengan membagikan kuesioner kepada seluruh pegawai di kantor kelurahan Losari, Pagentan, Candirenggo kecamatan Singosari kabupaten Malang.Kepuasan Kerjasebagai variabel intervening dapat berpengaruh secara parsial (partial mediation) pada hubungan variabel Motivasi Kerja dan Kinerja Pegawai.Asal responden sebagai variabel moderating dalam tugas akhir ini dapat dibuktikan yaitu memperkuat hubungan antara variabel Motivasi Kerja dan Kinerja Pegawai",,"Kata Kunci : variabel intervening, variabel moderating, kausal step, product of coefficient, uji MRA, uji nilai mutlak selisih, uji residual, MSI"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149938,PENERAPAN TEORI KONTROL STOKASTIK OPTIMAL PADA PORTOFOLIO ASET BERESIKO DAN ASET TIDAK BERESIKO; THE APPLICATION OF OPTIMAL STOCHASTIC CONTROL THEORY IN RISKY ASSET AND RISK-FREE ASSET,"CHANDRA PURWANA, N",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Di era yang semakin maju ini banyak cara yang bisa digunakan untuk berinvestasi guna memperoleh keuntungan. Salah satu cara berinvestasi yaitu dengan diversifikasi yakni membentuk portofolio. Terdapat banyak kemungkinan pembentukan portofolio, sehingga perlu dipilih portofolio mana yang lebih optimal dalam menghasilkan keuntungan. Pada tahun 1969, Robert C Merton mempublikasikan hasil kerjanya yang dikenal dengan istilah Masalah Portofolio Merton. Masalah Portofolio Merton membahas mengenai seberapa besar uang yang harus dialokasikan investor untuk berinvestasi di dua aset berbeda yaitu aset beresiko dan aset tidak beresiko. Model portofolio investasi yang akan berbentuk persamaan diferensial stokastik akan diselesaikan menggunakan metode Lemma Ito. Dengan persamaan HJB sebagai syarat cukup untuk keoptimalan, dapat ditentukan kontrol u yang memberikan hasil optimal. Dengan proporsi optimal ini investor akan memperoleh hasil yang maksimal dalam berinvestasi di dua aset berbeda yaitu aset beresiko dan aset tidak beresiko.",,"Kata Kunci : Portofolio Merton, Lemma Ito, Persamaan HJB, kontrol u"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149941,MODEL GOMPERTZ DAN LOGISTIK UNTUK KURVA SIGMOID (Studi Kasus : Pertumbuhan Perkecambahan Biji Tanaman Padi yang Diberi 3 Perlakuan),"SITI ISPRIYASIH, Zulaela",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Banyak ditemukan data dalam bidang ilmu pertanian, biologi, mikrobiologi dan ilmu medis yang berbentuk kurva sigmoid (kurva pertumbuhan yang grafiknya berbentuk seperti huruf S). Model yang digunakan untuk memodelkan kurva sigmoid disebut model pertumbuhan sigmoid. Model pertumbuhan sigmoid sendiri menggambarkan pertumbuhan tanaman, sel atau organisme yang mengikuti bentuk S terhadap waktu. Model pertumbuhan sigmoid dalam bidang ilmu pertanian digunakan untuk mengetahui kemampuan pertumbuhan suatu tanaman. Pada skripsi ini, kurva sigmoid dimodelkan menggunakan model pertumbuhan Gompertz dan model pertumbuhan Logistik. Lalu kedua model pertumbuhan tersebut dibandingkan berdasarkan nilai SSE-nya untuk memperoleh model pertumbuhan yang paling cocok digunakan untuk memodelkan kurva sigmoid. Pada studi kasus digunakan data perkecambahan biji tanaman padi yang diberi 3 perlakuan (ekstrak kenikir, ekstrak mahoni dan ekstrak cengkeh). Diperoleh hasil, model pertumbuhan Gompertz adalah model yang paling cocok digunakan untuk memodelkan kurva sigmoid",,"Kata Kunci : Kurva Sigmoid, Model Gompertz, Model Logistik, Perkecambahan Biji Tanaman Padi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149178,PORTOFOLIO OPTIMAL MENGGUNAKAN METODE MEAN-GINI; OPTIMAL PORTFOLIO BY MEAN-GINI METHOD,"VITA NOER QOMARIA, Abdurrakhman",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode Mean-Variance merupakan metode yang umum digunakan dalam pembentukan portofolio, metode Mean-Variance diperkenalkan oleh Markowitz lebih dari 50 tahun yang lalu. Akan tetapi, metode Mean-Variance tidak tepat untuk digunakan pada kondisi return saham yang tidak berdistribusi normal, perhitungan menggunakan Mean-Variance memerlukan waktu yang cukup lama. Shalit dan Yitzhaki (1984) memperkenalkan metode Mean-Gini sebagai metode alternatif dari metode Mean-Variance, metode Mean-Gini tidak dipengaruhi oleh kondisi distribusi dari return saham, Gini merupakan ukuran risiko investasi yang dalam perhitungannya lebih mudah. Metode Mean-Gini menarik bagi investor karena Mean-Gini memungkinkan investor untuk memilih portofolio yang dianggap rendah bagi investor lain. Dalam skripsi ini akan dijelaskan mengenai pembentukan portofolio optimal dengan metode Mean-Gini, yang kemudian akan dibandingkan dengan metode Mean-Variance, ukuran yang digunakan sebagai pembanding adalah Sharpe Ratio.",,"Kata Kunci : portofolio, alternatif, Sharpe Ratio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149179,MODEL REGRESI PARAMETRIK UNTUK DATA TAHAN HIDUP TERSENSOR INTERVAL; PARAMETRIC REGRESSION MODEL FOR INTERVAL CENSORED SURVIVAL DATA,"DEVIYANTI ROCHANA, Danardono",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi parametrik pada data tersensor interval adalah metode analisis data yang bertujuan mencari suatu hubungan antara varibel respon yang tersensor interval dengan variabel prediktor dengan asumsi populasi mempunyai distribusi data tertentu. Pada kasus penelitian di lapangan, tidak mudah untuk mendapatkan data lengkap sehingga memungkinkan adanya data tersensor, seperti kasus tersensor interval yaitu kejadian yang menjadi perhatian hanya diketahui terjadi sebelum dan sesudah waktu tertentu, karena penelitian dilakukan secara periodik. Di satu sisi, dengan adanya data tersensor akan menyebabkan analisis regresi menjadi lebih kompleks dan nonlinea. Beberapa model regresi parametrik yang akan dibahas adalah model regresi Eksponensial, Weibull, Log Logistik dan Log Normal. Dalam konteks data tersensor interval, kesimpulan dari model parametrik sangat kuat dengan perubahan asumsi distribusi dan umumnya lebih informatif daripada model nonparametrik. Metode estimasi yang digunakan untuk memperoleh estimasi parameter regresi adalah metode Maximum Likelihood Estimation (MLE) dan metode iterasi Newton Raphson dengan bantuan software R. Analisis dilanjutkan dengan menguji koefisien parameter pada model regresi yang dihasilkan. Dari hasil uji parsial parameter regresi tersebut, kemudian akan dicari model terbaiknya dengan ukuran Akaike Information Criterion (AIC) yang terkecil.",,"Kata Kunci : Model Regresi Parametrik, Data Tersensor Interval, Maximum Likelihood Estimation (MLE), Newton Raphson, Akaike Information Criterion (AIC)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149947,ESTIMASI MODEL MIXTURE LOGISTIK WEIBULL; ESTIMATING OF A LOGISTIC WEIBULL MIXTURE MODELS,"ADHYANA SETYO PRATIWI, Subanar",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Model mixture merupakan model gabungan dari dua atau lebih distribusi. Model mixture juga diketahui sebagai split-population models. Metode ini mengelompokkan data di dalam suatu dataset menjadi kelompok-kelompok data yang sebelumnya tidak terdefinisikan. Model mixture logistik Weibull merupakan model gabungan dari model logistik dan model Weibull dengan turut mempertimbangkan adanya lama waktu hidup. Model logistik menunjukkan probabilitas kegagalan suatu peristiwa dan model Weibull menunjukkan distribusi dari data waktu tahan hidup. Untuk mencari estimasi dari model mixture logistik Weibull digunakan metode Maximum Likelihood Estimator (MLE). Estimasi ML didasarkan pada asumsi bahwa data waktu kegagalan berdistribusi Weibull. Pembahasan akan diakhiri dengan studi kasus mengenai faktor-faktor yang mempengaruhi lama waktu hidup pasien kanker rahim. Dengan pemrograman R i386 2.15.0, diperoleh kesimpulan bahwa model logistik Weibull sesuai digunakan untuk data kanker rahim.",,"Kata Kunci : model mixture logistik Weibull, mixture survival, Maximum Likelihood Estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149183,REGRESI POISSON KELAS LATEN; LATENT CLASS POISSON REGRESSION,"SITRA PADMA ANDAYANINGRUM, Danardono",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis Regresi Poisson Kelas Laten mengakomodasi heterogenitas dalam model regresi poisson untuk model cacah. Model yang dikembangkan mengasumsikan heterogeneity yang timbul dari distribusi kedua intercept dan koefisien variabel penjelas. Kita mengasumsikan bahwa distribusi campuran berupa diskrit, menghasilkan formula model campuran yang terbatas. Estimasi model melalui metode maksimum likelihood dengan algoritma Ekspektasi Maksimasi (EM Algorithm), yang diaplikasikan pada data klaim asuransi kendaraan Malaysia pada tahun 1998-2000. Regresi Poisson Kelas Laten yang digunakan pada data asuransi Malaysia kelas yang paling baik digunakan berjumlah tiga kelas.",,"Kata Kunci : Regresi Poisson Kelas Laten, algoritma EM, count data, heterogeneity"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149444,MODEL POISSON NON-HOMOGEN DENGAN TITIK-UBAH Studi Kasus : Gempa Bumi di Daerah Istimewa Yogyakarta dan Sekitarnya; NON-HOMOGENEOUS POISSON MODEL WITH A CHANGE-POINT Case Study : An Earthquake in Yogyakarta and Surrounding Areas,"EVI FATIMAH, Gunardi",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Banyak penerapan proses Poisson mengasumsikan bahwa laju kejadian bersifat konstan (homogen). Namun asumsi proses Poisson homogen tidak selalu dapat terpenuhi. Ada kalanya laju kejadian berupa fungsi dari waktu, yang disebut dengan fungsi intensitas. Kejadian semacam ini merupakan proses Poisson nonhomogen (non-homogeneous Poisson process/NHPP). NHPP memerlukan model Poisson non-homogen agar dapat mengakomodasi laju kejadian yang tidak homogen. Selain masalah laju kejadian yang tidak homogen, masa transisi yang mungkin dialami sebuah sistem juga memerlukan suatu penyelesaian. Titik waktu dimana masa transisi terjadi biasa disebut dengan istilah titik ubah (change-point). Untuk mengatasi masalah adanya titik ubah dikembangkan model titik ubah (change-points model). Analisis dengan model yang memiliki lebih dari dua parameter akan mempersulit peneliti untuk mendapatkan kesimpulan tentang parameter dari model dengan menggunakan prosedur standar inferensi klasik. Analisis Bayesian melalui metode Markov Chain Monte Carlo (MCMC) dengan algoritma Gibbs sampling bisa menjadi cara yang sesuai untuk mendapatkan inferensi tentang parameter. Paket program yang dapat digunakan untuk teknik ini salah satunya dengan software WinBUGS. Dalam skripsi ini, digunakan suatu model Poisson non-homogen untuk mempelajari peristiwa gempa bumi di DIY dan sekitarnya. Diasumsikan bahwa kejadian gempa bumi mengikuti NHPP. Dalam kasus ini digunakan fungsi intensitas generalisasi Goel-Okumoto. Juga diasumsikan adanya titik-ubah dan tidak adanya titik-ubah. Analisis masalah dilakukan dengan menggunakan pendekatan Bayesian melalui metode Markov Chain Monte Carlo (MCMC). Model terbaik dipilih menggunakan Deviance Information Criterion (DIC). Dalam kasus ini diperoleh kesimpulan bahwa model Poisson non-homogen dengan titik ubah menghasilkan nilai estimasi yang lebih baik jika dibandingkan dengan model tanpa titik ubah",,"Kata Kunci : Proses Poisson Non-Homogen (NHPP), Titik-ubah (Changepoint), Markov Chain Monte Carlo (MCMC)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149700,PEMODELAN HARGA WARAN BELI TIPE EROPA MENGGUNAKAN METODE BINOMIAL TREE (Studi Kasus Waran Bursa Efek Indonesia); EUROPEAN CALL WARRANT PRICING MODELLING USING BINOMIAL TREE METHOD (Case Study Warrant in Indonesia Stock Exchange ),"Adipuja Rahmadinata, Suryo Guritno",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam berinvestasi waran di lantai bursa, tentu akan dihadapkan dengan alternatif waran dan nilai harga kontraknya. Oleh karena itu, diperlukan suatu metode yang dapat melihat apakah harga waran yang ditawarkan tersebut telah sesuai secara teori. Salah satu metode yang banyak menjadi acuan yaitu Binomial Tree. Binomial Tree merupakan analisis matematika yang sangat mendasar untuk menghitung harga waran. Dalam skripsi ini membahas tentang model binomial Cox, Ross, and Rubinstein (CRR) yang mengasumsi bahwa harga saham mengikuti sebuah proses binomial. Metode Binomial Tree ini memperkirakan 2 (dua) kemungkinan harga saham pada setiap periode yang ditentukan oleh up dan down rasio. Hasil akhirnya yakni memprediksi harga waran yang wajar untuk suatu nilai saham. Disamping itu juga akan diuraikan suatu studi kasus waran yang diperdagangkan di Bursa Efek Indonesia (BEI) menggunakan metode Binomial Tree.",,"Kata Kunci : Waran, Binomial Tree, Cox, Ross, and Rubinstein (CRR)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149702,PENGAMBILAN SAMPEL KLASTER DUA TAHAP DENGAN PENDEKATAN ESTIMATOR HORVITZ-THOMPSON; TWO STAGE CLUSTER SAMPLING WITH HORVITZ-THOMPSON ESTIMATOR APPROACH,"IRWAN RIZADI, Sri Haryatmi",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Pengambilan sampel klaster dua tahap adalah metode pengambilan sampel untuk mengatasi populasi yang besar dengan tingkat keheterogenan yang besar pula. Sampel klaster dua tahap diperoleh dengan pertama kali memilih sampel klaster dari populasi kemudian memilih elemen sampel dari tiap sampel klaster yang terpilih. Dalam skripsi ini, Estimator Horvitz-Thompson digunakan untuk mengestimasi populasi menggunakan metode pengambilan sampel klaster dua tahap. Dalam pengambilan sampel klaster ini, estimator Horvitz-Thompson juga bisa digunakan untuk berbagai jumlah tahapan dalam pengambilan sampel bergantung pada probabilitas terpilihnya sampel dalam populasi. Dalam studi kasus dilakukan pengambilan sampel untuk mengestimasi rata-rata dan jumlah kamar hotel yang terdapat di Provinsi DIY 2011",,"Kata Kunci : Pengambilan Sampel Klaster Dua Tahap, Estimator Horvitz- Thompson,"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149205,PELUANG KEBANGKRUTAN DENGAN MODAL SOLVENCY; PROBABILITY OF RUIN WITH SOLVENCY CAPITAL,"YULIAWAN SETYO WIBOWO, Adhitya Ronnie Effendie",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Kebutuhan solvency didasarkan pada gagasan bahwa risiko dapat diterima jika memiliki modal yang cukup. Penentuan tingkat minimum modal bergantung pada bagaimana kita mempertimbangkan dan mengukur risiko yang ada. Faktor penting dalam mengukur resiko adalah bagaimana waktu diintegrasikan dalam proses. Dalam skripsi ini, dipelajari risiko pasar dari sebuah perusahaan asuransi jiwa dengan tingkat garansi tetap pada horizon waktu tertentu dan menginvestasikan aset yang dimiliki pada aset beresiko. Metode yang digunakan untuk mengukur resiko adalah metode Value at Risk dan tail Value at Risk dengan resiko statis (kewajiban hanya saat jatuh tempo) dan resiko waktu kontinu (kewajiban terjadi selama periode sampai waktu jatuh tempo).",,"Kata Kunci : modal solvency, ukuran resiko, peluang kebangkrutan"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149207,ESTIMASI CHANGE POINT PADA REGRESI SEGMENTASI LINEAR SEDERHANA MELALUI MODIFIED INFORMATION CRITERION (MIC);CHANGE POINT ESTIMATION FOR SIMPLE LINEAR SEGMENTED REGRESSION VIA MODIFIED INFORMATION CRITERION (MIC),"Yuda Anggara Wijayanta, Adhitya Ronnie Effendie",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi merupakan sebuah aplikasi statistik penting yang digunakan dalam banyak disiplin ilmu. Sebelum pengenalan hipotesis change point dalam penelitian regresi, statistikiawan menghadapi masalah yaitu tidak mampu membangun model regresi untuk beberapa dataset yang diamati. Semenjak hipotesis change point diperkenalkan dalam analisis statistik, studi dari perubahan model-model regresi juga mendapat tempat dalam analisis regresi. Kriteria-kriteria informasi biasanya digunakan untuk memilih model-model statistik yang kompeten. Kriteria-kriteria informasi tidak mendukung model yang paling cocok untuk data dan nilai penafsiran yang sedikit, tetapi model yang lebih sederhana dengan kecocokan yang baik. Dalam kriteria informasi, kompleksitas model merupakan faktor yang sangat penting untuk pemilihan model. Dalam skripsi ini, kompleksitas model telah disempurnakan dalam konteks permasalahan change point, dan memodifikasi kriteria informasi yang ada. Kriteria yang termodifikasi ditemukan konsisten dalam memilih model yang benar.",,Kata Kunci : Estimasi change point; Modified Information Criterion (MIC)
http://etd.repository.ugm.ac.id/home/detail_pencarian/149751,MODEL REGRESI ADITIF POISSON TERGENERALISASI; GENERALIZED POISSON ADDITIVE MODELS,"SITI FATONAH, N",2012 | Skripsi | PROGRAM STUDI STATISTIKA,"Model regresi Poisson, nilai mean dan variansi dari variabel dependen sama. Pada kenyataannya, sering dijumpai pada data dengan nilai variansi lebih besar daripada nilai mean-nya (overdispersi). Sehingga model regresi Poisson dianggap kurang tepat untuk mengatasi masalah tersebut. Sehingga digunakan model regresi aditif Generalized Poisson untuk mengatasi kasus overdispersi. Dalam skripsi ini, model regresi aditif Poisson dan Generalized Poisson akan diterapkan pada data asuransi mobil pribadi Kanada dengan jenis klaim Third Party Property Damage (TPPD). Berdasarkan hasil estimasi parameter dispersi, a, pada model regresi aditif Generalized Poisson diketahui bahwa terjadi kasus overdipersi. Setelah model regresi aditif Poisson dan Generalized Poisson di uji dengan menggunakan Pearson Chi-Square, AIC, BIC, maka disimpulan bahwa model regresi terbaik dihasilkan oleh model regresi aditif Generalized Poisson",,"Kata Kunci : model regresi Poisson, Generalized Poisson, overdispersi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150274,PERBANDINGAN PORTOFOLIO METODE MEAN-VARIANCE DENGAN METODE MINIMAX; COMPARISON OF MEAN-VARIANCE PORTFOLIO WITH MINIMAX PORTFOLIO,"Gideon Wahyu Putra, Dedi Rosadi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Metode Mean-Variance sudah umum digunakan dalam pembentukan model portofolio sejak 1952. Metode Mean-Variance menggunakan matriks variansikovariansi sebagai ukuran risiko dari portofolio. Jika investor ingin mengalokasikan modalnya pada banyak aset, tentu pembentukan portofolio dengan metode Mean-Variance akan menjadi tidak efisien mengingat perhitungan dalam pembuatan matriks variansi-kovariansi akan memakan waktu yang cukup lama. Metode Minimax merupakan alternatif dari metode Mean-Variance klasik, dimana metode ini menggunakan expected absolute deviation sebagai ukuran risiko, yang tentunya dalam perhitungannya lebih mudah dan cepat dibandingkan dengan metode Mean-Variance. Tujuan dari metode Minimax adalah meminimumkan risiko maksimum individual aset, sehingga risiko portofolio yang terbentuk akan menjadi kecil jika risiko individual asetnya kecil. Dalam skripsi ini akan dijelaskan bagaimana pembentukan portofolio optimal dengan kedua metode portofolio tersebut, dimana optimisasi portofolio Minimax dilakukan dengan menggunakan kondisi Kuhn-Tucker, serta akan dilakukan studi kasus untuk membandingkan performa kedua portofolio untuk menentukan metode mana yang memiliki performa yang lebih baik. Ukuran yang digunakan sebagai pembanding adalah indeks Sharpe.",,"Kata Kunci : portofolio, ukuran risiko, kondisi Kuhn-Tucker"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150275,ANALISIS DISKRIMINAN GAUSSIAN MIXTURE; DISCRIMINANT ANALYSIS BY GAUSSIAN MIXTURE,"NOVIA RINAWATI, Zulaela",2011 | Skripsi | PROGRAM STUDI STATISTIKA,Analisis diskriminan Gaussian mixture merupakan teknik statistika yang digunakan untuk mengelompokkan observasi dengan variabel laten pada model mixture merupakan bobot mixture setiap sub kelas. Pengelompokkan ini didasarkan pada probabilitas posterior yang diestimasi melalui metode maksimum likelihood (MLE) dengan algoritma Ekspektasi Maksimasi (EM Algorithm). Metode ini digunakan pada densitas data yang terdiri atas lebih dari satu distribusi Gaussian dengan cara menganggap tipe distribusi data tiap kelasnya dengan Gaussian mixture. Asumsi yang harus dipenuhi dalam pemodelan analisis diskriminan Gaussian mixture ini adalah tidak terdapat data yang outlier dan kesamaan matriks variansi-kovariansi. Dalam studi kasus dilakukan pengelompokkan observasi yaitu kadar limfosit pasien berdasarkan 8 variabel independennya.,,"Kata Kunci : Analisis diskriminan Gaussian mixture, Gaussian mixture model (GMM), Algoritma Ekspektasi Maksimasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149008,"OPTIMASI PORTOFOLIO ROBUST MENGGUNAKAN METODE MINIMUM VECTOR VARIANCE (Studi Kasus Saham Indeks LQ-45, FTSE-100, S&P-100); ROBUST PORTFOLIO OPTIMIZATION USE MINIMUM VECTOR VARIANCE METHOD (Case Study Stocks of Index LQ-45, FTSE-100, S&P-100)","FEBTIANA TIA PIKA, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Pembuatan portfolio dalam tujuan pengurangan resiko investasi dengan menggunakan vektor mean dan matriks kovariansi yang diestimasi menggunakan metode klasik ternyata menghasilkan performa portfolio yang kurang baik dan seringkali tidak sesuai dengan harapan yang diinginkan investor. Hal tersebut disebabkan data yang digunakan untuk mengestimasi vektor mean dan matriks kovariansi pada kenyataannya tidak berdistribusi normal dan seringkali estimasi parameter tersebut terganggu karena adanya outlier pada data. Pada tugas akhir ini, portfolio dibentuk dengan input vektor mean dan matriks kovariansi yang robust dimana estimasinya menggunakan suatu metode yang dapat mendeteksi observasi-observasi yang tidak lazim (outlier) pada data, yaitu metode Minimum Vector Variance (MVV). MVV merupakan metode untuk mengestimasi vektor mean dan matriks kovariansi yang robust yaitu dengan meminimumkan trace dari kuadrat matriks kovariansinya. Setelah estimasi kedua parameter pembentuk portfolio menjadi robust, kemudian proporsi portfolio yang optimal dicari dengan menggunakan metode Resampled Efficient Frontier berdasarkan model Mean-Variance dari Markowitz",,"Kata Kunci : Outlier, Robust, metode Minimum Vector Variance, model Mean- Variance Markowitz, metode Resampled Efficient Frontier"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149786,PEMODELAN SHORT RATE MODEL HULL-WHITE DAN BLACK-KARASINSKI MENGGUNAKAN KERANGKA TRINOMIAL TREE; SHORT RATE WITH HULL-WHITE AND BLACK-KARASINSKI MODELS USING TRINOMIAL TREE,"SAPTO BINTANG PERSADA, Danang Teguh Qoyyimi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Short rate merupakan nilai kontrol dalam menentukan estimasi harga obligasi zero coupon sehingga keberadaanya menjadi sangat penting. Skripsi ini membahas 2 model untuk mengkonstruksi short rate dalam kerangka trinomial tree, yaitu model Hull-White (HW) dan model Black-Karasinski (BK). Kedua model memasukkan unsur mean reversion di dalamnya dan mengijinkan time step berubah. Untuk menjamin time step konstan maka dibuat drift setiap level pada tiap periode harus berbeda. Model Hull-White berdistribusi normal sedangkan model Black-Karasinski mengikuti distribusi lognormal sehingga dapat mengeliminasi masalah short rate yang bernilai negatif pada model Hull-White",,Kata Kunci : short rate; Model Hull-White; Model Black-Karasinski; mean reversion; trinomial tree
http://etd.repository.ugm.ac.id/home/detail_pencarian/150305,ESTIMASI MODEL SHARED FRAILTY GAMMA PADA REGRESI COX DENGAN ALGORITMA MODIFIED EM (MEM); ESTIMATION OF SHARED GAMMA FRAILTY MODEL FOR COX REGRESSION BY MODIFIED EM (MEM) ALGORITHM,"MEGAWATI SUHARSONO PUTRI, Danardono",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam analisis survival dikenal suatu fungsi hazard, yang memuat tingkat kejadian seseorang akan mati pada suatu selang (interval) waktu tertentu, dengan syarat bahwa orang tersebut telah hidup sampai saat permulaan dari selang waktu yang akan diteliti. Regresi Cox juga dikenal sebagai model hazard proporsional (proportional hazard model ), dimana nilai hazard yang didapat dari fungsi hazard bernilai proporsional, artinya kovariat (variabel- variabel penjelas) sebanding dalam memberikan efek atau pengaruh terhadap suatu tingkat kejadian. Model frailty merupakan perluasan dari regresi Cox, dimana melalui model ini dapat diketahui unsur heterogenitas yang disebabkan oleh kovariat-kovariat yang tidak terukur. Dalam skripsi ini dibahas mengenai estimasi untuk model frailty yang berdistribusi Gamma dengan algoritma Modified EM yaitu dengan memanfaatkan metode Breslow untuk mengestimasi parameter beta dan algoritma Newton-Raphson sehingga menyederhanakan prosedur estimasi dan mengurangi waktu komputasi dengan pendekatan program linear.",,"Kata Kunci : Regresi Cox, Frailty, Modified EM, Newton-Raphson"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149548,MODEL ANALISIS FAKTOR KONFIRMATORI DENGAN METODE GENERALIZED LEAST SQUARES (Studi Kasus : Multidimensionalitas Tujuan Berprestasi Mahasiswa Psikologi Universitas Gadjah Mada); CONFIRMATORY FACTOR ANALYSIS MODEL USING GENERALIZED LEAST SQUARES METHOD ( Case Study : The Multidimensionality of Psychology Students's Achievement Goals of University of Gadjah Mada ) Diajukan sebagai salah satu syarat untuk memperoleh derajat Sarjana Sains Ilmu Matematika,"FRANSISKA NANINGTYAS, Abdurakhman",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis faktor konfirmatori dilakukan dengan tujuan untuk mengkonfirmasi apakah suatu konstruk dapat dikonfirmasi dengan data empirisnya. Oleh karena itu, analisis ini harus dilakukan dengan konsep atau teori yang mendukung. Skripsi ini membahas penggunaan metode Generalized Least Squares untuk mengestimasi parameter pada analisis faktor konfirmatori. Estimasi nilai ? yang meminimumkan fungsi kesesuaian FGLS tidak dapat diperoleh secara langsung. Oleh karena itu, pada skripsi ini digunakan pendekatan dengan algoritma Gauss-Newton. Pembahasan dilengkapi dengan studi kasus mengenai multidimensionalitas tujuan berprestasi mahasiswa Psikologi Universitas Gadjah Mada",,Kata Kunci : analisis faktor konfirmatori; Generalized Least Squares
http://etd.repository.ugm.ac.id/home/detail_pencarian/150065,BIAYA GARANSI TERDISKON KEBIJAKAN NONRENEWING FREE REPLACEMENT PADA SISTEM MULTIKOMPONEN; DISCOUNTED WARRANTY COST NONRENEWING FREE REPLACEMENT POLICY FOR MULTICOMPONENT SYSTEMS,"ARIEF MADE ASTUTI, Adhitya Ronnie Effendi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Banyak faktor yang harus dipertimbangkan dalam memodelkan biaya garansi terdiskon (DWC) dari repairable systems produk meliputi struktur sistem, proses kerusakan komponen, metode diskon sebagaimana kebijakan garansi itu sendiri yaitu Nonrenewing Free Replacement Warranty. Dampak waktu perbaikan atas kerusakan komponen diasumsikan menjadi minimal, maka proses nonhomogeneous Poisson digunakan untuk menggambarkan proses kerusakan. Dua tipe metode diskon dipaparkan yaitu : metode diskon kontinu dan metode diskon diskrit. Nilai ekspektasi dan variansi DWC dibahas untuk menentukan cadangan dana garansi yang diperlukan per lot penjualan. Dalam aplikasinya, kita membahas perhitungan pada sistem seri, paralel, seri-paralel, dan paralel-seri.",,Kata Kunci : Biaya garansi terdiskon; Cadangan dana garansi; Minimal repair; NHPP; sistem multikomponen.
http://etd.repository.ugm.ac.id/home/detail_pencarian/150068,CREDIT SCORING MENGGUNAKAN LEARNING VECTOR QUANTIZATON (LVQ); CREDIT SCORING USING LEARNING VECTOR QUANTIZATION (LVQ),"RAHAYUNI WASISARINI, Subanar",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis resiko kredit memegang peranan penting pada proses kredit. Credit scoring merupakan teknik yang paling umum digunakan untuk mengevaluasi tingkat kelayakan kredit pada pemohon kredit. Salah satu metode untuk menganalisis credit scoring yaitu jaringan syaraf tiruan dengan metode LVQ. Pengujian jaringan yang telah dilakukan pada data kredit diperoleh hasil bahwa jaringan syaraf tiruan yang paling optimal adalah jaringan syaraf tiruan dengan learning rate 0,4 maksimum epoh 3000, akurasi tipe I adalah 11,5%, akurasi tipe II adalah 95,5%, dan total akurasi 74%. Jaringan syaraf tiruan LVQ mampu memprediksi tipe debitur buruk sedangkan backpropagation tidak mampu memprediksi tipe debitur buruk.",,"Kata Kunci : risiko kredit, credit scoring, jaringan syaraf tiruan, LVQ, learning rate, maksimum epoh, akurasi tipe I, akurasi tipe II, total akurasi, backpropagation"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150071,DIAGRAM KONTROL RS BERDASARKAN JUMLAH PERINGKAT UNTUK DATA BEBAS-DISTRIBUSI; RS CONTROL CHART BASED ON SUM OF RANKS FOR DISTRIBUTION-FREE DATA,"RONA VIRGANANDA, Subanar",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Diagram kontrol merupakan salah satu alat yang digunakan untuk mengukur stabilitas suatu proses. Diagram kontrol yang baik yaitu diagram kontrol yang sensitif dalam mendeteksi pergeseran proses yang kecil. Pergeseran proses dapat dilihat dari nilai Average Run Length (ARL). Nilai ARL yang kecil menandakan bahwa pergeseran proses dapat dideteksi sedini mungkin. Diagram kontrol yang efektif yaitu diagram kontrol yang mempunyai nilai ARL yang kecil. Diagram kontrol yang umumnya dipakai adalah diagram kontrol Shewhart. Diagram kontrol ini merupakan diagram kontrol parametrik yang memiliki keterbatasan dimana data kualitas yang akan dianalisis harus memenuhi asumsi normalitas. Padahal fakta yang ada di lapangan sangat jarang ditemukan data yang mengikuti distribusi normal, bahkan seringkali data proses tidak diketahui dengan pasti tentang distribusi apa yang mendasarinya. Oleh adanya keterbatasan tersebut, muncul suatu metode alternatif yaitu diagram kontrol nonparametrik berdasarkan jumlah peringkat yang disebut diagram kontrol RS dimana tidak mengharuskan dipenuhinya asumsi normalitas. Asumsi dari diagram kontrol ini adalah data harus berasal dari populasi yang identik. Diagram kontrol nonparametrik untuk variabel ini dapat digunakan untuk menganalisis data kualitas variabel berdistribusi apapun, serta memiliki kemampuan dalam mendeteksi pergeseran kecil pada proses. Dengan demikian diagram kontrol RS lebih fleksibel digunakan untuk mengukur stabilitas suatu proses.",,"Kata Kunci : Nonparametrik, Diagram kontrol Shewhart, Diagram kontrol RS, ARL, Jumlah peringkat."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149560,LINEAR SUPPORT VECTOR REGRESSION (LSVR),"IHDA IHSANIA, Zulaela",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Laju pertumbuhan penduduk di Indonesia cukup tinggi, sehingga kepadatan penduduk di Indonesia juga tinggi. Hal ini menunjukkan bahwa program Keluarga Berencana yang dilakukan di Indonesia belum berhasil dengan baik. Analisis regresi merupakan alat stastisik yang bermanfaat untuk mengetahui hubungan antara dua variabel atau lebih. Analisis regresi bertujuan untuk mengestimasi dan memprediksi rata-rata populasi atau nilai rata-rata variabel dependen berdasarkan nilai variabel independen yang diketahui. Salah satu metode yang akhir-akhir ini banyak digunakan dan mendapat perhatian lebih dalam pattern recognition adalah Support Vector Machine (SVM). Metode ini dapat digunakan untuk kasus klasifikasi dua kelas, multi kelas, dan regresi. Dalam penelitian ini, SVM diaplikasikan dalam kasus regresi. Dalam skripsi ini yang akan dibahas adalah penyelesaian model regresi linear dalam parameter dengan menggunakan metode Linear Support Vector Regression (LSVR) dengan aplikasinya untuk mengetahui faktor-faktor apa saja yang mempengaruhi banyaknya anak yang diahirkan oleh seorang wanita dalam sebuah keluarga di Indonesia.",,Kata Kunci : SVM; LSVR; Regresi Linier
http://etd.repository.ugm.ac.id/home/detail_pencarian/150333,OPTIMAL LINDUNG NILAI TERHADAP RESIKO BERMACAM-MACAM KURS MATA UANG (Studi Kasus: Optimal Lindung Nilai untuk Investor Indonesia dan Eropa); OPTIMAL HEDGING AGAINST MULTIPLE CURRENCY EXCHANGE RISK (Case Study: Optimal Hedging for Indonesian and Eropa Based Investor),"BONDAN DWI WIJAYANTI, Danang Teguh Qoyyimi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Skripsi ini membahas besarnya rasio lindung nilai optimal untuk portofolio dengan bermacam-macam mata uang yang digunakan. Optimalisasi mengarah pada fungsi objektif mean-variance dengan bermacam-macam parameter penghindaran resiko. Data dibawa pada pilihan parameter yang diusulkan, yang membawa vektor optimal lindung nilai berskala invariant. Hasil empiris diberikan untuk investor Indonesia dan Eropa. Karena vektor rasio lindung nilai optimal bergantung pada matriks kondisional variansi-kovariansi yang melibatkan data return kurs mata uang maka model multivariat DCC-GARCH dapat digunakan untuk diestimasi. Strategi optimal lindung nilai dibandingkan dengan strategi beberapa lindung nilai yang biasa digunakan.",,"Kata Kunci : optimal lindung nilai, DCC-GARCH"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149822,REDUKSI DIMENSI DATA DENGAN ANALISIS KOMPONEN UTAMA ROBUST; DATA DIMENSION REDUCTION WITH ROBUST PRINCIPAL COMPONENT ANALYSIS,"ANGELIA PARAMITHA KUSUMASTUTI, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis komponen utama merupakan salah satu metode reduksi data yang sangat populer. Metode ini digunakan untuk mereduksi variabel awal yang biasanya mempunyai jumlah yang sangat banyak dan saling berkorelasi menjadi variabel baru (selanjutnya disebut sebagai komponen utama) dengan jumlah yang lebih sedikit dan sudah tidak saling berkorelasi lagi. Analisis komponen utama klasik yang didasarkan pada penghitungan matriks kovariansi data menjadi kurang bisa dipercaya jika data penelitian mengandung sejumlah outlier. Hal ini dikarenakan vektor mean dan matriks kovariansi sangat sensitif dengan keberadaan outlier. Untuk mengatasi permasalahan tersebut maka diperkenalkan suatu konsep Robust Principal Component Analysis (ROBPCA) Skripsi ini membahas suatu pendekatan baru dalam analisis komponen utama robust yaitu dengan menggabungkan konsep projection pursuit (PP) dan estimator matriks kovarian robust (MCD, FAST-MCD). Komponen utama yang diperoleh dari metode ROBPCA adalah komponen utama yang tidak terlalu banyak terpengaruh dengan keberadaan outlier. Pembahasan akan diakhiri dengan studi kasus mengenai penerapan metode ROBPCA pada data penelitian yang mengandung outlier. Kemudian hasil analisis dengan metode ROBPCA akan dibandingkan dengan metode klasik. Pada studi kasus ini diperoleh kesimpulan bahwa metode ROBPCA lebih efisien daripada metode klasik",,"Kata Kunci : analisis komponen utama robust, analisis komponen utama klasik; outlier; projection pursuit; minimum covariance determinant (MCD) dan fast-MCD."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149823,LOKAL DEPENDENSI PADA ANALISIS FAKTOR KELAS LATEN; LOCAL DEPENDENCE IN LATENT CLASS FACTOR ANALYSIS,"BENEDIKTA HASTIKA CHANDRA YUNITA, Suryo Guritno",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis faktor kelas laten merupakan teknik statistika yang digunakan untuk mengelompokkan objek dengan variabel manifes kategorik dan variabel laten kategorik berskala ordinal. Pengelompokan ini didasarkan pada probabilitas pengelompokan posterior yang diestimasi melalui metode maksimum likelihood dengan algoritma Ekspektasi Maksimasi (EM Algorithm). Kelebihan dari analisis ini adalah dapat digunakan dalam jumlah variabel laten yang lebih dari satu. Asumsi yang harus dipenuhi dalam pemodelan analisis faktor kelas laten adalah lokal independensi variabel manifes dalam setiap kelas laten. Namun dalam hal ini, masalah yang terjadi pada analisis faktor kelas laten mempunyai hasil lokal dependensi pada tiap variabel manifesnya sehingga diperlukan metode untuk menangani permasalahan tersebut yaitu metode penggabungan variabel manifes, metode indikator berganda, dan metode loglinier. Dalam studi kasus dilakukan klasifikasi tingkat kemiskinan warga desa Girikerto.",,Kata Kunci : Analisis faktor kelas laten; variabel laten; variabel manifes; lokal dependensi.
http://etd.repository.ugm.ac.id/home/detail_pencarian/150336,APLIKASI METODE UNIVERSAL COKRIGING UNTUK ANALISIS GEOSTATISTIKA; APPLICATION OF UNIVERSAL COKRIGING METHOD FOR GEOSTATISTICAL ANALYSIS,"RAFIKA HADHINATI SYAFIRDI, Sardjono",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Kriging adalah metode untuk mengestimasi besarnya nilai yang mewakili suatu titik yang tidak tersampel berdasarkan titik-titik tersampel yang berada disekitarnya dengan mempertimbangkan korelasi spasial yang ada dalam data tersebut. Kriging merupakan penerapan analisis data geostatistika yang sering digunakan. Pada penelitian ini dipelajari metode perluasan dari universal kriging. Universal kriging mampu menganalisis data dengan kecenderungan trend tertentu, namun metode ini tidak mampu mengestimasi cadangan mineral dengan memperhitungkan pengaruh variabel lain yang disebut dengan co-variable sehingga perlu dipelajari metode yang mampu digunakan untuk mengestimasi dengan memperhitungkan pengaruh co-variable nya yaitu metode cokriging. Metode universal cokriging digunakan pada kasus data kandungan mineral tersampel memiliki trend tertentu dengan memperhitungkan pengaruh co-variable nya. Hasil akhir yang diperoleh adalah estimasi cadangan mineral dan peta yang menggambarkan zona potensial mineral tersebut.",,"Kata Kunci : kriging, universal kriging, cokriging, universal cokriging."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149569,PENGARUH VARIABEL IMAGE TERHADAP PEMBENTUKAN SIKAP DAN KEPERCAYAAN [Studi Kasus: Pengaruh Merk Dagang dari Perusahaan Penerbangan Domestik di Indonesia Terhadap Pembentukan Sikap dan Kepercayaan]; IMAGE VARIABLE EFFECT ON ATTITUDE AND BELIEFS FORMATION [Case Study: Brand Name Effect from Indonesian Domestic Airlines on Attitude and Beliefs Formation],"FARHANA SWEETA FITRIANA, Dedi Rosadi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Model mengenai pembentukan sikap banyak digunakan di bidang ekonomi, psikologi, psikologi sosial dan pemasaran. Bagi kalangan periset pasar, model pembentukan sikap yang paling terkenal adalah model Fishbein. Model ini menyatakan bahwa pembentukan sikap dipengaruhi oleh kepercayaan. Lebih lanjut, dikarenakan kemungkinan adanya efek halo sehingga berimplikasi bukan saja sikap dipengaruhi kepercayaan tetapi kepercayaan juga dapat dipengaruhi sikap. Selain itu pembentukan sikap dan kepercayaan juga dapat dipengaruhi faktor eksternal salah satunya adalah variabel image. Skripsi ini membahas mengenai pengaruh variabel image terhadap pembentukan sikap dan kepercayaan. Pembahasan akan diakhiri dengan studi kasus yang menyelidiki pengaruh dari merk dagang perusahaan penerbangan domestik di Indonesia terhadap pembentukan sikap dan kepercayaan. Metode yang digunakan untuk mengestimasi adalah metode dua tahap kuadrat terkecil (2SLS).",,Kata Kunci : Model Multiatribut; Model Fishbein; Efek Halo; Variabel Image; Persamaan Simultan; Metode Dua Tahap Kuadrat Terkecil (2SLS)
http://etd.repository.ugm.ac.id/home/detail_pencarian/150337,MODEL PERSAMAAN STRUKTURAL DENGAN METODE WEIGHTED LEAST SQUARE; STRUCTURAL EQUATION MODELING USING WEIGHTED LEAST SQUARE METHOD,"WURI AYU WIDATI, Sardjono",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Model persamaan struktural dilakukan untuk menguji hubungan antara variabel yang kompleks untuk memperoleh gambaran menyeluruh mengenai keseluruhan model. Analisis ini dapat menguji secara bersama-sama hubungan antara konstruk independen dengan dependen serta hubungan antara indikator dengan konstruk (variabel laten). Karya tulis ini membahas penggunaan metode Weighted Least Square dalam estimasi parameter model persamaan struktural yang merupakan suatu metode yang tidak terpengaruh oleh dilanggarnya asumsi normalitas multivariat. Estimasi nilai ? yang meminimumkan fungsi kesesuaian FWLS tidak dapat diperoleh secara langsung. Oleh karena itu, digunakan pendekatan algoritma Gauss-Newton untuk melakukan iterasi. Pembahasan dilengkapi dengan studi kasus mengenai hubungan antara komitmen professional terhadap komitmen organisasi dan kepuasan kerja para auditor di BPKP.",,"Kata Kunci : Structural Equation Modeling, Weighted Least Square."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149570,ESTIMASI MODEL SEEMINGLY UNRELATED REGRESSION DENGAN METODE MAKSIMUM LIKELIHOOD; ESTIMATION OF SEEMINGLY UNRELATED REGRESSION MODEL WITH MAXIMUM LIKELIHOOD METHOD,"AFRI NUR FAUZAMI, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Model Seemingly Unrelated Regression (SUR) merupakan suatu sistem persamaan regresi linear yang terdiri dari beberapa persamaan regresi yang saling berhubungan. Persamaan regresi dalam sistem saling berhubungan karena error antar persamaan saling berkorelasi secara contemporaneously. Dalam hal ini, metode Kuadrat Terkecil (Ordinary Least Square, OLS) dapat digunakan untuk mengestimasi parameter pada masing-masing persamaan, tetapi metode ini tidak efisien karena mengabaikan informasi bahwa error antara persamaan saling berkorelasi. Efisiensi dari estimasi parameter dapat ditingkatkan dengan memasukkan unsur korelasi ini dalam perhitungan. Dalam skipsi ini digunakan metode estimasi Maksimum Likelihood (Maximum Likelihood, ML) untuk mengestimasi parameter model SUR. Estimasi ML dapat mengakomodasi adanya korelasi antar error dengan baik. Estimasi ML didasarkan pada asumsi bahwa error estimasi berdistribusi normal multivariat. Uji rasio Likelihood (Likelihood Ratio, LR) digunakan untuk menguji adanya korelasi contemporaneosly pada error estimasi. Pembahasan akan diakhiri dengan studi kasus mengenai faktor-faktor yang mempengaruhi Pendapatan Asli Daerah (PAD) Kabupaten Bantul dan Kabupaten Sleman. Pada studi kasus disimpulkan bahwa estimasi ML lebih baik dari estimasi OLS",,Kata Kunci : Seemingly Unrelated Regression; Maximum Likelihood; Kuadrat Terkecil; Uji Likelihood Ratio
http://etd.repository.ugm.ac.id/home/detail_pencarian/149826,PERMODELAN VARIABEL YANG BERPENGARUH DAN OPTIMISASI MENGGUNAKAN METODE PERMUKAAN RESPON – RANCANGAN SUSUNAN TERPUSAT (RSM- CCD); MODELING VARIABLES INFLUENCE AND OPTIMIZATION USING RESPON SURFACE METHODOLOGY-CENTRAL COMPOSITE DESIGN (RSM-CCD),"IRMA WIDIASTUTI, Suryo Guritno",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Masalah yang sering dijumpai oleh peneliti dalam berbagai bidang secara umum mengenai variabel yang akan diteliti y dan variabel prediktor x1 , x2 ,..., xk. Peneliti harus mengira-ngira fungsi f yang tidak diketahui dengan model empirik yang tepat, ? ( , ,..., ) ?? 1 2 n y f x x x , dimana ? merupakan eror dalam sistem. Biasanya fungsi f adalah model orde pertama atau polinomial orde kedua. Model empirik ini disebut model permukaan respon. Identifikasi dan pencarian model dari data eksperimen pada model respon permukaan memerlukan rancangan percobaan, teknik regresi dan metode optimisasi. Ketiganya dikombinasikan menjadi metode permukaan respon (RSM). Rancangan susunan terpusat (CCD) telah banyak dipelajari oleh statistisi dalam analisis permukaan respon dan merupakan model orde kedua yang paling populer. Dalam skripsi ini, optimisasi dengan kriteria desain ortogonal akan didiskusikan untuk fitting model regresi permukaan respon orde kedua ketika model yang terbentuk adalah model orde kedua. Fokus yang diambil adalah pada pemodelan, pengaruh variabel proses dan optimasinya dengan pendekatan statistik menggunakan Metode Permukaan Respon- Rancangan Susunan Terpusat (RSM- CCD).",,Kata Kunci : metode permukaan respon (RSM); Rancangan susunan terpusat (CCD)
http://etd.repository.ugm.ac.id/home/detail_pencarian/149579,"OPTIMASI PORTOFOLIO UNTUK DATA TIDAK NORMAL MENGGUNAKAN PENDEKATAN MEAN ABSOLUTE DEVIATION PADA MEAN VARIANCE (Studi Kasus Saham KLBF.JK, SMSM.JK, JSMR.JK, dan INDF.JK);PORTFOLIO OPTIMIZATION FOR UNNORMAL DATA USING MEAN ABSOLUTE DEVIATION APPOACH FOR MEAN VARIANCE (Case Study Stocks of KLBF.JK, SMSM.JK, JSMR.JK, dan INDF.JK)","Rr ANNEKE MARYTA, Herni Utami",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Pembentukan portofolio umumnya didasari dengan asumsi normalitas return data. Asumsi ini penting karena apabila data berdistribusi normal maka karakteristik data dapat digambarkan oleh dua parameter, rata-rata dan variansi. Tapi pada kenyataannya, banyak data tidak memiliki return normal yang apabila diabaikan akan membuat hasil tidak akurat. Pada tugas akhir ini, portofolio dibentuk dengan menggunakan pendekatan Mean Absolute Deviation (MAD) pada optimasi portofolio Mean Variance. MAD merupakan ukuran sebaran data yang digunakan sebagai alternatif untuk return data tidak normal. Pada studi kasus digunakan data KLBF.JK, SMSM.JK, JSMR.JK, dan INDF.JK dengan empat nilai tetapan median. Hasil terbaik adalah hasil berdasar nilai sharpe ratio tertinggi",,Kata Kunci : Portofolio; Model Mean Variance; pendekatan Mean Absolute Deviation
http://etd.repository.ugm.ac.id/home/detail_pencarian/149075,REGRESI LINEAR TERSEGMENTASI (Studi Kasus : Tanaman Jarak Pagar (Jatropha curcas); SEGMENTED LINEAR REGRESSION (Implementation : Jarak Pagar Plants (Jatropha curcas),"DEWITA UTAMI, Zulaela",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Regresi linear adalah model yang mempunyai fungsi linear baik dalam variabel maupun dalam parameter. Syarat dari regresi linear adalah fungsi tersebut linear sepanjang interval variabel independen. Apabila ditemui kasus regresi X terhadap Y mengikuti hubungan linier tertentu pada interval nilai X tertentu, tetapi juga memiliki hubungan linier tertentu pada interval X yang lain, maka pada kasus tersebut tidak bisa digunakan regresi linear karena tidak mencukupi deskripsi, sehingga harus digunakan regresi linear tersegmentasi. Regresi linear tersegmentasi merupakan bentuk regresi yang meliputi berbagai model linear yang cocok dengan data untuk setiap interval X. Breakpoint merupakan nilai X dimana slope fungsi linear berubah Berdasarkan perbandingan mean square error dari ketiga model, yaitu regresi linear, regresi linear tersegmentasi, dan regresi power, diperoleh hasil bahwa regresi linear tersegmentasi adalah model yang paling baik untuk memodelkan pengaruh jumlah cabang primer terhadap diameter batang pada tanaman jarak pagar.",,"Kata Kunci : Regresi linear, segmentasi, breakpoint, tanaman jarak pagar"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148565,ESTIMASI JOINT MAKSIMUM LIKELIHOOD UNTUK MODEL TEORI RESPON BUTIR EMPAT PARAMETER LOGISTIK [Studi Kasus : Karakteristik Soal PPDB SMAN 1 Pati 2010]; JOINT MAXIMUM LIKELIHOOD ESTIMATION FOR ITEM RESPONSE THEORY FOUR PARAMETER LOGISTICS MODEL [Case Study : Test characteristic PPDB SMAN 1 Pati 2010],"KARTINI, Sri Haryatmi Kartiko",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Model 4-PL IRT adalah model IRT dikotomus unidimensional yang dapat digunakan untuk mengetahui karakteristik butir yang mencakup tingkat kesukaran, daya pembeda, probabilitas menjawab butir dengan benar secara tebakan dan probabilitas menjawab butir dengan kecerobohan. Kurva yang mendeskripsikan hubungan antara parameter kemampuan subyek dan probabilitas menjawab dengan benar disebut Item Characteristic Curve (ICC). Dalam skripsi ini dilakukan estimasi parameter dengan menggunakan metode Joint Maksimum Likelihood. Metode tersebut kemudian diaplikasikan pada data seleksi Penerimaan Peserta Didik Baru (PPDB) SMA Negeri 1 Pati tahun 2010.",,"Kata Kunci : Item Response Theory (IRT), Item Characteristic Curve (ICC), Metode estimasi Joint Maksimum Likelihood, model 4-PL IRT."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150101,PENDEKATAN BAYESIAN UNTUK KOMPONEN VARIANSI DALAM RANCANGAN BUJUR SANGKAR LATIN; BAYESIAN APPROACH FOR COMPONENT VARIANCE IN LATIN SQUARE DESIGN,"MIRA ANUGRAH TINAMPI, Dedi Rosadi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Pada suatu percobaan bila unit-unit percobaan relatif heterogen, dibutuhkan suatu rancangan percobaan yang dapat mengendalikan variasi yang terjadi pada percobaan tersebut. Cara menghilangkan dua jenis variasi dapat menggunakan Rancangan Bujur Sangkar Latin (RBSL) yaitu percobaan dengan cara melaksanakan pemblokan dua arah. Dalam setiap analisis rancangan percobaan akan dihasilkan nilai variansi pada parameter penduga. Metode penduga parameter yang baik akan memberikan penduga yang tak bias, menghasilkan ragam kecil dan efisien. Komponen variansi pada suatu percobaan sangat mungkin menghasilkan nilai yang negatif. Pendekatan Bayes pada model acak rancangan bujur sangkar latin memiliki potensi yang cukup baik digunakan untuk pendugaan karena diperoleh nilai variansi total pada model yang lebih kecil dibandingkan dengan metode kuadrat terkecil dan metode maksimum likelihood karena Pada kasus pemberian perlakuan exercise pada sapi Simmental, analisis variansi menunjukkan bahwa pemberian perlakuan exercise tidak memberikan pengaruh dalam peningkatan kualitas semen sapi Simmental, terlihat dari persentase sperma yang terdapat pada semen sapi Simmental serta menghasilkan komponen variansi yang bernilai negatif.",,"Kata Kunci : RBSL, Pendekatan Maksimum Likelihood, Pendekatan Bayesian, Komponen Variansi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150362,ANALISIS FAKTOR-FAKTOR YANG MEMPENGARUHI KONSUMEN DALAM MEMILIH MOTOR MATIC MERK HONDA BERDASARKAN METODE FUZZY PHA; ANALYSIS FACTORS THAT ARE INFLUENCING CONSUMER IN SELECTING HONDA MATIC MOTORCYCLE BASED ON FUZZY PHA METHOD,"WINDY ROSTAN TRININGRUM, Dedi Rosadi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Konsumen seringkali bingung dalam mengambil keputusan untuk membeli suatu produk karena banyaknya faktor yang mempengaruhi terhadap pilihan-pilihan yang ada. Dari penelitian didapatkan ketidakpastian penilaian yang terlalu subjektif untuk data yang kualitatif. Permasalahan di atas dapat diselesaikan dengan metode Fuzzy PHA dengan cara memakai penilaian dalam interval sehingga data yang kualitatif dapat memberikan penilaian yang lebih objektif. Penelitian ini bertujuan untuk menganalisis faktor yang mempengaruhi konsumen dalam memilih motor matic merk Honda. Data diambil dengan menyebar kuesioner. Dari hasil kuesioner dilakukan penghitungan konsistensi rasio (CR). Jika CR < 0,10 artinya konsisten, dapat digunakan untuk penelitian selanjutnya. Berdasar hasil penelitian, faktor yang paling mempengaruhi konsumen adalah Faktor Harga dan Desain, kemudian Faktor Kualitas dan yang terakhir Faktor Pelayanan.",,"Kata Kunci : Konsistensi Rasio, Metode Fuzzy PHA, Motor matic"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149083,ESTIMASI HARGA MINIMAL PADA KEBIJAKAN GARANSI RENEWING TUNGGAL SATU DIMENSI; MINIMUM COST ESTIMATE UNDER ONE-DIMENSION SINGLE RENEWING WARRANTY,"KUKUH AGENG PRIBOWO, Adhitya Ronnie Effendie",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam dunia industri,persaingan dalam pemasaran semakin ketat dan membutuhkan strategi yang selalu baru sehingga dapat menarik pasar dan secara perlahan menguasai pasar.Salah satu faktor yang dapat menarik minat konsumen adalah kepercayaan pelanggan terhadap produk dan rasa aman saat memutuskan untuk membeli.Kepercayaan ini melekat erat pada ketahanan produk yang tercermin pada kemampuan perusahaan dalam memberikan garansi.Semakin membuat konsumen merasa aman maka pelanggan akan semakin loyal terhadap produk.Loyalitas inilah yang akan menguatkan pasar dan juga nama baik produk khususny dan perusahaan pada umumnya.Akan tetapi jaminan paska jual ini tentunya juga memberikan beban biaya yang harus diperhitungkan oleh pihak perusahaan.Maka dari itu perlu pengetahuan untuk menetapkan harga yang seimbang yaitu yang tidak merugikan pihak perusahaan dan tidak juga terlalu memberatkan konsumen.",,"Kata Kunci : Garansi, reliabilitas, renewing, free replacement, pro-rata"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149595,METODE DELTA PADA ANALISA REGRESI DENGAN MEDIATOR; DELTA METHOD ON REGRESSION ANALYSIS WITH MEDIATOR,"DIKO REZA ARTHA, Zulaela",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Proses mediasi terjadi ketika terdapat variabel ketiga yang berperan sebagai perantara dalam hubungan antara variabel independen dengan variabel dependen. Variabel yang menjadi perantara disebut variabel mediasi atau mediator. Persamaan regresi dapat digunakan dalam mengkaji proses mediasi. Pengujian hipotesa mediasional atau signifikansi keberadaan mediator dalam proses mediasi, didasarkan pada nilai estimasi dan standar error efek mediasi. Metode delta dapat digunakan untuk memperoleh estimasi standar error efek mediasi. Karya tulis ini membahas proses mediasi secara konseptual serta peranan metode delta pada pengujian signfikansi efek mediasi.",,Kata Kunci : mediasi secara konseptual; peranan metode delta pada pengujian signfikansi efek mediasi.
http://etd.repository.ugm.ac.id/home/detail_pencarian/149084,ESTIMASI PROBABILITAS KEGAGALAN BERSYARAT DENGAN MENGGABUNGKAN MODEL KMV DAN METODE CONDITIONAL VALUE AT RISK; CONDITIONAL PROBABILITY OF DEFAULT ESTIMATION WITH COMBINE THE KMV MODEL AND CONDITIONAL VALUE AT RISK METHOD,"TECLA SEPTYANINGSIH, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Conditional Value at Risk (CVaR) merupakan metode yang digunakan untuk mengukur risiko pasar yang ekstrim atau untuk mengukur besarnya kerugian yang diperkirakan nilainya lebih dari Value at Risk. Model KMV merupakan model yang digunakan untuk meramalkan default dengan menghitung probabilitas default perusahaan untuk waktu kapanpun yang diberikan. Dalam tugas akhir ini, penulis menggabungkan model KMV dan metode CVaR untuk membangkitkan sebuah model yang digunakan untuk mengukur resiko kredit dalam kondisi pasar yang ekstrim berdasarkan probabilitas kegagalan bersyarat. Data yang digunakan dalam studi kasus adalah data total aset perusahaan, liabilitas jangka pendek dan liabilitas jangka panjang sebagai default point, dan jangka waktu jatuh tempo",,"Kata Kunci : Conditional Value at Risk/CVaR, Model KMV, Risiko Kredit, Distance to Default, Conditional Distance to Default, Probability of Default, Conditional Probability of Default"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150368,LATENT ROOT REGRESSION ANALYSIS UNTUK MENANGANI KASUS NON-PREDICTIVE MULTIKOLINEARITAS; LATENT ROOT REGRESSION ANALYSIS FOR DEALING NON PREDICTIVE MULTICOLLINEARITY,"ERMA APRILIANA, Sri Haryatmi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,N,,"Kata Kunci : Latent Root Regression Analysis, non-predictive multikolinearitas, multikolinearitas"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149359,ESTIMASI BREAK-POINT PADA REGRESI SEGMENTASI LINEAR SEDERHANA MELALUI EMPIRICAL LIKELIHOOD; BREAK-POINT ESTIMATION VIA EMPIRICAL LIKELIHOOD FOR A SEGMENTED SIMPLE LINEAR REGRESSION,"M. HASAN SIDIQ K, Zulaela",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi linear merupakan salah satu analisis dalam statistika yang bertujuan untuk mengetahui hubungan kelinearan antara dua variabel atau lebih, sehingga salah satu variabel dapat diduga dari variabel lainnya. Jika variabel yang dianalisis hanya berjumlah dua buah, yang terdiri dari satu buah variabel prediktor dan satu buah variabel respon, maka disebut dengan analisis regresi linear sederhana. Model regresi segmentasi linear sederhana merupakan perluasan dari model regresi linear sederhana. Model regresi segmentasi digunakan jika variabel prediktor memiliki lebih dari satu pola hubungan dengan variabel respon. Selain itu, juga digunakan pada kasus dimana variabel prediktor dan respon memiliki pola hubungan yang tetap namun pada saat prediktor berada pada nilai-nilai tertentu ternyata memiliki persamaan garis regresi yang berbeda. Jika analisis regresi linear biasa diterapkan untuk data yang seperti itu, mungkin saja model regresi yang diperoleh kurang representatif. Sehingga untuk mengatasinya digunakan regresi segmentasi. Pada pembentukan model regresi segmentasi nilai break-point harus diestimasi. Dalam skripsi ini akan dibahas mengenai estimasi break-point pada regresi segmentasi linear sederhana dengan memanfaatkan konsep empirical likelihood.",,Kata Kunci : Analisis regresi linear; break-point; empirical likelihood
http://etd.repository.ugm.ac.id/home/detail_pencarian/148851,ESTIMASI CADANGAN KLAIM IBNR (Incurred But Not Reported) MENGGUNAKAN METODE COPULA; THE ESTIMATION OF IBNR CLAIM RESERVES USING COPULA METHOD,"NINDYASARI EKA RISTYANI, Adhitya Ronnie Effendie",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam prakteknya, hampir setiap hari akan terjadi klaim di sebuah perusahaan asuransi. Akan tetapi biasanya klaim-klaim tersebut tidak dilaporkan pada hari yang sama dan ketika periode akuntansi berakhir, banyak klaim terjadi yang masih belum dilaporkan. Untuk memiliki perkiraan rugi dan laba dari sebuah perusahaan asuransi, diperlukan ditemukannya cadangan untuk klaim tersebut. Secara ideal cadangan dalam perusahaan asuransi harus cukup jumlahnya dan jangan berlebih-lebihan, guna untuk membayar semua kerugian yang terjadi. Oleh karena itu masalahnya adalah bagaimana menghitung tepat terjadinya cadangan klaim namun yang belum dilaporkan, yang selalu menjadi perhatian aktuaris dalam perusahaan asuransi. Langkah yang dilakukan adalah mengumpulkan data ukuran klaim dan waktu pengembangan (waktu saat klaim sampai waktu pembayaran), menentukan distribusi marginal dari masing-masing data, menentukan copula terbaik untuk distribusi bersama, menghitung cadangan klaim IBNR menggunakan copula terbaik. Copula merupakan salah satu alternatif dalam penghitungan cadangan klaim IBNR.",,"Kata Kunci : Copula, Copula Archimedean, Cadangan Klaim IBNR"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149375,PORTOFOLIO OPTIMAL DENGAN BAYES-STEIN ESTIMATOR; OPTIMAL PORTFOLIO WITH BAYES-STEIN ESTIMATOR,"Pulung Jati Sudarminto, Suryo Guritno",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam analisis portofolio, ketidakpastian nilai parameter mengantarkan pada pilihan portofolio yang suboptimal. Hasilnya loss pada utilitas investor merupakan suatu fungsi dari estimator tertentu yang dipilih berdasarkan expected return. Jadi, ini adalah estimasi simultan dari rata-rata normal dalam suatu loss function yang terspesikasi dengan baik. Pada keadaan ini, rata-rata sampel klasik tidak dapat diterima. Skripsi ini menunjukkan estimator Bayes empiris yang sederhana yang seharusnya mengungguli rata-rata sampel dalam konteks portofolio. Hasil simulasi menunjukkan estimator Bayes-Stein ini memberikan hasil yang signifikan pada masalah pemilihan portofolio",,Kata Kunci : portofolio; estimator Bayes-Stein; utilitas; ketidakpastian
http://etd.repository.ugm.ac.id/home/detail_pencarian/149384,ESTIMASI PARAMETER MODEL SEEMINGLY UNRELATED REGRESSION (SUR) MENGGUNAKAN METODE GENERALIZED LEAST SQUARE (GLS); PARAMETER ESTIMATION OF SEEMINGLY UNRELATED REGRESSION (SUR) MODEL USING GENERALIZED LEAST SQUARE (GLS) METHOD,"PUSPA DEWI MULAWATI, Abdurakhman",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi merupakan suatu analisis yang digunakan untuk menjelaskan hubungan antara satu variabel respon dengan satu atau beberapa variabel prediktor dalam suatu persamaan tunggal. Suatu perkembangan dari analisis regresi yang disebut Seemingly Unrelated Regression adalah suatu model yang digunakan pada suatu sistem persamaan atau terdapat beberapa persamaan tunggal yang dianalisis secara bersama-sama. Model Seemingly Unrelated Regression digunakan jika galat dari persamaan dalam suatu sistem persamaan berkorelasi secara contemporaneously (secara serentak). Langkah estimasi pada model Seemingly Unrelated Regression adalah mendapatkan galat dari masing-masing persamaan dengan menggunakan metode Ordinary Least Square (OLS), kemudian digunakan Lagrange Multiplier untuk mengetahui korelasi antar galat. Jika galat berkorelasi maka model diestimasi kembali dengan menggunakan metode Generalized Least Square (GLS).",,Kata Kunci : Seemingly Unrelated Regression; contemporaneously; Ordinary Least Square; Lagrange Multiplier; Generalized Least Square
http://etd.repository.ugm.ac.id/home/detail_pencarian/148904,ANALISIS BAYESIAN MODEL TITIK UBAH TUNGGAL PADA FUNGSI KEGAGALAN KONSTAN DAN APLIKASINYA MENGGUNAKAN ALGORITMA GIBBS SAMPLING; BAYESIAN ANALYSIS FOR SINGLE CHANGE-POINT MODEL IN CONSTANT HAZARD FUNCTION AND ITS APPLICATION USING GIBBS SAMPLING ALGORITHM,"RODIQ ROVIAN ALTRIYANTO, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Fungsi kegagalan konstan menunjukkan bahwa tingkat kegagalan bersifat konstan untuk setiap periode waktu. Namun, dalam selang periode ini dimungkinkan terjadi fenomena perubahan tingkat kegagalan tersebut sehingga perlu diajukan model titik ubah sebagai alternatif solusinya. Pendekatan Bayesian dilakukan untuk mengestimasi parameter tingkat kegagalan dan titik ubah. Metode Markov Chain Monte Carlo (MCMC) digunakan untuk membantu menyelesaikan analisis Bayesian dengan algoritma Gibbs sampling, yang merupakan salah satu algoritma dari MCMC. Kriteria faktor Bayes juga dikembangkan untuk memilih model terbaik antara model fungsi kegagalan konstan dan model titik ubah tunggal pada fungsi kegagalan konstan. Model titik ubah mampu mengakomodasi perubahan tingkat kegagalan yang terjadi dengan baik. Metodologi ini diimplementasikan pada data riil dari waktu antar kerusakan mobil suatu perusahaan persewaan mobil.",,"Kata Kunci : Kegagalan konstan, titik ubah, Gibbs sampling, metode MCMC, faktor Bayes"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149431,EKSPEKTASI ONGKOS GARANSI DUA DIMENSI NON-RENEWING FREE REPLACEMENT WARRANTY MENGGUNAKAN METODE COPULA; THE EXPECTATION OF TWO DIMENTIONAL WARRANTY COST NON-RENEWING FREE REPLACEMENT WARRANTY USING COPULA METHOD,"LINA ISTIQOMAH, Adhitya Ronnie Effendie",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Garansi merupakan suatu kesepakatan kontraktual antara produsen dan konsumen berkaitan dengan penjualan produk. Dengan adanya penawaran garansi akan menimbulkan biaya tambahan yang disebut ongkos garansi. Sehingga estimasi ongkos garansi yang akurat sangat penting untuk dilakukan. Dalam tugas akhir ini akan dilakukan studi garansi dua dimensi dengan kebijakan Non- Renewing Free Replacement Warranty untuk produk non-repairable, dimana garansi dibatasi oleh umur dan jarak pemakaian. Untuk membangun distribusi bersama kedua variabel tersebut digunakan fungsi copula, yaitu merupakan suatu metode dalam pemodelan distribusi gabungan yang tidak mengasumsikan normalitas data sehingga dapat digunakan untuk berbagai distribusi. Pada persamaan integral dua dimensi yang rumit, pendekatan simulasi memberikan metode alternatif untuk perhitungan tanpa membutuhkan penyelesaian analitik. Pada akhirnya, perhitungan ekspektasi ongkos garansi dua dimensi pada skripsi ini menggunakan metode simulasi, dimana pasangan variabel random untuk kegagalan ke-i dibangun dari distribusi marginal yang bersesuaian dengan masing-masing distribusi kegagalan awal. Selanjutnya untuk perhitungan estimasi ekspektasi banyaknya renewal digunakan distribusi bersama copula archimedian terbaik dari data, sehingga kemudian bisa diperoleh nilai ekspektasi ongkos garansi per unit penjualan",,Kata Kunci : Archimedean; copula; garansi dua dimensi; non-renewing FRW
http://etd.repository.ugm.ac.id/home/detail_pencarian/149687,PERBANDINGAN PORTOFOLIO OPTIMAL MODEL BLACK-LITTERMAN PENDEKATAN BAYES TERHADAP PORTOFOLIO OPTIMAL CAPITAL ASSET PRICING MODEL (Studi Kasus Pada Saham-Saham LQ-45 di BEI Periode Juni 2010 - Juni2011),"FAUZAN AZHARI, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Model Black Litterman (BLM), model yang berkembang pada tahun 90 an yaitu suatu model untuk optimisasi portofolio dengan menggunakan estimasi dari mean return yang terboboti suatu kombinasi dari vektor mean untuk mean return yang berdasar pada model kesetimbangan dan ‘dugaan’ yang merefleksikan pandangan/views investor tentang return. Dengan mengamati empat saham dari indeks LQ 45 yaitu AALI, ASII, ITMG, dan BMRI akan dibentuk suatu portofolio yang diharapkan dapat menghasilkan keuntungan yang lebih baik dengan menggunakan model Black Litterman dibandingkan dengan model CAPM",,Kata Kunci : Portofolio Optimal; CAPM; Model Black Litterman
http://etd.repository.ugm.ac.id/home/detail_pencarian/150199,GRAFIK PENGENDALI ROBUST BERDASARKAN MEDIAN ABSOLUTE DEVIATION (MAD); ROBUST CONTROL CHART BASED ON MEDIAN ABSOLUTE DEVIATION (MAD),"Fitri Purwani, Subanar",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Grafik pengendali adalah salah satu alat yang paling kuat yang digunakan untuk mendeteksi perilaku yang menyimpang dalam proses industri. Grafik pengendali Shewhart S adalah salah satu yang paling banyak digunakan sebagai teknik pengendalian proses statistik yang dikembangkan untuk mengontrol variabilitas proses berdasarkan asumsi dasar bahwa distribusi yang mendasari karakteristik kulitas adalah normal. Ketika asumsi normalitas yang mendasari tidak terpenuhi, metode robust adalah salah satu metode statistik yang paling sering digunakan sebagai pilihan dalam situasi seperti itu. Dipresentasikan sebuah pendekatan sederhana untuk mengestimasi secara kuat standar deviasi proses s berdasarkan estimator skala yang sangat robust, yang disebut dengan median absolute deviation dari median sampel (MAD). Metode ini menyediakan sebuah alternatif untuk grafik pengendali Shewhart S. Contoh numerik berdasarkan simulasi digunakan untuk menggambarkan tampilan dari metode robust dan membandingkannya dengan metode Shewhart. Metode robust terlihat lebih baik daripada metode Shewhart dan mempunyai sifat yang baik untuk distribusi tidak normal khususnya berekor tebal.",,"Kata Kunci : Shewhart, Robust, S, MAD, PGR"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150200,MODEL GROWTH CURVE LINEAR; LINEAR GROWTH CURVE MODEL,"BRIGITA ANITA SARI, Abdurakhman",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Multivariate Analysis of Variance (MANOVA) adalah sebuah metode analisis untuk mengetahui apakah ada perbedaan yang nyata pada beberapa variabel respon antar grup. Suatu perkembangan dari MANOVA yang disebut model growth curve adalah sebuah model Generalized Multivariate Analysis of Variance (GMANOVA) yang digunakan khususnya untuk menganalisis masalah pertumbuhan pada runtun waktu yang pendek (short time series) antara lain dalam bidang biologi dan kesehatan. Tujuan dari analisis growth curve adalah pembandingan parameter untuk mengetahui apakah terdapat perbedaan pertumbuhan variabel respon yang diamati pada suatu runtun waktu tertentu, pada beberapa grup yang ada. Model growth curve linear (linear growth curve model) digunakan untuk menganalisis respon pertumbuhan yang linear. Langkah dari analisis growth curve adalah pengujian asumsi model growth curve, estimasi parameter model growth curve menggunakan metode Generalized Least Square Estimator (GLSE), dan pembandingan parameter model growth curve.",,"Kata Kunci : linear growth curve model, Generalized Multivariate Analysis of Variance, Multivariate Analysis of Variance, Generalized Least Square Estimator"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150201,PITA KEPERCAYAAN UNTUK PERHITUNGAN PREMI DALAM ASURANSI KESEHATAN; CONFIDENCE BANDS FOR PREMIUMS CALCULATION IN HEALTH INSURANCE,"HENDRA PERDANA, Adhitya Ronnie Effendie",2011 | Skripsi | PROGRAM STUDI STATISTIKA,PITA KEPERCAYAAN UNTUK PERHITUNGAN PREMI DALAM ASURANSI KESEHATAN; CONFIDENCE BANDS FOR PREMIUMS CALCULATION IN HEALTH INSURANCE,,"Kata Kunci : model multi status, intensitas transisi, probabilitas transisi, Confidence Band, EP bands, premi asuransi kesehatan."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150204,"ESTIMASI PARAMETER REGRESI NONLINIER MENGGUNAKAN METODE ITERASI BERNDT, HALL, HALL, DAN HAUSMAN (BHHH); ESTIMATING PARAMETER OF NONLINEAR REGRESSION USING BERNDT, HALL, HALL, AND HAUSMAN (BHHH) ITERATION","ASNA FAUZIYAH, Abdurakhman",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis regresi adalah analisis yang digunakan untuk mengetahui hubungan antara dua variabel atau lebih. Sejauh ini, kita telah mengenal regresi linier, atau regresi dengan parameter yang linier. Berdasarkan kelinieran parameter dalam model regresi, ada dua macam model regresi yaitu model regresi linier, dan model regresi nonlinier. Model regresi nonlinier dapat diselesaikan dengan beberapa macam cara. Metode maksimum likelihood adalah salah satu metode yang dapat digunakan untuk mengestimasi parameter dalam model regresi nonlinier. Dalam skripsi ini, digunakan iterasi Berndt, Hall, Hall, dan Hausman (BHHH). Algoritma BHHH ini hanya membutuhkan turunan pertama dari fungsi log-likelihood dan relatif mudah untuk digunakan. Untuk menguji model nonlinier yang cocok digunakan, dilakukan verifikasi dengan uji Rasio Likelihood.",,"Kata Kunci : Regresi nonlinier, metode maksimum likelihood, BHHH."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150205,PEMODELAN JALUR PARTIAL LEAST SQUARE HIERARKI PADA PENGUKURAN FORMATIF (HIPLS-PM); HIERARCHICAL PARTIAL LEAST SQUARE PATH MODELING ON FORMATIVE MEASUREMENT (HIPLS-PM),"PARLITASARI, Abdurakhman",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"PLS mempunyai banyak kelebihan dibanding SEM, diantaranya tidak mensyaratkan sampel harus dalam jumlah banyak dan tidak harus berdistribusi normal multivariat, serta dapat menangani model pengukuran bertipe formatif. Variabel laten dalam penelitian terkadang merupakan variabel yang multidimensi, yaitu dipengaruhi oleh variabel laten lainnya. Nilai dari variabel laten endogen (second order factor) dipengaruhi oleh variabel laten eksogen (first order factor) yang ada dalam model. Ada beberapa penelitian yang hanya mempunyai data untuk indikator-indikator variabel eksogen saja, sedangkan indikator untuk variabel endogen tidak ada karena variabel endogen tersebut merupakan gambaran keseluruhan (global score) dari variabel eksogen. Indikator yang digunakan untuk membentuk variabel endogen tersebut adalah indikator-indikator dari semua variabel eksogen yang ada, sehingga terjadi pengulangan indikator. Untuk mendapatkan nilai variabel laten endogen tersebut digunakan pendekatan superblok atau yang lebih dikenal dengan PLS-PM hierarki (HIPLS-PM).",,"Kata Kunci : pendekatan komponen hierarki, konstruk orde tinggi, ordinary least square (OLS), indikator formatif, superblok"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149694,TEKNIK RESPONSE BASED UNIT SEGMENTATION (REBUS) DALAM PARTIAL LEAST SQUARE PATH MODELING (PLS-PM); A RESPONSE BASED FOR DETECTING UNIT SEGMENTS IN PARTIAL LEAST SQUARE PATH MODELING,"DAVID CANDRA PURNAMA, Dedi Rosadi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Segmentasi dalam kerangka model Partial Least Square - Path Modeling (PLS-PM) merupakan salah satu isu kritis dalam ilmu-ilmu sosial. Asumsi bahwa data yang dikumpulkan berasal dari sebuah populasi homogen sering dianggap tidak masuk akal. Sejumlah teknik klaster yang diterapkan pada level variabel manifest pun dianggap tidak efektif dalam mengatasi masalah heterogenitas pada estimasi model jalur. Oleh sebab itu, ketiga model yang dikembangkan dalam PLS, yakni Finite Mixture-PLS (FIMIX-PLS), PLS-Typological Path Modeling (PLS-TPM) dan REBUS-PLS dipercaya dapat menjadi solusi dari masalah ini. REBUS-PLS menutupi kelemahan 2 metode sebelumnya dengan mengklaster data berdasarkan nilai pendekatan antar unit antar model lokal yang disebut indeks closeness measure (CM) yang dianggap lebih tepat sehingga akan didapat model terbaik yang dapat dijadikan acuan peneliti",,Kata Kunci : Partial Least Square; FIMIX-PLS; PLS-TPM; REBUS-PLS; Klaster; Indeks Closeness Measure
http://etd.repository.ugm.ac.id/home/detail_pencarian/150206,PENENTUAN BOBOT RISIKO KREDIT DENGAN PENDEKATAN FOUNDATION INTERNAL RATING BASED (F-IRB); THE DETERMINATION OF WEIGHTS CREDIT RISK WITH FOUNDATION INTERNAL RATING BASED APPROACH (F-IRB),"DEWI ANTARI ESTI HARTANTI, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Foundation Internal Ratings Based (F-IRB) merupakan salah satu pendekatan yang digunakan untuk menghitung risiko kredit. Penjelasan pendekatan ini terdapat di dalam “International Convergence of Capital Measurement and Capital Standards – a Revised Framework” atau lebih dikenal dengan Basel II. Pendekatan F-IRB ini memperbolehkan suatu bank dapat menggunakan model internal rating mereka sendiri dalam mengelompokkan eksposur-eksposurnya. Manfaat dari pendekatan ini bank dapat memperoleh perhitungan risiko yang lebih sensitif sebab perhitungan ini berdasarkan pada profil risiko bank. Tujuan utamanya adalah untuk menghitung bobot dari risiko kredit. Menghitung bobot risiko sama dengan menghitung jumlah modal yang yang harus disisihkan untuk mengatasi risiko kredit, sehingga Bank dapat menghitung besarnya risiko yang dihadapi Bank tersebut apabila eksposurnya mengalami default. Komponen risiko pada FIRB antara lain Probability of Default (PD), Loss Given Default (LGD), Eksposur At Default (EAD), dan Maturity (M). Model Probability of Default (PD) menggunakan Default Probability Three Factor Structural Model (Cho-Hoi Hui, 2002), dimana model ini yang menggabungkan nilai aset perusahaan dan nilai nominal kewajiban perusahaan (Face Value) dengan tingkat suku bunga. Sedangkan nilai Loss Given Default (LGD) dan Eksposur At Default (EAD) telah ditentukan oleh lembaga credit rating (pengawas) tersebut. Nilai Maturity (M) dapat menggunakan maturity yang telah ditentukan oleh lembaga credit rating (pengawas) atau menggunakan nilai yang diperoleh dari informasi bank.",,"Kata Kunci : Basel II, Risiko Kredit, Capital Charge, ATMR, PD, LGD, EAD, M."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149696,CAPM STANDAR DAN OFFICER DALAM MENGESTIMASI HARGA HARAPAN RETURN DENGAN ADANYA PAJAK; STANDARD CAPM AND OFFICER’S CAPM FOR ESTIMATING EXPECTED RETURN WITH DIFFERENTIAL TAXATION,"Anggi Novita Kartikasari, Gunardi",2011 | Skripsi | PROGRAM STUDI STATISTIKA,Salah satu asumsi Capital Asset Pricing Model (CAPM) standar adalah tidak adanya pajak pendapatan. Banyak peneliti mencoba untuk melepaskan asumsi tersebut karena pada kenyataannya investor akan dikenakan pajak tertentu. Investor akan mengisyaratkan return yang tinggi sebelum adanya pajak. Officer menggunakan rumus CAPM tersebut untuk memasukkkan efek kredit imputasi yang merupakan sistem perpajakan di Australia. Bentuknya hampir sama dengan CAPM standar hanya rumus return saja yang berbeda. Model Officer ini mengsumsikan bahwa pajak pada capital gain dan ordinary income adalah sama. Martin Lally dan Tony Van-Zijl telah mengembangkan CAPM dalam kasus adanya tarif pajak yang berbeda antara capital gain dan ordinary income (dividen) serta adanya sistem kredit imputasi atas dividen. Yaitu dengan menggunakan model Officer yang dimodifikasi dengan adanya penambahan nilai ?,,Kata Kunci : Capital Asset Pricing Model; pajak personal
http://etd.repository.ugm.ac.id/home/detail_pencarian/149449,PEMODELAN MULTILEVEL 2-LEVEL PADA REGRESI LINEAR GANDA,"Nindya Arintyas Wigati, Zulaela",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Model multilevel adalah pengembangan teknik yang cukup popular pada analisa data hierarki yang juga merupakan teknik inferensi dalam statistika yang dirancang untuk memfasilitasi penarikan kesimpulan pada data berbentuk hierarki. Model multilevel ini mempunyai beberapa nama lain seperti permodelan hierarki linear, pemodelan koefisien random, atau estimasi Bayes empiris, nama-nama tersebut juga sering digunakan tergantung kepada fungsinya pada bidang penelitian masing-masing orang. Namun, dasar dari ilmu ini adalah sama yaitu dari data yij yang diberikan. Menggambarkan observasi ke-i pada grup ke-j, dimana grup ke-j mempunyai sebanyak nj observasi. Meskipun beberapa level mungkin dibutuhkan, tapi pada skripsi ini hanya akan mebahas kasus dimana unit grup pertaman dikelompokan kedalam unit grup kedua. Secara periodik dimaksudkan kepada studi kasus dimana murid (level-1) tersarang pada sekolah (level-2)",,Kata Kunci : Model Multilevel; Multilevel Linear; Multilevel Regresi
http://etd.repository.ugm.ac.id/home/detail_pencarian/150223,ANALISIS PROFIL MELALUI MULTIDIMENSIONAL SCALING UNTUK MENGGAMBARKAN POLA PROFIL NILAI UJIAN SEKOLAH; PROFILE ANALYSIS THROUGH MULTIDIMENSIONAL SCALING TO DESCRIBE THE PROFILE PATTERN OF SCHOOL EXAMINATION SCORE,"BETY RIYANA PUTRI, Zulaela",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis Profil Melalui Multidimensional Scaling (PAMS) merupakan teknik dengan menggunakan metodelogi Multidimensional Scaling (MDS) untuk memodelkan profil seseorang. Prosedur PAMS dimulai dengan sebuah matrik data di mana baris mewakili seseorang dan kolom mewakili nilai mereka pada setiap variabel observasi. Studi kasus ini mendeskripsikan secara eksplisit model MDS untuk data profil, mengaplikasikan model PAMS, dan menggunakan teknik bootstrap untuk mengestimasi standar error dari koordinat MDS. Prosedur PAMS terdiri dari MDS sederhana, estimasi parameter subjek, estimasi standar error dari nilai skala, dan menemukan signifikansi statistik dari nilai skala.",,"Kata Kunci : analisis MDS, analisis PAMS, bootstrap, parameter subjek, standar error, nilai skala."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150234,RANCANGAN LATIS SEIMBANG; BALANCED LATTICE DESIGN,"PRAMITA PRATIWI, Herni Utami",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Rancangan percobaan merupakan salah satu langkah penting dalam pencapaian hasil untuk sebuah percobaan. Dalam penerapannya, rancangan percobaan memiliki banyak macam dan kegunaan. Salah satu bentuk rancangan percobaan adalah rancangan latis seimbang yang merupakan bagian dari rancangan blok tidak lengkap seimbang. Rancangan ini tepat digunakan ketika sejumlah perlakuan yang cukup besar akan diujikan. Rancangan latis seimbang merupakan rancangan dengan bentuk khusus dimana ukuran blok (k) lebih kecil daripada jumlah perlakuan (t) dan jumlah perlakuan merupakan bentuk kuadrat dari ukuran blok, serta jumlah ulangan merupakan ukuran blok ditambah satu. Salah satu bentuk penggunaan rancangan ini adalah untuk membandingkan hasil dari varietas padi. Dengan menggunakan rancangan latis seimbang ini semua pasangan varietas dapat dibandingkan dengan tingkat presisi yang sama.",,Kata Kunci : LATIS SEIMBANG
http://etd.repository.ugm.ac.id/home/detail_pencarian/150235,MODEL ANALISIS FAKTOR KONFIRMATORI 2 LEVEL [Studi Kasus: Kecerdasan Anak Berdasarkan Groninger Inttelegence Test]; TWO LEVEL CONFIRMATORY FACTOR ANALYSIS MODEL [Case Study:Children’s Inttelegence Based on Groninger Inttelegence Test ],"INTAN NIRMALA, Abdurakhman",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Analisis faktor konfirmatori dilakukan dengan tujuan untuk mengkonfirmasi apakah suatu konstruk dapat dikonfirmasi dengan data empirisnya. Analisis ini harus didukung oleh teori yang kuat. Pada analisis faktor konfirmatori klasik, analisis difokuskan kepada sejumlah unit observasi yang independen satu sama lain. Akan tetapi, analisis tersebut tidak bisa diterapkan ketika unit observasi tersarang dalam suatu struktur hierarki atau multilevel. Struktur hierarki memberikan satu masalah tersendiri, karena asumsi standar bahwa unit observasi bersifat independently identically distributed tidak ditemukan. Individu-individu dalam grup yang sama menghasilkan observasi yang saling berkorelasi. Menangani data berstruktur hierarki dengan menggunakan model klasik akan memberikan hasil yang menyesatkan. Karya tulis ini membahas model analisis faktor konfirmatori 2 level untuk menangani data dengan struktur hierarki 2 level. Spesifikasi model dilakukan dengan between and within formulation dan digunakan pendekatan kemungkinan maksimum untuk mengestimasi parameter dari model analisis faktor konfirmatori 2 level. Estimasi nilai ? yang meminimumkan fungsi kesesuain tidak dapat diperoleh secara langsung. Oleh karena itu, digunakan algoritma EM Step ((Expectation Maximization Step) untuk melakukan iterasi. Pembahasan dilengkapi dengan studi kasus mengenai kecerdasan anak berdasarkan Groninger Inttelegence Test dari penelitian Van Peet.",,"Kata Kunci : Analisis Faktor Konfirmatori 2 Level, Between and Within Formulation , Estimasi Kemungkinan Maksimum, Iterasi EM Step."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149242,OPTIMALISASI HARGA EKSPEKTASI GARANSI DUA DIMENSI BERDASARKAN STRATEGI LAYANAN IMPERFECT REPAIR; OPTIMAL COST OF A TWO DIMENSIONAL WARRANTY SERVICING STRATEGY WITH AN IMPERFECT REPAIR OPTION,"WANDI RIDHO PUTRA, Adhitya Ronnie Effendie",2011 | Skripsi | PROGRAM STUDI STATISTIKA,"Jika produsen menjual produk dengan garansi maka akan timbul ongkos tambahan (yang disebut ongkos garansi) untuk melayani klaim selama masa garansi. Penurunan ongkos garansi merupakan persoalan penting bagi produsen. Tiga cara untuk menurunkan ongkos garansi antara lain meningkatkan keandalan produk, melakukan tindakan perawatan dan menerapkan strategi layanan garansi. Penelitian ini membahas strategi layanan garansi untuk produk yang dapat diperbaiki (repairable) yang dijual dengan garansi dua dimensi dimana daerah yang ditawarkan relatif besar. Sebagai contoh mobil diberi garansi 3 tahun atau 36.000 km, tergantung yang lebih dulu dicapai. Untuk produk repairable yang dijual dengan Free Replacement Warranty (FRW), produsen memiliki pilihan untuk memperbaiki atau mengganti produk rusak dengan produk baru. Tingginya ongkos replacement dibandingkan dengan perbaikan menjadi dasar munculnya strategi imperfect repair.",,Kata Kunci : garansi dua dimensi; strategi imperfect repair; laju penggunaan produk; ekspektasi ongkos garansi
http://etd.repository.ugm.ac.id/home/detail_pencarian/148744,REGULARIZED GENERALIZED STRUCTURED COMPONENT ANALYSIS UNTUK MODEL PERSAMAAN STRUKTURAL BERBASIS KOMPONEN REGULARIZED GENERALIZED STRUCTURED COMPONENT; ANALYSIS FOR COMPONENT-BASED STRUCTURAL EQUATION MODELING,"Ardy Arya Pradhana, Danang T. Qoyyimi",2010 | Skripsi | PROGRAM STUDI STATISTIKA,"Generalized Structured Component Analysis (GSCA) telah dikemukakan sebagai pendekatan pada model persamaan struktural (SEM) berbasis komponen . Dalam penggunaannya, GSCA sering kali memuat multikolinieritas, yakni korelasi tinggi diantara variabel eksogen. Namun metode GSCA belum dapat menyelesaikan permasalahan tersebut. Jadi dalam penelitian ini dikemukakan perluasan dari GSCA yang menggabungkan sebuah tipe ridge kedalam estimasi GSCA dalam sebuah kerangka tunggal bertujuan untuk menyelesaikan multikolinieritas secara lebih efektif. Sebuah algoritma An alternating regularized least square (ARLS) dikembangkan untuk mengestimasi parameternya",,"Kata Kunci : Generalized Structured Component Analysis(GSCA), multikolinieritas, regulasi tipe ridge, Alternating Regularized Least Square(ARLS)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148256,ANALISIS CREDIT SCORING MENGGUNAKAN METODE K-NEAREST NEIGHBORS (Studi Kasus Bank Perkreditan Rakyat di Tasikmalaya Jawa Barat); CREDIT SCORING ANALYSIS USING K-NEAREST NEIGHBORS METHOD (Case Study Bank Perkreditan Rakyat in Tasikmalaya West Java ),"Maharlesa Putri, Dedi Rosadi",2010 | Skripsi | PROGRAM STUDI STATISTIKA,"Dalam praktik seringkali dijumpai data yang beratribut besar serta tidak memenuhi asumsi distribusi normalitas. Oleh karena itu diperlukan suatu metode yang dapat menyelesaikan permasalahan tersebut. Salah satu metode yang diusulkan yakni metode k-Nearest Neighbors. k-Nearest Neighbors merupakan metode klasifikasi yang paling mendasar dan sederhana, hanya dengan mengukur kedekatan suatu kasus baru dengan kasus lama yang tersimpan dalam database. Metode tersebut merupakan salah satu metode statistik nonparametrik (bebas distribusi), yang merupakan perpaduan statistika, machine learning, database dan visualisasi. Metode ini dapat diaplikasikan dalam berbagai bidang, dalam hal ini akan dibahas aplikasi untuk Credit Scoring. Hasil akhirnya yakni memprediksi kemungkinan seorang debitor mengalami gagal bayar (default), lebih lanjutnya dapat memprediksi “good” atau “bad” debitor. Disamping itu juga akan diuraikan suatu studi kasus serta program aplikasi yang dapat dipergunakan untuk menganalisa data dengan metode dimaksud.",,"Kata Kunci : Credit Scoring, k-Nearest Neighbors, Klasifikasi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148536,ESTIMASI HARGA OBLIGASI MENGGUNAKAN PENDEKATAN EKSPONENSIAL DARI UKURAN SENSITIFITAS HARGA OBLIGASI TERHADAP YIELD; ESTIMATION OF BOND PRICES USING EXPONENTIAL APPROACH OF SENSITIVITY MEASUREMENT ON BOND PRICES AND YIELD,"FEBRA ARIANI, Abdurrakhman",2010 | Skripsi | PROGRAM STUDI STATISTIKA,"Harga Obligasi ditawarkan dalam bentuk persentase dari nilai nominal. Imbal hasil yang diperoleh dinyatakan sebagai Yield, yaitu hasil yang akan diperoleh investor apabila berinvestasi obligasi. Yield merupakan hal yang penting dalam merumuskan strategi investasi obligasi, untuk mencapai imbal hasil yang lebih besar dibanding dengan risiko yang ditanggung. Strategi tersebut dapat dilakukan dengan mempelajari sensitivitas harga obligasi terhadap yield, karena ukuran sensitivitas harga obligasi dapat digunakan untuk memperkirakan perubahan harga obligasi dalam menanggapi perubahan tingkat yield. Ukuran sensitivitas tersebut adalah Durasi dan Konveksitas. Dalam skripsi ini digunakan pendekatan Eksponensial dari nilai sensitifitas obligasi untuk mengestimasi harga obligasi.",,"Kata Kunci : Durasi, Konveksitas, Harga Obligasi dan Yield"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148285,ANALISIS FAKTOR KELAS LATEN; LATENT CLASS FACTOR ANALYSIS,"RINTO RETNADI FAUZI, Zulaela",2010 | Skripsi | PROGRAM STUDI STATISTIKA,Analisis faktor kelas laten merupakan teknik statistika yang digunakan untuk mengelompokkan objek dengan variabel manifes kategorik dan variabel laten kategorik berskala ordinal. Pengelompokan ini didasarkan pada probabilitas pengelompokan posterior yang diestimasi melalui metode maksimum likelihood dengan algoritma Ekspektasi Maksimasi (EM Algorithm). Kelebihan dari analisis ini adalah dapat digunakan dalam jumlah variabel laten yang lebih dari satu. Asumsi yang harus dipenuhi dalam pemodelan analisis faktor kelas laten adalah lokal independensi variabel manifes dalam setiap kelas laten. Dalam studi kasus dilakukan klasifikasi tipe responden yang terlibat dalam Political Action Survey.,,"Kata Kunci : Analisis faktor kelas laten, variabel laten, lokal independensi, EM Algorithm"
http://etd.repository.ugm.ac.id/home/detail_pencarian/148352,PEMODELAN PERSAMAAN STRUKTURAL DENGAN HUBUNGAN KUADRATIK [Studi Kasus: Model Kausal Pendidikan Orangtua dan Anak di SDN Karangwuni I Sleman]; QUADRATIC STRUCTURAL EQUATION MODELING [Case Study: Causal Modeling of Parent’s and Child’s Education in SDN Karangwuni I Sleman],"WIDYA IRMANINGTYAS, Dedi Rosadi",2010 | Skripsi | PROGRAM STUDI STATISTIKA,"Pemodelan persamaan struktural (structural equation modeling, SEM) dengan variabel laten telah banyak digunakan dalam penelitian ilmu sosial dan perilaku. Semakin berkembangnya penelitian dalam bidang ini diiringi dengan meningkatnya kompleksitas (kerumitan) dari hubungan-hubungan antara beberapa variabel yang dianalisis. Hal tersebut diindikasikan dengan semakin banyaknya analisis yang mengandung hubungan nonlinear, salah satunya hubungan kuadratik. Untuk mengatasi hal tersebut, diperkenalkan SEM kuadratik yang memungkinkan adanya hubungan kuadratik dalam model struktural. SEM kuadratik menunjukkan bahwa variabel prediktor berinteraksi dengan dirinya sendiri. Dalam skripsi ini dibahas analisis SEM kuadratik dengan teknik pendekatan model kuadratik indikator tunggal Ping dengan dua tahap estimasi yang menggunakan metode estimasi kemungkinan maksimum tegar. Pembahasan akan diakhiri dengan studi kasus mengenai hubungan antara tingkat pendidikan orangtua dengan tingkat pendidikan anak yang diharapkan di SDN Kawangwuni I Sleman Yogyakarta.",,"Kata Kunci : Pemodelan Persamaan Struktural, Hubungan Kuadratik, Indikator Tunggal dengan Dua Tahap Estimasi, Kemungkinan Maksimum Tegar."
http://etd.repository.ugm.ac.id/home/detail_pencarian/150839,JARINGAN SARAF TIRUAN :BACKPROPAGATJON SEBAGAI  EARLY WARNING SYSTEM (EWS) KEBANGKRUTAN PERUSAHAAN DI INDONESIA ( Studi Kasus Perusahaan Sektor Manufaktur yang Delisted dan Listing  di Bursa Efek Jakarta periode 1998 s/d 2003),"IRWANSYAH, Adhitya Ronnie Effendie",2006 | Skripsi | PROGRAM STUDI STATISTIKA,"Untuk mengantisipasi munculnya kesulitan keuangan pada perusahaan, perlu disusun suatu sistem yang dapat memberikan peringatan dini (Early Warning Systems) adanya  problematik keuangan yang mengancam operasional perusahaan tersebut. Data yang digunkan sebagai  variabel input atau indikator kebangkrutan dalam penelitian ini adalah rasio keuangan perusahaan  satu tahun sebelum dinyatakan bangkrut. Data rasio keuangan perusahaan yang dinyatakan bangkrut dan  tidak bangkrut memiliki pola yang mampu diprediksi dan diklasifikasikan. Jaringan saraf tiruan  khususnya metode Backpropagation Neural Network (BpNN) dengan metode pelatihan Gradien descent  dengan Momentum (GdM) merupakan metode yang mampu melakukan proses prediksi dan klasifikasi  berdasarkan pola-pola tertentu. Dalam skripsi ini digunkan dua fungsi aktivasi yaitu Sigmoid bipolar dan Logistik sigmoid.  Berdasarkan nilai SSE, MSE, MAE model terbaik yang dapat digunakan sebagai EWS kebangkrutan untuk  fungsi aktivasi sigmoid bipolar adalah  JST[S-12-1] dengan akurasi  prediksi 97,78%. Sedangkan  untuk fungsi aktivasi logistik sigmoid model terbaik adalah JST[S-10-1] dengan akurasi prediksi  97,09%.",,"Kata Kunci : Jaringan  Saraf  Tiruan,  Backpropagation,  Gradien  descent, Bankruptcy  Prediction, Sigmoid  Bipolar,  Logistik  sigmoid,  Early  Warning Systems."
http://etd.repository.ugm.ac.id/home/detail_pencarian/76299,MODEL AVERAGING BAYESIAN PADA REGRESI LINIER,"YULIA INDAH PERMATA SARI, Drs. Zulaela Dipl.Med.Stats,M.Si.",2014 | Skripsi | STATISTIKA,"Statistika banyak digunakan dalam berbagai bidang. Salah satunya adalah bidang kesehatan. Permasalahan dalam bidang kesehatan sangat banyak, contohnya kematian bayi. Kematian bayi merupakan permasalahan serius yang memerlukan perhatian sendiri. Di Indonesia Angka Kematian Bayi (AKB) masih cukup tinggi dibandingkan di beberapa negara ASEAN lainnya. Padahal AKB sebagai tolak ukur keberhasilan pembangunan kesehatan di suatu negara. 	Pada skripsi ini akan dianalisis AKB menggunakan model averaging bayesian dengan  occam's window karena kasus tersebut melibatkan ketidakpastian model. Model averaging bayesian akan mempertimbangkan model yang tidak pasti dalam pemilihan variabel dengan mengkombinasikan model-model yang terbentuk dari variabel prediktor. Studi yang digunakan adalah AKB di Indonesia pada tahun 2012. Data yang digunakan  terdiri dari satu variabel dependen dan lima variabel independen. Hasil dari estimasi dengan metode model averaging bayesian akan dibandingkan dengan analisis regresi berganda. Model averaging bayesian memiliki nilai SSE yang lebih rendah dan tingkat ketepatan prediksi yang lebih tinggi dibandingkan dengan analisis regresi berganda.",Statistics is used in many areas. One of them is in health area. One of the issues in health area is infant mortality. Infant mortality is a serious issue that need special attention. Indonesia's Infant Mortality Rate (IMR) is higher than other countries in ASEAN. Whereas IMR is an indicator of the level of health development in a country. In this thesis will be analyzed IMR using Bayesian Model Averaging (BMA) with occam's window because that case involves uncertainty model. BMA will consider uncertainty model in variable selection with combine models are formed from predictor variables. The reference that will be used as data is Indonesia's IMR in 2012. The data consists of one dependent variable and five predictors variables. The result and estimation with BMA method will be compared with multiple regression analysis. BMA method has less SSE value and higher accuracy than multiple regression analysis.,"Kata Kunci : model averaging bayesian, occam's window, analisis regresi beganda, angka kematian bayi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/75543,GENERALIZED ESTIMATING EQUATIONS MENGGUNAKAN METODE BOOTSTRAP BERPASANGAN,"SAWITRI, Dr. Danardono, M.PH",2014 | Skripsi | STATISTIKA,"Metode bootstrap merupakan metode yang digunakan untuk mengestimasi suatu distribusi populasi yang tidak diketahui. Metode bootstrap dapat digunakan untuk mengestimasi inferensi statistik seperti bias dan standar error. Metode bootstrap berpasangan adalah suatu metode resampling dengan mempertahankan korelasi pasangan variabel dependen dan variabel independennya.  Metode bootstrap berpasangan digunakan dalam Generalized Estimating Equations. Generalized Estimating Equations merupakan salah satu metode dalam masalah regresi untuk data longitudinal. Studi kasus yang dilakukan menggunakan data real, yaitu pada penurunan jarak gigi marmut.","Bootstrap method is a method which used to estimate the unknown population distribution. Bootstrap method can be used to estimate the statistic inferences such as bias and standard error. Paired bootstrap method is a resampling method by maintain the correlation of dependent variable and independent variable. Paired bootstrap method is used to generalized estimating equations. Generalized estimating equations is one of method for longitudinal data. The case studies is applied to real data, the decrease in marmot tooth spacing.","Kata Kunci : data longitudinal, Generalized Estimating Equations, bootstrap berpasangan, resampling."
http://etd.repository.ugm.ac.id/home/detail_pencarian/74776,KONSTRUKSI KURVA YIELD MENGGUNAKAN METODE NELSON SIEGEL SVENSSON ALGORITMA DIFFERENTIAL EVOLUTION,"NUR ALIFAH, Dr. Abdurakhman, M.Si",2014 | Skripsi | STATISTIKA,"Penelitian ini mempelajari tentang teori kurva yield metode Nelson Siegel Svensson algoritma Differential Evolution kemudian dibandingkan dengan kajian empiris kurva yield yang menggunakan metode Nelson Siegel Svensson. Pada studi kasus penelitian ini, data diambil dari transaksi obligasi Pemerintah Indonesia tanggal 16 Februari 2011. Dari analisis MSE (Mean Square Error) didapatkan bahwa metode Nelson Siegel Svensson algoritma Differential Evolution  memberikan hasil empiris lebih baik dalam memodelkan kurva yield dibandingkan metode Nelson Siegel Svensson.","This research learns about yield curve theory  for Nelson Siegel Svensson method with Differential Evolution algorithm, then has been compared by empirical studying for the yield curve using Nelson Siegel Svensson method. In study case of this research, data was taken from Indonesia Goverment Transaction bond  at 16th February 2011. From the MSE (Mean Square Error) analysis, have been  shown  that Nelson Siegel  Svensson method with Differential Evolution algorithm gave better empirical results of yield curve model than  Nelson Siegel Svensson method.","Kata Kunci : kurva yield, Nelson Siegel Svensson, algoritma Differential Evolution/yield curve, Nelson Siegel Svensson, Differential Evolution algorithm"
http://etd.repository.ugm.ac.id/home/detail_pencarian/69917,OPTIMISASI PORTOFOLIO MENGGUNAKAN PENDEKATAN TELSER BERBASIS VALUE AT RISK,"SASHA M PASUHUK, Dr. Adhitya Ronnie Efendi, M.Sc",2014 | Skripsi | STATISTIKA,"Dalam menanamkan modalnya di pasar modal khususnya saham, investor memerlukan suatu metode optimisasi portofolio. Salah metode optimisasi dikembangkan oleh Markowitz (1952) yaitu metode mean-variance. Metode ini berkonsentrasi pada upside dan downside risk yang menggunakan pendekatan statistika standar deviasi sebagai ukuran risiko nya. Padahal beberapa investor menganggap bahwa risiko sebenarnya adalah risiko return negatif (dowside risk) sehingga dibutuhkan metode lain yang hanya berkonsentrasi pada dowside risk. Salah satunya dengan menerapkan prinsip safety first. Metode pendekatan Telser berbasis Value at Risk merupakan salah satu teknik optimisasi portofolio yang menerapkan prinsip safety first. Metode ini bertujuan untuk membentuk portofolio yang memaksimalkan expected return sekaligus memenuhi kendala Value at Risk. Kendala Value at Risk digunakan untuk meminimumkan risiko portofolio. Dalam penghitungannya return diasumsikan berdistribusi eliptikal. Kemudian dibawah asumsi tersebut dihitung formula optimisasi Telser dengan kendala VaR yang nantinya akan menghasilkan alokasi aset optimal untuk masing-masing saham. Pada studi kasus dibentuk portofolio yang terdiri dari saham-saham pada Bursa efek Indonesia, yaitu CTRP, TLKM, MNCN, BBRI, LSIP, SMCB, MEDC menggunakan metode pendekatan Telser, metode pendekatan Telser dengan kendala value at risk, dan mean variance. Kemudian kinerja ketiga portofolio tersebut dibandingkan dengan ukuran kinerja besarnya kentungan/ kerugian, tingkat pengembalian (rate of return) dan Sharpe ratio. Hasilnya, portofolio VaR Telser menunjukkan kinerja portofolio yang baik dengan risiko yang paling rendah di antara ketiga portofolio tersebut. Metode optimisasi portofolio dengan pendekatan Telser berbasis Value at Risk menghasilkan portfolio dengan risiko yang lebih rendah dibandingkan dengan optimisasi portofolio pendekatan Telser, dan mengahasilkan keuntungan yang lebih tinggi dibandingkan dengan metode optimisasi mean-variance. Oleh karena itu, optimisasi portofolio menggunakan pendekatan Telser berbasis Value at Risk ini bisa menjadi alternatif pilihan bagi investor.","To invest his money on a capital market, investor needs a portfolio optimization method. One of the portfolio optimization method was developed by Harry Markowitz (1952) called mean-variance optimization. This method using a standard deviation as a risk measure and itâ€™s focusing on both upside and downside risk. However, most of investors considered only downside risk as a real risk. To fit investorâ€™s perception another method had to be made. One of these models, focusing on downside risk is the saftey first principle. Value at Risk based Telser Approach is one of the portfolio optimization technique applying this safety first principle. This method aimed to maximize expected return subject to Value at Risk constraint. Value at Risk constraint is used to minimize the risk of the portfolio. In the analysis, return assumed to be eliptically distributed. Under this assumption, Telser optimization formula is constructed, which then gives optimal weights for each asset as result. The case study presents portfolio constraction using Telser approach, VaR Based Telser Approach, and meanvariance optimization techniques consist of seven stocks listed in Indonesia Stock Exchange i.e : CTRP, TLKM, MNCN, BBRI, LSIP, SMCB, and MEDC. Then, those three methods are compared by using measure of portfolio performance i.e : ammount of profit/ loss, rate of return, and Sharpe Ratio. As a result, VaR based Telser portfolio optimization gives a good portfolio performance with the lowest risk among those three methods. VaR Based Telser portfolio optimization is less risky than Telser portfolio optimization, and it gives higher profit than meanvariance method. So, VaR based Telser portfolio optimization is recommended for investor","Kata Kunci : Portofolio, prinsip safety first, optimisasi Telser, Value at Risk, Optimisasi Telser berbasis Value at Risk, optimisasi mean-variance, distribusi eliptikal."
http://etd.repository.ugm.ac.id/home/detail_pencarian/70686,ANALISIS KELOMPOK UNTUK DATA KATEGORIK BERDASARKAN ALGROITMA VEKTOR HAMMING DISTANCE (Studi Kasus : Klasifikasi Hewan Berdasarkan Morfologi dan Karakteristik Umum),"SHIDDIQ SUGIONO, Herni Utami,  S.Si.,  M.Si.",2014 | Skripsi | STATISTIKA,Analisis  kelompok  menggunakan  metode  K-Means  Clustering  yang  menggunakan  rata-rata  sebagai  pusat  dari  kelompok  tidak  lagi  berarti  jika  digunakan  dalam  kumpulan  data  yang  bersifat  kategorik.  Analisis  kelompok  menggunakan algoritma vektor  Hamming distance  dikhususkan untuk kumpulan  data  yang  bersifat  kategorik.  Teori  dalam  algoritma  ini  banyak  berdasar  pada  coding theory.  Algoritma ini tidak diperlukan model ataupun kriteria konvergensi  dalam jalannya algortima.  Algoritma ini merupakan alternatif dari algoritma yang  sebelumnya  telah  diajukan  seperti  K-modes  dan  Autoclass.  Studi  kasus  yang  dilakukan  adalah  pengklasifikasian  hewan  menurut  morfologi  dan  karakteristik  umumnya.  Pengklasifikasian  ini  bertujuan  untuk  mengelompokan  hewan  pada  kelompok/sub-populasi yang memiliki kemiripan tinggi. Diharapkan hasil ini bisa  digunakan untuk mempelajari hewan berdasarkan kemiripan sifatnya.,"Clustering analysis using K-Means clustering methods  which using  mean  as the center of cluster doesnâ€™t have any meaning anymore if the method  is  used  for  the  categorical  data.  Clustering  analysis  using  Hamming  distance  vector  algorithm  is  only  for  categorical  dataset.  Theory  that  used  in  this  algorithm  is  based on coding theory. It doesnâ€™t need any model or convergence criteria for this  algorithm.  This  algorithm  is  the  alternate  way  for  older  method  :  K-Modes  and  Autoclass.  The  case  studies  in  this  thesis  is  animal  classification  based  on  their  morphology  and  common  characteristic,  the  purpose  of  the  classification  is  grouping some animal into subpopulation. Thus, the result can be a reference to  studing animal from their simmilarities.","Kata Kunci :  Data  Kategorik,  Analsis  kelompok,  Vektor  Hamming  distance,  statistik uji Modified chi-squared"
http://etd.repository.ugm.ac.id/home/detail_pencarian/76062,Estimasi-S Robust untuk Regresi Spline Terpenalti,"HENNI PRATIKA, Dr. Adhitya Ronnie Effendie, M.Si, M.Sc",2014 | Skripsi | STATISTIKA,"Regresi spline terpenalti adalah salah satu metode yang saat ini sering digunakan untuk smoothing noisy data. Model regresi spline terpenalti adalah alat statistik yang popular untuk masalah fitting kurva karena fleksibilitas dan efisiensi dalam kumputasinya. Pada regresi spline, estimasi kurva regresi dapat diselesaikan dengan kuadrat terkecil terpenalti atau Penalized Least Square. Namun metode estimasi ini rentan terhadap kehadiran pencilan. Sehingga diperkenalkan metode estimasi-S robust terpenalti.   Oleh karena itu metode estimasi kuadrat terkecil untuk regresi spline terpenalti diganti dengan metode estimasi-S robust yang mampu menangani kehadiran pencilan dalam data. Dengan tetap menjaga pembentukan model spline dan menjaga bentuk penalti, meskipun menggunakan estimator-S daripada estimator kuadrat terkecil, didapatkan metode estimasi yang robust dan cukup fleksibel untuk menangkap trend non-linear dalam data. Dalam skripsi ini juga mempelajari bagaimana memilih secara robust parameter penalti ketika kemungkinan terdapat outlier pada data. Diberikan kriteria pemilihan parameter penalti robust berdasarkan generalized cross-validation yang juga didapat dari gambaran weighted penalized least square dari estimator S-regresi terpenalti. Contoh data simulasi dan data riil digunakan untuk menggambarkan efektivitas prosedur.","Penalized regression splines are one of the currently most used methods for smoothing noisy data. Penalized spline regression models are a popular statistical tool for curve fitting problems due to their flexibility and computational efficiency. In spline regression, regression curve estimation can be solved by Penalized Least Square. However, this estimation method vulnerable to the presence of outliers. Therefore we proposed robust Penalised-S estimation method. Hereby we replace the least squares estimation method for penalized regression splines by S-estimation method, that capable of addressing the presence of outlier in the data. By keeping the modeling of splines and by keeping the penalty term, though using S-estimators instead of least squares estimators, we arrive at an estimation method that is both robust and flexible enough to capture non-linear trends in the data. This Undergraduate thesis also learn how to choose the penalty parameter robust when there are possibilities of outliers in the data. We propose a robust penalty parameter selection criteria based on generalized cross-validation that also borrows from the weighted penalized least squares representation of the penalized S-regression estimator. Simulated data and a real data example are used to illustrate the effectiveness of the procedure.","Kata Kunci : estimasi-S robust, regresi spline terpenalti, estimasi-S terpenalti, parameter pemulus, regresi nonparametrik, rgcv"
http://etd.repository.ugm.ac.id/home/detail_pencarian/76063,YIELD CURVE CONSTRUCTION AND ZERO COUPON BOND PRICING USING CUBIC B-SPLINE METHOD,"RIZKI EVITASARI, Dr. Abdurakhman, M.Si",2014 | Skripsi | STATISTIKA,"Obligasi adalah salah satu instrumen investasi berpendapatan tetap. Keuntungan yang diterima oleh investor sampai jatuh tempo disebut dengan yield to maturity. Analisis yang menjelaskan tentang hubungan yield to maturity dengan waktu jatuh tempo disebut dengan struktur jangka waktu tingkat bunga atau term structure of interest rate. Struktur jangka waktu ini digambarkan dengan grafik yang disebut dengan yield curve. Kurva yield ini memuat yield sebagai koordinat x dan waktu jatuh tempo sebagai koordinat y.   Pada skripsi ini akan dipelajari teori pembentukan kurva yield dengan menggunakan metode cubic B-spline yang kembangkan oleh Baki (2006). Metode ini perluasan dari metode cubic spline yang dipelopori oleh McCulloch (1975). Studi kasus penelitian ini diambil dari obligasi pemerintah Indonesia berkupon nol dengan seri SPN pada saat Bahan Bakar Minyak langka, yaitu tanggal 26-29 Agustus 2014. Hasil pemodelan diperoleh kesimpulan bahwa berdasarkan kriteria nilai MSYE yang minimum dapat dipilih model cubic B-spline terbaik dengan tiga titik knot. Titik knot optimum yang diperoleh adalah k1= 155, k2= 290 dan k3= 300. Nilai MSYE ini merupakan nilai yang minimum diantara semua kemungkinan titik knot. Kurva yield dengan tiga titik knot pada tanggal 29 Agustus 2014 terlihat paling mendekati yield to maturity dibandingkan tanggal 26, 27, 28 Agustus 2014.","Bond is one of fixed-income investment instruments. The income that will returns to the investor until maturity is called by yield to maturity. An analysis that explained the relationship between yield to maturity and time to maturity is called term structure of interest rate. This term structure design into graph which called yield curve. This curve has yield in coordinat x and maturity in coordinat y.   This research learn about yield curve theory using cubic B-spline method introduced by Baki (2006). This method is extended paper from McCulloch (1975). The research case study is taken from indonesian zero coupon bond transactions at gasoline rare on August, 26-29 2014. The result is the minimum value of MSYE to choose best cubic B-spline  with three knots points. Optimum knot points obtained among others are k1= 155, k2= 290 and k3= 300 which produce a minimum MSYE. This MSYE value is the minimum value among all possible knot points. Yield curve with three knot poin on 29 August is better than the others.","Kata Kunci : Bond pricing, yield curve, cubic B-spline, MSYE, knots point"
http://etd.repository.ugm.ac.id/home/detail_pencarian/71714,ANALISIS TEKNIKAL SAHAM MENGGUNAKAN INDIKATOR TRUE STRENGTH INDEX (TSI),"NABILA KENCANA, Prof. Dr. Sri Haryatmi Kartiko, M.Sc.",2014 | Skripsi | STATISTIKA,"Analisis teknikal saham menggunakan grafik sebagai medianya, digunakan untuk memprediksi pergerakan harga saham sehingga kita dapat mengetahui kapan harus membeli dan menjual saham untuk memperoleh keuntungan yang maksimal. Terdapat banyak sekali indikator yang digunakan dalam analisis teknikal saham. Indikator True Strength Index (TSI) merupakan indikator teknikal yang berbasis momentum yang membantu trader menjelaskan kondisi jenuh beli dan jenuh jual dari saham dengan cara menggabungkan momentum jangka pendek dari sekuritas dengan lag keuntungan dari moving average. Exponential Moving Average (EMA) dengan periode 25 dihitung dari selisih dari dua harga, dan kemudian EMA dengan periode 13 dihitung dari hasil EMA sebelumnya, hal ini membuat indikator lebih sensitif untuk kondisi pasar yang berlaku. Setelah data dihaluskan, digunakan angka -20 dan +20 digunakan untuk mengidentifikasi dimana saham jenuh beli dan jenuh jual. Akan dibuktikan juga bahwa TSI dengan menggunakan EMA adalah yang terbaik daripada menggunakan SMA maupun WMA. Indikator TSI adalah variasi dari indikator Relative Strength Index (RSI).","Technical analysis using a graph as method, itâ€™s used to predicting price movement of its stocks so that we know when we have to buy or sell our stocks in a way gaining maximum profits. There are a lot of indicators that can be used at stock technical analysis. True Strength Index (TSI) indicator in a technical momentum indicator that helps traders determine overbought and oversold conditions of a security by incorporating the short-term purchasing momentum of the market with the lagging benefits of moving average. Generally is a 25-day Exponential Momentum (EMA) is applied to the difference between two share prices, and then a 13-day EMA is applied to the result, making the indicator more sensitive to prevailing market condotions. After the data is smoothed, value of -20 dan +20 used to identify levels where a security is overbought or oversold. Itâ€™ll proof TSI based on EMA is the best than TSI based on SMA and WMA. TSI indicator is a variation of the Relative Strength Index (RSI).","Kata Kunci : Saham, analisis teknikal, SMA, EMA, WMA, TSI, RSI"
http://etd.repository.ugm.ac.id/home/detail_pencarian/71715,REGRESI RIDGE DENGAN BENTUK Q DAN FAKTOR SHRINKAGE  MENGGUNAKAN SINGULAR VALUE DECOMPOSITION,"RIZKI ARISTA SP, Drs. Zulaela,Dipl.Med.Stats.,M.Si",2014 | Skripsi | STATISTIKA,"Salah satu asumsi yang harus dipenuhi dalam regresi linier berganda adalah tidak ada multikolinieritas atau tidak ada hubungan linier diantara variabel bebas. Multikolinearitas menyebabkan MSE dari penduga kuadrat terkecil menjadi sangat besar sehingga estimator yang diperoleh dari metode kuadrat terkecil tidak tepat. Pada kenyataannya, yang diharapkan pada sebuah penelitian adalah model yang memiliki ragam minimum, meskipun berbias. Metode regresi ridge dengan bentuk Q dan faktor shrinkage merupakan salah satu cara untuk mengatasi masalah multikolinieritas karena menghasilkan MSE yang kecil meskipun bias yang dihasilkan relatif kecil. Metode ini menyusutkan koefisien regresi linier menggunakan faktor shrinkage dan mengontrol penyusutannya menggunakan bentuk Q. Metode ini merupakan modifikasi dari metode regresi ridge biasa dengan menggunakan singular value decomposition yaitu menguraikan s matrik dari variabel bebas X menjadi 3 komponen tanpa mengubah karakteristik variabel- variabel bebasnya.","One of the assumptions in a multiple linear regression is no multicollinearity or no linear relationship between the independent variables. Multicollinearity causes the MSE of the least squares estimators become so great and the estimator obtained from the least squares method is not appropriate. In fact, the study is expected in a model that has minimum variance, though biased. Ridge regression with Q-shape and shrinkage factors is one way to solve the problem of multicollinearity because it produces small MSE despite the relatively small bias. This method reduced the coefficient of linear regression using shrinkage factors to shrink the coefifisien and controlling them using the Q-shape. This method is a modification of the ordinary ridge regression method using singular value decomposition by change the component of independent variable without changing the characteristics of the independent variables.",Kata Kunci : -
http://etd.repository.ugm.ac.id/home/detail_pencarian/72486,PERBANDINGAN UJI T STATISTIK RATA-RATA TERBOBOT DENGAN RATA-RATA TIDAK TERBOBOT PADA DATA KEUANGAN: (Studi Kasus Yield Obligasi Muncipal dan Korporasi Amerika Serikat dengan Rating AAA,"INDAH MENTARI, Dr. Gunardi, M.Si.",2014 | Skripsi | STATISTIKA,"Rata-rata adalah teknik penjelasan kelompok atau ukuran pusat dari sekumpulan data. Biasanya suatu pertanyaan statistik akan muncul saat dua sampel random, diambil dari dua populasi normal, yaitu apakah rata-rata kedua populasi ini sama atau tidak. Untuk menguji kesamaan rata-rata tersebut, biasanya digunakan uji-t mean dua sampel. Pada skripsi ini akan dijelaskan perluasan dari uji-t mean dua sampel yaitu dengan menggunakan pembobotan. Metode ini menyajikan pengembangan dari dua sample t-test untuk kasus di mana nilai-nilai sampel harus diberi bobot yang tidak setara . Ini merupakan hal umum dalam pemodelan keuntungan di mana beberapa sampel dianggap lebih dapat diandalkan dibandingkan yang lain dalam memprediksi rata-rata populasi. Disini ditunjukkan dengan contoh data pengembalian yang diterima oleh investor yang menggunakan uji-t tidak terbobot dapat mengarah pada kesimpulan yang salah dibandingkan uji-t terbobot","Means is technic to explanation the group or size in center set of data. Usually, a common statistical question arises when two random samples are taken from two different normal populations, possibly with different variances, and wish to know whether or not the means of the two populations are equal. To examine the similarity of the average, typically use t-test mean two samples. In this minithesis will describe about expansion of t-test mean two samples using tstatistic weighted means. This methode present a generalization of the two-sample t-test for equality of the means to the case where the sample values are to be given unequal weights. This is a natural situation in financial return modelling where some samples are considered more reliable than others in predicting a common mean. In this case show with an example of yield data that using the standard unweighted t-test can lead to the wrong statistical conclusion.",Kata Kunci : -
http://etd.repository.ugm.ac.id/home/detail_pencarian/76072,Segmentasi Karakteristik Debitur Menggunakan Metode K-Means Cluster dan Multi-class Support Vector Machines,"AFINA NURSEHA, Dr. Herni Utami, M.Si.",2014 | Skripsi | STATISTIKA,"Analisis kredit menggunakan teknik statistika telah mengalami perkembangan dan mendapat perhatian yang besar. Salah satu teknik analisa kredit untuk mengantisipasi adanya kredit macet yaitu dengan mengelompokkan karakteristik debitur. Salah satu metode untuk mengelompokkan sejumlah data debitur yaitu menggunakan metode K-means Cluster (clustering non-hierarki). Setelah data dikelompokkan, selanjutnya dilakukan prediksi pada dataset (testing data) dan dihitung tingkat akurasi kebenarannya menggunakan metode multi-class support vector machines. Metode multi-class support vector machines merupakan pengembangan dari metode support vector machines yang dapat digunakan untuk mengklasifikasikan data yang memiliki lebih dari dua kelas (multi kelas). Salah satu metode yang dapat digunakan untuk mengimplementasikan metode multiclass support vector machines adalah metode one-against-one (satu lawan satu). Penelitian ini dilakukan dengan berbagai variasi proporsi training data dan testing data serta nilai sigma dari fungsi Kernel berupa Radial Basis Function.","Credit analysis using statistical technique has been developed and received considerable research attention. One of the techniques of credit analysis to anticipate any bad debts is classifying the debtors characteristics. One of methods for classifying a number of the debtor data is K-means cluster method (clustering non-hierarki). Once the data classified, then conducted the predictions on dataset (testing data) and calculated the accuracy of righteousness using the multi-class support vector machines method. Methods of multi-class support vector machines are development of support vector machines that can be used to classify data which have over than two classes (multi-kelas). One of the methods that can be used to implement the multi-class support vector machines is a one-against-one method. This study was conducted with a variety of proportions from training data and testing data as well as the sigma value of the Kernels function in the form of Radial Basis Function.","Kata Kunci : Debtor Segmentation, K-means Cluster, Support Vector Machines, Multi-class Support Vector Machines, One-Against-One"
http://etd.repository.ugm.ac.id/home/detail_pencarian/70697,ESTIMASI NILAI DATA HILANG MENGGUNAKAN IMPUTASI GANDA DENGAN METODE REGRESI,"MONICA RINDAYU G. K., Drs. Sardjono, S.U",2014 | Skripsi | STATISTIKA,"Data merupakan salah satu poin penting dalam setiap analisis data, karena tidak akan mungkin analisis dilakukan tanpa data. Data yang digunakan diharapkan merupakan data yang baik. Namun pada kenyataannya, seringkali data tidak sesuai dengan yang kita harapkan. Data yang tidak lengkap menyebabkan proses penarikan kesimpulan menjadi lebih sulit. Jika data yang hilang diabaikan, maka menyebabkan kesimpulan yang bias atau tidak valid. Oleh karena itu, muncullah berbagai metode untuk mengestimasi nilai yang hilang tersebut. Salah satunya adalah imputasi ganda dengan menggunakan metode regresi. Metode ini digunakan untuk mengestimasi nilai data yang hilang pada variabel dependen. Pembahasan diakhiri dengan studi kasus mengenai estimasi nilai data hilang pada variabel persentase penduduk miskin.","Data is one of the important points in every data analysis as it is impossible to conduct data analysis without data. The data used is expected to be a good data. In fact, it is commonly found that the data doesnâ€™t meet the expectation. Incomplete data causes the difficulty in drawing the conclusion. If missing data are ignored, it causes the conclusion are bias or invalid. Therefore, there are various methods for estimating the missing value. One of them is multiple imputation using regression method. This method is used to estimate the missing value on the dependent variable. The discussion ended with a case study of the missing data value estimation in a percentage variable of the poor inhabitants.","Kata Kunci : regresi linear, data hilang, imputasi ganda, imputasi ganda menggunakan metode regresi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72235,PENENTUAN HARGA OPSI EROPA MODEL TRINOMIAL DENGAN TEKNIK EKSTRAPOLASI,"IKHA FITRIA HERDYANTI, Dr. Abdurakhman, S.Si., M.Si",2014 | Skripsi | STATISTIKA,"Penelitian mengenai bagaimana penentuan harga opsi terus mengalami perkembangan dari waktu ke waktu. Salah satunya dengan menggambarkan pergerakan harga saham yang mengikuti model pohon trinomial. Dengan model ini diasumsikan bahwa pergerakan harga saham untuk periode ke depan mengikuti 3 kondisi yaitu harga saham akan naik, cenderung tetap, atau justru akan menurun. Namun, hasil yang diperoleh memiliki kekonvergenan yang lambat sehingga diperlukan suatu metode untuk mempercepat konvergensi dengan ekstrapolasi. Metode ekstrapolasi yang sering digunakan adalah teknik ekstrapolasi Richardson yang merupakan teknik ekstrapolasi yang cukup populer. Ide awal dari penggunaan teknik ekstrapolasi Richardson adalah dengan melakukan eliminasi pada beberapa bagian awal dari ekspansi asimtotis fungsi pendekatan yang bergantung pada barisan stepsize untuk mendapatkan pendekatan yang lebih baik.","Research on how option pricing has continued to experienced over time. One of them is by describing stock price movements that follow the trinomial tree model. With this model, it is assumed that the movement of stock prices for the period ahead following three conditions: stock prices will increase, tend to constant, or it will decline. However, the results have a slow convergence so it is required a method to accelerate the convergence with the extrapolation. A frequently used method of extrapolation is the Richardson extrapolation technique which is quite popular extrapolation technique. The initial idea of the use of Richardson extrapolation technique is to do elimination on some early part of the asymptotic expansion from approached function that depends on the stepsize line to get a better approach.","Kata Kunci : trinomial, ekstrapolasi Richardson, stepsize"
http://etd.repository.ugm.ac.id/home/detail_pencarian/75052,Penentuan Harga Obligasi Bencana Alam Gempa Bumi dengan Sebaran Nilai Ekstremum Rampat dan Model Suku Bunga Cox-Ingersoll-Ross (CIR),"EZRA PUTRANDA SETIAWAN, Dr. Gunardi, M.Si.",2014 | Skripsi | STATISTIKA,"Indonesia merupakan daerah yang rawan bencana gempa bumi, karena terletak di daerah perbatasan lempeng-lempeng bumi. Bencana gempa bumi dapat menimbulkan kerusakan, kerugian, serta dampak ekonomis yang sangat besar. Oleh karena itu, diperlukan pemindahan resiko bencana dari negara atau perusahaan (reasuransi) sedemikian rupa sehingga dapat dikumpulkan dana yang cukup untuk menutup kerugian bencana tersebut, misalnya melalui penerbitan obligasi bencana alam (catastrophe bond).  	Suatu obligasi bencana alam diterbitkan oleh perusahaan special purpose vehicle (SPV). Dana yang diperoleh sebagai hasil penjualan kepada investor akan diinvestasikan bersama dengan dana yang diperoleh dari perusahaan sponsor. Bila terjadi bencana alam dalam jangka waktu tertentu, aliran dana dari SPV kepada investor akan dipotong atau bahkan dihentikan untuk diberikan kepada sponsor guna menutup kerugian akibat bencana alam tersebut. Khusus untuk bencana gempa bumi, penentuan besar pemotongan dapat dilakukan secara parametrik berdasarkan magnitude gempa. Studi kasus menunjukkan bahwa probabilitas maksimum magnitude gempa bumi dapat dianalisis menggunakan sebaran nilai ekstremum rampat (generalized extreme value / GEV distribution). 	Untuk penentuan harga obligasi bencana alam ini, digunakan asumsi suku bunga mengambang seturut model Cox-Ingersoll-Ross (CIR). Di bawah model ini, telah diperoleh harga tiga macam obligasi bencana alam yakni obligasi tanpa kupon, obligasi dengan nilai akhir tetap, dan obligasi dengan kupon maupun nilai akhir beresiko.  Selanjutnya dari simulasi diperoleh hubungan antara harga jual obligasi dengan parameter-parameter model suku bunga CIR, distribusi GEV, besar kupon, maupun pemotongan aliran dana obligasi kepada investor.","Indonesia is a country with huge risk of earthquake, because of their position in the border of earthâ€™s tectonic plate. An earthquake could raise very high amount of damage, loss, and other economic impacts. We need a mechanism to transferring the risk of earthquake from the government or the (reinsurance) company, as it could collect enough money for implementing the rehabilitation and reconstruction program, for example is by issuing catastrophe bonds.  	A catastrophe bond issued by a special-purpose-vehicle (SPV) company, then sold to the investor. The revenue from this transaction is joined with the money (premium) from the sponsor company and then invested in other product. Before the time-of-maturity, if a catastrophe happened, cash flow from the SPV to the investor will discounted or stopped, and the cash flow is paid to the sponsor company to compensate their loss because of this catastrophe event. When we consider the earthquake only, the amount of discounted cash flow could determined based on the earthquakeâ€™s magnitude.  A case study with Indonesian earthquake magnitude show that the probability of maximum magnitude can modelled by generalized extreme value (GEV) distribution. 	In pricing this catastrophe bond, we assumed stochastic interest rate that follow the Cox-Ingersoll-Ross (CIR) interest rate model.  We develop formulas for pricing three types of catastrophe bond, namely zero coupon bonds, â€˜coupon only at riskâ€™ bond, and â€˜principal and coupon at riskâ€™ bond.  Relationship between price of the catastrophe bond and CIR modelâ€™s parameter, GEVâ€™s parameter, percentage of coupon, and discounted cash flow rule then explained via Monte Carlo simulation.","Kata Kunci : Obligasi bencana alam, model suku bunga Cox-Ingersoll-Ross (CIR), sebaran nilai ekstremum rampat, simulasi, magnitude gempa, harga."
http://etd.repository.ugm.ac.id/home/detail_pencarian/70965,"MANAJEMEN PERSEDIAAN MENGGUNAKAN MODEL Q,r DENGAN TIMEWEIGHTED BACKORDERS","IDA NURFAIZAH RAHMAH, Dr. Abdurakhman, S.Si., M.Si.",2014 | Skripsi | STATISTIKA,"Persediaan adalah merupakan salah satu unsur paling aktif dalam operasi perusahaan yang secara kontinue diperoleh, diubah, yang kemudian dijual kembali. Pada dasarnya persediaan juga merupakan sumber daya yang menganggur (idle resources), yang berarti jika persediaan berlebih menyebabkan investasi sia-sia, akan tetapi bila tidak ada persediaan akan sulit mengantisipasi fluktuasi permintaan atau hal-hal lain yang menyebabkan terjadinya kekurangan. Ketika dihadapkan dengan kehabisan persediaan, reaksi pelanggan berbeda-beda, tergantung pada bagaimana hal itu mempengaruhi bisnis masing-masing. Beberapa peka terhadap frekuensi stockout sementara yang lain menganggap jumlah backorder lebih penting. Dalam jenis usaha tertentu seperti mesin atau elemen penting, bagaimanapun durasi stockout merupakan elemen penting. Dengan demikian time-weighted backorders adalah tindakan tepat dari stockout dalam situasi seperti ini. . Keputusan yang menyangkut â€œberapa banyak dan kapan harus melakukan pemesananâ€ merupakan masalah utama dalam manajemen persediaan. Model persediaan Q,r dengan time-weighted backordes diyakini mampu menyelesaikan masalah itu, sistem ini akan menentukan titik posisi persediaan kapan harus memesan barang (R), dan jumlah barang yang harus selalu dipesan (Q) ketika persediaan berada tepat pada titik itu menggunakan metode HMMS multi-item.","Inventory is one of the most active ingredient in the company's operations are continuously acquired, modified, which then resold. Basically supplies are also a idle resources, which means that if the excess inventory causes wasted investment, but if there is no inventory would be difficult to anticipate fluctuations in demand or other things that cause deficiencies. When faced with stockout, customers react differently, depending upon how it affects their respective businesses. Some are sensitive to the frequency of stockout while others regard number of backorders to be more important. In certain types of businesses such as engines or critical elements, however the duration of stockout is an important element. As such, time - weighted backorders are appropriate measures of stockout in a situation like this. Decisions concerning \\"" how much and when to place an order \\"" is a major problem in inventory management. Inventory model Q,r with time - weighted backordes believed to be able to resolve the problem, the system will determine the inventory position of the point of when to order goods (R), and the number of items that should always be booked (Q) when the inventory is right at that point using the method HMMS multi- items.","Kata Kunci : manajemen persediaan, time-weighted backorders, HMMS model, persediaan multi-item"
http://etd.repository.ugm.ac.id/home/detail_pencarian/73787,Model Marginal Data Longitudinal Biner menggunakan Rantai Markov,"ROCHYATI, Dr. Danardono, MPH.",2014 | Skripsi | STATISTIKA,"Data longitudinal adalah data yang pengumpulannya dilakukan berkali-kali selama jangka waktu tertentu. Data longitudinal biasanya akan berkorelasi serial dalam subyek. Jelasnya, jika yit merepresentasikan observasi subyek ke -i waktu ke -t , maka subyek i memuat respon berulang yit. Karena observasinya diambil dari subyek yang sama akibatnya respon berulang ini berkorelasi. Analisis data longitudinal dimana variabel responnya biner itu perlu diperhatikan dari sudut pandang inferensi likelihood, dimana mengharuskan spesifikasi yang lengkap dari model stokastik untuk individu. Dalam skripsi ini digunakan rantai Markov biner â€“yang merupakan mekanisme stokastik dasar- untuk menggambarkan model marginal dalam data longitudinal dengan memperhatikan efek random. Dalam skripsi ini, analisis model marginal data longitudinal biner menggunakan rantai Markov dengan efek random diaplikasikan untuk menganalisis penelitian pada 577 bayi yang terkena penyakit Infeksi Saluran Pernafasan Akut (ISPA) di Purworejo. Analisis ini menghasilkan estimasi yang tidak jauh berbeda dengan analisis regresi logistik, tetapi dengan analisis ini dapat dihitung estimasi parameter dependensi dan etimasi variansi.","Longitudinal data is collected repeatedly over time. Longitudinal data usually have correlation in a form of serial correlation within the subjects. Obviously, if Yit represents the i-th observation subject to time-t, the subject I will take back the response Yit accordingly. Observation result is obtained from the same subject, thus result of these repeated responses are correlated. Analysis of longitudinal data where the response variable is binary is considered as likelihood inference, which requires a complete specification of the stochastic model for the individual. The Markov chain is applied in this study which is a binary-stochastic mechanism to describe the policy-marginal model in longitudinal data with random effects observed. In this thesis, the analysis of binary longitudinal data marginal models using Markov chain with random effects applied to analyze the study in 577 affected infants Acute Respiratory Infection (ARI) in Purworejo. This analysis yields an estimate that is not much different from the logistic regression analysis, but the analysis can be calculated dependencies and variance estimation.","Kata Kunci : data longitudinal biner, model marginal, rantai Markov, odds ratio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/76096,KLASIFIKASI MENGGUNAKAN METODE BOOSTING,"RINA PUSPITA SARI, Dr. Herni Utami, M.Si",2014 | Skripsi | STATISTIKA,"Di dalam tugas akhir ini dibahas metode Boosting yang digunakan dalam klasifikasi.  Dewasa ini, kecermatan dan ketepatan dalam hal klasifikasi data merupakan hal yang  sangat  penting.  Hasil  klasifikasi  data  dengan  beberapa  metode  yang  telah dikembangkan akan mempengaruhi sejumlah keputusan, misalnya dalam hal kedokteran (emergency  medicine),  cuaca,  bioinformatika  dan  bidang-bidang  lainnya. Salah satu metode yang diperkenalkan adalah metode Boosting. Metode boosting adalah metode yang mengkombinasikan pengklasifikasi lemah menjadi pengklasifikasi yang kuat.  Di dalam metode boosting diperkenalkan adanya iterasi.  Dalam tiap iterasi learning-nya, dibangun model hasil prediksi data latih dan kemudian disampel ulang untuk masuk pada itrasi berikutnya.  Pada umumnya hal ini akan meningkatkan tingkat keakurasian dalam pengklasifikasi. Metode Boosting juga baik digunakan pada data yang persebarannya tidak seimbang atau sering dikatakan imbalance.  Pada kesimpulannya diperoleh bahwa penggunaan metode Boosting pada klasifikasi memberikan hasil yang lebih baik dengan tingkat keakurasian yang tinggi.","In this thesis discussed in Boosting methods are used in classification. Today, precision and accuracy in the classification of data is very important. Data classification results with some of the methods that have been developed will affect a number of decisions, for example in the case of medicine (emergency medicine), the weather, bioinformatics and other fields. One method is introduced Boosting method. The method is a method which combines boosting weak classifiers into a strong classifier. In the boosting method introduced any iteration. In each iteration of his learning, the results of prediction models constructed training data, and then re-sampled to get in on the next itrasi. In general, this will increase the level of accuracy of the classifiers. Boosting methods are also good to use for data spreading is often said to be unbalanced or imbalance. In conclusion, it was found that the use of Boosting methods on classification gives better results with a high degree of accuracy.","Kata Kunci : boosting, adaboost, adabag"
http://etd.repository.ugm.ac.id/home/detail_pencarian/70487,ESTIMASI MAXIMUM LIKELIHOOD DAN RESTRICTED MAXIMUM LIKELIHOOD  UNTUK MODEL LINEAR EFEK CAMPURAN,"ADIANTO PANGARIBUAN, Dr. Gunardi, M.Si",2014 | Skripsi | STATISTIKA,"Model linear efek campuran merupakan salah satu metode regresi parametrik linear untuk data berkelompok, longitudinal, atau data pengamatan yang berulang untuk mengetahui hubungan variabel dependen dan independen. Model linear efek campuran terdiri dari parameter efek tetap dan efek random. Estimasi parameter dari model linear efek campuran dapat diperoleh menggunakan metode estimasi maximum likelihood dan restricted maximum likelihood dengan distribusi multivariat normal dari model marginal. Koefisien efek random pada model dapat diprediksi menggunakan ekspektasi bersyarat. Pemilihan model terbaik menggunakan uji rasio likelihood beserta Akaike Information Criterion (AIC) dan Bayesian Information Criterion (BIC). Pemilihan metode estimasi terbaik menggunakan mean square error. Aplikasi data untuk model linear efek campuran yaitu registrasi pemeriksaan ibu hamil di salah satu Puskesmas di Yogyakarta.","Linier mixed effects model (LME) is one of parametric linier regression for clustered, longitudinal, or repeated measures data that quantifies the relationship between a continous dependent variabel and independent variable. LME may include both fixed effects parameters and random effects. Estimation parameter of LME can be achieved using maximum likelihood estimation and restricted maximum likelihood method with multivariate normal distribution from the marginal model. Random effects coefficient from the model can be predicted using conditional expectation. The best model selection using the likelihood ratio test alongside with Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The best method estimation selection using mean square error. LME is applied to real data, the examination of pregnant woman in Community Health Center in Yogyakarta.","Kata Kunci : model linear efek campuran, model marginal, efek random, maximum likelihood, restricted maximum likelihood, ekspektasi bersyarat, uji rasio likelihood, AIC dan BIC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/71776,ANALISIS SURVIVAL WAKTU DISKRIT,"DJATI SAMPURNA, Dr. Danardono, MPH",2014 | Skripsi | STATISTIKA,"Model hazard proposional waktu kontinu didasarkan pada asumsi yang tidak realistis bahwa efek prediktor pada saat terjadinya event adalah konstan sepanjang waktu. Ada beberapa kasus bahwa prediktor nilainya bisa bervariasi dari waktu ke waktu. Maka dari itu analisis survival waktu diskrit digunakan sebagai pendekatan umum analisis survival untuk mengatasi masalah ini. Prediktor yang nilainya bervariasi dari waktu ke waktu dapat dimodelkan seperti time-invariant. Singer dan Willet (1993) menunjukkan bahwa model ini bisa sesuai menggunakan analisis regresi logistik dan menggunakan prosedur maksimum likelihood untuk mengestimasi parameternya. Menggunakan data penderita AIDS dengan prediktor STRATUM dan CD4, akan dijelaskan perbedaan kedua prediktor tersebut dalam menginterpretasikannya.","The continuous-time proportional hazard model based on the assumption which is not realistic that the predictor effect when the event occur is constant all the time. There are some cases that the value of predictor can be varies from time to time. Because of that, the discrete-time survival analysis is used as common approach survival analysis to solve this problem. The predictor which value is varies from time to time can be modeled such as time-invariant. Singer and Willet (1993) showed that the model can be fit to use the logistic regression analysis and to use the likelihood maximum procedure to estimate itâ€™s parameter. By use the AIDS data patient with STRATUM and CD4 predictors, will be explained the different between both of predictors in interpretation.","Kata Kunci : analisis survival, data longitudinal, regresi logistik"
http://etd.repository.ugm.ac.id/home/detail_pencarian/68963,ALOKASI OPTIMAL UKURAN SAMPEL DATA LONGITUDINAL DUA GRUP DENGAN RESPON KONTINU DAN MATRIKS KOVARIAN COMPOUND SYMMETRY,"CAMELIA FALISA, Drs. Zulaela, Dipl. Med. Stats., M.Si.",2014 | Skripsi | STATISTIKA,"Ukuran sampel adalah banyaknya individu yang diamati dalam suatu penelitian atau percobaan. Penentuan ukuran sampel yang tidak tepat akan mengakibatkan sampel tersebut tidak dapat merepresentasikan populasinya dengan baik. Estimasi ukuran sampel dan alokasi optimum harus sesuai dengan hipotesis desain studinya. Begitu juga dengan desain studi longitudinal yang merupakan pengamatan berulang atas suatu individu atau observasi terhadap waktu. Skripsi ini membahas alokasi optimal ukuran sampel untuk sata longitudinal dua grup dengan respon kontinu dan matriks kovarian compound symmetry. Tujuan menentukan alokasi optimal adalah untuk memberikan acuan estimasi ukuran sampel dan ukuran pengulangan optimal kepada peneliti, yang meminimumkan biaya apabila power ditentukan oleh peneliti atau memaksimumkan power jika biaya ditentukan oleh peneliti. Sehingga peneliti mendapatkan ukuran sampel yang efektif dan efisien.",Sample size is the number of individual used in research or experiment. Innacurate sizing of the sample will prevent the sample from representing the population properly. Sample size estimation and optimal allocation must be appropriate with hipothesis of study design. So that longitudinal study which are recuring observation to individu or observation with time. This paper examines optimal allocation for two-groups longitudinal data with continuous response and compound symmetry covariance matrix and provide methods to find the optimal combination of number of participans and number of repeated measures for maximizing power under a fixed budgetary constraint or minimizing budget under fixed power constraint. So the investigator get effective and efficient sample size.,"Kata Kunci : ukuran sampel, data longitudinal, alokasi optimal, matriks kovarian compound symmetry"
http://etd.repository.ugm.ac.id/home/detail_pencarian/66152,VALUASI PREMI SYARIAH MENGGUNAKAN DEFLATOR DENGAN  PENDEKATAN MODEL HULL-WHITE,"RISMA ARYANI, Dr Adhitya Ronnie Effendie S.Si, , M.Sc",2014 | Skripsi | STATISTIKA,"Dalam prinsip ekonomi islam tidak mengenal adanya konsep nilai waktu terhadap uang (time-value of money) dan pelarangan riba dalam berbagai bentuknya. Prinsip ini tidak menghilangkan faktor acak dalam perhitungan return pada suatu rencana investasi yang berbeda pada prinsip ekonomi konvensional. Maksudnya, tidak menghilangkan faktor ketidakpastian yang ada dalam perolehan keuntungan atau kerugian suatu usaha. Pengamatan perlakuan perubahan secara acak ini dilakukan pada pergerakan fluktuasi equivalen rate. Fluktuasi equivalen rate tersebut diambil dari historical data periode sebelumnya pada suatu lembaga dinar. Untuk memperlihatkan pergerakan fluktuasi equivalen rate tersebut digunakan adanya pendekatan model Hull-White yang sebelumnya dilakukan proses estimasi terhadap parameter-parameternya. Selanjutnya pergerakan fluktuasi equivalen rate dan taksiran nilai equivalen rate periode ke depan tersebut digambarkan dengan menggunakan regresi Ordinary Least Square. Dalam skripsi ini juga diperkenalkan diskretisasi persamaan model Hull-White sebagai deflator yang digunakan untuk faktor diskon syariah.","One of the principles in Islamic economics is the in existence concept of time value of money and prohibition of usury (riba) in any forms. This principle does not remove random factor in profit-earning orloss of a business. The observation of random factor treatment is done at equivalent rate fluctuation movement. Fluctuation of equivalent rate is taken from historical data of previous periodic of a dinar institution. To show the movement of equivalent rate fluctuation, Hull-White model approach is utilized as previously estimation process is done towards the parameters. Furthermore, the equivalent rate fluctuation and equivalent rate value estimation of the next periodic is illustrated using Ordinary Least Square Regresion. Discretization of Hull-White Model equation is introduced as deflator which is used as Islamic discounting factor.","Kata Kunci : ekonomi syariah, time-value of money , equivalen rate, deflator, model Hull-White"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72042,DISTRIBUSI BETA WEIBULL UNTUK ANALISIS DATA SURVIVAL,"VINIE FRANCISCA, Rianti Siswi Utami, S.Si., M.Sc.",2014 | Skripsi | STATISTIKA,"Dalam skripsi ini dijelaskan tentang estimasi parameter dari model distribusi beta Weibull.Distribusi beta Weibull adalah sebuah distribusi hasil modifikasi antara fungsi beta dan distribusi Weibull yang memiliki empat parameter. Distribusi beta Weibull memiliki beberapa submodel khusus untuk beberapa parameter yang diberi nilai tertentu. Distribusi beta Weibull memiliki fungsi hazard yang berbentuk bathtup, unimodal, turun, dan naik. Untuk mengestimasi parameter dari model distribusi beta Weibull, digunakan metode maksimum likelihood dan BFGS.Dapat ditunjukkan bahwa distribusi beta Weibull lebih baik dibandingkan dengan distribusi Weibull.","This study explains about the parameter estimation of the beta Weibull distribution model. Beta Weibull distribution is a result of the distribution modification between the beta function and Weibull distribution which have four parameters. Beta Weibull distribution has several specific submodels of several parameters which are bathtup, unimodal, increasing, and decreasing. For estimating the parameter of beta Weibull distribution model, it is used the likelihood mazimum method and BFGS. It shows that beta Weibull distribution is better than Weibull distribution.","Kata Kunci : beta Weibull, maksimum likelihood, BFGS, bathtup, unimodal"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72044,KLASIFIKASI DENGAN METODE RANDOM FOREST DAN ANALISIS DISKRIMINAN LINEAR,"CUT ASYRAF ANZILA, Adhitya Ronnie Effendie, S.Si., M.Sc.",2014 | Skripsi | STATISTIKA,"Pohon Keputusan telah banyak digunakan pada berbagai macam masalah karena melihat efisiensi waktu dalam menganalisa dan keakuratan klasifikasinya. Salah satu pengembangan dari pohon keputusan adalah metode klasifikasi Random Forest. Penggunaan metode random forest untuk menghasilkan pohon gabungan telah memberikan dugaan yang lebih tinggi akurasinya dibandingkan dengan pohon tunggal. Analisis klasifikasi klasik yang biasa digunakan adalah Analisis Diskriminan Linear. Namun biasanya Analisis Diskriminan akan menghasilkan prediksi dibawah metode Random Forest. Untuk meningkatkan akurasi dari Analisis Diskriminan Linear ini, akan digunakan seleksi fitur dari Random Forest. Seleksi fitur merupakan sebuah tahapan penting dalam proses klasifikasi, karena fitur yang terseleksi sangat mempengaruhi tingkat akurasi dari klasifikasi. Pada dataset yang memiliki banyak fitur membutuhkan proses untuk mereduksi fitur yang dianggap kurang penting. Setelah menyeleksi fitur tahap selanjutnya adalah mengklasifikasikan data.","Decision Tree has been widely used in various cases due the time efficiency in the analysis and classification accuracy. One of the development of Decision Tree is Random Forest classification method. The use of this method to produce the joint tree have provided more accurate allegation compared to the single tree. The common used for classification analysis is Linear Discriminant Analysis. Usually, Linear discriminant Analysis provide lower accuracy compared to the Random Forest Methods. In order to increase the accuracy of Linear Discriminant Analysis, features selection will be used with Random Forest. The features selection is an important stage in classification proccess, because the selected features will greatly affect the accuracy of classification. In the dataset with many features require a proccess to reduce the insignificant feature. After selecting the features, the data will be classified with Linear Discriminant Analysis.","Kata Kunci : Klasifikasi, Random Forest, Analisis Diskriminan Linear, seleksi fitur."
http://etd.repository.ugm.ac.id/home/detail_pencarian/71021,REGRESI NONPARAMETRIK KERNEL DAN METODE THEIL DALAM MEMODELKAN HUBUNGAN KURS RUPIAH DENGAN IHSG,"I GB.YOGISWARA PATRA, Drs. Zulaela, M.Si.",2014 | Skripsi | STATISTIKA,"Analisis regresi  adalah suatu analisis  yang sering digunakan untuk memodelkan  hubungan  antara  dua  variabel  atau  lebih.  Ada  regresi  parametrik  dan  regresi  nonparametrik.  Pada  regresi  nonparametrik,  ada  banyak  metode  untuk  mengestimasi  model  persamaan,  seperti  metode  Theil  dan  regresi  kernel.  Keduanya akan digunakan untuk memodelkan data nilai tukar rupiah terhadap US  Dollar  dan  IHSG  yang  terindikasi  memiliki  outlier  dan  tidak  memenuhi  asumsi  regresi parametrik. Akan dibandingkan pula dengan nilai MSE, MAE dan MAPE  untuk melihat metode mana yang lebih baik pada kasus ini.","Regression  analysis  is  an  analysis  that  is  often  used  to  model  the  relationship  between  two  or  more  variables.  There  parametric  regression  and  nonparametric  regression.  In  nonparametric  regression,  there  are  many  methods  for  estimating  equation models, such as the  Theilâ€™s method  and kernel regression. Both will be  used to model the data of the  exchange rate  and the JCI has indicated outliers and  do not meet the assumptions of parametric regression.  Similarly, the value will be  compared to MSE, MAE and MAPE to see which method is better in this case.","Kata Kunci : regresi nonparamterik, metode Theil, kernel, outlier"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72045,MODEL ADITIF CAMPURAN TERGENERALISASI,"JAMILATUZ ZAHRO, Herni Utami, M.Si",2014 | Skripsi | STATISTIKA,"Generalized additive models (GAM) merupakan perluasan dari regresi linier biasa, dengan mengganti fungsi linier menjadi fungsi aditif sehingga model ini dapat digunakan meskipun hubungan variabel respon dan variabel prediktor tidak linier. Serta, variabel responpada GAM merupakan keluarga eksponensial. Namun GAM tidak dapat digunakan jika ada dua efek dalam suatu model yaitu efek tetap dan efek acak. Generalized additive mixed models (GAMM) ini diharapkan lebih efisien dalam mengidentifikasi sebaran pengaruh komponenen acak sehingga mampu menerangkan lebih tepat pengaruh komponen acak tersebut dalam suatu model. Penggunaan generalized additive mixed models untuk data variabel kuantitatif dengan estimasi fungsi penghalus menggunakan smoothing spline, dan estimasi parameter menggunakan Maximum Likelihood Estimation (MLE) tidak dapat diselesaikan secara analitik, sehingga estimator dihitung dengan memaksimumkan fungsi log-likelihood secara numerik menggunakan metode Newton-Raphson dengan menggunakan ekspektasi turunan kedua dari fungsi log-likelihood yang dinamakan teknik fisher scoring.","The generalized additive models (GAM) is an extension of the usual linear regression by generalizing linear functions into an additive function so that this model can be used even though the relationship of the response variable and linear predictor variables is not linear. And the response variable exponential family. But GAM can not be used if there are two effect influencing the model, that is fixed effect and random effect. The generalized additive mixed models (GAMM) is expected to be more efficient in identifying the effect of the distribution of the random component that is able to explain precisely the effect of a random component in a model. The use of generalized additive mixed models for quantitative variables with the estimation of data smoothing using smoothing spline functions, and parameter estimation using Maximum Likelihood Estimation (MLE) can not be solved analytically, so the estimator is computed by maximizing the log-likelihood function numerically using the Newton-Raphson method with using the expected second derivative of the log-likelihood function called fisher scoring techniques.","Kata Kunci : Generalized additive mixed models, maximum likelihood estimation, smoothing spline, fisher scoring."
http://etd.repository.ugm.ac.id/home/detail_pencarian/72814,PENGKLASTERAN DATA RUNTUN WAKTU BERBASIS DENSITAS PERAMALAN,"ORIEZA FEBRIANDHANI, Prof. Drs. Subanar, Ph.D",2014 | Skripsi | STATISTIKA,"Suatu metode pengklasteran data runtun waktu diperkenalkan, dengan berbasis pada densitas probabilitas dari peramalan pada titik atau rentang waktu tertentu. Pertama, prosedur bootstrap dikombinasikan dengan estimator nonparametrik kernel untuk memperoleh estimasi dari densitas peramalan. Hasil estimasi densitas peramalan ini kemudian digunakan untuk membentuk matriks ketakmiripan yang selanjutnya dipakai untuk melakukan pengklasteran. Terakhir, aplikasi metode ini pada dataset riil juga akan dibahas.","A clustering method for time series is introduced, based on probability density of the forecast. First, autoregressive bootstrap procedure combined with nonparametric kernel estimator is applied to data to obtain estimation of the forecast densities. The estimated forecast densities are the used to construct the dissimilarity matrix and hence to perform clustering. Finally, application of this method in real dataset are discussed.","Kata Kunci : Pengklasteran, Runtun Waktu, Autoregression Bootstrap, Estimator Kernel, Indeks Produksi Industri"
http://etd.repository.ugm.ac.id/home/detail_pencarian/66932,ESTIMASI HARGA OBLIGASI MENGGUNAKAN PENDEKATAN DURASI DAN KONVEKSITAS HEATH JARROW MORTON DARI UKURAN SENSITIVITAS HARGA OBLIGASI TERHADAP YIELD (Studi Kasus : Obligasi Pemerintah Indonesia),"FEBTIO ADI WIBAWANTO, Dr. Abdurakhman, M.Si.",2014 | Skripsi | STATISTIKA,"Obligasi sebagai instrumen investasi yang dikelompokkan pada sekuritas berpendapatan tetap memiliki risiko akibat dari perubahan tingkat bunga pasar. Dalam hal ini, investor membutuhkan manajemen risiko obligasi untuk meminimalkan risiko akibat dari perubahan tingkat bunga (yield) dan mendapatkan imbal hasil (return) sesuai yang diinginkan. Durasi dan konveksitas merupakan kombinasi yang cocok digunakan dalam manajemen risiko. Pendekatan Durasi dan konveksitas Heath Jarrow Morton untuk obligasi berkupon merupakan salah satu cara untuk mengukur sensitivitas harga obligasi yang diperkenalkan oleh Manfred Fruhwirth (2001) dengan dua contoh model HJM yang populer dengan struktur volatilitas deterministik yaitu model volatilitas konstan (Ho/Lee, 1986) dan model volatilitas eksponensial (Hull/White, 1990). Dalam skripsi akan dibahas bagaimanakah menghitung durasi dan konveksitas Heath Jarrow Morton menggunakan model volatilitas konstan untuk mengetahui estimasi perubahan harga obligasi yang lebih akurat akibat dari perubahan yield dengan membandingkan estimasi harga obligasi menggunakan pendekatan tradisional dan eksponensial dalam yield waktu kontinu. Selanjutnya, Mengkonstruksi bobot/proporsi obligasi yang optimal untuk membentuk portofolio obligasi dengan menggunakan metode Moment.","Bonds as invesment instrument are classified in fixed income securities have risks from changes in yields. In this case, investors require bonds for risk management to minimize the risks resulting from change in yields amd obtain the desired return. Duration and convexity are used in a suitable combination of risk management. Approach duration and convexity Heath Jarrow Morton for coupon bonds are one way to sensitivity measure of bond price introduced by Manfred Fruhwirth (2001) with two popular examples of HJM models with deterministic volatility structure. They are constant volatility model (Ho/Lee, 1986) and exponential volatility model (Hull/White, 1990). In this graduation paper will be discussed how to calculate the duration and convexity Heath Jarrow Morton with constant volatility model to estimate bond price changes more accurate from change in yields by comparing the estimated price of the bond using traditional approach and exponential approach in yield contionuous time. Furthermore, constructing the weight/proportion of optimal bonds to form bond portfolio with moment method.","Kata Kunci : harga obligasi, durasi, konveksitas, heath jarrow morton, return, yield, metode moment, bond price, duration, convexity, heath jarrow morton, return, yield, moment method"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72337,COPULA DOUBLE EXPONENSIAL UNTUK PREDIKSI MODEL KERUGIAN AGGREGATE,"PUJI LESTARI, Dr. Adhitya Ronnie Effendi",2014 | Skripsi | STATISTIKA,"Aggregate loss atau kerugian aggregate adalah total kerugian harus ditanggung oleh perusahaan asuransi dalam suatu periode waktu tertentu dalam suatu kontrak. Hal yang menarik dalam model kerugian aggregate adalah mengenai prediksi banyak klaim maupun besar klaim. Skripsi ini mengembangkan prediktor dari kerugian aggregate dengan menggunakan data longitudinal. Pada data longitudinal, salah satu pertemuan data dari data crossection kelas risiko dengan data klaim asuransi terdahulu yang tersedia untuk masing-masing kelas risiko Untuk membantu menjelaskan dan memprediksi banyak klaim dan besar klaim kita membutuhkan variabel penjelas. Model distribusi marginal klaim dalam skripsi ini menggunakan model linear tergeneralisasi (GLM). Banyak klaim direpresentasikan menggunakan model regresi Poisson yang bersyarat pada variabel latent. Variabel laten mempunyai hubungan antara banyak klaim, sehingga distribusi bersama antara keduanya dimodelkan dengan menggunakan copula elliptical. Skripsi ini menampilkan ilustrasi dengan mengambil contoh data klaim kendaraan dikota Massachusetts. Estimasi parameter dari variabel laten dari proses klaim di peroleh dan didapat simulasi prediksi.","Aggregate loss model is total amount paid on all claims occurring in a fixed time period on a defined set of insurance contracts. For a model of aggregate losses, the interest is in predicting both the claims number process as well as the claims amount process. This minithesis develops predictor of aggregate losses using a longitudinal data. In longitudinal data, one encounters data from cross-section of risk classes with a history of insurance claims available for each risk class. To help explain and predict both the claims number and claims amount process we need explanatory variables. For the marginal claims distributions this minithesis uses generalized linear models (GLM). The claims number process is represented using a Poisson regression models that is conditioned on a sequence of latent variables. These latent variables drive the serial dependencies among claims number, their joint distribution is represented using an elliptical copula. This minithesis presents an illustrative example of Massachusetts automobile claims. Estimates of the latent claims process parameters are derived and simulated predictions are provided.","Kata Kunci : aggregate loss, longitudinal data, elliptical copula"
http://etd.repository.ugm.ac.id/home/detail_pencarian/69526,APLIKASI GENERALIZED RIDGE REGRESSION UNTUK MENGATASI MASALAH MULTIKOLINEARITAS,"VALENDRA GRANITHA SHANDIKA P., Dr. Abdurakhman S.Si., M.Si",2014 | Skripsi | STATISTIKA,"Metode kuadrat terkecil adalah salah satu metode penaksiran parameter yaitu metode untuk menduga koefisien regresi. Jika salah satu asumsi regresi klasik yaitu asumsi no multikolinearitas tidak terpenuhi, estimasi parameter dengan menggunakan metode kuadrat terkecil menjadi kurang valid, bahkan jika terjadi multikolinearitas sempurna dapat menyebabkan parameter beta tidak dapat diestimasi. Hubungan linier antar variabel independen ini menyebabkan variansi parameter beta menjadi besar, errornya pun besar. Padahal nilai estimasi yang diinginkan adalah yang memiliki nilai variansi dan error yang kecil. Salah satu penanganan multikolinearitas ini adalah dengan regresi ridge. Konsep dari regresi ridge adalah menambahkan tetapan bias sebesar k yang merupakan matriks diagonal, ke dalam matriks korelasi ' X X . Dalam skripsi ini akan dibahas mengenai salah satu metode estimasi parameter k yaitu dengan metode Generalized Ridge Regression. Dengan metode ini, nilai parameter ridge k yang dihasilkan tidak hanya satu macam, nilai k yang didapat berbeda untuk tiap variabel independennya.","Least square method is one of parameter estimation method, it is a method for estimating regression coefficient. If any of classical regression assumption, that is no multicollinearity is not met, parameter estimation using least square method becomes less valid, even if there is perfect multicollinearity then it causes beta parameters canâ€™t be estimated. Linear relationship between the independent variables causes the variance parameter beta becomes large, the error was large. In fact, the estimated value we desired is a small variance and error. For handling this multicollinearity problem, one of the way is using ridge regression. The concept of ridge regression is by adding a k biased constant which is a diagonal matrix, to the correlation matrix ' X X . In this paper we will discuss one of the k parameter estimation methods, namely Generalized Ridge Regression. This method obtain the k ridge parameter that is not a single parameter but multiple k ridge parameters. Those k ridge parameters are different for each independent variables.","Kata Kunci : Metode Kuadrat Terkecil, Multikolinearitas, Regresi Ridge, Generalized Ridge Regression."
http://etd.repository.ugm.ac.id/home/detail_pencarian/66970,PORTOFOLIO OPTIMAL DENGAN METODE BEST BETA CAPM ( Studi Kasus Pada Saham â€“ Saham LQ-45 Periode 2010 â€“ 2013 ),"ASTRIANI KUSUMANINGRUM, Dr. Abdurakhman, M.Si.",2014 | Skripsi | STATISTIKA,"Masalah 'best-beta' muncul yaitu pada potensi kesalahan pada model asset pricing Sharpe-Lintner-Hitam (CAPM) yang telah diakui. Dengan memasukkan variable target kedalam preferensi investor, diperoleh suatu best beta CAPM yang membahas perbandingan teori CAPM dan analisis sederhana dalam meningkatkan akurasi penetapan harga. Pengamatan empiris menunjukkan bahwa BCAPM diharapkan memprediksi hasil yang lebih baik dibandingkan dengan CAPM yaitu sekitar 20% sampai 30% per tahun. Kita tidak dapat menemukan, kita bisa setidaknya memperbaiki, kita dapat memberikan sedikit perkembangan.","The issue of 'best-betaâ€™ arises as soon as potential errors in the Sharpe- Lintner-Black capital asset pricing model (CAPM) are acknowledged. By incorporating a target variable into the investor preferences, this study derives a best-beta CAPM (BCAPM) that maintains the CAPM's theoretical appeal and analytical simplicity yet unambiguously improves its pricing accuracy. Empirical observations suggest that the BCAPM predicts expected returns better than the CAPM by 20% to 30% annually. Where we cannot invent, we may at least improve; we may give somewhat of novelty to that which was old, condensation to that which was diffuse, perspicuity to that which was obscure, and currency to that which was recondite.","Kata Kunci : Resiko, Capital Asset Pricing Model ( CAPM ), Beta, Portofolio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/76188,RANCANGAN BUJUR SANGKAR GRAECO LATIN,"ROMIE PRIYASTAMA, Prof. Drs. Suryo Guritno, M.Stats., Ph.D",2014 | Skripsi | STATISTIKA,"Rancangan percobaan adalah salah satu cara terbaik untuk mendapatkan hasil yang lebih baik pada sebuah percobaan. Salah satu bentuk rancangan percobaan adalah rancangan bujursangkar Graeco Latin yang merupakan bagian dari rancangan kelompok lengkap. Pada dasarnya rancangan bujursangkar graeco latin merupakan gabungan dari dua rancangan bujursangkar yang saling ortogonal yang mana rancangan bujursangkar yang satu terdiri dari huruf Latin sedangkan rancangan bujursangkar yang lain terdiri dari huruf Yunani. Rancangan ini bertujuan untuk mengendalikan tiga sumber keragaman dari tiga arah. 	Parameter-parameter yang terdapat pada model rancangan bujursangkar Graeco Latin diestimasi dengan menggunakan metode kuadrat terkecil yaitu metode yang meminimumkan jumlah kuadrat kesalahan. Pada penelitian kali ini digunakan analisis variansi untuk mengetahui perbedaan pencampuran bensin dan uji lanjut perbandingan ganda untuk memperoleh nilai perbedaan rata-rata antar perlakuan. 	Ada empat faktor yang digunakan pada percobaan untuk mengetahui keefektifan penggunaan bahan bakar antara lain hari percobaan, merk mobil, pengemudi dan pencampuran bensin. Dengan menggunakan rancangan bujur sangkar graeco latin ini dapat diketahui bahwa hanya faktor pencampuran bensin yang berpengaruh secara signifikan dalam mengefisienkan penggunaan bahan bakar yang diukur melalui jarak tempuh per liter.","Design of experiment is one of the best way to get better result on a experiment. Graeco-latin square is one of the design of experiment. Basically, graeco latin square design is a combined of two square design which mutually ortogonal which part the one square design consist of Latin letters whereas the other one consist of Greek letters. This design intent on controlling three variation source from three ways. Parameters that contained in Graeco Latin will be estimated with least square method that is method which is minimization the sum square of error. On this research will be used analysis of variation to find out  the difference of fuel mixing and multiple comparison analysis to obtain the difference mean between treatment. There are four factors as used in experimentation to find out the effectiveness for using fuel that is day test, brand of car, drivers and fuel mixing. With using Graeco latin square design is readily ascertainable that only fuel-mixing factor which had an effect significantly on using fuel which measurably through distance per liter.","Kata Kunci :  design of experiment, graeco latin, ortogonal"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72096,MENENTUKAN OPSI BELI BARRIER DOWN AND OUT TIPE EROPA DENGAN VOLATILITAS MODEL GARCH (Studi Kasus Saham Melco Crown Entertainment Limited),"LARAS NUR NOVIANI, Yunita Wulan Sari, S.Si., M.Si",2014 | Skripsi | STATISTIKA,"Produk derivatif merupakan kontrak finansial turunan dari produk acuan  seperti saham, obligasi, atau suku bunga. Salah satu contoh produk derivatif yang  populer ialah opsi beli barrier down and out tipe Eropa. Opsi ini merupakan opsi  beli yang letak barrier-nya berada di bawah harga beli saham dan opsi akan  dinonaktifkan jika harga beli saham menembus atau keluar dari nilai barrier. Opsi  jenis ini digunakan untuk memanfaatkan kecenderungan perilaku harga saham  yang selalu di atas.  Harga opsi beli barrier down and out  tipe Eropa dapat  ditentukan dengan formula Black-Scholes. Semua parameter dalam formula  Black-Scholes, yaitu harga saham di pasar, harga pelaksanaan (K) , waktu sampai  jatuh tempo (T) , nilai barrier (B), tingkat bunga bebas resiko (r) dapat diketahui  kecuali volatilitas (ðœŽ).  Dalam skripsi ini akan dibahas bagaimana mengestimasi volatilitas dengan  model GARCH untuk penentuan harga opsi. Dimana model GARCH merupakan  model yang sederhana dan modelnya tidak hanya bergantung pada residual data  sebelumnya, tetapi juga bergantung pada nilai volatilitas data sebelumnya. Selain  itu model ini dapat mengatasi volatility clustering pada data return.","A derivative product is a financial contract which derives its value from  the performance of another entity such as an asset, bond, or interest rate. One of  the most popular derivative is European down and out barrier call option. It is an  option with barrier level at under price of asset and option is extinguished on the  price of the underlying asset breaching a barrier. This option is used to take  advantage of underlying asset that always on the top. Pricing  down and out  barrier option can be determined with Black-Scholes formula. All parameters in  the Black-Scholes formula, exercise price (K), time to maturity (T), price of asset  at t=0 (ð‘†0) , barrier value (B), risk free interest rate (r) can be known, except a  volatility (Ïƒ). This research is tell about how to estimate volatility with GARCH model  to pricing European down and out barrier call option. Where GARCH model is a  simple model and its not only dependent to before residual data, but also  dependent to before volatility. Beside that GARCH model can be overcome  volatility clustering in return data","Kata Kunci : Opsi call Eropa, Opsi barrier down and out , Volatilitas, GARCH"
http://etd.repository.ugm.ac.id/home/detail_pencarian/71843,VALUE AT RISK NONPARAMETRIK UNTUK CLAIM SEVERITY PADA  ASURANSI KERUGIAN MENGGUNAKAN ESTIMASI KERNEL  BERTRANSFORMASI GANDA,"DIAH PUTRI RAMADHANI, Prof. Dr.rer.nat. Dedi Rosadi,S.Si.,M.Sc.,",2014 | Skripsi | STATISTIKA,"Asuransi melibatkan dua pihak yaitu pihak penanggung dan pihak tertanggung. Pihak penanggung berkewajiban membayar pertanggungan sedangkan pihak tertanggung berkewajiban membayar sejumlah uang sebagai kompensasinya. Hal ini membuat perusahaan asuransi harus menentukan harga premi. Salah satu ukuran yang digunakan sebagai patokan adalah ukuran resiko, dan salah satu cara menghitung resiko adalah metode Value at Risk. Value at Risk dimodelkan sesuai dengan bentuk dari distribusi kerugian. Untuk data data asuransi yang bersifat heavy tailed dapat digunakan metode Estimasi Kernel Bertransformasi Ganda","Insurance involves two parties namely the insured and the insured (insurance company). The insurer must pay some amounts to cover insured when they have a loss, while the insured is obliged pay a premium as a compensation. This makes insurance companies should determine the price of premium. One measure that is used as a benchmark is a measure of risk, and one way to calculate the risk is Value at Risk method from a loss function. Value at risk is one of the method to measure the risk from a loss function. But It should be modeled as fit as possible with the distribution of the data. For the insurance data with heavy tail, Double Transformed Kernel Estimation for Value at Risk can be used","Kata Kunci : Klaim Asuransi, Estimator Kernel, Value at Risk, Distribusi Kerugian, Disribusi Modified Champernowne, Distribusi Beta(3,3)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/71845,PEMODELAN TOPIK UNTUK MEDIA SOSIAL MENGGUNAKAN LATENT DIRICHLET ALLOCATION,"RUSKE ILLA KENGKEN, Prof. Dr. rer. nat. Dedi Rosadi, S.Si., M.Sc.",2014 | Skripsi | STATISTIKA,Berkembangnya analisis media sosial saat ini memberikan suatu kebutuhan baru. Kita dituntut untuk dapat menyimpulkan opini atau argumen dalam kumpulan dokumen yang sangat besar seperti pada media sosial secara cepat dan efisien. Dari opini yang didapat kita dapat menyimpulkan sebuah informasi utama yang tersembunyi dan dapat digunakan untuk analisis lebih lanjut. Pemodelan topik atau topic models merupakan perkembangan dari analisis teks yang bermanfaat dalam pemodelan data tekstual dengan tujuan menemukan topik yang tersembunyi didalamnya. Salah satu model yang akan dibahas adalah model probabilitas Latent Dirichlet Allocation (LDA). Model Latent Dirichlet Allocation (LDA) merupakan sebuah model probabilitas dari data tekstual dimana dapat menjelaskan korelasi antara kata-kata dengan tema semantik yang tersembunyi didalam dokumen tersebut. Estimasi parameter yang digunakan dalam model adalah metode Bayesian. Metode Bayesian adalah sebuah metode yang memberikan nilai estimasi melalui distribusi posterior. Untuk model ini perhitungan estimasi dari distribusi posterior sangat kompleks sehingga digunakan estimasi Gibbs sampling. Dalam skripsi ini diterapkan model probabilitas Latent Dirichlet Allocation (LDA) untuk data yang bersumber dari salah satu platform media sosial yaitu Twitter. Tujuannya adalah untuk mengetahui berita apa yang dominan dibicarakan masyarakat di Twitter dalam periode tertentu. Hasil dari pemodelan topik ini adalah berupa topik utama dari seluruh opini masyarakat yang diinterpretasikan menjadi berita yang paling dominan dibicarakan masyarakat.,"The rise of social media analysis is currently providing a new requirement. We are required to conclude an opinion or argument in a document such as the enormous social media data as quickly and efficiently. Opinion obtained from us may infer a hidden key information and can be used for further analysis. Topic models is model for corpus to finding topics hidden in it. One model that will be discussed is Latent Dirichlet Allocation (LDA) probability model. Latent Dirichlet Allocation (LDA) is a probability model of textual data which can explain the correlation between the words with a hidden semantic theme in the document. Estimation of the parameters used in the model is a Bayesian method. Bayesian method is a method that provides value estimates through the posterior distribution. For this model the estimated calculation of the posterior distribution is very complex, therefore Gibbs sampling estimation is then used. In this paper, Latent Dirichlet Allocation (LDA) probability model is applied for data that have their source from one of the social media platform, Twitter. The aim is to know what dominant news are talking about on Twitter in a given period. The outcome of this topic models is a main topic of the entire public opinions which is then interpreted to be the most dominant news people talk about.","Kata Kunci : pemodelan topik, latent Dirichlet allocation, Bayesian, Gibbs sampling, text mining, analisis teks, twitter"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72102,Estimasi Model Spatial Autoregressive dan Model Spatial Error dengan Matriks Pembobot Spasial Tipe Rook,"NOVIANA, Drs. Zulaela, Dipl.Med.Stats, M.Si",2014 | Skripsi | STATISTIKA,"Analisis regresi klasik merupakan suatu metode dalam analisis statistik yang dapat mengetahui bagaimana suatu variabel yang mempengaruhi dapat menduga variabel yang lain yang dipengaruhi. Analisis data menggunakan regresi klasik terkadang efek wilayah kurang diperhatikan sehingga dapat menghasilkan kesimpulan yang kurang akurat bahkan dapat menghasilkan estimasi yang bias sehingga hasil analisis kurang baik digunakan untuk mengambil suatu keputusan, oleh karena itu diperlukan analisis yang mampu mangatasi masalah efek wilayah ini. Regresi spasial yang merupakan generalisasi dari analisis regresi klasik yang memuat unsur autoregressive atau error di dalam model untuk mendapatkan kesimpulan yang lebih tepat dengan data yang mengandung efek wilayah. Pada kasus Crude Birth Rate (CBR) signifikan mengandung efek error namun tidak mengandung efek autoregressive sehingga Spatial Error Model lebih tepat digunakan daripada model yang mengandung efek autoregressive pada Spatial Autoregressive Model.","Classical regression analysis is a method of statistical analysis to determine how the independent variables affect the dependent variables. Classical regression analysis sometimes ignores the element of region in data that has been collected in some regions, it will produce biased estimates so the results of the analysis are less well used to take a decision. So necessary to use a spatial regression analysis that generalization of the classical regression analysis which includes elements of the autoregressive or error in the model to get a more precise conclusion. In the case of CBR contain significant errors, but does not contain the effect of the autoregressive effect that Spatial Error model is more appropriate than the model containing the autoregressive effect on the Spatial Autoregressive Model.","Kata Kunci : regresi klasik, autoregressive, error, Spatial Error Model, Spatial Autoregressive Model"
http://etd.repository.ugm.ac.id/home/detail_pencarian/73897,Pemodelan Lokasi-Skala untuk Data Ordinal Bertingkat,"LUAILI NURUL HUSNA, Drs. Zulaela, Dipl.Med.Stats, M.Si",2014 | Skripsi | STATISTIKA,"Kasus data yang memuat variabel respon diskrit, khususnya ordinal sering ditemukan dalam penelitian. Untuk data yang mempunyai variabel respon berskala ordinal, dapat digunakan analisis regresi ordinal. Ketika sampel data mempunyai struktur hierarki, analisis regresi ordinal tidak dapat digunakan lagi karena asumsi independensi antar pengamatan tidak terpenuhi. Selain itu, model ordinal mengasumsikan bahwa pengaruh variabel prediktor sama sepanjang logit kumulatif. Namun, pada kenyataannya, asumsi ini tidak mudah dipenuhi. Untuk mengatasi permasalahan tersebut, dapat digunakan model lokasi-skala untuk data ordinal. Model ini memuat parameter lokasi dan parameter skala sehingga dapat digunakan untuk membagi derajat variansi di dalam subjek dan variansi antar subjek.","We often find data with discrete respon variable especially ordinal in observation. For data which have ordinal respon, we can use ordinal regression analysis. When data has a hierarchial structure, ordinal regression canâ€™t used because independent assumption between observations are not met. In addition, ordinal models asssume that influence of covariates are same across the cumulative logits. But, in fact this assumptions are not easy to met. To hold this problems, we can use location-scale models for hierarchical ordinal data. This model accomodate location parameters and scale parameters, so it can used to partition the degree of within-subjects and between-sujects variance.",Kata Kunci : Ordinal models; Proportional odds assumption; Cluster; Cumulative logit; Withih-subjects variance; Between-subjects variance.
http://etd.repository.ugm.ac.id/home/detail_pencarian/72368,CREDIT SCORING ADAPTIF MENGGUNAKAN KERNEL LEARNING METHODS,"MUHAMAD RASHIF HILMI, Prof. Dr.rer.nat. Dedi Rosadi, S.Si., M.Sc.",2014 | Skripsi | STATISTIKA,"Credit scoring merupakan suatu metode berbasis analisis statistika yang digunakan untuk mengukur besaran resiko kredit. Metode klasifikasi yang paling populer diadopsi di industri credit scoring adalah analisis diskriminan linier dan regresi logistik. Namun, metode tersebut mempunyai beberapa keterbatasan. Yaitu memerlukan seleksi variabel untuk regresi logistik dan data harus mengikuti distribusi tertentu untuk analisis diskriminan linear. Berdasarkan informasi tersebut, sulit untuk mengotomatisasi proses pemodelan data ketika lingkungan atau populasi terjadi perubahan. Metode Kernel adalah salah satu solusi dari permasalahan tersebut. Metode ini tidak memerlukan upaya pemilihan variabel dan dapat selalu konvergen ke solusi yang optimal dan memberikan hasil yang sama tanpa menghadapi masalah numerik atau harus kehilangan informasi. Hal ini memungkinkan pemodel untuk merancang proses penilaian kredit secara dinamis dalam praktek di mana model keputusan dapat diperbarui dan diperbaiki dengan kedatangan informasi baru.","Credit scoring is a method based on statistical analysis that used to measure the amount of credit risk. The most popular methods of classification adopted in the credit scoring industry are linear discriminant analysis and logistic regression. However, the method has some limitations. Those methods require the selection of variables for logistic regression and also the data must follow a certain distribution for linear discriminant analysis. Based on that information, it is difficult to automate the process of data modeling occurs when the environment or a population changes. Kernel method is one of the solutions to these problems. This method does not require effort and variable selection can always converge to the optimal solutions and provide the same results without encountering numerical problems or losing information. It enables modelers to design a credit scoring process dynamically in practice where decision model can be updated and improved with the arrival of new information.","Kata Kunci : Manajemen Resiko, Penilaian Kredit, Metode Kernel, Support Vector Machines"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72881,MODEL RUNTUN WAKTU UNTUK MEMODELKAN DATA DERET BERKALA JANGKA PANJANG,"DEWINTA PUTRI, Yunita Wulan Sari, S.Si., M.Sc.",2014 | Skripsi | STATISTIKA,"Terdapat dua jenis data yang dikenal dalam analisis runtun waktu yaitu data short memory dan long memory. Short memory adalah data yang memiliki ciri proses jangka pendek sedangkan long memory adalah data yang memiliki ciri proses jangka panjang. Data long memory hanya mampu dianalisis secara akurat menggunakan metode ARFIMA (Autoregressive Fractionally Integrated Moving Average). Tujuan penulisan skripsi ini untuk menjelaskan bagaimana melakukan pemodelan data dengan metode ARFIMA secara tepat dengan langkah-langkah analisis data dengan metodologi Box Jenkins dan mampu mengaplikasikan metode peramalan tersebut pada data real. Data runtun waktu yang bersifat long memory ditandai dengan plot Fungsi Autokorelasi (Autocorrelation Function (ACF)) yang tidak turun secara eksponensial melainkan menurun secara lambat atau hiperbolik. Model ARFIMA merupakan pengembangan dari model Autoregressive Moving Average (ARIMA), dengan nilai pembedaan (differencing) bernilai pecahan. Dalam penulisan skripsi ini, identifikasi orde parameter dilakukan secara eksploratori dengan melihat pada correlogram ACF dan PACF. Langkah-langkah pemodelan ARFIMA sebagai berikut: 1. menguji normalitas data long memory; 2. membuat plot runtun waktu, plot ACF, plot PACF, menguji ketidakstasioneran data; 3. melakukan transformasi data menggunakan transformasi logaritma jika data tidak stasioner dalam variansi dan melakukan differencing jika data tidak stasioner dalam mean; 4. mengestimasi parameter model; 5. melakukan diagnostic checking dengan model yang telah memenuhi asumsi residual white noise dan pemilihan model ARFIMA terbaik dengan kriteria memiliki nilai AIC terkecil diantara kemungkinan model.","There are two types of data are known in the analysis of time series that are short memory and long memory. Short memory is data that has a characteristic process of short-term memory while long memory is data that has a characteristic process of long-term memory. The long memory data is only capable of long accurately analyzed using ARFIMA (Autoregressive Fractionally Integrated Moving Average). The purpose of this thesis to explain how to do data modeling with appropriate ARFIMA method with steps of data analysis by the method of Box Jenkins were able to apply these forecasting methods to real data. Data time series that are characterized by long memory has autocorrelation function plot (Autocorrelation Function (ACF)) which does not go down exponentially but slowly declining or hyperbolic. ARFIMA model is the development of models Autoregressive Integrated Moving Average Model ( ARIMA ), with the value of the distinction ( differencing ) is real value. In writing this essay, identification of order parameter is done by looking at the ACF and PACF correlogram. ARFIMA modeling steps are as follows: 1. normality test for long memory data; 2. create a time series plot, ACF plot, plot of PACF, stationary test;3. perform data transformation using logarithmic transformation if the data is not stationary in the variance and perform differencing if the data is not stationary in the mean; 4. estimating the model parameters; 5. perform diagnostic checking with the model that has met the assumption of white noise residuals and best ARFIMA model selection criteria has the smallest AIC value among probability models.","Kata Kunci : runtun waktu, data deret berkala jangka panjang, ARFIMA, estimasi GPH."
http://etd.repository.ugm.ac.id/home/detail_pencarian/75451,MODEL AUTOREGRESI SPASIAL,"NOVAN DWI ATMAJA, Dr. Herni Utami, M.Si.",2014 | Skripsi | STATISTIKA,"Model regresi spasial adalah model yang terbentuk dari regresi umum yang mendapatkan pengaruh spasial (lokasi). Anselin (1988). Beberapa model dependensi spasial yang dapat terbentuk berdasarkan dependensi yang terdapat pada model yaitu Spatial Autoregressive Model (SAR) dependensi nilai respon antar lokasi, Spatial Error Model (SEM) dependensi nilai galat antar lokasi. Pengujian Spatial Dependence dilakukan untuk melihat data setiap variabel memiliki pengaruh spasial pada lokasi. Pemodelan didasarkan pada pengaruh dependensi spasial, sehingga sebelum dilakukan pemodelan perlu dilakukan pengujian pengaruh spasial yang terkandung dalam data menggunakan statistik uji Lagrange Multiplier (LM). Pada tugas akhir ini, berdasarkan hasil pemodelan Spatial Autoregressive Model (SAR) diketahui bahwa faktor-faktor yang berpengaruh terhadap angka putus sekolah usia SMP di Jawa Tengah adalah rata-rata anggota rumah tangga dan persentase desa terpencil di tiap kabupaten/kota.","Spatial regression model is a model that formed from the general regression to get the effect of spatial (location). Anselin (1988). Some models spatial dependence can be formed based on the dependence contained in the model, that is Spatial Autoregressive Model (SAR) response rate dependence between locations, Spatial Error Model (SEM) error value dependence between locations. Spatial Dependence test performed to see data for each variable has an effect on the spatial location. The modeling is based on the effect of spatial dependence, so that prior to modeling necessary to test the spatial effect of data using statistical Lagrange Multiplier (LM) test. In this thesis, based on the results of modeling Spatial Autoregressive Model (SAR) note that the factors affecting of junior high school dropout rates in Central Java is the average of household members and percentage of remote villages in each district/city.","Kata Kunci : Spatial, Lagrange Multiplier, SAR, SEM"
http://etd.repository.ugm.ac.id/home/detail_pencarian/71101,COPULA BERSYARAT UNTUK MENGESTIMASI VALUE at RISK (Studi Kasus Saham Nasdaq dan S&P500),"MELVINA OCHTORA DAMANIK, Dr. Gunardi, M.Si.",2014 | Skripsi | STATISTIKA,"Value at Risk (VaR) memerankan peran penting dalam bidang risiko manajemen sekarang ini.Banyak metode yang digunakan dalam perhitungan VaR seperti metode Variansi-Kovariansi, simulasi historis dan sebagainya. Umumnya pendekatan-pendekatan tersebut mengasumsikan data return berdistribusi normal dan ukuran dependensi diantara saham-saham portofolio menggunakan korelasi linear. Dalam dunia finansial asumsi normalitas jarang terpenuhi dan sesuai dengan namanya, korelasi linear tidak dapat mendeteksi hubungan dependensi yang non linear sehinggaestimasi VaR kurang akurat.Konsep Copula merupakan alat yang sangat powerful dalam memodelkan distribusi gabungan untuk berbagai bentuk distribusi marginal data.Ukuran dependensi yang digunakan juga berbasis copula seperti tail dependensi. Penulisan skripsi ini membahas tentang konsep copula dan aplikasinya dalam mengestimasi Value at Risk suatu portofolio yang terdiri dari 2 indeks saham. Data yang digunakan adalah return-return indeks saham Nasdaq dan S&P500 pada periode 13 Agustus 2012 hingga 10 Maret 2014 dimana masingmasing return saham dimodelkan dengan ARMA-GARCH untuk didapatkan residualnya yang untuk selanjutnya digunakan untuk pemodelan Copula dan estimasi VaR.","Value at Risk (VaR) plays a central role in risk management nowadays. There are several methods that can be used to estimate VaR such as the Variance- Covariance, historical simulation and etc. Generally, these methods assume the normal distribution for the stock returns and the dependence between the stocks of portfolio estimated by linear correlation. In finance the normality is rarely an adequate assumptions and the linear correlation can not detect the the non linear dependence so the estimated VaR is not accurate. The concept of Copula is a very powerful tool for modeling joint distribution for any marginal distribution. Dependency measurement that used too is based on copula namely tail dependency. This minithesis studies about the concept of Copula and its application to estimates Value at Risk of a portfolio that composed by two stock indices ( Nasdaq and S&P500 period 13 August 2012 to 10 March 2014) which is each returns modeled by ARMA-GARCH obtain the residuals furthermore that used to model the Copula and estimate VaR.",Kata Kunci : -
http://etd.repository.ugm.ac.id/home/detail_pencarian/70592,ANALISIS TEKNIKAL SAHAM MENGGUNAKAN VARIABLE INDEX DYNAMIC AVERAGE (VIDYA),"ROSELINA YOLANDA, Dr. Abdurakhman, S.Si., M.Si.",2014 | Skripsi | STATISTIKA,"Keuangan kita dan aksi pasar berubah setiap menit dalam setiap harinya. Pasar ini bersifat dinamis karena pedagang terus-menerus menyesuaikan dengan perubahan persepsi dan para partisipan. Oleh karena itu, kita membutuhkan indikator dinamis yang dapat mengubah periode waktu dengan cara menganalisis aksi pasar. Variable Index Dynamic Average (VIDYA) merupakan salah satu indikator yang mengadaptasi indikator Exponential Moving Average (EMA) yang secara dinamis dapat mengikuti pergerakan harga saham. Sama seperti Exponential Moving Average (EMA), VIDYA juga membutuhkan bobot dalam metode perhitungannya. Dalam skripsi ini, bobot EMA digunakan untuk menentukan bobot VIDYA. VIDYA dapat dihitung dengan menggunakan tiga metode yang berbeda, seperti standar deviasi, Chande Momentum Oscillator, dan koefisien determinasi. Metode perhitungan tersebut digunakan untuk menentukan indeks volatilitas yang berfungsi untuk memprediksi pergerakan harga saham dan memprediksi tren di masa yang akan datang. Selain itu, VIDYA dapat digunakan sebagai strategi trading dan dapat menentukan sinyal jual atau beli saham dengan cara menggunakan titik breakout. Kata kunci: analisis teknikal, exponential moving average, indeks volatilitas, standar deviasi, koefisien determinasi, indikator momentum","Our financial and market action change every minute of every day. These markets are dynamic because traders constantly adjust to changing perceptions and participants. Therefore we need dynamic indicators that vary the time periode used in analyzing market action. Variable Index Dynamic Average (VIDYA) is an indicator that adapted Exponential Moving Average (EMA) that can dynamically follow the movements of stock prices. Equal as Exponential Moving Average (EMA), VIDYA also need weight in its calculation methods. In this paper, the weight of EMA is used to determine the weight of VIDYA. VIDYA can be calculated using three different methods, such as standard deviation, Chande Momentum Oscillator (CMO), and coefficient of determination. These calculation method is used to determine the volatility index that worth to predict stock price movements and predict the trend in the future. In addition, VIDYA can be used as trading strategy and determine buy signal or sell signal that using the breakout point. Keyword: technical analysis, volatility index, exponential moving average, standard deviation, coeffitient of determination, momentum indicator","Kata Kunci : analisis teknikal, exponential moving average, indeks volatilitas, standar deviasi, koefisien determinasi, indikator momentum"
http://etd.repository.ugm.ac.id/home/detail_pencarian/69589,EKSPEKTASI ONGKOS GARANSI DUA DIMENSI MENGGUNAKAN KEBIJAKAN NON- RENEWING KOMBINASI FRW DAN PRW,"IRMA YUNIAR PANGESTI RAHAYU, Dr. Danardono MPH.",2014 | Skripsi | STATISTIKA,"Garansi adalah sebuah obligasi yang diberikan pada produk yang mewajibkan perusahaan untuk menetapkan ganti rugi kepada konsumen jika produk tersebut gagal berfungsi dari penggunaan normalnya setelah pembelian selama periode garansi yang telah ditentukan. Dengan adanya penawaran garansi akan menimbulkan biaya tambahan yang disebut ongkos garansi sehingga estimasi ongkos garansi sangat penting untuk dilakukan. Dalam tugas akhir ini akan dilakukan studi garansi dua dimensi dengan kebijakan Non- Renewing Kombinasi Free Replacement Warranty dan Pro Rata Warranty pada produk nonrepairable, dimana garansi dibatasi oleh umur dan tingkat pemakaian. Perhitungan ekspektasi biaya garansi dua dimensi menggunakan distribusi gabungan dari distribusi variabel waktu dan pemakaian, yaitu distribusi bivariat. Pemodelan kegagalan mengikuti distribusi bivariat pareto, dengan parameter yang diperoleh melalui metode MLE. Pada penghitungan ekspektasi ongkos garansi kebijakan FRW, digunakan jenis integral konvolusi dan transformasi laplace yang memerlukan bantuan software untuk mendapatkan solusinya, begitu pula dalam penghitungan ekspektasi refund pada kebijakan PRW.","Warranty is a bond that is given to products that require companies to assign compensation to consumers if the product fails to function normally after the purchase of usage during the specified warranty period. Offering warranty will generate expense which is called warranty cost. Therefore, estimation of warranty cost is very important to be done. This paper will studying two dimentional warranty that is limited by time and usage, with non-renewing free replacement warranty policy for non- repairable product. Calculation of expected warranty costs using a two-dimensional joint distribution of variable distribution and usage time, in example bivariate distribution. Modeling of failure to follow the bivariate Pareto distribution, with parameters obtained through MLE method. In calculating the expected cost of warranty policies FRW, used type of convolution integral and Laplace transform that use software to get the solution, as well as in the calculation of the expected refund policy PRW.",Kata Kunci : Garansi adalah sebuah obligasi yang diberikan pada produk yang mewajibkan perusahaan untuk menetapkan ganti rugi kepada konsumen jika produk tersebut gagal berfungsi dari penggunaan normalnya setelah pembelian selama periode garansi yang telah ditentukan.
http://etd.repository.ugm.ac.id/home/detail_pencarian/67036,METODE BOOTSTRAP DALAM CADANGAN KLAIM IBNR (Incurred But Not Reported) BOOTSTRAP METHOD IN IBNR CLAIM RESERVES,"MUSTIKA RIZKY AMALIA, Dr. Adhitya Ronnie Effendie.",2014 | Skripsi | STATISTIKA,"Incurred but not reported (IBNR) adalah jenis klaim pada asuransi nonjiwa yang sudah terjadi namun belum dilaporkan kepada perusahaan asuransi, hal ini dapat disebabkan karena waktu yang dibutuhkan dalam menjalankan prosedurprosedur dalam melengkapi berkas pengajuan klaim seperti prosedur hukum dan beberapa prosedur administratif lainnya. Adanya efek periode kejadian dan periode pengembangan membuat prediksi IBNR sulit untuk dimodelkan Menurut Pinheiro (2001), Kaas (2008), Shapland dan Leong (2010), distribusi yang cocok untuk memodelkan cadangan klaima adalah over dispersed Poisson (ODP) dimana mean data tidak lebih besar dari variansinya. Pada skripsi ini penulis mengaplikasikan metode bootstrap untuk memperoleh prediksi cadangan IBNR dalam model ODP. Bootstrap adalah suatu metode resampling yang digunakan untuk mengestimasi atau menaksir suatu parameter. (Efron (1979)). Studi kasus yang digunakan adalah data klaim IBNR periode 1999-2008 untuk kompensasi yang diberikan kepada karyawan dari seluruh perusahaan yang bergerak di bidang industri di Amerika Serikat dalam juta dolar US. Hasil dari pemodelan bootstrap diperoleh total cadangan klaim IBNR sebesar 155477,9 juta dollar US dengan standar error prediksi yaitu 7366,258.","Incurred but not reported (IBNR) is kind of claim in non-life insurance which already incurred but not reported to the insurance company, it can be happen because they need time to pass some procedures in completing submission documents such as law submission and other administrative procedures. The effect of original time and development time in IBNR made their prediction difficult to be modeled. According to Pinheiro (2001), Kaas (2008), Shapland dan Leong (2010), the distribution which modeled by claim reserves is over dispersed Poisson (ODP) which its mean is not larger than its variance. In this thesis, bootstrap method was applied to acquire claim reserve prediction of IBNR in ODP model. Bootstrap is a resampling method which can be used to estimates certain parameters. (Efron, 1979) Case study used in this thesis is claim data of IBNR on period 1999-2008 for the workerâ€™s compensation from all industry companies in United States in million dollars US. The result of bootstrap models are, total reserve of IBNR is 155477,9 dollar US with standard error of prediction is 7366,258.","Kata Kunci : cadangan klaim, IBNR, generalized linear model, over dispersed poisson, bootstrap."
http://etd.repository.ugm.ac.id/home/detail_pencarian/68061,MODIFIKASI CADANGAN PREMI METODE FULL PRELIMINARY TERM PADA ASURANSI JIWA DWIGUNA MODEL DISKRIT,"MH ROESANGGIT P, Dr. Gunardi, M.Si",2014 | Skripsi | STATISTIKA,"Asuransi jiwa merupakan usaha seseorang untuk mengurangi resiko yang ditimbulkan jika seseorang meninggal dunia. Dalam asuransi jiwa, pemegang polis atau tertanggun membayar sejumlah uang kepada perusahaan asuransi jiwa pada periode tertentu dengan niali tertentu yang disebut premi. Sementara itu, Perusahaan Asuransi berjanji akan memberikan sejumlah uang jika terjadi sesuatu seperti yang telah diperjanjikan misalnya kematian atau tetap hidup di akhir kontrak polis yang disebut manfaat. Oleh karena kewajiban memenuhi janji tersebut, maka perusahaan asuransi melakukan perhitungan berapa dana yang dibutuhkan jika terjadi klaim dimasa mendatang. Dana yang harus ada itulah yang disebut cadangan premi. Perhitungan cadangan ada beberapa macam, diantara metode prospektif dan Full Preliminary Term. Pada perhitungan cadangan prospektif cadangan premi kotor yang dihasilkan lebih kecil dari cadangan premi bersihnya. Dengan metode full preliminaty term, cadangan premi kotor yang dihasilkan besarnya sama dengan cadangan premi kotornya. Jika premi kotor lebih kecil dari premi kotor full preliminary term, maka cadangan full preliminary term menghasilkan nilai yang lebih kecil dari cadangan metode prospektif.","Life insurance is an effort to reduce the loss risk due to policy holderâ€™s death. In life insurance, policy holder must pay amount of fund periodically to the insurance company which is called premium. And insurance company promises to give amount of fund to the policy holder's family if the policy holder died which is called benefit. Therefore their liability, insurance company must calculate how much the fund they should be prepared if the event occur in the future. The fund which insurance company must prepared is called by reserve premium. There are several method of reserve premium calculation, some of them are prospective method and full preliminary term method. Prospective method produces the gross premium reserve that is less than net premium reserve. By using full preliminary term method, insurance company can produces the gross premium reserve that is equal to net premium. And the premium reserve which produced by full preliminary term method is less than the prospectiveâ€™s premium reserve if the gross premium reserver is less than the full preliminary termâ€™s gross premium.","Kata Kunci : Asuransi Jiwa, Cadangan Premi, Full Preliminary Term"
http://etd.repository.ugm.ac.id/home/detail_pencarian/75753,Analisis Fleksibel Bayesian Untuk Regresi Kuantil Terpenalti Dengan Menggunakan Algoritma MCMC Gibbs Sampling,"AFIFKA FITRI NUGRAHWATI, Dr. Abdurakhman, M.Si",2014 | Skripsi | STATISTIKA,"Regresi kuantil terpenalti dapat digunakan untuk mengatasi keterbatasan regresi linear dalam menganalisis data yang bentuknya tidak simetris, terdapat pencilan dan distribusi data yang tidak homogen. Regresi kuantil terpenalti dengan LASSO (Least Absolute Shrinkage and Selection Operator) dan Adaptive Lasso penalty dapat diestimasi menggunakan metode fleksibel bayesian yakni suatu metode analisis berdasarkan pada informasi yang berasal dari sampel dan informasi prior. Gabungan informasi sampel dan informasi prior ini dinamakan posterior. Dalam mencari distribusi posterior untuk parameter yang cukup banyak sering kali mengalami kesulitan. Teknik khusus yang dapat digunakan untuk mempermudah yaitu dengan menggunakan simulasi MCMC (Marcov Chain Monte Carlo) Gibbs sampling yang juga dapat meningkatkan model fit dan mereduksi variansi dari distribusi posterior. Pada software R terdapat paket bayesQR untuk analisis regresi kuantil terpenalti dengan metode bayesian secara lengkap.  Studi kasus dalam skripsi ini membahas hubungan antara tingkat serum antigen prostat spesifik dengan sejumlah tindak klinis pada pria yang hendak menerima prostatektomi radikal. Hasil estimasi regresi kuantil terpenalti dengan metode fleksibel bayesian dibandingkan dengan metode OLS, regresi kuantil dan regresi kuantil bayesian. Dengan menggunakan nilai R2 dan MSE diperoleh kesimpulan bahwa regresi kuantil terpenalti dengan metode fleksibel bayesian menghasilkan estimasi yang lebih akurat dan presisi daripada estimasi dengan metode lainnya.","Penalized quantile regression can be used to overcome the limitation of linear regression to analyze data which not symmetric, outlier existed, and distribution of data is not homogeneous. Penalized quantile regression with LASSO (Least Absolute Shrinkage and Selection Operator) and Adaptive Lasso penalty can be estimated by using flexible bayesian method which based on the information derived from the sample and prior information. The combination of sample and prior information is called by posterior. So difficult to find a posterior distribution which include by many parameters. Therefore, there is a special techniques which will make easily, that can be used by through MCMC (Marcov Chain Monte Carlo) Gibbs Sampling which make improve the model fit and reduce the variance of posterior distribution. R software provides a bayesQR package for doing bayesian analysis in penalized quantile regression completely. The case study in this thesis is analyze the relationship between level of prostate specific antigen and number of clinical measures in men who were about to receive a radical prostatectomy.  The estimation result of penalized quantile regression using flexibel bayesian method compered with simple linear regression using OLS, quantile regression, and bayesian quantile regression method. By using value of R2 and MSE, provide a conclusion that estimation of penalized quantile regression with flexibel bayesian method more accurate and precision than others estimation.","Kata Kunci : Penalized Quantile Regression, Lasso Penalty, Adaptive Lasso Penalty, Flexible Bayesian, Marcov Chain Monte Carlo, Gibbs sampling, bayesQR"
http://etd.repository.ugm.ac.id/home/detail_pencarian/70635,PEMODELAN KREDIBILITAS RATEMAKING MENGGUNAKAN ELLIPTICAL COPULA,"WAHYU HIDAYAT, Dr. Danardono MPH.",2014 | Skripsi | STATISTIKA,"Program asuransi sebenarnya bukan hal baru. Berbagai kalangan memahami, asuransi sebagai lembaga keuangan pengelola risiko memang diperlukan keberadaannya. Tetapi, kesadaran untuk mengikuti program asuransi belum dimiliki semua orang. Dalam kehidupan manusia, faktor risiko adalah suatu yang mungkin terjadi. Mulai dari risiko kehilangan aset, risiko sakit, cacat total hingga risiko kehilangan jiwa atau meninggal dunia. Dengan sering terjadinya bencana alam, kecelakaan membuat orang ataupun pelaku usaha mulai melirik asuransi sebagai pilihan utama untuk mengurangi kerugian atas risiko yang mungkin terjadi di kemudian hari. Salah satu cara untuk mengukur risiko adalah dengan kredibilitas ratemaking menggunakan copula. Kredibilitas ratemaking adalah teknik untuk memprediksi ekspektasi klaim di masa depan berdasarkan kelas risiko, dengan berdasarkan klaim masa lalu dan kelas risiko yang terkait.","Insurance is not something new in this time. People understand it as a financial institution managing risk that should really exist. However, awareness of the insurance program is not granted to everyone. In human life, a risk factor may occur. Starting from the risk of loss of assets, the risk of illness, total disability until the risk of loss of life or death. With the frequent occurrence of natural disasters, accidents or entrepreneurs making people began to look at insurance as a primary option for reducing losses on risks that may occur in the future. A good way to measure risk is the credibility ratemaking using copula. Credibility ratemaking is a technique to predict the expected future claims by the class of risk, based on past claims and class associated risks.","Kata Kunci : Kata Kunci : klaim, kredibilitas ratemaking, copula"
http://etd.repository.ugm.ac.id/home/detail_pencarian/68082,OPTIMISASI PORTOFOLIO CAMPURAN,"CHANDIKA HANDAYANI, Dr.Abdurakhman,M.Si.",2014 | Skripsi | STATISTIKA,"Saham merupakan salah satu instrumenyang sering dipakai dalam  investasi. Tingkat pengembalian  (return)saham dan besarnya risiko yang  ditanggung investor merupakan hal  yang perlu diperhatikan. Untuk  mengoptimalkan return dan meminimalkan risiko dapat dibentuk portofolio  saham. Return dihitung dari harga penutupan saham bulanan pada masing-masing  aset yang terdaftar pada NASDAQ-100.   Optimisisasi portofolio campuran merupakan kombinasi aset berisiko dan  sebuah aset bebas aset risiko dengan sharpe ratioyang tinggi yang dikenal dengan  portofoliotangency. Aset berisiko adalah aset-aset yang tingkat returnaktualnya  dimasa depan masih mengandung ketidakpastian, misalnya saham. Aset bebas  risiko adalah aset yang tingkat return dimasa depan sudah bisa ditentukan pada  saat ini, misalnya Treasury Bill","Stock is one instrument that is often used in investment. The rate of  expected return of shares and amount of risk borne by the investor are things that  need attention. To optimize returns and minimize the risk of a stock portfolio can  be formed. Return is calculated from the monthly closing price on each of the  assets listed on the NASDAQ-100.   Optimization mixed portofolio is a combination of risky assets and a risk free asset with the highest Sharpe ratio, known as tangency portfolio. Risk assets  are assets that the future rate of return aktualnyaa still contains uncertainties, such  as stocks. Risk-free asset is an asset that level of return in the future can already  be determined at this time,such as Treasury Bills.","Kata Kunci :  Aset Berisiko, Aset Bebas Risiko, Portofolio Tangency"
http://etd.repository.ugm.ac.id/home/detail_pencarian/73719,ANALISIS DISKRIMINAN FLEKSIBEL FLEXIBLE DISCRIMINANT ANALYSIS,"RASITA NINGRUM, Herni Utami., S.Si., M.Si.",2014 | Skripsi | STATISTIKA,"Analisis diskriminan fleksibel adalah pengembangan dari analisis diskriminan linear, yang digunakan untuk menyelesaikan masalah klasifikasi pada regresi non-parametrik. Flexibel discriminant analysis (FDA) menggantikan langkah regresi linear dengan regresi non-parametrik, salah satu regresi nonparametrik yang dapat digunakan adalah Multivariate Adaptive Regression Splines (MARS). MARS bertujuan untuk memprediksi. Model MARS yang terbaik adalah yang menghasilkan nilai Generalized Cross Validation (GCV) minimum. Asumsi yang harus dipenuhi dalam pemodelan analisis diskriminan fleksibel adalah tidak terdapat outlier, kesamaan matriks variansi-kovariansi. Dalam studi kasus dilakukan pengelompokkan observasi, yakni kadar dissolved oxygen (DO) dalam air berdasarkan 5 variabel prediktornya, yaitu: fosfat, COD,BOD, TSS dan Ph.","Flexible discriminant analysis is development of linear discriminant analysis, which is used to solve the problem of classification on non-parametric regression. Flexible discriminant analysis replaces parametric regression with non-parametric regression, one of the non-parametric regression can be used is Multivariate Adaptive Regression Splines (MARS). MARS aims to predict, the best MARS models which is have minimum of Generalized Cross Validation (GCV). The assumptions is used in flexible discriminant analysis modeling is that there is no outlier data, homogeneity variance-covariance matrix. In case study, classify dissolved oxygen levels of water based on 5 predictor variables.","Kata Kunci : Analisis diskriminan linear, analisis diskriminan fleksibel, Multivariate Adaptive Regression Splines (MARS)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/72696,PERBANDINGAN ESTIMATOR CENSORED LEAST ABSOLUTE DEVIATIONS (CLAD) DAN SYMMETRICALLY CENSORED LEAST SQUARES (SCLS) UNTUK MODEL REGRESI TOBIT (Studi Kasus : Analisis Faktor-Faktor yang Mempengaruhi Partisipasi Perempuan dalam Perekonomian Rumah Tangga di Provinsi Daerah Istimewa Yogyakarta),"VANIA PRIMA AMELINDA, Prof. Subanar, Ph.D",2014 | Skripsi | STATISTIKA,"Dalam tugas akhir ini, dibahas suatu metode alternatif untuk estimator Maximum Likelihood untuk data tersensor atau yang sering disebut dengan Model Tobit. Ada dua metode alternatif yang akan dibahas dan dibandingkan yaitu Censored Least Absolute Deviations (CLAD) dan Symmetrically Censored Least Squares (SCLS). Tidak seperti metode Maximum Likelihood, metode CLAD konsisten dan asimtotis normal serta robust digunakan untuk data yang tidak memenuhi asumsi normalitas dan homoskedastisitas. Sementara metode SCLS masih mengasumsikan kesimetrisan (dan independensi) dari distribusi error, namun tetap konsisten walaupun residual tidak berdistribusi identik dan tetap robust untuk data heterokedastik. Untuk studi kasus, digunakan data Survei Angkatan Kerja Nasional (SAKERNAS) 2013 untuk memodelkan faktor-faktor yang mempengaruhi partisipasi perempuan dalam perekonomian rumah tangga di provinsi Daerah Istimewa Yogyakarta.","This graduating paper discusses alternatives for maximum likelihood estimation of the censored regression or censored â€˜Tobitâ€™ model. There are two alternative methods that will be discusses and compared: Censored Least Absolute Deviations (CLAD) and Symmetrically Censored Least Absolute Deviations (SCLS). Unlike maximum likelihood estimator, CLAD is consistent and asymptotically normal for a wide class of error distributions and robust to heterokedasticity. Meanwhile, SCLS is not completely general, since it is based upon the assumption of symmetrically (and independently) distributed error terms. However this estimator will be consistent even though the residuals are not identically distributed and remain robust to heterokedasticity data. In this case study, the researcher use National Labor Force Survey Data 2013 to examine factors that influence womenâ€™s participant in domestic economy of Yogyakarta province.","Kata Kunci : Regresi Semi Parametrik Tersensor, Model Tobit, Censored Least Absolute Deviations, Simmetrically Censored Least Squares, Bootstrap, Algoritma ILPA, Algoritma Newton Type."
http://etd.repository.ugm.ac.id/home/detail_pencarian/72186,PEMBENTUKAN PORTOFOLIO OBLIGASI BERKUPON TETAP DENGAN MODEL INDEKS TUNGGAL (Studi Kasus Pada Obligasi Korporasi Periode 2010-2011),"WINDU PRAMANA P.B., Dr. Abdurakhman, M.Si",2014 | Skripsi | STATISTIKA,"Tujuan dari makalah ini adalah untuk menetukan portofolio optimal dengan menggunakan model indeks tunggal. Model indeks tunggal merupakan satu dari banyak model lain untuk menentukan portofolio optimal. Pembentukan portofolio optimal dapat mengurangi risiko investasi menjadi pertimbangan investor untuk menolak. Disisi lain juga untuk membentuk pengembalian yang diharapkan untuk batas tertinggi portofolio. Obligasi merupakan investasi alternatif untuk mendapatkan keuntungan bagi investor dengan risiko di dalam area toleransi. Ketentuan dasar dalam menentukan portofolio berdasarkan model indeks tunggal adalah dengan ERB â‰¥ C* akan diterimas sebagai portofolio optimal. Setelah diketahui saham mana yang termasuk kedalam portofolio optimal, Kemudian dapat ditentukan proporsi dana yang diinvestasikan pada setiap obligasi. Hasil analisis terdapat 3 obligasi yang masuk kedalam poertofolio optimal yakni : PPGD 12B, JMPD14JM10 dan PPGD13B. Proporsi dana untuk masing-masing obligasi adalah 7,22%, 9,01%, dan 83,77%. Dari portofolio optimal yang telah terbentuk memberikan pengembalian yang diharapkann sebesar 0,09288 dan risiko portofolio yang terbentuk sebesar 0,00147 %.","The purpose of this paper is to determine optimal portofolio by using single index model. Single index model is one of many other models to create optimal portofolio. Creating optimal portofolio can reduce investment risk to become tolerable by risk averse investor. In the other it is creating expected return to the highest limit that this portofolio can archieve. Bond also the alternative investment instrument to gain wealth to its investor which is risk in in tolerable area. The basic rule in determining portfolio member based on single index model is all shares with ERB â‰¥ C* includes in portfolio optimal. After knowing which shares that included in optimal portfolio, then can determine the proportions invested for each shares. Analysis result is 3 shares that are selected as the optimal portfolio, they are : PPGD12B, JMPD14JM10 and PPGD13B. Proportion for each shares are 7,22 %, 9,01 %, and 83,77 %. From the optimal portfolio formed , it gets portfolio expected return E(Rp) at 0,09288. and portfolio risk at 0,00147 %.","Kata Kunci : Model Indeks Tunggal, ERB dan Cut-off Point (C*)."
http://etd.repository.ugm.ac.id/home/detail_pencarian/72955,PEMILIHAN PORTOFOLIO DENGAN MENGGUNAKAN PEMODELAN MIXTURE OF MIXTURE,"MIDIAN RAJAGUKGUK, Dr. Gunardi, M.Si.",2014 | Skripsi | STATISTIKA,"Perkembangan teknologi, informasi dan komunikasi yang sangat cepat, secara tidak disadari telah mewarnai kemajuan penciptaan instrumen investasi secara global dalam pasar modal. Berbagai macam instrumen telah diperdagangkan secara terbuka dan mampu memberikan fasilitas serta kesempatan kepada investor yang cukup leluasa untuk memperbesar macam cara investasi. Keuntungan yang diperoleh bisa sangat besar dan cepat dengan melakukan investasi secara simultan atau bersamaan dengan tepat. Hal ini akan memaksa para investor untuk sadar akan perlunya manajemen resiko yang dapat mewarnai perjalanan pengambilan keputusannya. Metode yang cukup tua dan populer serta sederhana secara statistik dalam penghitungan resiko investasi melalui portofolio adalah Value at Risk (VaR). Metode ini memberikan cara perhitungan nilai kerugian minimum atas portofolio yang dipilih pada tingkat kepercayaan tertentu. Dengan membagi ke dalam beberapa segmen berdasarkan waktu dan aktivitas perekonomian dunia dan kemudian melakukan pemodelan portofolio berdasarkan data dari beberapa saham (BBNI.JK, BNGK.JK, BMRI.JK, AALI.JK, emas) menggunakan mixture of mixture akan merepresentatifkan dalam menjelaskan pola return dan sekaligus menggunakan hasilnya untuk menghitung besarnya proposi besaran dana yang akan dialokasikan.","The development of technologi, information and communication is spread out and indirectly has colored creation advances global investment instrument in the capital markets. A wide variety of instruments have been traded publicy and is able to provide facilities and opportunities to investors sufficient flexibility to increase investment ways. Benefits could be very large and fast by investing as simultaneous or concurrent with the right. This will force investors to be aware of the need for risk management decision making can enliven. The method is quite old and popular as well as simple statistical calculation of risk in the portfolio is invested in Value at Risk (VaR). This method provides a way of calculating the value of the minimum loss over the selected portfolio at a certain confidence level. By dividing into several segments based on time and world economic activity and then do a modeling portfolio from the several stocks (BBNI.JK, BNGK.JK, BMRI.JK, AALI.JK, gold) using a mixture of mixture density will represents in explaining patterns return and simultaneously use the result to calculate the amount of risk to be accepted.","Kata Kunci : Teori Portofolio, Bayes Faktor, Sturuktur Perkalian Distribusi, Mixture of Mixture Modeling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/64776,MODEL HAZARD PROPORSIONAL UNTUK DATA UJI HIDUP TERSENSOR INTERVAL,"DUNIE ANATON, Dr. Danardono, MPH.",2013 | Skripsi | STATISTIKA,"Model proporsional hazard adalah model yang dapat diterima dan digunakan secara luas untuk analisis tahan hidup, karena hasil estimasinya bermanfaat serta mudah dipahami oleh peneliti kesehatan. Secara esensi keuntungan dari metode ini adalah tanpa asumsi distribusi dan hasil estimasinya natural untuk resiko dari kegagalan dihubungkan dengan vektor kovariat. Sebagian besar aplikasi, data mungkin tersensor interval. Dengan data tersensor interval, berarti bahwa variabel random dari yang menjadi perhatian hanya diketahui terletak pada interval. Pada kasus ini, informasi yang dimiliki untuk masing-masing individu hanya waktu kejadian (event) yang terjadi dalam interval, waktu terjadinya kejadian (event) pasti tidak diketahui. Fungsi survival merupakan fungsi yang sangat penting dalam studi medis dan kesehatan. Menggunakan metode Turnbull, estimasi MLE nonparametrik dapat dicari dalam keadaan tersensor interval. Hasil yang diberikan untuk menguji hipotesis dari koefisien regresi nol mengarahkan untuk melakukan generalisasi dari uji log-rank untuk membandingkan beberapa kurva survival menggunakan metode skor.","The proportional hazards model is the most widely accepted and utilized model for survival analysis, because results in an estimate that is meaningful and understood by medical investigators. The advantages of the methods are that it is essentially distribution-free and it results in a natural estimate for the risk for failure associated with a vector of covariates. In most applications, the data may be interval-censored. By interval-censored data, we mean that a random variable of interest is known only to lie in an interval. In such cases, the only information we have for each individual is that their event time falls in an interval, but the exact time is unknown. The survival function is perhaps the most important function in medical and health studies. Using Turnbull method, a nonparametric estimation of survival function can also be found in such interval-censored situations. Results given for testing the hypothesis of a zero regression coefficient lead to a generalization of the log-rank test for comparison of several survival curves using score method.","Kata Kunci : Model Hazard Proporsional, Data Tersensor Interval, MLE Nonparametrik, Uji Skor"
http://etd.repository.ugm.ac.id/home/detail_pencarian/67091,ANALISIS CHURN PADA PELANGGAN TELEKOMUNIKASI MENGGUNAKAN ALGORITMA C4.5,"LINTANG GUSTIKA PARATU, Prof.Dr.rer.nat Dedi Rosadi, S.Si., M.Sc",2013 | Skripsi | STATISTIKA,"Industri telekomunikasi seluler telah berkembang pesat, persaingan antar provider menjadi sangat ketat. Dibutuhkan alat analisis yang akurat untuk mempertahankan pelanggan lama bersamaan dengan mendapatkan pelanggan yang baru. Pada prakteknya mendapatkan pelanggan baru membutuhkan cara yang lebih susah daripada mempertahankan, membutuhkan biaya yang lebih besar, dan cara yang lebih atraktif. Pada skripsi ini akan dibahas bagaimana analisis untuk mengetahui pelanggan yang bagaimana yang akan pindah (churn) dari provider tersebut, sehingga lebih lanjut akan ditentukan mana pelanggan yang akan dipertahankan oleh provider tersebut. Dalam skripsi ini akan menggunakan alat analisis pengklasifikasian dengan nama Algoritma C4.5. Algoritma C4.5 ini dirasa cukup akurat untuk mengklasifikasikan apakah pelanggan akan pindah (churn) atau tidak dengan tampilan pohon keputusan agar mudah dipahami kebanyakan orang pada umumnya.","Mobile telecommunications industry has been growing rapidly, competition among providers becomes very tight. It needs an accurate analytical tool to retain existing customers along with getting new customers. In practice, gain new customers more difficult than maintain current customers, requiring greater cost, and way more attractive. This paper will discuss the analysis to determine how the customer is going to move (churn) from the provider, then provider can choose which customers should be retained. This paper will use classification analysis C4.5 algorithm. C4.5 algorithm is deemed sufficiently accurate to classify whether a customer will move (churn) or not and it displays the decision tree that easily understood by most people in general.",Kata Kunci :
http://etd.repository.ugm.ac.id/home/detail_pencarian/66326,APLIKASI MODEL KREDIBILITAS HIRARKI TIGA LEVEL  PADA INDUSTRI ASURANSI UMUM,"RIDA PERWITASARI, Dr. Adhitya Ronnie Effendie, M.Sc.",2013 | Skripsi | STATISTIKA,"Dalam inferensi statistika, teori kredibilitas adalah ilmu yang mengkombinasikan gagasan tentang stabilitas, presisi, dan kemampuan reaksi terhadap data-data yang baru saja terjadi. Teori kredibilitas ini mempelajari proses penghitungan premi berdasarkan pengalaman masa lampau, serta mengkombinasikan antara pengalaman individual dan pengalaman kolektif. Pada kehidupan sehari-hari, seringkali ditemui struktur hirarki dalam berbagai bidang, seperti pada bidang industri, bisnis, asuransi, dan lain sebagainya. Model kredibilitas hirarki dapat dipandang sebagai alat untuk mendistribusikan premi secara fair pada portofolio heterogen yang diklasifikasikan secara hirarki. Ide fundamental dari teori kredibilitas hirarki adalah dengan membagi suatu portofolio yang besar menjadi beberapa subportofolio yang lebih homogen berdasarkan kriteria-kriteria tertentu. Dalam skripsi ini akan dianalisis cara mendapatkan estimator kredibilitas hirarki dan parameter struktural yang melibatkan sifat matematik ekpektasi bersyarat dan kovariansi bersyarat. Studi kasus dalam skripsi ini menganalisis estimasi rasio kerugian kredibilitas di antara 12 negara yang tergabung dalam asuransi perlindungan hukum (legal protection insurance) RIAD yang diklasifikasikan menurut kelompok pendapatan berdasarkan pengklasifikasian dari Bank Dunia.","In statistical inference, credibility theory achieves the compromise between the notions of stability, precision, and responsiveness to the most recent events. Credibility theory studies how to calculate premium based on past experiences and combine between individual experiences and collective experiences. In practice, we often find hierarchical structure in many line business, such as in the industrial, business, insurance, etc. Hierarchical credibility model is seen as a tool to distribute premium fairly among a heterogeneous portfolio classified in a hierarchical structure. Fundamental idea from this theory is divide a given population into homogeneous subportfolio based on certain citeria. This thesis will analyzed the way to get hierarchical credibility estimator and structural parameter that involve mathematical properties of conditional expectations and of conditional covariances. Case study in this thesis analyze loss ratio credibility estimation among 12 countries that joined legal protection insurance RIAD that classified based on World Bankâ€™s income group.","Kata Kunci : kredibilitas hirarki, ekspektasi bersyarat, kovariansi bersyarat"
http://etd.repository.ugm.ac.id/home/detail_pencarian/63003,MODEL ADITIF TERGENERALISASI,"NURLIA HIKMANANDA, Prof. Drs. Subanar, Ph.D.",2013 | Skripsi | STATISTIKA,Pemodelan hubungan antara variabel respon dan prediktor tidak selalu mengikuti asumsi linearitas dan variabel respon berdistribusi normal. Hastie dan Tibshirani (1986) mengadaptasikan model aditif ke model linear tergeneralisasi yang disebut sebagai model aditif tergeneralisasi. Model aditif tergeneralisasi merupakan pemodelan yang sesuai untuk mengatasi adanya kenonlinearan dalam hubungan antara variabel respon dan prediktor serta tidak membatasi distribusi variabel respon hanya pada distribusi normal saja akan tetapi distribusi-distribusi lain dalam keluarga eksponensial dapat dipergunakan dalam model ini. Model aditif tergeneralisasi mengganti komponen linear yang ada pada model linear tergeneralisasi dengan jumlahan fungsi yang diestimasi menggunakan algoritma local scoring. Komponen aditif dari model aditif tergeneralisasi merupakan jumlahan fungsi tunggal yang dimiliki oleh setiap prediktor sehingga dapat diketahui kontribusi dari setiap prediktor terhadap respon. Ilustrasi diberikan dalam studi kasus menggunakan software R.,"Modeling relationship between respond variable and predictor is not always following linearity and normality assumption. Hastie and Tibshirani (1986) adapted additive models to generalized linear models that is called by generalized additive models. Generalized additive models is modeling technique that appropriates for overcomes nonlinearity in the relationship between respond variable and predictor, and does not limited respond variable to normal distribution but other distributions in the exponential family allowing to use in this model. Generalized additive models replace linear component on the generalized linear models with sum of functions which is estimated using local scoring algorithm. Additive component in generalized additive models is sum of univariat function of every predictors, so we can see contribution of each predictor to respond. Illustration is given in case study using R software.","Kata Kunci : regresi nonlinear, model aditif, model aditif tergeneralisasi, local scoring"
http://etd.repository.ugm.ac.id/home/detail_pencarian/67112,PERBANDINGAN MODEL REGRESI NONPARAMETRIK SPLINE DAN REGRESI NONPARAMETRIK KERNEL,"YUNI KURNIA P., Dr. Abdurakhman S.Si, M.Si.",2013 | Skripsi | STATISTIKA,"Analisis Regresi merupakan salah satu alat statistika yang banyak digunakan untuk mengetahui hubungan antara sepasang variable atau lebih. Analisis regresi dibagi dua yaitu regresi parametrik dan regresi nonparametrik. Regresi nonparametric memiliki beberapa metode smoothing, seperti regresi spline dan kernel. Tujuan utamanya adalah membandingkan kedua metode tersebut untuk mengestimasi model regresi nonparametrik. Data yang digunakan untuk membandingkan kedua metode tersebut yaitu data pertuumbuhanbalita.","Regression analysis is a statistical tool that is widely used to determine the relationship between a pair of variables or more. Regression analysis is divided into two, namely parametric regression and nonparametric regression. Nonparametric regression has several smoothing methods, such as spline and kernel regression. Its main purpose is to compare the two methods for estimating the nonparametric regression model. The data were used to compare the two methods of toddler growth data.","Kata Kunci : Regresi nonparametrik, regresi spline, regresi kernel."
http://etd.repository.ugm.ac.id/home/detail_pencarian/66626,UNIT-LINKED LIFE INSURANCE CONTRACT WITH MINIMUM DEATH BENEFIT GUARANTEE USING EUROPEAN OPTION APPROACH,"ADHA ROZAK, Dr.Gunardi, M.Si,",2013 | Skripsi | STATISTIKA,"Kontrak asuransi jiwa unit-linked merupakan proteksi sekaligus investasi. Selain akan mendapat perlindungan dari suatu sebab tertentu yang merugikan, juga kita akan mendapat hasil investasi dari instrumen keuangan tertentu. Dewasa ini , perusahaan asuransi telah kreatif dalam mengkreasikan berbagai macam jaminan untuk ditambahkan ke dalam asuransi unit-linked murni, seperti pengembalian premi, asset garansi, asuransi renter, ratchet. Pada skripsi ini digunakan jaminan minimum kematian sebesar harga kontrak saham dengan pendekatan opsi tipe eropa. Sehingga jika harga saham ketika tertanggung jatuh, akan mendapatkan nilai kontrak dan sebaliknya jika harga saham melejit tinggi maka akan mendapatkan harga saham tersebut.","Unit-Linked life insurance contract is protection and investment. In addition to the protection of a specified outcome occured , we are also going to get a return on investment of certain financial instruments. Today, insurance companies have been creative in the creation of a wide range of collateral to be added to the pure unit-linked insurance, such as the premium refunding, asset guarantee, renter insurance, ratchet. In this graduation paper the guarantee will be used is value of contract with european option. So if the value of asset decrease so badly, the insured will get the value of contract. Vise Versa, value of asset increase greatly , the insured get that value.",Kata Kunci :
http://etd.repository.ugm.ac.id/home/detail_pencarian/64066,ANALISIS REGRESI HAZARD ADITIF DENGAN MODEL LIN DAN YING,"RAHMASARI NUR AZIZAH, Dr. Danardono, MPH.,",2013 | Skripsi | STATISTIKA,"Data antar kejadian (data survival) merupakan data yang berupa lama waktu hingga suatu kejadian terjadi. Apabila waktu kejadian dipengaruhi oleh satu atau beberapa variabel yang lain, maka dapat digunakan analisis regresi untuk memodelkan pengaruh dari variabel independen tersebut. Salah satu analisis regresi yang dapat digunakan adalah analisis regresi hazard aditif dengan model Lin dan Ying. Pada model hazard aditif Lin dan Ying, koefisien regresi bersifat konstan, nilainya tidak bergantung pada waktu. Metode yang digunakan untuk mengestimasi koefisien regresi pada model ini menyerupai maximum partial likelihood pada regresi Cox. Estimasi dari koefisien regresi dapat diperoleh dari persamaan score equation yang didapat dengan meniru score equation model Cox. Score equation model Cox merupakan turunan dari log partial likelihoodnya. Dalam skripsi ini, analisis regresi hazard aditif dengan model Lin dan Ying digunakan untuk menganalisis variabel-variabel yang mempengaruhi kegagalan pengobatan pasien penderita penyakit TBC di Puskesmas Mantang, Lombok Tengah. Risk difference juga dihitung untuk menjelaskan pengaruh masing-masing variabel tersebut. Diberikan pengerjaan alternatif menggunakan analisis regresi hazard dengan model Aalen, dimana grafik dari fungsi regresi kumulatifnya digunakan untuk menginterpretasikan pengaruh dari variabelvariabel independen terhadap kegagalan pengobatan. Dapat terlihat bahwa analisis regresi hazard aditif dengan menggunakan model Lin dan Ying memiliki kemudahan dalam hal interpretasi pengaruh dari masing-masing variabel, jika dibandingkan dengan analisis regresi hazard aditif dengan model Aalen.","Time to event data (survival data) is data of length of time until the event occurs. If the event time is affected by other independent variables, regression analysis can be used to analyze the effects of those independent variables. One of some kinds of regression analysis that can be used is additive hazard regression with Lin and Ying model. In Lin and Ying additive hazard model, the regression coefficients are constants, time-independent. The method that can be used to estimate regression coefficients in this model is similar with maximum partial likelihood method in Cox regression. The estimation of regression coefficients can be obtained from score equation which is obtained from mimicing the score equation from Cox model. Score equation of Cox model is the derrivative of the partial likelihood. In this paper, additive hazard regression analysis with Lin and Ying model is used to analyze some variables that affect the failure of medication of TBC patients in Puskesmas Mantang, Lombok Tengah. Risk Differences are also computed to explain each of the effects of independent variables. It is also presented here the alternative method, hazard regression analysis with Aalen model, which uses the graph of cumulative regression functions to interprete the effects of independent variables. It can be seen that additive hazard regression with Lin and Ying model has advantage in the interpretation of the effects of independent variables, compared to additive hazard regression with Aalen model","Kata Kunci : data antar kejadian, analisis regresi hazard aditif, model Lin dan Ying, maximum partial likelihood, regresi Cox, analisis regresi hazard dengan model Aalen"
http://etd.repository.ugm.ac.id/home/detail_pencarian/64323,ANALISIS BAYESIAN PADA REGRESI LOGISTIK MULTIVARIAT DENGAN ALGORITMA MCMC RANDOM WALK METROPOLIS,"ARIF MARJUKI, Herni Utami, S.Si, M.Si.",2013 | Skripsi | STATISTIKA,"Di banyak area aplikasi, seperti epidemiologi dan penelitian biomedis, regresi logistik merupakan pendekatan standar untuk analisis data biner maupun data kategorik. Pendekatan umum untuk data jenis ini dapat digunakan pendekatan generalized estimating equation (GEE, Zeger dan Liang, 1986). Meskipun pendekatan GEE memecahkan masalah data biner ataupun data kategorik, namun pendekatan ini bergantung pada asumsi sampel besar.Dalam skripsi ini menggunakan pendekatan metode Bayesian untuk mengestimasi data biner. Pendekatan Bayesian sering kali menghasilkan perhitungan yang rumit dimana melalui integrasi numerik dengan dimensi integral yang cukup besar. Dengan menggunakan algoritma Markov Chain Monte Carlo (MCMC) didapatkan perkiraan distribusi posterior yang tepat, algoritma ini juga tidak memerlukan pembenaran asumsi sampel besar. Algoritma ini juga menghasilkan perhitungan yang cepat dan efisien. Metode estimasi Bayesian melibatkan informasi prior dari parameter yang digunakan. Skripsi ini termotivasi oleh kebutuhan untuk mengembangkan metode Bayesian pada regresi logistik multivariat dengan distribusi prior noninformatif.","In many application, such as epidemiologic and biomedical studies, logistic regression is the standard approach for the analysis of binary and ordered categorical data. Common frequentist approaches, which can be used for data of this type, via generalized estimating equation (GEE, Zeger and Liang, 1986). Although the GEE approach solve this problem, the justification relies on large sample arguments. In this paper we follow a Bayesian approach to estimaste and inference, for multivariate binary and categorical data. Bayesian approach often produce a high complexity calculation and high dimensions integral. By using Markov chain Monte Carlo (MCMC) algorithms to obtain estimates of exact posterior distributions, there is no need to rely on large sample justifications. Itâ€™s also fast and eficien in calculation. Bayesian methods involves the prior information of the parameters to estimate the posterior distribution. This article is motivated by the need to develop Bayesian methods for multivariate logistic regression, which allow simple noninformative prior distribution.","Kata Kunci : Multivariate binary data, Logistic regression, Bayesian statistic, MCMC algorithm, Metropolis-Hasting algorithm."
http://etd.repository.ugm.ac.id/home/detail_pencarian/66884,OPTIMASI PERMUKAAN RESPON KUADRATIK MENGGUNAKAN ANALISIS RIDGE,"YUFAN PUTRI ASTRINI, Yunita Wulan Sari, S.Si., M.Sc.",2013 | Skripsi | STATISTIKA,"Metode permukaan respon adalah suatu metode yang bertujuan untuk menentukan kondisi operasi yang optimal dalam suatu percobaan. Karakteristik respon dari kondisi operasi dapat berupa respon yang maksimum, minimum, atau saddle. Pada beberapa situasi dimana titik stasioner tidak memiliki nilai, misalnya titik stasioner berada pada titik saddle atau titik stasioner yang menyebabkan titik respon minimum atau maksimum berada di luar daerah percobaan, perlu dilakukan suatu analisis khusus untuk menanganinya. Analisis ini disebut analisis ridge. Analisis ridge adalah suatu prosedur yang sangat berguna untuk mengoptimasi prediksi respon model kuadratik pada bentuk hyperspherical terpusat. Prosedur ini menghitung estimasi ridge pada respon optimum untuk meningkatkan jari-jari dari pusat percobaan sebenarnya. Studi kasus yang digunakan dalam skripsi ini adalah optimasi kekuatan torque lampu TL terhadap tekanan low air, low air cooling, dan natural gas. Rancangan percobaan menggunakan central composite design (CCD) dengan jumlah percobaannya yaitu 23 run. Hasil analisis menunjukkan bahwa kondisi optimal diperoleh ketika tekanan low air 3,80125 Kpa, low air cooling 7,3515 Kpa, dan natural gas 4,97 Kpa, yaitu pada saat kekuatan torque mencapai 3,11125 Newton meter.","Response surfaces method is a method that aims to determine the optimal operating conditions in an experiment. The response characteristics of operating conditions can be a maximum, minimum, or saddle point. In some situations where a stationary point has no value, for example, a stationary point is at the saddle point and stationary point which causes minimum or maximum response point outside the original region, needs to be done a specific analysis to handle that. This analysis called ridge analysis. Ridge analysis is a useful procedure to optimize response prediction in hyperspherical centralized quadratic model. This procedure calculates the estimated ridge on the optimum response for increasing radii from the center of the actual experiments. The case study used in this thesis is the optimization of the power torque TL lights against low air, low air cooling, and natural gas pressure. Experimental design using central composite design (CCD) with the number of experiments at 23 runs. The analysis showed that the optimal condition is obtained when the pressure of low air 3,80125 Kpa, low air cooling 7,3515 Kpa, and natural gas 4,97 Kpa, that is when the strength of the torque reaches 3,11125 Newton meter.","Kata Kunci : metode permukaan respon, analisis ridge dalam metode permukaan respon, central composite design (CCD)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/63050,ESTIMASI MAKSIMUM LIKELIHOOD PADA MODEL GROWTH CURVE DAN APLIKASINYA PADA MODEL GROWTH CURVE LINEAR,"FAUZIA FATMANINGRUM, Prof. Dr. Sri Haryatmi, M.Sc.",2013 | Skripsi | STATISTIKA,"Model growth curve merupakan model Generalized Multivariate Analysis of variance (GMANOVA) yang khusus digunakan untuk menganalisis masalah pertumbuhan pada runtun waktu yang pendek. Untuk kasus pertumbuhan subjek secara linear, digunakan model growth curve linear. Tujuan dari analisis model growth curve adalah pembandingan parameter untuk mengetahui variabel respon yang diamati pada suatu runtun waktu tertentu, pada beberapa grup yang ada. Dimana pada beberapa bidang ilmu seperti ilmu hewan, hortikultura, uji klinis, ilmu kedokteran, psikologi, eksperimen psikologi, biologi, dan sosial diperlukan analisis mengenai masalah pembandingan pertumbuhan suatu subjek yang diamati ulang dalam runtun waktu tertentu untuk kasus lebih dari 2 grup atau perlakuan. Masalah tersebut mempunyai kemiripan dengan masalah pada MANOVA, hanya saja variabel responnya ditetapkan sebagai pertumbuhan subjek runtun waktu tertentu atau yang biasa disebut kasus repeated measure. Repeated measure mengacu pada situasi di mana pengukuran beberapa variabel respon yang diperoleh, selama beberapa periode waktu, dari setiap unit eksperimental. Biasanya, respon yang diambil dari waktu ke waktu, dalam waktu mingguan / bulanan. Dalam skripsi ini langkah yang digunakan untuk menganalisis growth curve adalah pengujian asumsi model growth curve, estimasi parameter model growth curve menggunakan metode Maximum likelihood Estimator (MLE) untuk mengestimasi parameter model growth curve, dan pembandingan parameter model growth curve.","Growth curve model is a model of Generalized Multivariate Analysis of variance (GMANOVA) that is specifically used to analyze the growth problem in a short time series. For the case of linear growth of the subject, the model used linear growth curve. The purpose of the analysis is the comparison of the model growth curve parameters to determine the response variables were observed at a particular time series, on some existing group. Where in several fields of science such as animal science, horticulture, clinical trials, medical science, psychology, experimental psychology, biology, and social analysis is required on the issue of comparing the observed growth a subject back in a certain time series for the case of more than 2 groups or treatment. The problem has some similarities with the MANOVA problem, it's just that the response variable defined as the growth of a particular subject or time series which is called the case of repeated measure. Repeated measure refers to a situation in which several variables measuring the response obtained, for some period of time, from each experimental unit. Typically, responses are taken from time to time, in the weekly / monthly. In this paper the steps used to analyze the growth curve is the growth curve testing of model assumptions, the estimated growth curve model parameters using the Maximum likelihood estimator (MLE) to estimate the growth curve model parameters, and benchmarking parameters of growth curve models.","Kata Kunci : Model Growth Curve Linear, Maximum likelihood Estimator, Generalized Multivariate Analysis of Variance, Multivariate Analysis of Variance, Repeated Measure"
http://etd.repository.ugm.ac.id/home/detail_pencarian/64330,ANALISIS TEKNIKAL SAHAM DENGAN INDIKATOR STOCHASTIC RSI (RELATIVE STRENGTH INDEX),"MUHAMMAD RIO NUGRAHA, Dr. RerNat Dedi Rosadi, S.Si, M.Sc,",2013 | Skripsi | STATISTIKA,"Analisis teknikal saham merupakan suatu metode analisis yang menggunakan pengujian atas harga di masa lampau untuk tujuan prediksi pergerakan harga di masa yang akan datang. Analisis teknikal saham baik digunakan para pelaku trading (trader) dalam membantu memberikan prediksi kapan waktu yang tepat untuk masuk dan keluar dari pasar saham. Terdapat berbagai macam indikator yang digunakan dalam analisis teknikal, salah satunya adalah Stochastic RSI (Relative Strength Index). Stochastic RSI (Relative Strength Index) merupakan analisi indikator yang perhitungan mirip dengan Stochastic Oscilator namun untuk data lowest dan highest biasanya diganti dengan data tertinggi dan terendah dari RSI (Relative Strenth Index) sehingga diprediksi dapat menghasilkan prediksi yang lebih tepat dari indikator lainnya, dimana dalam skripsi ini akan menggunakan periode 25 yang selanjutnya akan dibandingkan sinyal prediksi yang lebih tepat antara Stochastic Oscilator dan Stochastic RSI.","Technical analysis of stock is an analytical method which tests the previous price in order to predict the future of the price movenment. The technical analysis of stock is good to help traders in predicting the right time to get in and out of the stock market. There are some indicators used in technical analysis, one of them is Stochastic RSI (Relative Strength Index). Stochastic RSI (Relative Strength Index) is an indicator that the calculation analysis similar to the Stochastic Oscillator but for the data of choices and usually replaced with the highest highs and the lowest of data from RSI (Relative strenth Index) which is predicted to produce more accurate predictions than other indicators, where the this thesis will use the 25 period will be compared to a more precise prediction of the signal between the Stochastic oscillator and Stochastic RSI.","Kata Kunci : Analisis teknikal, Stochastic Oscillator, RSI (Relative Strength Index), Stochastic RSI (Relative Strength Index)."
http://etd.repository.ugm.ac.id/home/detail_pencarian/64080,APLIKASI SUKU BUNGA MODEL COX-INGERSOLL-ROSS (CIR) DALAM PERHITUNGAN PREMI ASURANSI JIWA DWIGUNA,"HERMAWATI SETYANINGSIH, Yunita Wulan Sari, S.Si., M.Sc.",2013 | Skripsi | STATISTIKA,Asuransi jiwa dwiguna adalah kombinasi antara asuransi jiwa berjangka dan asuransi jiwa dwiguna murni. Polis ini menjanjikan pembayaran manfaat kepda ahli waris tertanggung bila tertanggung meninggal dalam jangka waktu mengikuti polis atau pembayaran manfaat kepada tertanggung bila ia hidup sampai akhir masa kontrak asuransi. Jadi artinya polis asuransi jiwa dwiguna memiliki dua elemen yaitu perlindungan jiwa dan tabungan sehingga tertanggung dan ahli waris dapat memperoleh manfaat. Perhitungan premi asuransi jiwa biasanya dengan asumsi suku bunga bergerak tetap sepanjang waktu. Asumsi ini tidak sesuai dengan kenyataan bahwa suku bunga bergerak secara fluktuatif. Sehingga diperlukan model tingkat suku bunga stokastik yang telah mempertimbangkan pergerakan tingkat suku bunga secara fluktuatif. Pada skripsi ini akan dihitung harga premi tahunan asuransi jiwa dwiguna untuk tingkat bunga mengikuti model CIR. Model Cox-Ingersoll-Ross (CIR) merupakan salah satu model stokastik yang menggambarkan perubahan tingkat bunga untuk jangka waktu yang pendek. Model suku bunga CIR merupakan perluasan dari model suku bunga Vasicek. Model suku bunga CIR mengasumsikan suku bunga selalu positif. Parameterparameter model CIR akan diestimasi dengan metode estimasi kuadrat terkecil bersyarat.,"Endowment insurance is a combination of term life insurance and pure endowment life insurance. This policy promises payment of benefit to insured's heir if the insured dies within a period of policy or payment of benefit to insured if he/she lives until the end of insurance contract. So it means that endowment life insurance policy has two elements, namely the protection of life and savings, so the insured and his/her heir can obtain the benefit. Calculation premiums of insurance are usually based on the assumption that interest rates move steadily over time. This assumption is not consistent with the fact that interest rates on life insurance volatile moves. So that the required model of stochastic interest rates that have considered the movement of interest rate fluctuate. In this essay, will be calculated yearly premium value of life insurance endowment for interest rates follow CIR models. Cox-Ingersoll-Ross (CIR) model is one of stochastic models that describe changes in the interest rate for a short period. Interest rates CIR model is an extended of the Vasicek model. CIR interest rate model assumed interest rate is always positive. CIRâ€™s parameters are estimated by conditional least squares estimation method.","Kata Kunci : suku bunga, asuransi jiwa dwiguna, Cox-Ingersoll- Ross (CIR), estimasi kuadrat terkecil bersyarat"
http://etd.repository.ugm.ac.id/home/detail_pencarian/64346,RANCANGAN D-OPTIMAL UNTUK OPTIMASI VARIABEL RESPON PADA METODE PERMUKAAN RESPON ORDE DUA,"FATMAH BUDIANI ARINI, Yunita Wulan Sari, S.Si, M.Sc.",2013 | Skripsi | STATISTIKA,"Metode Permukaan Respon adalah suatu metode yang digunakan untuk mencari hubungan antara variabel independen dan variabel dependen kemudian mengoptimalisasi variabel dependen tersebut. Secara umum Metode Permukaan Respon terdiri dari Merancang Percobaan, memodelkan variabel yang berpengaruh, dan melakukan optimalisasi. Metode Permukaan Respon mempunyai banyak rancangan percobaan, mulai dari rancangan klasik seperti Central Composite Design (CCD), Box-Behnken Design (BBD) maupun Computer Generated Design seperti D- Optimal. Rancangan D-Optimal hanya membutuhkan sedikit unit percobaan sehingga dapat meminimumkan biaya percobaan. Kriteria D-Optimal digunakan untuk meminimalkan variansi dari estimasi parameter dengan cara memaksimalkan determinan determinan matriks informasinya. Pemilihan titik-titik dari rancangan tergantung pada model yang dipilih dan banyaknya unit percobaan yang diinginkan. Dalam Skripsi ini, Kriteria D-Optimal diaplikasikan pada data total kandungan lipid pada mikroalga Chroococcus Sp berdasarkan 3 variabel independen yaitu kerapatan sel, komposisi nitrogen, komposisi posfor. dapat disimpulkan pada saat kerapatan sel 3.206026, komposisi nitrogen=0.264478 g/l dan komposisi posfor=0.396292 g/l akan menghasilkan total lipid pada Chroococcus Sp yang optimum sebesar 7.159689 %.","Response Surface Method is a method used to find the relationship between the independent variables and the dependent variable then optimize the dependent variable. In general, response surface method consists of designing experiments, modeling influential variable, and perform optimization. Response surface methods have many experimental designs, classic designs such as the Central Composite Design (CCD), Box-Behnken design (BBD) and Computer Generated as D-Optimal Design. Optimal design requires little experiment so as to minimize the cost of the experiment. D-Optimal criteria are used to minimize the variance of the parameter estimates by maximizing the determinant of the information matrix determinant. The selection of the points of the design depends on the model chosen and the desired number of experimental units. In this thesis, D-Optimal criteria applied to the data on the total lipid of microalgae Chroococcus Sp, 3 independent variables are based on the cell density, the composition of nitrogen, phosphorus composition. it can be concluded at the cell desdity=3.206026, composition of nitrogen=0.264478 g/l and phosphorus composition=0.396292 g/l will result in the total lipid optimum 7.159689%.","Kata Kunci : Metode Permukaan Respon, Rancangan D-Optimal, Matriks Informasi, Lipid, Mikroalga Chroococcus Sp."
http://etd.repository.ugm.ac.id/home/detail_pencarian/64347,ESTIMASI PARAMETER MODEL REGRESI LOGISTIK MENGGUNAKAN METODE JACKKNIFE,"HANA FITIANINGRUM, Herni Utami, S.Si., M.Si.",2013 | Skripsi | STATISTIKA,"Jackknife merupakan salah satu metode estimasi inferensi statistika yang berbasis komputer. Prinsip kerjanya adalah menggunakan komputer dalam membangkitkan data dari sampel asli yang berukuran kecil untuk mendapatkan sampel tiruan. Sampel tiruan diperoleh dengan cara menghapus suatu observasi dari sampel asli yang selanjutnya dapat digunakan untuk menghitung nilai estimator. Salah satu kelebihan dari metode Jackknife adalah tidak membutuhkan asumsi apapun mengenai distribusi dari sampel yang dimiliki. Tujuan utama dari metode ini adalah untuk memperoleh estimasi yang sebaik-baiknya berdasarkan data yang minimal dengan bantuan komputer. Metode Jackknife dapat digunakan pada data berpasangan untuk keperluan rasio dan dalam kasus model regresi. Metode yang sering dipakai untuk menyelesaikan masalah Dalam skripsi ini, metode Jackknife diterapkan untuk mengestimasi parameter model regresi logistik. Model regresi logistik merupakan salah satu bentuk analisis regresi untuk mengetahui suatu hubungan sebab akibat (kausalitas) apabila variabel respon Y hanya memiliki 2 kemungkinan nilai/hasil atau data bersifat dikotomus. regresi logistik adalah metode Maximum Likelihood Estimation (MLE) dimana proses penaksiran parameter didahului oleh pembentukan fungsi likelihood. Metode Jackknife dalam estimasi parameter model regresi logistik tersebut diilustrasikan dalam penentuan tingkat kebangkrutan perusahaan perbankan di Indonesia yang dipilih secara acak. Berdasarkan hasil analisis yang diperoleh, metode Jackknife mampu memperkecil standar error sampai dengan Jackknife terhapus-2.","Jackknife is one of the estimation methods, computer-based statistical inference. Its working principle is using a computer in generating original data from a small sample to get an pseudo sample. Pseudo sample is obtained by removing an observation from the original sample can then be used to calculate the value of the estimator. One of the jackknife methodâ€™s advantage is no need of any assumptions regarding the distribution of the sample possessed. The main purpose of this method is to obtain the best possible estimate based on minimal data with the help of computers. A jackknife method can be used on paired data for purposes ratio and in the case regression models. In this paper, a jackknife method is applied to estimate the parameters of a logistic regression model. Logistic regression model is a form of regression analysis to determine a causal relationship (causality) when the response variable Y has only two possible values / results or data are dichotomous. The method which is often used to solve the logistic regression problem is Maximum Likelihood Estimation (MLE) where the parameter estimation process is preceded by the formation of likelihood function. Jackknife method in estimating parameters of the logistic regression model is illustrated in the determination of the level of bankruptcies in the Indonesian banking firm selected randomly. Based on the results of the analysis, jackknife method is able to reduce the standard errors to jackknife deleted-2.","Kata Kunci : Metode Jackknife, Regresi Logistik, Maximum Likelihood Estimation (MLE)"
http://etd.repository.ugm.ac.id/home/detail_pencarian/67164,PORTOFOLIO OPTIMAL MENGGUNAKAN CAPITAL ASSET PRICING MODEL DENGAN KEYAKINAN HETEROGEN,"ATHIKAH DIAN A, Prof. Dr. rer. nat. Dedi Rosadi, S.Si., M.Sc.",2013 | Skripsi | STATISTIKA,"Investasi merupakan suatu bentuk penanaman modal dengan harapan nantinya akan mendapatkan keuntungan. Semakin tinggi pengaharapan investor akan keuntungan, semakin tinggi pula risiko yang dihadapi. Untuk dapat meminimalkan risiko dalam investasi, investor dapat melakukan portofolio (diversifikasi) saham yaitu dengan melakukan investasi pada banyak saham sehingga risiko kerugian pada satu saham dapat ditutup dengan keuntungan pada saham yang lainnya. Salah satu model faktor tunggal yang dapat digunakan untuk mencari bobot masingmasing saham pada portofolio adalah Capital Asset Pricing Model (CAPM) yang didasarkan pada asumsi bahwa investor memiliki keyakinan yang homogen tentang mean dan kovariansi aset berisiko. Asumsi ini telah diperdebatkan oleh banyak pihak dari tahun ke tahun, karena dinilai tidak realistis dan tidak sesuai dengan pasar yang sebenarnya, di mana setiap investor memiliki keyakinan yang berbeda-beda akan keuntungan dan risiko di periode mendatang. Maka, dicari portofolio saham dengan menggunakan CAPM dengan asumsi keyakinan investor yang heterogen untuk dibandingkan tingkat keuntungan dan risikonya dengan portfolio yang dicari dengan menggunakan CAPM pada asumsi homogen. Sehingga didapatkan bahwa portofolio yang didapatkan saat mengasumsikan pasar heterogen, menghasilkan pengembalian yang lebih besar dan risiko yang lebih kecil.","Investment is a form of financial activity which the investor put on their wealth on the market with the expectation of profit. The higher expectation of investment return, the higher risk that should be faced. Investor can minimalizing the investment risk by asset portfolio (diversification), which is investing their wealth on many assets, so that the risk of one asset can be covered by anothe assets. One of the single factor model that can be used to find the weight of each assets on portfolio is Capital Asset Pricing Model (CAPM). This model assume that investor under homogeneous belief about the future mean and covariance return of risky assets. This assumption has been discussed by many researchers since years ago even until now, since it is not realistic and agree with the real condition of the market, where each investors has their own belief about the future expected mean and covariance return. This work provide portfolio with CAPM under heterogeneous belief of future mean and covariance return to be compared with CAPM under homogeneous belief of future mean and covariance return, for its rate of return and risk. Then the result show that portfolio with CAPM under heterogeneous belief deliver higher return and lower risk than portfolio with CAPM under homogeneous belief.","Kata Kunci : Saham, Portofolio, CAPM, Keyakinan Heterogen, Return, Aset Bebas Risiko."
http://etd.repository.ugm.ac.id/home/detail_pencarian/66929,ANALISIS DATA MINING CUACA MENGGUNAKAN ATURAN ASOSIASI DAN REGRESI LOGISTIK KERNEL,"HEBNU PRIYAMBODO, Dr. Gunardi, M.Si.",2013 | Skripsi | STATISTIKA,Pengetahuan tentang pola dan hubungan memegang peranan sangat penting dalam pengambilan kebijakan pada bidang pertanian. Tingkat curah hujan merupakan salah satu faktor yang mempengaruhi produktifitas tanaman komoditas pangan. Aturan asosiasi (association rule) dan Regresi logistik kernel (Kernel Rgresson Logistik) merupakan teknik data mining untuk membantu dalam pengambilan suatu keputusan. Pemanfaatan model dalam mengggambarkan suatu probabilitas serta menemukan aturan-aturan yang terjadi dalam data jumlah besar. Penulis melakukan penelitian terhadap data yang diperoleh dari Badan Meteorologi Klimatologi dan Geofisika (BMKG) serta Badan Pusan Statistik (BPS). Tugas akhir ini memberikan kesimpulan bahwa regresi logistik kernel dengan bantuan grafik dapat menghasilkan pola tingkat curah hujan serta gambaran pada bidang pertanian. Sedangkan association rule menghasilkan aturan-aturan tersembunyi yang terdapat dalam suatu data set.,"Knowledge about patterns and relationships plays an important role in agriculture policy making. The level of rainfall is one of the factors affect the productivity of plants for food commodities. Association rules and kernel logistic regression (KLR) are data mining techniques to assist in making a decision. Model utilization depicts a probability and finds the rules occur in large amounts of the data. The author conduct a study of the data obtained from the Indonesian Meteorological, Climatological and Geophysical - Badan Meteorologi, Klimatologi, dan Geofisika (BMKG) and Statistics Indonesia - Badan Pusat Statistik (BPS). This final project aims to give conclusion that the kernel logistic regression; with the help of graphs can produce patterns of rainfall levels and the description of agriculture, while association rules producing hidden rules in the data set.","Kata Kunci : Curah hujan, Regresi logistik kernel, Aturan sosiasi, Data mining, Komoditas pangan, data set."
http://etd.repository.ugm.ac.id/home/detail_pencarian/66933,PEMODELAN PERSAMAAN REGRESI SPLINE KUADRATIK DENGAN MENENTUKAN TITIK - TITIK KNOT YANG OPTIMAL (Studi Kasus Pertambahan Persentase Penduduk dengan Persentase Penerimaan Tenaga Kerja Baru),"TRI EFENDI, Drs. Zulaela, Dipl. Med. Stats., M.Si",2013 | Skripsi | STATISTIKA,"Analisis regresi digunakan untuk melihat pengaruh variabel prediktor terhadap variabel respon dengan sebuah kurva regresi. Pendekatan yang digunakan untuk menentukan kurva regresi yaitu pendekatan parametrik dan nonparametrik. Regresi spline merupakan salah satu model pendekatan kearah pengepasan data dengan tetap memperhitungkan kemulusan kurva regresi, yang merupakan modifikasi dari fungsi polynomial tersegmen. Bentuk estimator spline sangat di pengaruhi oleh nilai parameter penghalus ï¬ yang pada hakekatnya adalah penentuan lokasi titik â€“ titik knot. Pemilihan ï¬ optimal merupakan persoalan yang sangat penting dalam estimasi regresi spline. Model regresi spline kuadratik diterapkan pada suatu study kasus mengenai persentase pertumbuhan penduduk Indonesia dan persentase penerimaan tenaga kerja baru di Indonesia dengan memilih nilai MSE dan GCV( Generalized Cross Validation ) yang optimum.","Regression analysis was used to see the effect of predictor variables on the response variable with a regression curve . The approach used to determine the regression curve parametric and nonparametric approaches . Spline regression models is one approach towards fitting the data with smoothness considering the regression curve , which is a modification of segmented polynomial function . Spline estimator shape is influenced by the smoothing parameter value that is essentially a determination of the location of the point - the point knots . Selection of optimal is a very important issue in spline regression estimation . Quadratic spline regression model is applied to a case study of the Indonesian population growth percentage and percentage of new recruitment in Indonesia with GCV ( Generalized Cross Validation ) and MSE selecting optimum .",Kata Kunci : -
http://etd.repository.ugm.ac.id/home/detail_pencarian/64126,ANALISIS REGRESI RIDGE DUA TAHAP UNTUK PERMASALAHAN MULTIKOLINEARITAS,"ESTIRA WORO ASTRINI, Prof. Drs. Subanar, Ph.D",2013 | Skripsi | STATISTIKA,"Analisis regresi adalah analisis statistika yang dilakukan untuk memodelkan hubungan antara variabel dependen dan variabel independen. Dalam asumsi yang terdapat pada analisis regresi klasik salah satunya adalah tidak terdapat multikolinearitas. Jika terdapat multikolinearitas dalam model regresi, hal itu dapat menyebabkan hasil estimasi menggunakan metode kuadrat terkecil menjadi tidak valid. Seiring perkembangan waktu, mulailah ditemukan berbagai analisis regresi modern. Dan salah satu analisis regresi modern yang dapat mengatasi permasalahan multikolinearitas adalah analisis regresi Ridge yang pertama kali diperkenalkan oleh A.E Hoerl dan Kennard pada tahun 1970. Seperti halnya analisis regresi klasik yang berkembang menjadi analisis regresi modern, regresi Ridge pun mengalami perkembangan. Salah satunya adalah analisis regresi Ridge dua tahap yang baru diperkenalkan oleh Hussein Eledum dan Mostafa Zahri pada tahun 2013. Analisis regresi Ridge dua tahap ini merupakan gabungan antara metode regresi kuadrat terkecil dua tahap dengan metode regresi Ridge biasa. Dalam skripsi ini, analisis regresi Ridge dua tahap diaplikasikan pada analisis faktor-faktor yang mempengaruhi jumlah uang beredar di Amerika sehingga memperoleh model yang tepat dan bebas dari multikolinearitas.","Regression analysis is a statistical analysis that used to perform model relationship between dependent variable and independent variable. One of the assumption in classical regression analysis is there is no multicollinearity problem. If there is multicollinearity in the regression model, it could cause the results of model that using the method of Least Squares estimator becomes invalid. Over the years, there are a lot of variety modern regression analysis. And one of the modern regression analysis that can overcome the multicollinearity problem is the Ridge regression analysis. Ridge regression analysis was first introduced by A.E Hoerl and Kennard in 1970. Two Stages Ridge regression analysis recently introduced by Hussein Eledum and Mostafa Zahri in 2013. Two Stages Ridge Regression analysis method is a combination of Two Stage Least Squares and Ordinary Ridge Regression. In this paper, two stages of Ridge regression analysis was applied to the analysis of the factors that affect the amount of money circulating in the U.S. to obtain the model that free from multicollinearity problem.","Kata Kunci : analisis regresi kuadrat terkecil dua tahap, analisis regresi Ridge, analisis regresi Ridge dua tahap."
http://etd.repository.ugm.ac.id/home/detail_pencarian/64127,REGRESI SPLINES BENTUK-TERBATAS MONOTON (MONOTONE SHAPE-RESTRICTED REGRESSION SPLINES),"ESTRI PURWANI, Drs. Zulaela, Dipl. Med. Stats., M.Si.",2013 | Skripsi | STATISTIKA,"Analisis regresi merupakan analisis statistika yang sering digunakan untuk menyelidiki hubungan antara variabel prediktor dengan variabel respon. Jika asumsi bentuk parametrik diketahui, maka regresi parametrik dapat dilakukan. Tetapi jika asumsi bentuk parametriknya tidak diketahui maka estimasi fungsi regresi dapat dilakukan dengan regresi nonparametrik. Metode regresi nonparametrik yang sering digunakan adalah regresi splines karena menggunakan lebih sedikit parameter dalam proses estimasi. Regresi splines mampu memodelkan data yang mempunyai karakteristik berbeda dalam interval ??????, ?????? . Proses estimasi dilakukan dengan membagi ??????, ?????? menjadi beberapa sub interval yang mempunyai kesamaan karakteristik. Regresi splines sangat sensitif terhadap penentuan jumlah dan lokasi titik knot sehingga diperlukan kriteria yaitu Generalized Cross-Validation untuk menentukan jumlah dan lokasi titik knot yang optimal. Dalam aplikasi nyata, variabel prediktor dan respon diketahui mempunyai bentuk tertentu seperti monotonisitas. Asumsi monotonisitas ini dapat diterapkan ke dalam proses estimasi regresi splines yang kemudian dinamakan regresi splines bentuk-terbatas monoton. Estimasi fungsi regresi ini dapat diperoleh menggunakan kombinasi linier dari basis fungsi yaitu I-splines dan membatasi koefisiennya agar bernilai positif. Regresi splines dengan pembatasan bentuk ini memiliki Mean Squared Error (MSE) yang lebih kecil dan R-square yang lebih besar daripada regresi splines tanpa pembatasan bentuk. Dalam skripsi ini, analisis regresi splines bentuk terbatas monoton diaplikasikan untuk menganalisis hubungan umur dan tinggi badan balita di posyandu Sakura, kelurahan Caturharjo, kecamatan Pandak, kabupaten Bantul. Kemudian hasil estimasi regresi splines bentuk terbatas monoton dibandingkan dengan regresi splines dan regresi linier sederhana. Dengan melihat hasil estimasi kurva regresi, MSE dan R-square diperoleh kesimpulan bahwa regresi splines bentuk-terbatas monoton merupakan model yang terbaik dibandingkan dengan model lainnya.","Regression analysis is a statistical analysis that is often used to explore the relationship between predictor variable and response variable. If the assumption of a parametric form was known, parametric regression can be performed. But if the parametric form was not known, the estimated regression functions can be performed with nonparametric regression. Nonparametric regression method that is often preferred is regression splines because it uses less parameters in the estimation process. Regression splines can model the data that have different characteristics in the interval ??????, ??????. Estimation process is done by deviding ??????, ?????? to be some sub intervals that have similar characteristics. Regression splines is known to be sensitive to number and location of knot so it need Generalized Cross-Validation to determine the optimal number and location of knot. In many practical settings, the predictor and response variable are known to preserve certain shape restrictions such as monotonicity. That monotonicity assumptions can be imposed on the regression splines estimation process, which is then called monotone shape-restricted regression splines. Estimate monotone shape-restricted function can be obtained using linear combination of I-splines basis functions and restrict the coefficients of these basis function to be positive. The restricted version have smaller mean squared error (MSE) and greater Rsquare than unrestricted version. In this paper, monotone shape-restricted regression splines is applied to analyze the relationship of age and toddler height in posyandu Sakura, Caturharjo village, Pandak subdistrict, Bantul regency. Then monotone shaperestricted regression splines estimation results are compared with regression splines and simple linear regression. By looking at the estimation results of the regression curve, MSE and R-square, it is concluded that monotone shaperestricted regression splines better than others.","Kata Kunci : regresi nonparametrik, regresi splines, knot, regresi splines bentukterbatas monoton, I-splines"
http://etd.repository.ugm.ac.id/home/detail_pencarian/66186,OPTIMISASI PORTOFOLIO ROBUST MENGGUNAKAN SECOND-ORDER CONE PROGRAMMING (SOCP),"DESSY PARAMITA, Dr.rer.nat. Dedi Rosadi, M.Sc.",2013 | Skripsi | STATISTIKA,"Optimisasi portofolio merupakan salah satu metode seleksi portofolio yang terkenal dan paling banyak digunakan. Teknik optimisasi portofolio pertama kali dikembangkan oleh Markowitz (1952), yaitu model mean-variance. Walaupun model ini didukung oleh teori yang kuat dan memiliki kemudahan dalam komputasi, mean-variance menunjukkan beberapa kelemahan, salah satunya sangat sensitif terhadap perubahan parameter input. Untuk mengurangi sensitivitas model ini, dikenalkan teknik optimisasi portofolio robust. Dalam optimisasi portofolio robust, parameter inputnya dianggap tidak pasti, dalam hal ini terletak dalam sebuah interval konfidensi yang selanjutnya disebut himpunan ketidakpastian (uncertainty set). Hal ini disebabkan karena pada realita mengestimasi nilai kedua parameter ini tidak mudah, selain itu nilainya selalu berubah setiap saat. Setelah menentukan himpunan ketidakpastian, masalah optimisasi akan diselesaikan untuk kasus terburuk yakni kondisi dengan nilai pengembalian (expected return) portofolio minimum dan risiko portofolio maksimum. Masalah optimisasi dalam analisis portofolio robust ini akan dibawa ke dalam bentuk second-order cone programming (SOCP) yang dapat diselesaikan dengan metode titik interior primal-dual. Studi kasus dilakukan dengan membentuk portofolio yang terdiri dari saham-saham yang terdaftar di Bursa Efek Indonesia, yaitu AALI, ADRO, ASRI, BBRI, CPIN, TLKM dan UNVR, menggunakan teknik SOCP dan mean-variance. Kedua metode ini dibandingkan dengan mengamati kinerja portofolio yang diukur dari tingkat pengembalian (rate of return) portofolio dan nilai indeks Sharpe. Hasilnya, portofolio robust SOCP menunjukkan kinerja yang lebih baik daripada portofolio mean-variance.","Portfolio optimization is one of the best known and most widely used methods in financial portfolio selection. The first portfolio optimization technique called mean-variance model was developed by Harry Markowitz (1952). Despite the strong theoretical support and the availability of efficient computation provided by mean-variance, the model presents several practical pitfalls. One of them is that the model is often sensitive to the change in input parameter. To reduce the sensitivity of mean-variance model, the robust portfolio optimization technique has been proposed. In this approach, the input parameter are expected to lie within a confidence interval, which is described as uncertainty sets. This is because in reality, it is very difficult to estimate the correct values of these parameters and the values change every time. After determining the uncertainty sets, the analysis is carried out for the worst-case scenario under the model, i.e: model with minimum expected return and maximum risk. The optimization problem is reduced to a second-order cone programming (SOCP) which could be solved via primal-dual interior point method. The case study presents the portfolio construction of several stocks listed in Indonesia Stock Exchange i.e: AALI, ADRO, ASRI, BBRI, CPIN, TLKM and UNVR, using the SOCP and mean-variance optimization techniques. These two methods are compared by measuring the portfolio performance under their rate of return and Sharpe ratio. As aresult, the SOCP robust portfolio performs better than the mean-variance portfolio.","Kata Kunci : Portofolio, optimisasi robust, optimisasi mean-variance, second-order cone programming"
http://etd.repository.ugm.ac.id/home/detail_pencarian/61331,ANALISIS PARTIAL LEAST SQUARES REGRESI (PLS-R) PARTIAL LEAST SQUARES REGRESSION (PLS-R) ANALYSIS,"INGGRIT RABERTA, Dr.Abdurakhman,S.Si.,M.Si.",2013 | Skripsi | STATISTIKA,"Metode Partial Least Squares Regresi (PLS-R) adalah teknik regresi linear antara variabel prediktor X yang bersifat multivariat dengan variabel respon Y. Model linear optimal PLS-R dibangun dari hasil reduksi variabel prediktor X. Proses reduksi tersebut menghasilkan variabel baru yang disebut sebagai komponen utama. Proses estimasi parameter-parameter yang ada pada model regresi ini, digunakan Algoritma Nonlinear Iterative Partial Least Squares (NIPALS). Metode PLS-R tersebut akan digunakan untuk studi kasus mengenai faktor-faktor yang mempengaruhi Pendapatan Asli Daerah (PAD) Kabupaten Sleman. Data menunjukkan adanya multikolinearitas, sehingga pada kondisi ini regresi OLS tidak cocok digunakan, dan digunakan PLS-R sebagai suatu alternatif dalam mengatasi multikolinearitas data. Hasil analisis studi kasus disimpulkan bahwa metode PLS-R mampu menangani masalah multikolinearitas.","Partial Least Squares Regression (PLS-R) method is regression linear technique for multivariate predictor variable X with response variable Y. The best linear PLS-R model is build by reducing predictor variable X. This reduction process produces new variable that called principal component. To estimate the regression model parameters, Nonlinear Iterative Partial Least Squares (NIPALS) Algorithm is used. PLS-R method will be used for case study about influencing factors of Regional Income in Sleman District. Based on data, thereâ€™re multicollinearity, so OLS Regression is not suitable for this case and we use PLS-R as an alternative solution for multicollinearity.","Kata Kunci : Partial Least Square Regresi, PLS-R, komponen utama, NIPALS, regresi berganda, Pendapatan Asli Daerah, multikolinearitas"
http://etd.repository.ugm.ac.id/home/detail_pencarian/60568,MODELLINIERTERGENERALISASIPADAVARIABELRESPONCACAH,"CHOLIFATUL HUSNA, Dr.Abdurakhman,S.Si.,M.Si.",2013 | Skripsi | STATISTIKA,"Modelliniertelahditerapkanbertahunâ€“tahundalamanalisisstatistik,terutamadatakontinu.Tehnikinididasarkanpadaasumsidistribusinormaldalamvariabelacaknyadanadanyahubunganlinierantararataâ€“ratadenganvariabelpenjelasnya.Modelinitelahmengalamiperkembangandenganmemberikanasumsiyanglebihlonggarpadadistibusinya.Datatidakterbataspadadistribusinormaltetapimerupakananggotadistribusikeluargaeksponensial.Modelinidinamakanâ€œModelLinierTergeneralisasiâ€.ModelinipertamadiperkenalkanolehNelderdanWedderburn(1972). Khususuntukvariabelresponcacah,ModelLinierTergeneralisasiyangdigunakanadalahregresiPoissondanregresiNegatifBinomial.KeduaregresiiniakanditerapkanpadadatadenganvariabelresponcacahkemudiannantinyaakandilakukanpemilihanmodeldengancriteriaAICuntukengetahuimodelliniertergeneralisasiterbaik.","LinearModelhasbeenappliedduringthroughyearsinanalysingstatistics,especiallycontinuousdataanalysis.Thistechnicsbasedonassumptionatnormaldistributionintherandomcomponentandalsoexistenceoftherelationoflinearbetweenmeanwithsystemticcomponent(itsexplanatoryvariable).Thislinearmodelhereinafterexperiencesdevelopmentbygivingassumptionwhichmoreclearanceeitheratthedistributionoratrelationbetweenmanwiththesimetikcomponent.Distributiondatawillnolongerlimitedtonormaldistributionbutismemberfromdistributionofexponentialfamily.Thismodelnamed\""GeneralizedLinierModels\"".ThismodelfirstlyisintroducedbyNelderandWedderburn(1972). Especiallyforthecountresponsevariable,generalizedlinearmodelswereusedPoissonregressionandNegativeBinomialregression.BothregressionwillbeappliedtothedatawiththecountresponsevariablewillbedonelaterwiththemodelselectioncriteriaAICtodeterminethebetGeneralizedLinearModel.","Kata Kunci : ModelLinierTergeneralisasi,DistribusiKeluargaEkponensial,RegresiPoisson,RegresiNegatifBinomial,datacacah,AIC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/62885,Penentuan Harga Obligasi Callable dengan Suku Bunga Black Derman Toy Menggunakan Pohon Binomial,"HELIDA HAERINI, Yunita Wulan Sari, S.Si., M. Sc.",2013 | Skripsi | STATISTIKA,"Suku bunga mempunyai peran penting dalam penentuan harga aset finansial, salah satunya adalah obligasi. Obligasi callable merupakan obligasi yang memberikan hak kepada penerbit untuk membeli kembali obligasi pada harga tertentu sepanjang umur obligasi tersebut. Digunakan metode pohon binomial dalam megestimasi harga obligasi. Untuk tingkat bunga yang konstan, tidaklah sulit untuk menentukan harga obligasi. Akan tetapi pada kenyataannya, pergerakan tingkat bunga berubah-ubah secara tidak pasti dan merupakan proses stokastik sehingga untuk mengamatinya diperlukan suatu model tingkat bunga stokastik. Dalam skripsi ini, digunakan suku bunga acuan model faktor tunggal dengan asumsi no-arbitrage yang dikembangkan pada tahun 1990 oleh Fischer Black, Emanuel Derman, dan William Toy dan diasumsikan bahwa suku bunga berdistribusi lognormal, yaitu suku bunga Black Derman Toy (BDT). Dalam suku bunga BDT digunakan bisection method untuk mencari nilai drift. Suku bunga BDT juga memasukkan unsur mean reversion di dalamnya. Pada penentuan pohon binomial, dilakukan pemodelan suku bunga terlebih dulu secara maju dan selanjutnya secara mundur dalam menentukan harga obligasi.","Interest rate has an important role in the pricing of financial assets, one of them is a bond. Callable bond is a bond that gives the issuer the right to buy back the bonds at a certain price over the life of the bonds. In estimating bond pricing used binomial tree. For a constant interest rate, it is not difficult to determine the bond price. But in fact, interest rates move up and down uncertainy and it is a stochastic process. So to observe the stochastic process, needed interest rate stochastic model. In this thesis, used single factor interest rate model that developed in 1990 by Fischer Black, Emanuel Derman, and William Toy and assumed that short rate has a lognormal distribution and has no-arbitrage assumption. That model is Black Derman Toy (BDT) short rate model. BDT short rate model used in the bisection method to find the value of drift. BDT short rate model also incorporate mean reversion in it. In determining the binomial tree, the interest rate is modeled forward first, and then backwards in determining bond prices.","Kata Kunci : suku bunga, obligasi callable, Black Derman Toy, bisection method, pohon binomial"
http://etd.repository.ugm.ac.id/home/detail_pencarian/66982,SEQUENTIAL KRIGING DALAM GEOSTATISTIKA,"MUHAMAD AJI IKHSANTO, Yunita Wulan Sari, S.Si., M.Sc.",2013 | Skripsi | STATISTIKA,"Kriging adalah suatu teknik perhitungan untuk menghitung estimasi dari suatu variabel teregional yang menggunakan pendekatan bahwa data yang dianalisis dianggap sebagai suatu realisasi dari suatu variable acak, dan keseluruhan variable acak yang dianalisis akan membentuk suatu fungsi acak dengan menggunakan model structural variogram. Semivariogram isotropy adalah semivariogram yang dipengaruhi oleh jarak antar titik sampel. Sequential kriging setara dengan simple kriging. Kumpulan data (data set) dibagi ke dalam beberapa subset dan tiap subset memungkinkan data tunggal. Dalam tersedianya suatu data tambahan, estimator sekuensial memperbaiki estimasi sebelumnya dengan menggunakan bobot linier dari data yang baru dan estimasi sebelumnya di suatu lokasi.","Kriging is a computation technique to calculate estimates of a variable teregional the approach that the data being analyzed is considered as a realization of a random variable, and overall random variable is analyzed to form a random function using a structural model of variogram. Semivariogram semivariogram isotropy is affected by the distance between sample points. Sequential kriging is equivalent to simple kriging. Data set (data set) is divided into several subsets and each subset allows a single data. In the availability of additional data, sequential estimators improve previous estimates using a linear weighting of new data and the previous estimate of a location.","Kata Kunci : Kriging, semivariogram isotropy, simple kriging, sequential kriging"
http://etd.repository.ugm.ac.id/home/detail_pencarian/67497,ANALISIS REGRESI ROBUST menggunakan PEMBOBOT WELSCH dan PEMBOBOT HAMPEL,"RICKY JOSUA SIMANJUNTAK, Drs. Zulaela, Dipl.Med.Stats., M.Si",2013 | Skripsi | STATISTIKA,"Dalam beberapa studi kasus yang ditemukan saat melakukan penelitian, kita sering menjumpai adanya data yang memiliki pencilan. Untuk melakukan analisis regresi dengan data tersebut, maka kita dapat melakukan analisis regresi robust. Analisis ini memiliki beberapa metode dan banyak fungsi yang dapat digunakan untuk mencari parameter koefisien untuk variabel independen dalam menentukan variabel dependen. Salah satu metode yang dapat digunakan adalah Estimasi-S. Metode estimasi-S memiliki breakdown point yang tinggi (50%) dan fungsi yang digunakan adalah fungsi dengan pembobot welsch dan pembobot hampel. Laporan skripsi kali ini bertujuan untuk membandingkan kedua fungsi tersebut dengan menggunakan estimasi S serta membandingkan pula dengan metode kuadrat terkecil (dengan/ tanpa pencilan). Data yang digunakan dalam studi kasus laporan skripsi ini adalah data kualitas air dari Laporan Hasil Uji di Balai Laboratorium Kesehatan Yogyakarta dengan periode pada tahun 2012.","In several case studies that found when doing research, we often find the data that have outliers. To perform regression analysis with the data, then we can perform robust regression analysis. This analysis has several methods and many functions that can be used to find the parameter coefficients for the independent variables in determining the dependent variable. One method that can be used is estimated-S. Estimation methods-S has a high breakdown point (50%) and the function that used are weighted function Welsch and weighting function Hampel. This time the report thesis aims to compare both these functions by using the estimated S and compares well with the least squares method (with / without outliers). The data used in this report is a case study of water quality data from the Report of Test Results in Health Laboratory Yogyakarta with the period in 2012.","Kata Kunci : Pencilan, Analisis Regresi Robust, Estimasi-S, Pembobot Welsch, Pembobot Hampel."
http://etd.repository.ugm.ac.id/home/detail_pencarian/62636,METODE FUZZY ANALYTICAL HIERARCHY PROCESS DALAM PENGAMBILAN KEPUTUSAN PENYALURAN KREDIT (Studi Kasus Penyaluran Kredit pada PD. BPR BKK Kebumen Cabang Puring),"EKA SETYANINGSIH, Dr. Gunardi, M.Si",2013 | Skripsi | STATISTIKA,"Bank merupakan badan usaha yang menghimpun dana dari masyarakat dalam bentuk simpanan dan menyalurkannya kepada masyarakat dalam bentuk kredit ataupun bentuk-bentuk lainnya. Seiring dengan perjalanan waktu sesudah kredit direalisasikan, tidak dapat dipungkiri bank akan dihadapkan pada permasalahan risiko, yaitu risiko kredit bermasalah. Misalnya saja ketidakmampuan untuk membayar bunga dan ketidakmampuan mengembalikan kreditnya pada saat jatuh tempo. Oleh sebab itu bank harus bisa mengambil keputusan yang tepat dan efektif dalam penyaluran kredit kepada calon debitur. Terkait dengan hal di atas, maka metode Analytical Hierarchy Process (AHP) dapat digunakan untuk membantu menyelesaikan masalah tersebut. AHP digunakan manakala keputusan yang diambil melibatkan banyak faktor, dimana pengambil keputusan mengalami kesulitan dalam membuat bobot setiap faktor tersebut. Dengan demikian AHP dapat memecahkan suatu situasi yang kompleks, tidak terstruktur ke dalam beberapa komponen dalam susunan yang hirarki, dengan memberi nilai subjektif tentang pentingnya setiap variable secara relative, dan menetapkan variable mana yang memiliki prioritas paling tinggi guna mempengaruhi hasil pada situasi tersebut. Meskipun demikian penggunaan AHP dalam permasalahan Multi Criteria Decision Making (MCDM) sering dikritisi suhubungan dengan kurang mampunya pendekatan AHP untuk mengatasi faktor ketidakpresisian yang dialami oleh pengambil keputusan ketika harus memberikan nilai yang pasti dalam matriks perbandingan berpasangan. Oleh karena itu, untuk mengatasi kelemahan AHP yang ada maka dikembangkan suatu metode yang disebut Fuzzy AHP. Metode Fuzzy AHP merupakan penggabungan antara metode AHP dengan pendekatan Fuzzy.","Bank is agency that assemble fund from people in saving and distribute it to people in credit or other form. As time goes on after the credit is realized, it canâ€™t be denied that bank will facing the risk, that is problematical credit. For example, uncapability to pay interest and return credit at the time to maturity. Therefore, bank have to make dicision appropriately and effectively in credit distribution to prospective borrower. In relation, Analytical Hierarchy Process (AHP) can be used to help solve this problem. AHP is used when the taken decision involve many factors, that decision maker get difficulty in making weight to each factors. So that, AHP can solve a complex situation, unstructured into several component of hierarchy structure, with put on subjective value about the importance of each variable relatively, and decide what variable that has highest priority to influence the result on that situation. Although, the used of AHP in Multi Criteria Decision Making (MCDM) often get criticism because the uncapability of AHP approach to solve unprecisely factor that decision makers feel when give precise value in comparison matrix. Therefore, Fuzzy AHP method is developed to solve the existing weakness of AHP. It is merge of AHP and Fuzzy approach.","Kata Kunci : Analytic Hierarchy Process, Himpunan Fuzzy ,Fuzzy Analytic Hierarchy Process."
http://etd.repository.ugm.ac.id/home/detail_pencarian/67015,SEGMENTASI KARAKTERISTIK DEBITUR MENGGUNAKAN ALGORITMA X-MEANS,"NURLITA KUSUMA DEWI, Dr. Gunardi, M.Si.",2013 | Skripsi | STATISTIKA,Kejadian debitur gagal membayar atau menunggak pembayaran diistilahkan â€œdefaultâ€. Salah satu cara meminimalisir default kita dapat mengenali ciri-ciri debitur yang biasanya mengalami default menggunakan algoritma x- means. Clustering menggunakan algoritma x-means merupakan pengembangan dari k-means cluster. X-means membentuk cluster awal menggunakan k-means. Setiap cluster awal yang terbentuk dibagi menjadi dua cluster berdasarkan kreiteria BIC. Proses ini berulang hingga tidak ada lagi cluster yang dapat dibagi. X-means membutuhkan komputasi yang lebih sedikit daripada k-means dan mampu mengoptimalkan jumlah cluster yang terbentuk.,The event of debtor is failed to pay or arrears of payment is called default. One of the way to minimize default is we can recognize debtor characteristic who usually gets default experience using x-means algorithm. Clustering using x- means algorithm is a development from k-means cluster. X-means forms an initial cluster using k-means. Each formed initial cluster is divided into two clusters based on BIC criteria. This process is repeated until there is no cluster which can not be divided. X-means needs less computation than k-means and is capable to optimize the number of clusters formed.,"Kata Kunci : resiko kredit, segmentasi, x-means, k-means, BIC"
http://etd.repository.ugm.ac.id/home/detail_pencarian/63958,PERBANDINGAN OPTIMISASI PORTOFOLIO METODE MEAN- VARIANCE DENGAN METODE MEAN-SEMIVARIANCE,"SEPTI WAHYUNI, Yunita Wulansari, S.Si., M.Sc.",2013 | Skripsi | STATISTIKA,"Pada tahun 1952 Markowitz memelopori penggunaan metode Mean-Variance untuk permasalahan optimisasi portofolio, yang hingga saat ini metode Mean- Variance sangat populer untuk digunakan. Namun, metode Mean-Variance ini memiliki kekurangan bahwa data return harus berdistribusi normal. Faktanya, sangat sulit mendapatkan data saham yang memiliki return berdistribusi normal. Markowitz (1959) berpendapat bahwa â€œanalisis berdasarkan semivariansi cenderung menghasilkan portofolio yang lebih baik dibandingkan portofolio berdasarkan variansiâ€. Walaupun begitu, mengapa analisis portofolio dengan Mean-Variance lebih sering digunakan daripada Mean-Semivariance? Hal ini dikarenakan, tidak seperti matriks variansi-kovariansi yang bersifat simetrik dan eksogen, matriks semivariansi-semikovariansi bersifat tidak simetrik dan endogen. Sehingga dalam penghitungan bobot harus digunakan algoritma numerik yang jarang digunakan oleh para praktisi dan akademisi. Oleh karena itu, digunakanlah pendekatan heuristik yang berfungsi untuk mengubah matriks semivarian-semikovarian menjadi simetrik dan eksogen. Sehingga penghitungan bobot portofolio Mean-Semivariance bisa menggunakan metode yang sama dengan Mean-Variance. Optimisasi portofolio menggunakan Mean-Semivariance tidak memerlukan asumsi distribusi apapun, sehingga lebih mudah penggunaannya dibandingkan Mean-Variance. Penghitungannya pun mudah dan dengan pendekatan heuristik dihasilkan matriks semivarian-semikovarian yang memiliki bentuk dan penyelesaian yang sama dengan matriks varian-kovarian milik metode Mean- Variance. Pada skripsi ini akan dilakukan perbandingan empiris antara optimisasi portofolio Mean-Semivariance dengan optimisasi portofolio Mean-Variance. Lalu dalam studi kasus dilakukan pembentukan portofolio Mean-Variance dan juga portofolio Mean-Semivariance dengan kombinasi dari beberapa aset finansial yang berupa saham.","In 1952 Markowitz pioneered the use of Mean-Variance method for portfolio optimization problems, for which Mean-Variance method is very popular to use. However, Mean-Variance method has drawbacks that the return data should be normal distributed. In fact, it is very difficult to get data that has normal distributed return. Markowitz (1959) argued that â€œanalysis based on semivariance tend to produce better portfolios than those based on varianceâ€. However, why is the analysis of the portfolio with Mean-Variance more often used than Mean-Semivariance? This is because, unlike covariance matrix that is symmetric and exogenous, semicovariance matrix is asymmetric and endogenous. Thus in calculating the weights, numerical algorithms must be used that is rarely used by practitioners and academics. Therefore, heuristic approach used which fuction to change semicovariance matrix to be symmetric and exogenous. So calculating the weights of Mean-Semivariance portfolio could use the same with Mean-Variance portfolio. Portfolio optimization using Mean-Semivariance does not require any distribution assumptions, making it much easier to use than Mean-Variance. The calculations are easy and with heuristic approach obtained semicovariance matrix which has the same form and finishing with covariance matrix of Mean-Variance. In this thesis the empirical comparison will be made between Mean-Semivariance portfolio optimization with Mean-Variance portfolio optimization. Then in case studies portfolio formation carried out Mean-Variance portfolio and Mean- Semivariance portfolio with combination of multiple financial assets.","Kata Kunci :  portofolio, Mean-Semivariance, pendekatan heuristik"
http://etd.repository.ugm.ac.id/home/detail_pencarian/64233,OPTIMALISASI PORTOFOLIO MODEL BLACK-LITTERMAN  DAN CAPITAL ASSET PRICING MODEL OPTIMIZATION PORTFOLIO BLACK LITTERMAN MODEL  AND CAPITAL ASSET PRICING MODEL,"RIZKA NUR ASFARINA, Prof. Drs. Suryo Guritno, M.Stats., Ph.D",2013 | Skripsi | STATISTIKA,"Saham merupakan salah satu instrumen yang sering dipakai dalam investasi. Tingkat pengembalian (return) saham dan besarnya resiko yang ditanggung investor merupakan hal yang perlu diperhatikan. Untuk mengoptimalkan return dan meminimalkan resiko dapat dibentuk portofolio saham. Diasumsikan bahwa return saham tunggal dan portofolio berdistribusi normal. Return dihitung dari harga penutupan saham harian pada masing-masing aset yang terdaftar dalam bursa Indeks LQ-45. Bobot portofolio dengan model Black-Litterman memberikan hasil portofolio yang optimal. Teori pembentukan portofolio diawali oleh Markowitz dengan meanvariancenya di tahun 50an. Selanjutnya bermunculan teori tentang portofolio seperti CAPM dan Single index model. Hingga pada tahun 90an muncul model portofolio yang dikenal dengan Model Black-Litterman oleh Robert Litterman dan Fischer Black. Formula return model Black-Litterman dapat ditelusuri melalui berbagai pendekatan. Selain dengan pendekatan Bayes, formula Black- Litterman dijelaskan oleh Mankert (2003) melalui pendekatan teori sampling. Sebagai simulasi nilai return akan diamati 4 saham dari indeks LQ45 yaitu AALI, JSMR, INDF, PGAS akan dibentuk suatu portofolio yang diharapkan dapat memberikan keuntungan yang optimal dengan menggunakan model Black- Litterman.","Stock is one instrument that is often used in investment . Rate of return (return ) of stock and amount of risk borne by the investor is a thing to watch . To optimize returns and minimize the risk of a stock portfolio can be formed . It is assumed that a single stock and portfolio returns are normally distributed . Return is calculated from the daily closing price for each asset listed in the stock exchange LQ - 45 index . Portfolio weights with the Black- Litterman model give optimal portfolio yield . Preceded by the formation of portfolio theory by Markowitz mean - variancenya in the '50s . Furthermore sprung on portfolio theory like CAPM and Single index models . Until the late 90s appeared portfolio model , known as the Black - Litterman Model by Fischer Black and Robert Litterman . Return formula Black - Litterman Model can be traced through various approaches . In addition to the Bayes approach , the Black - Litterman formula described by Mankert (2003 ) through the sampling theory approach . As the simulation will return the value of the observed 4 shares of LQ45 is AALI , JSMR , INDF , PGAS will set up a portfolio that is expected to provide optimal benefits to using the Black - Litterman .","Kata Kunci : Model CAPM, Model Black Litterman, Teori Sampling"
http://etd.repository.ugm.ac.id/home/detail_pencarian/66800,MODEL STOKASTIK BERDASARKAN TEKNIK CHAIN-LADDER,"GALANG YUNAWAN, Drs. Danardono, MPH, Ph.D.",2013 | Skripsi | STATISTIKA,"Skripsi ini menyajikan pembentukan model statistik berdasarkan teknik chainladder. Proses pembentukan model statistik ini memanfaatkan beberapa asumsi dan pendekatan, baik yang bersifat deterministik maupun stokastik, dalam menentukan estimasi cadangan klaim Incurred but Not Reported (IBNR). Model ini dituangkan dalam bentuk generalized linear model yang menggunakan asumsi awal distribusi Poisson untuk incremental claim amounts. Selain itu, metode ini juga dapat mengatasi masalah negative incremental claims dengan memanfaatkan algoritma Verbeek yang merupakan pendekatan model Poisson. Teknik ini memberikan alternatif bagi perusahaan asuransi untuk menentukan secara tepat estimasi cadangan klaim di waktu yang akan datang.","This paper presents a technique based on statistical modeling of chain-ladder technique. This model building process utilizes some assumptions and approaches, both deterministic and stochastic, to determine the estimation of Incurred but Not Reported (IBNR) claim reserves. The model is written in the form of generalized linear models that uses a Poisson distribution for the initial assumption of incremental claim amounts. In addition, this method also could overcome the problems of negative incremental claims by using Verbeekâ€™s algorithm which is Poisson model approach. For insurance companies, this technique provides an alternative to determine accurately claim reserves estimate in the future.",Kata Kunci : -
http://etd.repository.ugm.ac.id/home/detail_pencarian/67056,PENENTUAN RETENSI OPTIMAL DAN HARGA PREMI DARI ASURANSI KE REASURANSI DENGAN VALUE AT RISK,"DIAN SARASWATI, Drs. Zulaela, Dipl. Med. Stats, M.Si.",2013 | Skripsi | STATISTIKA,"Pada asuransi umum, asuransi menanggung risiko kerugian dari benda yang diasuransikan oleh pemegang polis jika terjadi klaim. Kerugian tersebut bisa tinggi bisa juga rendah. Asuransi menemui kendala jika risiko kerugian yang ditanggung bernilai tinggi dalam hal ini berarti dana yang dibutuhkan untuk mangganti kerugian tersebut besar nilainya. Solusi dari kendala tersebut adalah dengan mengasuransikan kembali pada reasuransi. Hal itu berarti bahwa asuransi akan membagi risiko yang ditanggung dengan reasuransi. Asuransi berkewajiban membayar sejumlah dana kepada reasuransi yang disebut premi reasuransi. Reasuransi berkewajiban menanggung risiko sebagian risiko yang telah dilimpahkan kepadanya. Dalam pembagian risiko tersebut ditentukan batas yang mana mengacu pada kemampuan maksimal asuransi menanggung risiko yang disebut retensi. Batas ini ditentukan berdasarkan optimisasi Value at Risk. Kriteria optimisasi ditetapkan berdasarkan nilai minimal VaR dari risiko total asuransi, untuk menurunkan retensi optimal pada reasuransi stop loss. Hasil solusi optimal pada kriteria optimisasi memiliki beberapa karakteristik penting, antara lain: retensi optimal mempunyai analisis yang sangat sederhana; retensi optimal hanya bergantung pada asumsi distribusi kerugian dan faktor loading reasuransi. Dengan demikian asuransi dapat mengatasi kendala dari risiko kerugian yang bernilai tinggi, karena asuransi hanya menanggung kerugian maksimal sebesar retensi dan reasuransi menanggung kerugian sisanya.","The general Insurance must take over the loss risk of the item which is insured by policy holder. The loss risk can be high, can be low. The general Insurance will face a problem if the loss risk are high, in this case, means that the fund are used to pay the claim are high. And the solution is by reinsure the policy in reinsurance company. Its means that the insurance will share the risk with the reinsurance. The Insurance have to pay amount of fund periodically which is called premium reinsurance. The reinsurance must take over some of the loss risk. The general Insurance determine the retention of the loss risk by optimation of the value at risk. The resulting optimal solution of optimation criterion has several important characteristics, such as: the optimal retention has a very simple analytic form; the optimal retention depends only on the assumed loss distribution and the reinsurerâ€™s loading factor. Therefore, the insurance can solve the problem because the insurance will pay the claim at the retention or lower than retention, then the reinsurance will pay the rest of the loss risk.","Kata Kunci : Asuransi Umum, Retensi Optimal, Value at Risk (VaR)."
http://etd.repository.ugm.ac.id/home/detail_pencarian/63742,ANALISIS BAYESIAN UNTUK REGRESI KUANTIL DENGAN MENGGUNAKAN ALGORITMA GIBBS SAMPLING,"ANNISA HANIF, Prof. Dr. Subanar, Ph.D",2013 | Skripsi | STATISTIKA,"Regresi kuantil mendapatkan perhatian yang tinggi baik dari segi teoritis maupun dari sudut pandang empiris. Ini adalah suatu prosedur statistik dengan meminimalkan jumlahan dari asymmetrically weighted absolute dan dapat digunakan untuk memeriksa hubungan antara kuantil dari distribusi variabel dependen. Regresi kuantil dapat digunakan untuk mengatasi keterbatasan regresi linear dalam menganalisis sejumlah data yang berbentuk lonceng tidak simetris dan regresi kuantil sangat berguna jika distribusi data tidak homogen. Regresi Kuantil dapat diestimasi menggunakan metode Bayesian. Metode Bayesian adalah metode analisis yang berdasarkan pada informasi yang berasal dari sampel dan informasi prior. Gabungan informasi ini disebut posterior. Untuk mencari distribusi posterior seringkali menghasilkan perhitungan yang tidak dapat diselesaikan secara analitis sehingga digunakan pendekatan Gibbs sampling. Estimasi parameter dari model adalah mean dari distribusi posterior yang diperoleh dari proses Gibbs sampling tersebut. Dalam skripsi ini dibahas regresi kuantil menggunakan asymmetric Laplace distribution dari sudut pandang Bayesian. Digunakan algoritma Gibbs sampling untuk mencari estimator dari model regresi kuantil berdasarkan parameter lokasi dan skala dari mixture representation asymmetric Laplace distribution. Studi kasus dalam skripsi ini membahas faktor apa saja yang mempengaruhi harga emas. Hasil estimasi regresi kuantil dengan metode Bayesian akan dibandingkan dengan regresi linear menggunakan metode OLS, dan dibandingkan dengan metode regresi kuantil. Lalu didapatkan kesimpulan bahwa metode Bayesian lebih baik daripada estimasi yang lainnya","Quantile regression has received increasing attention both from a theoretical and from an empirical view point. It is a statistical procedure that minimizing sums of asymmetrically weighted absolute and can be used to explore the relationship between quantile of response distribution. Quantile regression can be used to overcome the limitation of linear regression to analyze data not symmetric and quantile regression is useful if the distribution of datais not homogeneous. Quantile regression can be estimated using Bayesian method. Bayesian method is a method of analysis based on information from sample and prior information. Combination of those informations is called posterior. For looking posterior dsitribution often result in calculation can not be solved with analytical so Gibbs sampling approach is used. Estimation of parameters in the model is the mean of the posterior distribution that obtained from Gibbs sampling process. In this paper discused quantile regression using an asymmetric Laplace distribution from Bayesian point of view. Gibbs sampling is used to find the estimator of quantile regression model based on a location-scale mixture representation asymmetric Laplace distribution. The case study in this paper discusses the factors that effect the gold price. The estimation result of quantile regression using Bayesian method will be compared with linear regression using OLS method, and compared with quantile regression method. And then it was concluded that the Bayesian method is better than the otherconclusion","Kata Kunci : Regresi Kuantil, Bayesian, Gibbs sampling, Asymmetric Laplace Distribution"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150797,DISTRIBUSI MAJEMUK DENGAN JUMLAHAN BOREL DAN FUNGSI REKURSIFNYA UNTUK PEMODELAN TOTAL BESAR KLAIM ASURANSI; ( COMPOUND DISTRIBUTION WITH BOREL SUMMANDS AND ITS RECURSIVE FUNCTION FOR MODELING TOTAL SIZE OF INSURANCE CLAIMS ),"I Gusti Ngurah Putra Pratama, Adhitya Ronnie Effendie",2015 | Skripsi | PROGRAM STUDI S1 STATISTIKA,"Total besar klaim asuransi adalah jumlah dari seluruh klaim yang diajukan dalam suatu periode. Total besar klaim dapat dimodelkan sebagai distribusi majemuk yang melibatkan distribusi frekuensi dan distribusi severity. Distribusi frekuensi adalah distribusi dari banyaknya klaim yang diajukan dalam suatu periode sedangkan distribusi severity adalah distribusi dari besarnya masing-masing klaim yang diajukan tersebut. Pada skripsi ini dibahas mengenai distribusi majemuk Poisson, Bartlett, dan Delaporte dengan jumlahan Borel yang dapat digunakan untuk memodelkan distribusi frekuensi. Pembentukan distribusi majemuk dengan jumlahan Borel ini menggunakan teori probabilitas dan kombinatorik. Dikembangkan juga suatu fungsi rekursif yang bertipe Panjer untuk menghitung fungsi massa probabilitas dari total besar klaim asuransi. Studi kasus dari skripsi ini menggunakan data klaim pada tahun 2012 dari salah satu perusahaan asuransi kendaraan bermotor di Indonesia. Estimasi parameter dari distribusi majemuk dengan jumlahan Borel menggunakan metode maksimum likelihood. Dari hasil uji kecocokan distribusi, hanya distribusi majemuk Poisson dan Bartlett dengan jumlahan Borel yang layak digunakan untuk memodelkan distribusi frekuensi. Berdasarkan perhitungan secara rekursif dengan menggunakan distribusi majemuk Poisson dan Bartlett dengan jumlahan Borel sebagai distribusi frekuensi, diperoleh bahwa distribusi probabilitas dari total besar klaim asuransi yang dihasilkan relatif sama. Nilai ekspektasi dari total besar klaim asuransi dalam satu hari adalah sebesar 9,8 juta rupiah dengan standar deviasi sebesar 11,2 juta rupiah.",,"Kata Kunci : distribusi Borel, distribusi majemuk, distribusi Poisson, distribusi Bartlett, distribusi Delaporte, rekursif Panjer, klaim asuransi."
http://etd.repository.ugm.ac.id/home/detail_pencarian/149939,"KLASIFIKASI SAHAM BERDASARKAN MODEL GARCH MENGGUNAKAN ALGORITMA AGGLOMERATIVE (Studi Kasus Saham BBNI.JK, GGRM.JK, INDF.JK, MDLN.JK, PNBN.JK, PNLF.JK,SMCB.JK, SMRA.JK); STOCK CLASSIFYING BASED ON GARCH MODEL USING AGGLOMERATIVE ALGORITHM (Case Study Applied on Stocks of BBNI.JK, GGRM.JK, INDF.JK, MDLN.JK,PNBN.JK, PNLF.JK,SMCB.JK,SMRA.JK)","ARSITA FAJRIN KHOIRUNISA, Adhitya Ronnie Effendie",2012 | Skripsi | PROGRAM STUDI S1 STATISTIKA,"Ekstensifikasi maupun intensifikasi pengembangan berbagai jenis industri mendorong perusahaan untuk membuka peluang investasi bagi masyarakat yang salah satunya dengan menawarkan saham perusahaan dalam bursa efek baik pada tingkat nasional maupun internasional. Kegiatan ini menambah banyaknya jumlah saham yang diperdagangkan. Melihat dari sisi lain, investor dihadapkan pada pilihan yang semakin beragam. Keragaman ini perlu dikelompokkan sesuai dengan tingkat resiko berdasarkan waktu (time varying risk) agar investor lebih mudah dalam memilih saham maupun membentuk portofolio. Skripsi ini akan menggunakan jarak antar GARCH sebagai ukuran ketidaksamaan dalam pengelompokkan (pengklasteran) 8 saham anggota indeks saham Kompas 100. Pembahasan dilanjutkan dengan pembentukan portofolio dengan metode sederhana yaitu portofolio mean variance. Perbandingan antara portofolio yang dibentuk berdasarkan klaster dan portofolio yang dibentuk tidak berdasarkan klaster turut disajikan dalam studi kasus",,"Kata Kunci : Model GARCH, Analisis Klaster, Algoritma Agglomerative, Portofolio"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150100,PENYELESAIAN POHON REGRESI CART (CLASSIFICATION AND REGRESSION TREE) DENGAN PENDEKATAN BAYESIAN; THE SOLVING OF CART (CLASSIFICATION AND REGRESSION TREE) REGRESSION TREE UNDER BAYESIAN FRAMEWORK,"ENDANG RAHAYU LESTARI, Danang Teguh Qoyyimi",2011 | Skripsi | PROGRAM STUDI S1 STATISTIKA,"Bentuk pencarian stokastik suatu analisis klasifikasi dan regresi pohon (CART) (Breiman et al., 1984) yang dilakukan menggunakan pendekatan Bayesian. Tujuannya adalah menyajikan suatu algoritma Bayesian yang meniru prosedur CART mengenai jumlah pemisahan nodes (splitting nodes), posisinya dan questions yang digunakan pada nodes yang tidak diketahui. Penambahan parameter dalam permasalahan dapat digunakan untuk membuat suatu inferensi menggunakan data. Suatu perkiraan distribusi probabilitas dengan pohon yang mungkin digunakan untuk menyelidiki reversible jump Markov chain Monte Carlo.",,"Kata Kunci : Metode Bayesian, classification tree, regression tree, reversible jump Markov chain Monte Carlo"
http://etd.repository.ugm.ac.id/home/detail_pencarian/149272,MODEL REGRESI BETA; BETA REGRESSION MODEL,"IRNA NOVITA LESTARI, Sri Haryatmi Kartiko",2012 | Skripsi | PROGRAM STUDI STATISTIK,"Regresi linier klasik seringkali digunakan untuk memodelkan hubungan antara variabel dependen (respon) dengan satu atau lebih variabel independen (prediktor) dimana respon bersifat kontinu dan berdistribusi normal. Apabila asumsi normalitas respon tidak terpenuhi, estimasi untuk model regresi linier tidak akurat. Regresi beta adalah metode alternatif untuk memodelkan hubungan antara variabel respon dan prediktor dimana respon berdistribusi beta. Analisis ini sangat bermanfaat pada kondisi dimana variabel respon kontinu dan pada interval terbuka (0,1), seperti rate dan proporsi. Model regresi beta menggunakan fungsi link yang tidak diketahui. Estimasi parameter regresi menggunakan estimasi maksimum likelihood dan parameter regresi beta interpretable dalam mean respon. Perhitungan estimator MLE tidak dapat diselesaikan secara analitik sehingga estimator dihitung dengan memaksimumkan fungsi log-likelihood secara numerik dengan menggunakan algoritma optimisasi nonlinear, dalam kasus ini digunakan algoritma BFGS.",,"Kata Kunci : regresi beta, kontinu, distribusi beta, maksimum likelihood estimation, proporsi"
http://etd.repository.ugm.ac.id/home/detail_pencarian/150395,PENENTUAN HARGA OBLIGASI BENCANA ALAM GEMPA BUMI DENGAN SEBARAN NILAI EKSTREMUM RAMPAT DAN MODEL SUKU BUNGA COX-INGERSOLL-ROSS (CIR); VALUATION OF CATASTROPHIC EARTHQUAKE BONDS WITH GENERALIZED EXTREME VALUE (GEV) DISTRIBUTION AND COX-INGERSOLL-ROSS (CIR) INTEREST RATE MODEL,"Ezra Putranda Setiawan, Gunardi",2014 | Skripsi | PROGRAM STUDI STATISTIKA JURUSAN MATEMATIKA,"Indonesia merupakan daerah yang rawan bencana gempa bumi, karena terletak di daerah perbatasan lempeng-lempeng bumi. Bencana gempa bumi dapat menimbulkan kerusakan, kerugian, serta dampak ekonomis yang sangat besar. Oleh karena itu, diperlukan pemindahan resiko bencana dari negara atau perusahaan (reasuransi) sedemikian rupa sehingga dapat dikumpulkan dana yang cukup untuk menutup kerugian bencana tersebut, misalnya melalui penerbitan obligasi bencana alam (catastrophe bond). Suatu obligasi bencana alam diterbitkan oleh perusahaan special purpose vehicle (SPV). Dana yang diperoleh sebagai hasil penjualan kepada investor akan diinvestasikan bersama dengan dana yang diperoleh dari perusahaan sponsor. Bila terjadi bencana alam dalam jangka waktu tertentu, aliran dana dari SPV kepada investor akan dipotong atau bahkan dihentikan untuk diberikan kepada sponsor guna menutup kerugian akibat bencana alam tersebut. Khusus untuk bencana gempa bumi, penentuan besar pemotongan dapat dilakukan secara parametrik berdasarkan magnitude gempa. Studi kasus menunjukkan bahwa probabilitas maksimum magnitude gempa bumi dapat dianalisis menggunakan sebaran nilai ekstremum rampat (generalized extreme value / GEV distribution). Untuk penentuan harga obligasi bencana alam ini, digunakan asumsi suku bunga mengambang seturut model Cox-Ingersoll-Ross (CIR). Di bawah model ini, telah diperoleh harga tiga macam obligasi bencana alam yakni obligasi tanpa kupon, obligasi dengan nilai akhir tetap, dan obligasi dengan kupon maupun nilai akhir beresiko. Selanjutnya dari simulasi diperoleh hubungan antara harga jual obligasi dengan parameter-parameter model suku bunga CIR, distribusi GEV, besar kupon, maupun pemotongan aliran dana obligasi kepada investor.",,"Kata Kunci : Obligasi bencana alam, model suku bunga Cox-Ingersoll-Ross (CIR), sebaran nilai ekstremum rampat, simulasi, magnitude gempa, harga."
